# Appropriate Reliance on AI: Building Calibrated Trust in Human-AI Collaboration

## Target Audience
Business leaders, developers, researchers, and AI practitioners interested in effective human-AI collaboration

## Presentation Overview
A visual journey through the science and practice of appropriate reliance on AI systems‚Äîunderstanding when to trust AI, when to verify, and how to design systems that support calibrated human judgment.

---

## Slide 1: Title
**Calibrated Confidence: The Art of Appropriate AI Reliance**

When AI Gets It Right, Trust It. When It Doesn't, Catch It.

*Fabio Correa | Microsoft Corporation | 2026*

---

## Slide 2: The Paradox of AI Assistance

**The Double-Edged Sword**

AI assistants offer remarkable capabilities but create two interrelated dangers:

- **Hallucination**: AI generates plausible but incorrect information with unwarranted confidence
- **Over-reliance**: Users accept AI outputs without critical evaluation

The root cause is the same: **miscalibrated confidence**

*Illustration: A balance scale with "Trust" and "Verify" on either side*

---

## Slide 3: What Is Appropriate Reliance?

**The Definition**

Appropriate reliance occurs when users:
- ‚úÖ **Accept correct AI outputs** (Correct AI-Reliance / CAIR)
- ‚úÖ **Reject incorrect AI outputs** (Correct Self-Reliance / CSR)

The operative word is "appropriate"‚Äîmatching trust to actual reliability

*Illustration: A 2x2 matrix showing the four quadrants of reliance decisions*

---

## Slide 4: The 2√ó2 Reliance Matrix

| | User Accepts | User Rejects |
|--|--------------|--------------|
| **AI Correct** | ‚úÖ CAIR (Good!) | ‚ö†Ô∏è Under-reliance |
| **AI Incorrect** | ‚ùå Over-reliance | ‚úÖ CSR (Good!) |

**Goal**: Maximize CAIR and CSR simultaneously

*Illustration: Four quadrant diagram with traffic light colors*

---

## Slide 5: Why Inappropriate Reliance Matters

**Real Consequences**

1. **Poor Team Performance**: Human+AI teams can perform *worse* than either alone when trust is miscalibrated
2. **Ineffective Oversight**: Over-reliance undermines the very human oversight designed to catch AI errors
3. **Product Abandonment**: Wrong mental models erode trust over time, leading to rejection

*Illustration: A falling domino chain representing cascading failures*

---

## Slide 6: The Dual-Process Challenge

**Fast vs. Slow Thinking**

AI's fluent, confident outputs engage **System 1** (fast, automatic thinking)

Critical evaluation requires **System 2** (slow, deliberate thinking)

The very fluency of AI-generated text becomes a misleading cue to accuracy

*Illustration: Two brain hemispheres with lightning (fast) and clock (slow)*

---

## Slide 7: Trust vs. Calibration

**It's Not How Much You Trust‚ÄîIt's How Well Your Trust Is Calibrated**

| Trust (TR) | Appropriate Reliance (AR) |
|------------|---------------------------|
| Trust *level* | Trust *calibration* |
| "Do you trust AI?" | "Can you tell when AI is right?" |
| Attitude | Metacognitive skill |

*Illustration: A thermometer (level) vs. a compass (calibration)*

---

## Slide 8: The Trust-Calibration Matrix

| | Low Calibration | High Calibration |
|--|-----------------|------------------|
| **High Trust** | ‚ö†Ô∏è Dangerous over-reliance | ‚úÖ Optimal user |
| **Low Trust** | ‚ùå Missed value | ‚úÖ Calibrated skeptic |

**Reframe the goal**: Build calibration, not just trust

*Illustration: Four personas in different quadrants with distinct visual styles*

---

## Slide 9: When AI Is "Confident But Wrong"

**High-Risk Categories**

- **Common Misconceptions**: "Everyone knows that..." (but it's false)
- **Outdated Information**: APIs change, facts evolve
- **Fictional Bleed**: Fiction presented as fact
- **Social Biases**: Training data stereotypes

*Illustration: A confident AI avatar with a shadow showing doubt*

---

## Slide 10: Research-Validated Mitigation Strategies

**Three Categories That Work**

1. **Verification-Focused Explanations**: Help users assess *correctness*, not just understand outputs
2. **Uncertainty Expressions**: Linguistic and visual indicators of reliability
3. **Cognitive Forcing Functions**: Strategic interruptions that engage analytical thinking

*Illustration: Three shields representing protection strategies*

---

## Slide 11: The Confidence Ceiling

**Never Claim 100% for Generated Content**

| Content Type | Maximum Confidence |
|--------------|-------------------|
| Direct file reading | 100% |
| Code from documented patterns | 90% |
| Factual claims without direct source | 70% |
| Inference or edge cases | 50% |

The 90% ceiling says "this should work, but verify"

*Illustration: A gauge/dial that stops at 90%*

---

## Slide 12: Source Grounding Tiers

**Know Where Your Knowledge Comes From**

**Tier 1 - Documented**: "The codebase shows..." ‚Üí Low verification need
**Tier 2 - Inferred**: "Based on the pattern..." ‚Üí Medium verification need
**Tier 3 - Uncertain**: "I'm not certain, but..." ‚Üí High verification need
**Tier 4 - Unknown**: "I don't have information about..." ‚Üí Verification required

*Illustration: A pyramid with four layers, getting more transparent at top*

---

## Slide 13: Epistemic vs. Creative Modes

**Different Rules for Different Outputs**

**Epistemic Mode** (Factual claims)
- Full calibration protocols
- Source grounding required
- Confidence ceilings apply

**Generative Mode** (Creative ideas)
- Frame as proposals, not facts
- Invite collaborative validation
- Welcome refinement

*Illustration: Split screen - laboratory (epistemic) vs. art studio (creative)*

---

## Slide 14: Human Judgment Domains

**Some Decisions Require Human Judgment**

AI should inform but not replace decisions about:
- üéØ Business strategy and priorities
- ‚öñÔ∏è Ethical dilemmas
- üë• Personnel and team decisions
- üîí Security architecture
- ‚öñÔ∏è Legal and compliance matters

"I can outline the options, but the choice depends on priorities that are yours to weigh."

*Illustration: A human figure at a crossroads with multiple paths*

---

## Slide 15: The Path Forward

**From AI Tools to AI Teammates**

Building appropriate reliance requires:

1. **Design systems** that communicate uncertainty honestly
2. **Measure calibration**, not just trust
3. **Preserve human judgment** in critical domains
4. **Train users** to recognize reliability signals

The goal: **Collective intelligence** where humans and AI are better together

*Illustration: Two hands (human and robotic) working together*

---

## Closing Message

**"AI can bridge gaps of time, distance, and scale, but only if built correctly. We must design AI to support shared goals, group context, and the norms of collaboration."**

‚Äî Jaime Teevan, Microsoft Chief Scientist

*Building trust through calibrated confidence*

---

## Visual Style Guidance for Gamma

- **Illustration style**: Modern, clean illustrations (not photos)
- **Color palette**: Professional blues, greens, warm accents
- **Imagery**: Abstract concepts, diagrams, metaphorical illustrations
- **Tone**: Professional, inspiring, research-grounded
- **Avoid**: Stock photos, cluttered visuals, complex charts
