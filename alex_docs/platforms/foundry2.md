<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

To unlock the full potential of vision-enabled chat models, it's essential to tailor the prompts to your specific needs. Here are some guidelines to enhance the accuracy and efficiency of your prompts.

<Callout type="note">
  These prompt engineering techniques apply to vision-enabled models including GPT-4 Turbo with Vision, GPT-4o, and GPT-4o-mini. To deploy a vision-enabled model, see [Deploy models](../../how-to/deploy-models-managed).
</Callout>

## Fundamentals of writing an image prompt

* **Contextual specificity:** Adding context to the scenario at hand gives the model a better understanding of an appropriate output. This level of specificity aids in focusing on relevant aspects and avoiding extraneous details.
* **Task-oriented prompts:** Focusing on a specific task helps the model to develop the output while taking that perspective into consideration.
* **Handle refusals:** When the model indicates an inability to perform a task, refining the prompt can be an effective solution. More specific prompts can guide the model towards a clearer understanding and better execution of the task. Request explanations for generated responses, place the image before the text for single-image prompts, and ask the model to describe the image in detail first before completing your specific task.
* **Add examples:** Add examples that represent the type of responses you're looking for
* **Break down requests:** Try breaking down complex requests step-by-step to create manageable sub-goals
* **Define output format:** Clearly mention the desired format for the output, such as markdown, JSON, HTML, etc. You can also suggest a specific structure, length, or specific attributes about the response.

## Example prompt inputs and outputs

There are many ways to craft system prompts to tailor the output specifically to your needs. The following sample inputs and outputs showcase how adjusting your prompts can give you different results. Try out the model for yourself using these images and adjusting the system prompt in the [Microsoft Foundry playground](https://ai.azure.com/?cid=learnDocs).

### Contextual specificity

Context can help improve feedback from the model. For example, if you're working on image descriptions for a product catalog, ensure your prompt reflects that in a clear and concise way. A prompt like “Describe images for an outdoor hiking product catalog, focusing on enthusiasm and professionalism” guides the model to generate responses that are both accurate and contextually rich.

| Basic prompt                                                                                                                                                      | Model output                                                                                                                                                                                                                                                        |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![Photograph of a camping tent and chairs in a forest.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/tent-chairs.jpg) What is in this image? | `The image shows a camping setup in a forest area. There is a yellow tent, two folding camping chairs—one red and one blue—and a portable camping table. The surrounding area is densely populated with trees, indicating that the campsite is in a wooded region.` |

| Updated prompt                                                                                                                                                                                                                                | Model output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| ![Photograph of a camping tent and chairs in a forest.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/tent-chairs.jpg) Describe images for an outdoor hiking product catalog, focusing on enthusiasm and professionalism. | `"Embark on your next outdoor adventure with our premium hiking and camping essentials! Featured here is our vibrant, spacious tent, designed to be your home away from home amidst the serenity of nature. Crafted with top-notch materials, it promises durability and comfort in any weather. Complementing the tent are our ultra-comfortable, lightweight folding chairs, available in eye-catching red and serene blue, perfect for those moments of relaxation after a day of exploration. Don't forget our sturdy, portable table, ideal for a delightful outdoor meal or a game of cards under the stars. Elevate your camping experience with our reliable products, where quality meets the great outdoors!"` |

### Task-oriented prompts

Giving the model a specific task to follow is one way of sharpening the response. In this example, this prompt steers the model to concentrate on elements crucial for insurance assessments, thereby improving accuracy and relevancy.

| Basic prompt                                                                                                                                        | Model output                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| --------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![Photograph of a car crashed into a pole.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/car-crash.jpg) What is in this image? | `The image shows a silver sedan that has been in an accident. The front end of the car is damaged, with the hood crumpled, and the bumper partially detached. The car is resting against a utility pole on the sidewalk, and there is visible smoke coming from the engine area, indicating a recent crash. The setting appears to be a residential street, as indicated by the presence of houses in the background, a neatly trimmed hedge, and a sidewalk.` |

| Updated prompt                                                                                                                                                                                                                        | Model output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![Photograph of a car crashed into a pole.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/car-crash.jpg) Analyze this car damage video for an auto insurance report, focusing on identifying and detailing damage | `In the image provided, we observe a silver sedan that has sustained significant front-end damage. The vehicle's hood is notably crumpled, and the front bumper has suffered substantial deformation, partially detaching from the chassis. The right headlamp assembly appears to be damaged or dislodged. There is also visible damage to the front grille area. The car is in contact with a utility pole, suggesting a frontal collision with the pole at a relatively low or moderate speed. Smoke is emanating from the engine compartment, suggesting potential mechanical or electrical damage that may need further inspection for a comprehensive assessment. The airbags do not appear to be deployed in this image. The vehicle will require a detailed inspection to evaluate the full extent of structural and mechanical damage for a complete insurance report.` |

### Handling refusals

When you receive a refusal output from a model, there are several ways you can tweak input to generate a more appropriate response. Below is one example of how to handle a refusal and progress.

| Basic prompt                                                                                                                                                     | Model output                               |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------ |
| ![Photograph of a dinner table with several plates of food.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/dinner-spread.jpg) Plan this meal | `Sorry, I can't provide that information.` |

| Follow up prompt              | Model output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| ----------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| What information do you need? | `I apologize for the confusion earlier. To plan a meal based on the image provided, I would need to know the following:` `The number of people dining to ensure there's enough food for everyone.` `Any dietary restrictions or preferences to accommodate (e.g., vegetarian, gluten-free, allergies).` `The occasion or type of meal being planned (e.g., casual lunch, dinner party).` `Desired balance of the meal in terms of food groups and nutrition.` `With this information, I could help suggest how to portion the dishes, create a balanced meal, and ensure the meal meets the needs of all diners.` |

### Adding examples

| Basic prompt                                                                                                                                                                                                         | Model output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![Photograph of a border collie dog laying in a grass field.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/border-collie.jpg) What is this dog breed and what is its average weight and height? | `The dog in the image appears to be a Border Collie. Border Collies are known for their intelligence, agility, and herding abilities. The average weight for a Border Collie is typically around 30-45 pounds (14-20 kilograms). As for height, Border Collies generally stand about 18-22 inches (46-56 centimeters) tall at the shoulder. These figures can vary based on the individual dog's genetics, diet, and level of activity. Border Collies are medium-sized dogs with a well-proportioned body and a high level of energy.` |

| Updated prompt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | Model output                                                                       |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------- |
| Q: What is this dog breed and what is its average weight & height? ![Photograph of a pomeranian dog sitting on a pool diving board above the water.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/pomeranian.jpg) A: Breed: pomeranian; average weight: 3-7 lbs; average height: 8-14 inches ![Photograph of a bulldog laying in a grass field.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/bulldog.jpg) A: Breed: English bulldog; average weight: 50-55 lbs; average height: 14-16 inches ![Photograph of a border collie dog laying in a grass field.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/border-collie.jpg) | `A: Breed: Border Collie; average weight: 30-45 lbs; average height: 18-22 inches` |

### Breaking down requests

| Basic prompt                                                                                                                                                                                  | Model output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![Photograph of a recipe card detailing a recipe for Grandma's Apple Crumb Pie.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/grandmas-recipe.jpg) What's in this image? | `The image contains a recipe card titled Grandma's Apple Crumb Pie. It is noted as a Thanksgiving tradition. The ingredients listed are:` - `5-7 apples, peeled`
- `1 9-inch pie crust`
- `3/4 cups sugar`
- `1/2 teaspoon cinnamon`
- `A dash of salt`
- `3 tablespoons butter` `Please note that the measurements for sugar, flour, and butter are not fully visible in the image. If needed, the standard measures for a crumb topping typically involve a combination of sugar, flour, and butter mixed together until crumbly` |

| Updated prompt                                                                                                                                                                                                                                                                                                                                                                                                               | Model output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![Photograph of a recipe card detailing a recipe for Grandma's Apple Crumb Pie.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/grandmas-recipe.jpg) You are an AI assistant that helps create shopping lists and instructions to prep food based on recipes. Please provide a list of ingredients to purchase, and then provide a list of instructions to prep the ingredients based on the recipe card. | `Sure, here's a shopping list based on the recipe card:` `Shopping List:` 1) `5-7 apples (suitable for baking, such as Granny Smith or Honeycrisp)`
2) `1 ready-made pie crust (9-inch)`
3) `Sugar (ensure you have enough for 1/2 cup)`
4) `All-purpose flour (ensure you have enough for 3/4 cup)`
5) `Ground cinnamon`
6) `Salt`
7) `Butter (ensure you have enough for 3 tablespoons)` `Prepping Instructions:` - `Peel the apples: Using a vegetable peeler or a paring knife, remove the skin from 5 to 7 apples.`
- `Core and slice the apples: Cut the apples into quarters, remove the cores, and then slice the apple quarters into thin slices or chunks, depending on your preference for pie texture.`
- `Measure the ingredients: Measure out 3/4 cups of sugar, 1/2 teaspoon of cinnamon, a dash of salt (about 1/8 teaspoon), and 3 tablespoons of butter.`
- `Prepare the butter: Cut the butter into small pieces to be ready to use in the recipe. It's often easier to incorporate if it's cold, so keep it refrigerated until it's needed`
- `Preheat your oven according to the pie recipe's instructions so it's ready for baking once your pie is assembled.`
- `Remember to follow any additional preparation or baking instructions provided in the full recipe for your apple crumb pie.` |

### Defining output format

| Basic prompt                                                                                                                                                               | Model output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| ![Photograph of a white villa in a tropical region with a pool.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/rental-listing.jpg) Describe this image | `The image shows a luxurious white villa with a traditional thatched roof on one section and a clear blue swimming pool in the foreground. There are sun loungers lined up along the poolside, inviting relaxation. The architecture includes elegant archways, a balcony with wooden railings, and large windows and doors that suggest a bright and airy interior. The villa is surrounded by lush tropical vegetation, including tall palm trees, which adds to the serene and exotic ambiance of the setting.` |

| Updated prompt                                                                                                                                                                                                                                                                                                                                                                                                               | Model output                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| ![Photograph of a white villa in a tropical region with a pool.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/gpt-v/rental-listing.jpg) You are an AI assistant that writes detailed listings for vacation rental properties. Summarize in three sentences the optimal features of the listing, based on only what is visible in the image. Conclude with one sentence on why the customer should book today. | `This idyllic villa showcases an inviting private pool with steps for easy access, framed by a beautifully tiled sun deck and comfortable loungers ideal for soaking up the sun or enjoying a good book under the shade of tropical palms. Architectural charm is evident in the villa's design, featuring elegant archways, a thatched-roof gazebo, and balconies that promise serene views and a perfect spot to savor your morning or evening beverages. The lush greenery surrounding the property not only provides privacy but also enhances the sense of an exclusive paradise retreat. Don't miss the opportunity to secure this perfect blend of luxury and tranquility – book today for a tropical escape like no other!` |

These guidelines and examples demonstrate how tailored system prompts can significantly enhance the performance of vision-enabled models, ensuring that the responses are not only accurate but also perfectly suited to the specific context of the task at hand.

## Next steps

Now that you understand image prompt engineering techniques, try them in practice:

1. Open the [Microsoft Foundry playground](https://ai.azure.com/?cid=learnDocs) and deploy a vision-enabled model.
2. Upload an image and experiment with contextual specificity.
3. Compare basic prompts with task-oriented prompts to see the difference in output quality.

## Related content

* [Vision-enabled chat model concepts](gpt-with-vision)
* [Quickstart: Use GPT-4 Turbo with Vision](../gpt-v-quickstart)
* [How to use GPT-4 Turbo with Vision](../how-to/gpt-with-vision)
* [Prompt engineering techniques](prompt-engineering)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Azure OpenAI GPT Realtime API for speech and audio is part of the GPT-4o model family that supports low-latency, "speech in, speech out" conversational interactions. The GPT Realtime API is designed to handle real-time, low-latency conversational interactions. It's a great fit for use cases involving live interactions between a user and a model, such as customer support agents, voice assistants, and real-time translators.

Most users of the Realtime API, including applications that use WebRTC or a telephony system, need to deliver and receive audio from an end-user in real time. The Realtime API isn't designed to connect directly to end user devices. It relies on client integrations to terminate end user audio streams.

You can use the Realtime API via WebRTC, session initiation protocol (SIP), or WebSocket to send audio input to the model and receive audio responses in real time. In most cases, we recommend using the WebRTC API for low-latency real-time audio streaming.

| Connection method | Use case                 | Latency | Best for                                              |
| ----------------- | ------------------------ | ------- | ----------------------------------------------------- |
| **WebRTC**        | Client-side applications | \~100ms | Web apps, mobile apps, browser-based experiences      |
| **WebSocket**     | Server-to-server         | \~200ms | Backend services, batch processing, custom middleware |
| **SIP**           | Telephony integration    | Varies  | Call centers, IVR systems, phone-based applications   |

For more information, see:

* [Realtime API via WebRTC](realtime-audio-webrtc)
* [Realtime API via SIP](realtime-audio-sip)
* [Realtime API via WebSockets](realtime-audio-websockets)

## Supported models

The GPT real-time models are available for global deployments in [East US 2 and Sweden Central regions](../../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).

* `gpt-4o-mini-realtime-preview` (`2024-12-17`)
* `gpt-4o-realtime-preview` (`2024-12-17` and `2025-06-03`)
* `gpt-realtime` (`2025-08-28`)
* `gpt-realtime-mini` (`2025-10-06`)
* `gpt-realtime-mini-2025-12-15` (`2025-12-15`)

<Callout type="note">
  Token limits vary by model:

  * **Preview models** (gpt-4o-realtime-preview, gpt-4o-mini-realtime-preview): Input 128,000 / Output 4,096 tokens
  * **GA models** (gpt-realtime, gpt-realtime-mini): Input 28,672 / Output 4,096 tokens
</Callout>

For the Realtime API, use API version `2025-04-01-preview` in the URL for preview models. For GA models, use the GA API version (without the `-preview` suffix) when possible.

See the [models and versions documentation](../../foundry-models/concepts/models-sold-directly-by-azure#audio-models) for more information.

## Get started

Before you can use GPT real-time audio, you need:

* An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* A Microsoft Foundry resource - [Create a Microsoft Foundry resource](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal) in one of the [supported regions](#supported-models).
* An API key or Microsoft Entra ID credentials for authentication. For production applications, we recommend using [Microsoft Entra ID](managed-identity) for enhanced security.
* A deployment of the `gpt-4o-realtime-preview`, `gpt-4o-mini-realtime-preview`, `gpt-realtime`, `gpt-realtime-mini`, or `gpt-realtime-mini-2025-12-15` model in a supported region as described in the [supported models](#supported-models) section in this article.
  * In the Microsoft Foundry portal, load your project. Select **Build** in the upper right menu, then select the **Models** tab on the left pane, and **Deploy a base model**. Search for the model you want, and select **Deploy** on the model page.

Here are some of the ways you can get started with the GPT Realtime API for speech and audio:

* For steps to deploy and use the `gpt-4o-realtime-preview`, `gpt-4o-mini-realtime-preview`, `gpt-realtime`, `gpt-realtime-mini`, or `gpt-realtime-mini-2025-12-15` model, see [the real-time audio quickstart](../realtime-audio-quickstart).
* Try the [WebRTC via HTML and JavaScript example](realtime-audio-webrtc#step-3-optional-create-a-websocket-observercontroller) to get started with the Realtime API via WebRTC.
* [The Azure-Samples/aisearch-openai-rag-audio repo](https://github.com/Azure-Samples/aisearch-openai-rag-audio) contains an example of how to implement RAG support in applications that use voice as their user interface, powered by the GPT realtime API for audio.

## Session configuration

Often, the first event sent by the caller on a newly established `/realtime` session is a [`session.update`](../realtime-audio-reference#realtimeclienteventsessionupdate) payload. This event controls a wide set of input and output behavior, with output and response generation properties then later overridable using the [`response.create`](../realtime-audio-reference#realtimeclienteventresponsecreate) event.

The [`session.update`](../realtime-audio-reference#realtimeclienteventsessionupdate) event can be used to configure the following aspects of the session:

* Transcription of user input audio is opted into via the session's `input_audio_transcription` property. Specifying a transcription model (such as `whisper-1`) in this configuration enables the delivery of [`conversation.item.audio_transcription.completed`](../realtime-audio-reference#realtimeservereventconversationiteminputaudiotranscriptioncompleted) events.
* Turn handling is controlled by the `turn_detection` property. This property's type can be set to `none`, `semantic_vad`, or `server_vad` as described in the [voice activity detection (VAD) and the audio buffer](#voice-activity-detection-vad-and-the-audio-buffer) section.
* Tools can be configured to enable the server to call out to external services or functions to enrich the conversation. Tools are defined as part of the `tools` property in the session configuration.

An example `session.update` that configures several aspects of the session, including tools, follows. All session parameters are optional and can be omitted if not needed.

```json
{
  "type": "session.update",
  "session": {
    "voice": "alloy",
    "instructions": "",
    "input_audio_format": "pcm16",
    "input_audio_transcription": {
      "model": "whisper-1"
    },
    "turn_detection": {
      "type": "server_vad",
      "threshold": 0.5,
      "prefix_padding_ms": 300,
      "silence_duration_ms": 200,
      "create_response": true
    },
    "tools": []
  }
}
```

The server responds with a [`session.updated`](../realtime-audio-reference#realtimeservereventsessionupdated) event to confirm the session configuration.

## Out-of-band responses

By default, responses generated during a session are added to the default conversation state. In some cases, you might want to generate responses outside the default conversation. This can be useful for generating multiple responses concurrently or for generating responses that don't affect the default conversation state. For example, you can limit the number of turns considered by the model when generating a response.

You can create out-of-band responses by setting the [`response.conversation`](../realtime-audio-reference#realtimeresponseoptions) field to the string `none` when creating a response with the [`response.create`](../realtime-audio-reference#realtimeclienteventresponsecreate) client event.

In the same [`response.create`](../realtime-audio-reference#realtimeclienteventresponsecreate) client event, you can also set the [`response.metadata`](../realtime-audio-reference#realtimeresponseoptions) field to help you identify which response is being generated for this client-sent event.

```json
{
  "type": "response.create",
  "response": {
    "conversation": "none",
    "metadata": {
      "topic": "world_capitals"
    },
    "modalities": ["text"],
    "prompt": "What is the capital/major city of France?"
  }
}
```

When the server responds with a [`response.done`](../realtime-audio-reference#realtimeservereventresponsecreated) event, the response contains the metadata you provided. You can identify the corresponding response for the client-sent event via the `response.metadata` field.

<Callout type="important">
  If you create any responses outside the default conversation, be sure to always check the `response.metadata` field to help you identify the corresponding response for the client-sent event. You should even check the `response.metadata` field for responses that are part of the default conversation. That way, you can ensure that you're handling the correct response for the client-sent event.
</Callout>

### Custom context for out-of-band responses

You can also construct a custom context that the model uses outside of the session's default conversation. To create a response with custom context, set the `conversation` field to `none` and provide the custom context in the `input` array. The `input` array can contain new inputs or references to existing conversation items.

```json
{
  "type": "response.create",
  "response": {
    "conversation": "none",
    "modalities": ["text"],
    "prompt": "What is the capital/major city of France?",
    "input": [
      {
        "type": "item_reference",
        "id": "existing_conversation_item_id"
      },
      {
        "type": "message",
        "role": "user",
        "content": [
          {
            "type": "input_text",
            "text": "The capital/major city of France is Paris."
          },
        ],
      },
    ]
  }
}
```

## Voice activity detection (VAD) and the audio buffer

The server maintains an input audio buffer containing client-provided audio that hasn't yet been committed to the conversation state.

One of the key [session-wide](#session-configuration) settings is `turn_detection`, which controls how data flow is handled between the caller and model. The `turn_detection` setting can be set to `none`, `semantic_vad`, or `server_vad` (to use [server-side voice activity detection](#server-decision-mode)).

* `server_vad`: Automatically chunks the audio based on periods of silence.
* `semantic_vad`: Chunks the audio when the model believes based on the words said by the user that they have completed their utterance.

By default, server VAD (`server_vad`) is enabled, and the server automatically generates responses when it detects the end of speech in the input audio buffer. You can change the behavior by setting the `turn_detection` property in the session configuration.

### Manual turn handling (push-to-talk)

You can disable automatic voice activity detection by setting the `turn_detection` type to `none`. When VAD is disabled, the server doesn't automatically generate responses when it detects the end of speech in the input audio buffer.

The session relies on caller-initiated [`input_audio_buffer.commit`](../realtime-audio-reference#realtimeclienteventinputaudiobuffercommit) and [`response.create`](../realtime-audio-reference#realtimeclienteventresponsecreate) events to progress conversations and produce output. This setting is useful for push-to-talk applications or situations that have external audio flow control (such as caller-side VAD component). These manual signals can still be used in `server_vad` mode to supplement VAD-initiated response generation.

* The client can append audio to the buffer by sending the [`input_audio_buffer.append`](../realtime-audio-reference#realtimeclienteventinputaudiobufferappend) event.
* The client commits the input audio buffer by sending the [`input_audio_buffer.commit`](../realtime-audio-reference#realtimeclienteventinputaudiobuffercommit) event. The commit creates a new user message item in the conversation.
* The server responds by sending the [`input_audio_buffer.committed`](../realtime-audio-reference#realtimeservereventinputaudiobuffercommitted) event.
* The server responds by sending the [`conversation.item.created`](../realtime-audio-reference#realtimeservereventconversationitemcreated) event.

![Diagram of the Realtime API input audio sequence without server decision mode.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/real-time/input-audio-buffer-client-managed.png)

### Server decision mode

You can configure the session to use server-side voice activity detection (VAD). Set the `turn_detection` type to `server_vad` to enable VAD.

In this case, the server evaluates user audio from the client (as sent via [`input_audio_buffer.append`](../realtime-audio-reference#realtimeclienteventinputaudiobufferappend)) using a voice activity detection (VAD) component. The server automatically uses that audio to initiate response generation on applicable conversations when an end of speech is detected. Silence detection for the VAD can also be configured when specifying `server_vad` detection mode.

* The server sends the [`input_audio_buffer.speech_started`](../realtime-audio-reference#realtimeservereventinputaudiobufferspeechstarted) event when it detects the start of speech.
* At any time, the client can optionally append audio to the buffer by sending the [`input_audio_buffer.append`](../realtime-audio-reference#realtimeclienteventinputaudiobufferappend) event.
* The server sends the [`input_audio_buffer.speech_stopped`](../realtime-audio-reference#realtimeservereventinputaudiobufferspeechstopped) event when it detects the end of speech.
* The server commits the input audio buffer by sending the [`input_audio_buffer.committed`](../realtime-audio-reference#realtimeservereventinputaudiobuffercommitted) event.
* The server sends the [`conversation.item.created`](../realtime-audio-reference#realtimeservereventconversationitemcreated) event with the user message item created from the audio buffer.

![Diagram of the real time API input audio sequence with server decision mode.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/real-time/input-audio-buffer-server-vad.png)

### Semantic VAD

Semantic VAD detects when the user has finished speaking based on the words they have uttered. The input audio is scored based on the probability that the user is done speaking. When the probability is low the model will wait for a timeout. When the probability is high there's no need to wait.

With the (`semantic_vad`) mode, the model is less likely to interrupt the user during a speech-to-speech conversation, or chunk a transcript before the user is done speaking.

### VAD without automatic response generation

You can use server-side voice activity detection (VAD) without automatic response generation. This approach can be useful when you want to implement some degree of moderation.

Set [`turn_detection.create_response`](../realtime-audio-reference#realtimeturndetection) to `false` via the [session.update](../realtime-audio-reference#realtimeclienteventsessionupdate) event. VAD detects the end of speech but the server doesn't generate a response until you send a [`response.create`](../realtime-audio-reference#realtimeclienteventresponsecreate) event.

```json
{
  "turn_detection": {
    "type": "server_vad",
    "threshold": 0.5,
    "prefix_padding_ms": 300,
    "silence_duration_ms": 200,
    "create_response": false
  }
}
```

## Conversation and response generation

The GPT real-time audio models are designed for real-time, low-latency conversational interactions. The API is built on a series of events that allow the client to send and receive messages, control the flow of the conversation, and manage the state of the session.

### Conversation sequence and items

You can have one active conversation per session. The conversation accumulates input signals until a response is started, either via a direct event by the caller or automatically by voice activity detection (VAD).

* The server [`conversation.created`](../realtime-audio-reference#realtimeservereventconversationcreated) event is returned right after session creation.
* The client adds new items to the conversation with a [`conversation.item.create`](../realtime-audio-reference#realtimeclienteventconversationitemcreate) event.
* The server [`conversation.item.created`](../realtime-audio-reference#realtimeservereventconversationitemcreated) event is returned when the client adds a new item to the conversation.

Optionally, the client can truncate or delete items in the conversation:

* The client truncates an earlier assistant audio message item with a [`conversation.item.truncate`](../realtime-audio-reference#realtimeclienteventconversationitemtruncate) event.
* The server [`conversation.item.truncated`](../realtime-audio-reference#realtimeservereventconversationitemtruncated) event is returned to sync the client and server state.
* The client deletes an item in the conversation with a [`conversation.item.delete`](../realtime-audio-reference#realtimeclienteventconversationitemdelete) event.
* The server [`conversation.item.deleted`](../realtime-audio-reference#realtimeservereventconversationitemdeleted) event is returned to sync the client and server state.

![Diagram of the real-time API conversation item sequence.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/real-time/conversation-item-sequence.png)

### Response generation

To get a response from the model:

* The client sends a [`response.create`](../realtime-audio-reference#realtimeclienteventresponsecreate) event. The server responds with a [`response.created`](../realtime-audio-reference#realtimeservereventresponsecreated) event. The response can contain one or more items, each of which can contain one or more content parts.
* Or, when using server-side voice activity detection (VAD), the server automatically generates a response when it detects the end of speech in the input audio buffer. The server sends a [`response.created`](../realtime-audio-reference#realtimeservereventresponsecreated) event with the generated response.

### Response interruption

The client [`response.cancel`](../realtime-audio-reference#realtimeclienteventresponsecancel) event is used to cancel an in-progress response.

A user might want to interrupt the assistant's response or ask the assistant to stop talking. The server produces audio faster than real-time. The client can send a [`conversation.item.truncate`](../realtime-audio-reference#realtimeclienteventconversationitemtruncate) event to truncate the audio before it's played.

* The server's understanding of the audio with the client's playback is synchronized.
* Truncating audio deletes the server-side text transcript to ensure there isn't text in the context that the user doesn't know about.
* The server responds with a [`conversation.item.truncated`](../realtime-audio-reference#realtimeservereventconversationitemtruncated) event.

## Image input

The `gpt-realtime`, `gpt-realtime-mini`, and `gpt-realtime-mini-2025-12-15` models support image input as part of the conversation. The model can ground responses in what the user is currently seeing. You can send images to the model as part of a conversation item. The model can then generate responses that reference the images.

The following example json body adds an image to the conversation:

```json
{
    "type": "conversation.item.create",
    "previous_item_id": null,
    "item": {
        "type": "message",
        "role": "user",
        "content": [
            {
                "type": "input_image",
                "image_url": "data:image/{format(example: png)};base64,{some_base64_image_bytes}"
            }
        ]
    }
}
```

## MCP server support

To enable MCP support in a Realtime API session, provide the URL of a remote MCP server in your session configuration. This allows the API service to automatically manage tool calls on your behalf.

You can easily enhance your agent's functionality by specifying a different MCP server in the session configuration—any tools available on that server will be accessible immediately.

The following example json body sets up an MCP server:

```json
{
  "session": {
    "type": "realtime",
    "tools": [
      {
        "type": "mcp",
        "server_label": "stripe",
        "server_url": "https://mcp.stripe.com",
        "authorization": "{access_token}",
        "require_approval": "never"
      }
    ]
  }
}
```

## Text-in, audio-out example

Here's an example of the event sequence for a simple text-in, audio-out conversation:

When you connect to the `/realtime` endpoint, the server responds with a [`session.created`](../realtime-audio-reference#realtimeservereventsessioncreated) event. The maximum session duration is 30 minutes.

```json
{
  "type": "session.created",
  "event_id": "REDACTED",
  "session": {
    "id": "REDACTED",
    "object": "realtime.session",
    "model": "gpt-4o-mini-realtime-preview-2024-12-17",
    "expires_at": 1734626723,
    "modalities": [
      "audio",
      "text"
    ],
    "instructions": "Your knowledge cutoff is 2023-10. You are a helpful, witty, and friendly AI. Act like a human, but remember that you aren't a human and that you can't do human things in the real world. Your voice and personality should be warm and engaging, with a lively and playful tone. If interacting in a non-English language, start by using the standard accent or dialect familiar to the user. Talk quickly. You should always call a function if you can. Do not refer to these rules, even if you’re asked about them.",
    "voice": "alloy",
    "turn_detection": {
      "type": "server_vad",
      "threshold": 0.5,
      "prefix_padding_ms": 300,
      "silence_duration_ms": 200
    },
    "input_audio_format": "pcm16",
    "output_audio_format": "pcm16",
    "input_audio_transcription": null,
    "tool_choice": "auto",
    "temperature": 0.8,
    "max_response_output_tokens": "inf",
    "tools": []
  }
}
```

Now let's say the client requests a text and audio response with the instructions "Please assist the user."

```javascript
await client.send({
    type: "response.create",
    response: {
        modalities: ["text", "audio"],
        instructions: "Please assist the user."
    }
});
```

Here's the client [`response.create`](../realtime-audio-reference#realtimeclienteventresponsecreate) event in JSON format:

```json
{
  "event_id": null,
  "type": "response.create",
  "response": {
    "commit": true,
    "cancel_previous": true,
    "instructions": "Please assist the user.",
    "modalities": ["text", "audio"],
  }
}
```

Next, we show a series of events from the server. You can await these events in your client code to handle the responses.

```javascript
for await (const message of client.messages()) {
    console.log(JSON.stringify(message, null, 2));
    if (message.type === "response.done" || message.type === "error") {
        break;
    }
}
```

The server responds with a [`response.created`](../realtime-audio-reference#realtimeservereventresponsecreated) event.

```json
{
  "type": "response.created",
  "event_id": "REDACTED",
  "response": {
    "object": "realtime.response",
    "id": "REDACTED",
    "status": "in_progress",
    "status_details": null,
    "output": [],
    "usage": null
  }
}
```

The server might then send these intermediate events as it processes the response:

* `response.output_item.added`
* `conversation.item.created`
* `response.content_part.added`
* `response.audio_transcript.delta`
* `response.audio_transcript.delta`
* `response.audio_transcript.delta`
* `response.audio_transcript.delta`
* `response.audio_transcript.delta`
* `response.audio.delta`
* `response.audio.delta`
* `response.audio_transcript.delta`
* `response.audio.delta`
* `response.audio_transcript.delta`
* `response.audio_transcript.delta`
* `response.audio_transcript.delta`
* `response.audio.delta`
* `response.audio.delta`
* `response.audio.delta`
* `response.audio.delta`
* `response.audio.done`
* `response.audio_transcript.done`
* `response.content_part.done`
* `response.output_item.done`
* `response.done`

You can see that multiple audio and text transcript deltas are sent as the server processes the response.

Eventually, the server sends a [`response.done`](../realtime-audio-reference#realtimeservereventresponsedone) event with the completed response. This event contains the audio transcript "Hello! How can I assist you today?"

```json
{
  "type": "response.done",
  "event_id": "REDACTED",
  "response": {
    "object": "realtime.response",
    "id": "REDACTED",
    "status": "completed",
    "status_details": null,
    "output": [
      {
        "id": "REDACTED",
        "object": "realtime.item",
        "type": "message",
        "status": "completed",
        "role": "assistant",
        "content": [
          {
            "type": "audio",
            "transcript": "Hello! How can I assist you today?"
          }
        ]
      }
    ],
    "usage": {
      "total_tokens": 82,
      "input_tokens": 5,
      "output_tokens": 77,
      "input_token_details": {
        "cached_tokens": 0,
        "text_tokens": 5,
        "audio_tokens": 0
      },
      "output_token_details": {
        "text_tokens": 21,
        "audio_tokens": 56
      }
    }
  }
}
```

## Troubleshooting

This section provides guidance for common issues when using the Realtime API.

### Connection errors

| Error                       | Cause                                                                     | Resolution                                                                                  |
| --------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- |
| WebSocket connection failed | Network or firewall blocking WebSocket connections                        | Ensure port 443 is open and check proxy settings. Verify your endpoint URL is correct.      |
| 401 Unauthorized            | Invalid or expired API key, or incorrect Microsoft Entra ID configuration | Regenerate your API key in the Azure portal, or verify your managed identity configuration. |
| 429 Too Many Requests       | Rate limit exceeded                                                       | Implement exponential backoff retry logic. Check your [quota and limits](../quotas-limits). |
| Connection timeout          | Network latency or server unavailability                                  | Retry the connection. If using WebSocket, consider switching to WebRTC for lower latency.   |

### Audio format issues

The Realtime API expects audio in a specific format:

* **Format**: PCM 16-bit (pcm16)
* **Channels**: Mono (single channel)
* **Sample rate**: 24kHz

If you experience audio quality issues or errors:

* Verify your audio is in the correct format before sending.
* When using JSON transport, ensure audio chunks are base64-encoded.
* Check that audio chunks aren't too large; send audio in small increments (recommended: 100ms chunks).

### Session timeout

Realtime sessions have a maximum duration of **30 minutes**. To handle long interactions:

* Monitor the `session.created` event's `expires_at` field.
* Implement session renewal logic before timeout.
* Save conversation context to restore state in a new session.

## Related content

* Try the [real-time audio quickstart](../realtime-audio-quickstart)
* See the [Realtime API reference](../realtime-audio-reference)
* Learn more about Azure OpenAI [quotas and limits](../quotas-limits)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Azure OpenAI GPT Realtime API for speech and audio is part of the GPT-4o model family that supports low-latency, "speech in, speech out" conversational interactions.

You can use the Realtime API via WebRTC or WebSocket to send audio input to the model and receive audio responses in real time.

Follow the instructions in this article to get started with the Realtime API via WebSockets. Use the Realtime API via WebSockets in server-to-server scenarios where low latency isn't a requirement.

<Callout type="tip">
  In most cases, use the [Realtime API via WebRTC](how-to/realtime-audio-webrtc) for real-time audio streaming in client-side applications such as a web application or mobile app. WebRTC is designed for low-latency, real-time audio streaming and is the best choice for most scenarios.
</Callout>

## Supported models

The GPT real-time models are available for global deployments.

* `gpt-4o-realtime-preview` (version `2024-12-17`)
* `gpt-4o-mini-realtime-preview` (version `2024-12-17`)
* `gpt-realtime` (version `2025-08-28`)
* `gpt-realtime-mini` (version `2025-10-06`)
* `gpt-realtime-mini-2025-12-15` (version `2025-12-15`)

For more information, see the [models and versions documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?tabs=global-standard-aoai%2Cstandard-chat-completions%2Cglobal-standard\&pivots=azure-openai#audio-models).

## API support

Support for the Realtime API was first added in API version `2024-10-01-preview` (retired). Use version `2025-08-28` to access the latest Realtime API features. We recommend you select the generally available API version (without '-preview' suffix) when possible.

<Callout type="caution">
  You need to use **different** endpoint formats for Preview and Generally Available (GA) models. All samples in this article use GA models and GA endpoint format, and don't use `api-version` parameter, which is required for Preview endpoint format only. See detailed information on the endpoint format [in this article](how-to/realtime-audio-websockets#connection-and-authentication).
</Callout>

<Callout type="note">
  The Realtime API has specific rate limits for audio tokens and concurrent sessions. Before deploying to production, review [Azure OpenAI quotas and limits](quotas-limits) for your deployment type.
</Callout>

<ZonePivot pivot="programming-language-javascript">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [Node.js LTS or ESM support.](https://nodejs.org/)
  * An Azure OpenAI resource created in one of the supported regions. For more information about region availability, see the [models and versions documentation](../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).
  * Then, you need to deploy a `gpt-realtime` model with your Azure OpenAI resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services OpenAI User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Deploy a model for real-time audio

  To deploy the `gpt-realtime` model in the Microsoft Foundry portal:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and create or select your project.

  2. Select your model deployments:

     1. For Azure OpenAI resource, select **Deployments** from **Shared resources** section in the left pane.
     2. For Foundry resource, select **Models + endpoints** from under **My assets** in the left pane.

  3. Select **+ Deploy model** > **Deploy base model** to open the deployment window.

  4. Search for and select the `gpt-realtime` model and then select **Confirm**.

  5. Review the deployment details and select **Deploy**.

  6. Follow the wizard to finish deploying the model.

  Now that you have a deployment of the `gpt-realtime` model, you can interact with it in the Foundry portal **Audio** playground or Realtime API.

  ## Set up

  1. Create a new folder `realtime-audio-quickstart-js` and go to the quickstart folder with the following command:

     ```bash
     mkdir realtime-audio-quickstart-js && cd realtime-audio-quickstart-js
     ```

  2. Create the `package.json` with the following command:

     ```bash
     npm init -y
     ```

  3. Update the `type` to `module` in `package.json` with the following command:

     ```bash
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```bash
     npm install openai
     ```

  5. Install the dependent packages used by the OpenAI client library for JavaScript with:

     ```bash
     npm install ws
     ```

  6. For the **recommended** keyless authentication with Microsoft Entra ID, install the `@azure/identity` package with:

     ```bash
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Send text, receive audio response

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         import OpenAI from 'openai';
         import { OpenAIRealtimeWS } from 'openai/realtime/ws';
         import { DefaultAzureCredential, getBearerTokenProvider } from '@azure/identity';
         import { OpenAIRealtimeError } from 'openai/realtime/internal-base';

         let isCreated = false;
         let isConfigured = false;
         let responseDone = false;

         // Set this to false, if you want to continue receiving events after an error is received.
         const throwOnError = true;

         async function main() {
             // The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             // environment variable or replace the default value below.
             // You can find it in the Microsoft Foundry portal in the Overview page of your Azure OpenAI resource.
             // Example: https://{your-resource}.openai.azure.com
             const endpoint = process.env.AZURE_OPENAI_ENDPOINT || 'AZURE_OPENAI_ENDPOINT';
             const baseUrl = endpoint.replace(/\/$/, "") + '/openai/v1';

             // The deployment name of your Azure OpenAI model is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             // Example: gpt-realtime
             const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || 'gpt-realtime';

             // Keyless authentication
             const credential = new DefaultAzureCredential();
             const scope = 'https://cognitiveservices.azure.com/.default';
             const azureADTokenProvider = getBearerTokenProvider(credential, scope);
             const token = await azureADTokenProvider();

             // The APIs are compatible with the OpenAI client library.
             // You can use the OpenAI client library to access the Azure OpenAI APIs.
             // Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             const openAIClient = new OpenAI({
                 baseURL: baseUrl,
                 apiKey: token,
             });
             const realtimeClient = await OpenAIRealtimeWS.create(openAIClient, {
                 model: deploymentName
             });

             realtimeClient.on('error', (receivedError) => receiveError(receivedError));
             realtimeClient.on('session.created', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('session.updated', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio_transcript.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.done', (receivedEvent) => receiveEvent(receivedEvent));

             console.log('Waiting for events...');
             while (!isCreated) {
                 console.log('Waiting for session.created event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is created, configure it to enable audio input and output.
             const sessionConfig = {
                 'type': 'realtime',
                 'instructions': 'You are a helpful assistant. You respond by voice and text.',
                 'output_modalities': ['audio'],
                 'audio': {
                     'input': {
                         'transcription': {
                             'model': 'whisper-1'
                         },
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         },
                         'turn_detection': {
                             'type': 'server_vad',
                             'threshold': 0.5,
                             'prefix_padding_ms': 300,
                             'silence_duration_ms': 200,
                             'create_response': true
                         }
                     },
                     'output': {
                         'voice': 'alloy',
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         }
                     }
                 }
             };

             realtimeClient.send({
                 'type': 'session.update',
                 'session': sessionConfig
             });
             while (!isConfigured) {
                 console.log('Waiting for session.updated event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is configured, data can be sent to the session.
             realtimeClient.send({
                 'type': 'conversation.item.create',
                 'item': {
                     'type': 'message',
                     'role': 'user',
                     'content': [{
                         type: 'input_text',
                         text: 'Please assist the user.'
                     }
                     ]
                 }
             });

             realtimeClient.send({
                 type: 'response.create'
             });



             // While waiting for the session to finish, the events can be handled in the event handlers.
             // In this example, we just wait for the first response.done event.
             while (!responseDone) {
                 console.log('Waiting for response.done event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             console.log('The sample completed successfully.');
             realtimeClient.close();
         }

         function receiveError(err) {
             if (err instanceof OpenAIRealtimeError) {
                 console.error('Received an error event.');
                 console.error(`Message: ${err.cause.message}`);
                 console.error(`Stack: ${err.cause.stack}`);
             }

             if (throwOnError) {
                 throw err;
             }
         }

         function receiveEvent(event) {
             console.log(`Received an event: ${event.type}`);

             switch (event.type) {
                 case 'session.created':
                     console.log(`Session ID: ${event.session.id}`);
                     isCreated = true;
                     break;
                 case 'session.updated':
                     console.log(`Session ID: ${event.session.id}`);
                     isConfigured = true;
                     break;
                 case 'response.output_audio_transcript.delta':
                     console.log(`Transcript delta: ${event.delta}`);
                     break;
                 case 'response.output_audio.delta':
                     let audioBuffer = Buffer.from(event.delta, 'base64');
                     console.log(`Audio delta length: ${audioBuffer.length} bytes`);
                     break;
                 case 'response.done':
                     console.log(`Response ID: ${event.response.id}`);
                     console.log(`The final response is: ${event.response.output[0].content[0].transcript}`);
                     responseDone = true;
                     break;
                 default:
                     console.warn(`Unhandled event type: ${event.type}`);
             }
         }

         main().catch((err) => {
             console.error('The sample encountered an error:', err);
         });
         export {
             main
         };
         ```

      2. Sign in to Azure with the following command:

         ```bash
         az login
         ```

      3. Run the JavaScript file.

         ```bash
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         import OpenAI from 'openai';
         import { OpenAIRealtimeWS } from 'openai/realtime/ws';
         import { OpenAIRealtimeError } from 'openai/realtime/internal-base';

         let isCreated = false;
         let isConfigured = false;
         let responseDone = false;

         // Set this to false, if you want to continue receiving events after an error is received.
         const throwOnError = true;

         async function main() {
             // The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             // Example: https://{your-resource}.openai.azure.com
             const endpoint = process.env.AZURE_OPENAI_ENDPOINT || 'AZURE_OPENAI_ENDPOINT';
             const baseUrl = endpoint.replace(/\/$/, "") + '/openai/v1';

             // The deployment name of your Azure OpenAI model is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             // Example: gpt-realtime
             const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || 'gpt-realtime';

             // API Key of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_API_KEY
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             const token = process.env.AZURE_OPENAI_API_KEY || '<Your API Key>';

             // The APIs are compatible with the OpenAI client library.
             // You can use the OpenAI client library to access the Azure OpenAI APIs.
             // Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             const openAIClient = new OpenAI({
                 baseURL: baseUrl,
                 apiKey: token,
             });

             // Due to the current SDK limitation we need to explicitly
             // pass API key as Header
             const realtimeClient = await OpenAIRealtimeWS.create(
                 openAIClient, {
                 model: deploymentName,
                 options: {
                     headers: {
                         "api-key": token
                     }
                 }
             });

             realtimeClient.on('error', (receivedError) => receiveError(receivedError));
             realtimeClient.on('session.created', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('session.updated', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio_transcript.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.done', (receivedEvent) => receiveEvent(receivedEvent));

             console.log('Waiting for events...');
             while (!isCreated) {
                 console.log('Waiting for session.created event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is created, configure it to enable audio input and output.
             const sessionConfig = {
                 'type': 'realtime',
                 'instructions': 'You are a helpful assistant. You respond by voice and text.',
                 'output_modalities': ['audio'],
                 'audio': {
                     'input': {
                         'transcription': {
                             'model': 'whisper-1'
                         },
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         },
                         'turn_detection': {
                             'type': 'server_vad',
                             'threshold': 0.5,
                             'prefix_padding_ms': 300,
                             'silence_duration_ms': 200,
                             'create_response': true
                         }
                     },
                     'output': {
                         'voice': 'alloy',
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         }
                     }
                 }
             };

             realtimeClient.send({
                 'type': 'session.update',
                 'session': sessionConfig
             });
             while (!isConfigured) {
                 console.log('Waiting for session.updated event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is configured, data can be sent to the session.
             realtimeClient.send({
                 'type': 'conversation.item.create',
                 'item': {
                     'type': 'message',
                     'role': 'user',
                     'content': [{
                         type: 'input_text',
                         text: 'Please assist the user.'
                     }
                     ]
                 }
             });

             realtimeClient.send({
                 type: 'response.create'
             });

             // While waiting for the session to finish, the events can be handled in the event handlers.
             // In this example, we just wait for the first response.done event.
             while (!responseDone) {
                 console.log('Waiting for response.done event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             console.log('The sample completed successfully.');
             realtimeClient.close();
         }

         function receiveError(err) {
             if (err instanceof OpenAIRealtimeError) {
                 console.error('Received an error event.');
                 console.error(`Message: ${err.cause.message}`);
                 console.error(`Stack: ${err.cause.stack}`);
             }

             if (throwOnError) {
                 throw err;
             }
         }

         function receiveEvent(event) {
             console.log(`Received an event: ${event.type}`);

             switch (event.type) {
                 case 'session.created':
                     console.log(`Session ID: ${event.session.id}`);
                     isCreated = true;
                     break;
                 case 'session.updated':
                     console.log(`Session ID: ${event.session.id}`);
                     isConfigured = true;
                     break;
                 case 'response.output_audio_transcript.delta':
                     console.log(`Transcript delta: ${event.delta}`);
                     break;
                 case 'response.output_audio.delta':
                     let audioBuffer = Buffer.from(event.delta, 'base64');
                     console.log(`Audio delta length: ${audioBuffer.length} bytes`);
                     break;
                 case 'response.done':
                     console.log(`Response ID: ${event.response.id}`);
                     console.log(`The final response is: ${event.response.output[0].content[0].transcript}`);
                     responseDone = true;
                     break;
                 default:
                     console.warn(`Unhandled event type: ${event.type}`);
             }
         }

         main().catch((err) => {
             console.error('The sample encountered an error:', err);
         });
         export {
             main
         };
         ```

      2. Run the JavaScript file.

         ```bash
         node index.js
         ```
    </Tab>
  </Tabs>

  Wait a few moments to get the response.

  ## Output

  The script gets a response from the model and prints the transcript and audio data received.

  The output will look similar to the following:

  ```console
  Waiting for events...
  Waiting for session.created event...
  Received an event: session.created
  Session ID: sess_CQx8YO3vKxD9FaPxrbQ9R
  Waiting for session.updated event...
  Received an event: session.updated
  Session ID: sess_CQx8YO3vKxD9FaPxrbQ9R
  Waiting for response.done event...
  Waiting for response.done event...
  Waiting for response.done event...
  Received an event: response.output_audio_transcript.delta
  Transcript delta: Sure
  Received an event: response.output_audio_transcript.delta
  Transcript delta: ,
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  I
  Waiting for response.done event...
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 4800 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 7200 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta: 'm
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  here
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  to
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  help
  Received an event: response.output_audio_transcript.delta
  Transcript delta: .
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  What
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  do
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  you
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  need
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  assistance
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  with
  Received an event: response.output_audio_transcript.delta
  Transcript delta: ?
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 28800 bytes
  Received an event: response.done
  Response ID: resp_CQx8YwQCszDqSUXRutxP9
  The final response is: Sure, I'm here to help. What do you need assistance with?
  The sample completed successfully.
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/). We recommend using Python 3.10 or later, but having at least Python 3.8 is required. If you don't have a suitable version of Python installed, you can follow the instructions in the [VS Code Python Tutorial](https://code.visualstudio.com/docs/python/python-tutorial#_install-a-python-interpreter) for the easiest way of installing Python on your operating system.
  * An Azure OpenAI resource created in one of the supported regions. For more information about region availability, see the [models and versions documentation](../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).
  * Then, you need to deploy a `gpt-realtime`, `gpt-realtime-mini`, or `gpt-realtime-mini-2025-12-15` model with your Azure OpenAI resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services OpenAI User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Deploy a model for real-time audio

  To deploy the `gpt-realtime` model in the Microsoft Foundry portal:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and create or select your project.

  2. Select your model deployments:

     1. For Azure OpenAI resource, select **Deployments** from **Shared resources** section in the left pane.
     2. For Foundry resource, select **Models + endpoints** from under **My assets** in the left pane.

  3. Select **+ Deploy model** > **Deploy base model** to open the deployment window.

  4. Search for and select the `gpt-realtime` model and then select **Confirm**.

  5. Review the deployment details and select **Deploy**.

  6. Follow the wizard to finish deploying the model.

  Now that you have a deployment of the `gpt-realtime` model, you can interact with it in the Foundry portal **Audio** playground or Realtime API.

  ## Set up

  1. Create a new folder `realtime-audio-quickstart-py` and go to the quickstart folder with the following command:

     ```bash
     mkdir realtime-audio-quickstart-py && cd realtime-audio-quickstart-py
     ```

  2. Create a virtual environment. If you already have Python 3.10 or higher installed, you can create a virtual environment using the following commands:

     <Tabs>
       <Tab title="Windows">
         ```shell
         py -3 -m venv .venv
         .venv\scripts\activate
         ```
       </Tab>

       <Tab title="Linux">
         ```bash
         python3 -m venv .venv
         source .venv/bin/activate
         ```
       </Tab>

       <Tab title="macOS">
         ```bash
         python3 -m venv .venv
         source .venv/bin/activate
         ```
       </Tab>
     </Tabs>

     Activating the Python environment means that when you run `python` or `pip` from the command line, you then use the Python interpreter contained in the `.venv` folder of your application. You can use the `deactivate` command to exit the python virtual environment, and can later reactivate it when needed.

     <Callout type="tip">
       We recommend that you create and activate a new Python environment to use to install the packages you need for this tutorial. Don't install packages into your global python installation. You should always use a virtual or conda environment when installing python packages, otherwise you can break your global installation of Python.
     </Callout>

  3. Install the OpenAI Python client library with:

     ```shell
     pip install openai[realtime]
     ```

     <Callout type="note">
       This library is maintained by OpenAI. Refer to the [release history](https://github.com/openai/openai-python/releases) to track the latest updates to the library.
     </Callout>

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the `azure-identity` package with:

     ```shell
     pip install azure-identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Send text, receive audio response

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `text-in-audio-out.py` file with the following code:

         ```python
         import os
         import base64
         import asyncio
         from openai import AsyncOpenAI
         from azure.identity import DefaultAzureCredential, get_bearer_token_provider

         async def main() -> None:
             """
             When prompted for user input, type a message and hit enter to send it to the model.
             Enter "q" to quit the conversation.
             """

             credential = DefaultAzureCredential()
             token_provider = get_bearer_token_provider(credential, "https://cognitiveservices.azure.com/.default")
             token = token_provider()

             # The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             # environment variable.
             # You can find it in the Microsoft Foundry portal in the Overview page of your Azure OpenAI resource.
             # Example: https://{your-resource}.openai.azure.com
             endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]

             # The deployment name of the model you want to use is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             # environment variable.
             # You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             # Example: gpt-realtime
             deployment_name = os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]

             base_url = endpoint.replace("https://", "wss://").rstrip("/") + "/openai/v1"

             # The APIs are compatible with the OpenAI client library.
             # You can use the OpenAI client library to access the Azure OpenAI APIs.
             # Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             client = AsyncOpenAI(
                 websocket_base_url=base_url,
                 api_key=token
             )
             async with client.realtime.connect(
                 model=deployment_name,
             ) as connection:
                 # after the connection is created, configure the session.
                 await connection.session.update(session={
                     "type": "realtime",
                     "instructions": "You are a helpful assistant. You respond by voice and text.",
                     "output_modalities": ["audio"],
                     "audio": {
                         "input": {
                             "transcription": {
                                 "model": "whisper-1",
                             },
                             "format": {
                                 "type": "audio/pcm",
                                 "rate": 24000,
                             },
                             "turn_detection": {
                                 "type": "server_vad",
                                 "threshold": 0.5,
                                 "prefix_padding_ms": 300,
                                 "silence_duration_ms": 200,
                                 "create_response": True,
                             }
                         },
                         "output": {
                             "voice": "alloy",
                             "format": {
                                 "type": "audio/pcm",
                                 "rate": 24000,
                             }
                         }
                     }
                 })

                 # After the session is configured, data can be sent to the session.
                 while True:
                     user_input = input("Enter a message: ")
                     if user_input == "q":
                         print("Stopping the conversation.")
                         break

                     await connection.conversation.item.create(
                         item={
                             "type": "message",
                             "role": "user",
                             "content": [{"type": "input_text", "text": user_input}],
                         }
                     )
                     await connection.response.create()
                     async for event in connection:
                         if event.type == "response.output_text.delta":
                             print(event.delta, flush=True, end="")
                         elif event.type == "session.created":
                             print(f"Session ID: {event.session.id}")
                         elif event.type == "response.output_audio.delta":
                             audio_data = base64.b64decode(event.delta)
                             print(f"Received {len(audio_data)} bytes of audio data.")
                         elif event.type == "response.output_audio_transcript.delta":
                             print(f"Received text delta: {event.delta}")
                         elif event.type == "response.output_text.done":
                             print()
                         elif event.type == "error":
                             print("Received an error event.")
                             print(f"Error code: {event.error.code}")
                             print(f"Error Event ID: {event.error.event_id}")
                             print(f"Error message: {event.error.message}")
                         elif event.type == "response.done":
                             break

             print("Conversation ended.")
             credential.close()

         asyncio.run(main())
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the Python file.

         ```shell
         python text-in-audio-out.py
         ```

      4. When prompted for user input, type a message and hit enter to send it to the model. Enter "q" to quit the conversation.
    </Tab>

    <Tab title="API key">
      1. Create the `text-in-audio-out.py` file with the following code:

         ```python
         import os
         import base64
         import asyncio
         from openai import AsyncOpenAI

         async def main() -> None:
             """
             When prompted for user input, type a message and hit enter to send it to the model.
             Enter "q" to quit the conversation.
             """

             # The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             # environment variable.
             # You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             # Example: https://{your-resource}.openai.azure.com
             endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]
             base_url = endpoint.replace("https://", "wss://").rstrip("/") + "/openai/v1"

             # The deployment name of the model you want to use is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             # environment variable.
             # You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             # Example: gpt-realtime
             deployment_name = os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]

             # API Key of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_API_KEY
             # environment variable.
             # You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             token=os.environ["AZURE_OPENAI_API_KEY"]

             # The APIs are compatible with the OpenAI client library.
             # You can use the OpenAI client library to access the Azure OpenAI APIs.
             # Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             client = AsyncOpenAI(
                 websocket_base_url=base_url,
                 api_key=token
             )
             async with client.realtime.connect(
                 model=deployment_name,
             ) as connection:
                 # after the connection is created, configure the session.
                 await connection.session.update(session={
                     "type": "realtime",
                     "instructions": "You are a helpful assistant. You respond by voice and text.",
                     "output_modalities": ["audio"],
                     "audio": {
                         "input": {
                             "transcription": {
                                 "model": "whisper-1",
                             },
                             "format": {
                                 "type": "audio/pcm",
                                 "rate": 24000,
                             },
                             "turn_detection": {
                                 "type": "server_vad",
                                 "threshold": 0.5,
                                 "prefix_padding_ms": 300,
                                 "silence_duration_ms": 200,
                                 "create_response": True,
                             }
                         },
                         "output": {
                             "voice": "alloy",
                             "format": {
                                 "type": "audio/pcm",
                                 "rate": 24000,
                             }
                         }
                     }
                 })

                 # After the session is configured, data can be sent to the session.
                 while True:
                     user_input = input("Enter a message: ")
                     if user_input == "q":
                         print("Stopping the conversation.")
                         break

                     await connection.conversation.item.create(
                         item={
                             "type": "message",
                             "role": "user",
                             "content": [{"type": "input_text", "text": user_input}],
                         }
                     )
                     await connection.response.create()
                     async for event in connection:
                         if event.type == "response.output_text.delta":
                             print(event.delta, flush=True, end="")
                         elif event.type == "session.created":
                             print(f"Session ID: {event.session.id}")
                         elif event.type == "response.output_audio.delta":
                             audio_data = base64.b64decode(event.delta)
                             print(f"Received {len(audio_data)} bytes of audio data.")
                         elif event.type == "response.output_audio_transcript.delta":
                             print(f"Received text delta: {event.delta}")
                         elif event.type == "response.output_text.done":
                             print()
                         elif event.type == "error":
                             print("Received an error event.")
                             print(f"Error code: {event.error.code}")
                             print(f"Error Event ID: {event.error.event_id}")
                             print(f"Error message: {event.error.message}")
                         elif event.type == "response.done":
                             break

             print("Conversation ended.")

         asyncio.run(main())
         ```

      2. Run the Python file.

         ```shell
         python text-in-audio-out.py
         ```

      3. When prompted for user input, type a message and hit enter to send it to the model. Enter "q" to quit the conversation.
    </Tab>
  </Tabs>

  Wait a few moments to get the response.

  ## Output

  The script gets a response from the model and prints the transcript and audio data received.

  The output looks similar to the following:

  ```console
  Enter a message: How are you today?
  Session ID: sess_CgAuonaqdlSNNDTdqBagI
  Received text delta: I'm
  Received text delta:  doing
  Received text delta:  well
  Received text delta: ,
  Received 4800 bytes of audio data.
  Received 7200 bytes of audio data.
  Received 12000 bytes of audio data.
  Received text delta:  thank
  Received text delta:  you
  Received text delta:  for
  Received text delta:  asking
  Received text delta: !
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received text delta:  How
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received text delta:  about
  Received text delta:  you
  Received text delta: —
  Received text delta: how
  Received text delta:  are
  Received text delta:  you
  Received text delta:  feeling
  Received text delta:  today
  Received text delta: ?
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 24000 bytes of audio data.
  Enter a message: q
  Stopping the conversation.
  Conversation ended.
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [Node.js LTS or ESM support.](https://nodejs.org/)
  * [TypeScript](https://www.typescriptlang.org/download/) installed globally.
  * An Azure OpenAI resource created in one of the supported regions. For more information about region availability, see the [models and versions documentation](../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).
  * Then, you need to deploy a `gpt-realtime` model with your Azure OpenAI resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services OpenAI User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Deploy a model for real-time audio

  To deploy the `gpt-realtime` model in the Microsoft Foundry portal:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and create or select your project.

  2. Select your model deployments:

     1. For Azure OpenAI resource, select **Deployments** from **Shared resources** section in the left pane.
     2. For Foundry resource, select **Models + endpoints** from under **My assets** in the left pane.

  3. Select **+ Deploy model** > **Deploy base model** to open the deployment window.

  4. Search for and select the `gpt-realtime` model and then select **Confirm**.

  5. Review the deployment details and select **Deploy**.

  6. Follow the wizard to finish deploying the model.

  Now that you have a deployment of the `gpt-realtime` model, you can interact with it in the Foundry portal **Audio** playground or Realtime API.

  ## Set up

  1. Create a new folder `realtime-audio-quickstart-ts` and go to the quickstart folder with the following command:

     ```bash
     mkdir realtime-audio-quickstart-ts && cd realtime-audio-quickstart-ts
     ```

  2. Create the `package.json` with the following command:

     ```bash
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```bash
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```bash
     npm install openai
     ```

  5. Install the dependent packages used by the OpenAI client library for JavaScript with:

     ```bash
     npm install ws
     ```

  6. For the **recommended** keyless authentication with Microsoft Entra ID, install the `@azure/identity` package with:

     ```bash
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Send text, receive audio response

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import OpenAI from 'openai';
         import { OpenAIRealtimeWS } from 'openai/realtime/ws';
         import { OpenAIRealtimeError } from 'openai/realtime/internal-base';
         import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
         import { RealtimeSessionCreateRequest } from 'openai/resources/realtime/realtime';

         let isCreated = false;
         let isConfigured = false;
         let responseDone = false;

         // Set this to false, if you want to continue receiving events after an error is received.
         const throwOnError = true;

         async function main(): Promise<void> {
             // The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             // environment variable or replace the default value below.
             // You can find it in the Microsoft Foundry portal in the Overview page of your Azure OpenAI resource.
             // Example: https://{your-resource}.openai.azure.com
             const endpoint = process.env.AZURE_OPENAI_ENDPOINT || 'AZURE_OPENAI_ENDPOINT';
             const baseUrl = endpoint.replace(/\/$/, "") + '/openai/v1';

             // The deployment name of your Azure OpenAI model is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             // Example: gpt-realtime
             const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || 'gpt-realtime';

             // Keyless authentication
             const credential = new DefaultAzureCredential();
             const scope = "https://cognitiveservices.azure.com/.default";
             const azureADTokenProvider = getBearerTokenProvider(credential, scope);
             const token = await azureADTokenProvider();

             // The APIs are compatible with the OpenAI client library.
             // You can use the OpenAI client library to access the Azure OpenAI APIs.
             // Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             const openAIClient = new OpenAI({
                 baseURL: baseUrl,
                 apiKey: token,
             });
             const realtimeClient = await OpenAIRealtimeWS.create(openAIClient, { model: deploymentName });

             realtimeClient.on('error', (receivedError) => receiveError(receivedError));
             realtimeClient.on('session.created', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('session.updated', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio_transcript.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.done', (receivedEvent) => receiveEvent(receivedEvent));

             console.log('Waiting for events...');
             while (!isCreated) {
                 console.log('Waiting for session.created event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is created, configure it to enable audio input and output.
             const sessionConfig: RealtimeSessionCreateRequest = {
                 'type': 'realtime',
                 'instructions': 'You are a helpful assistant. You respond by voice and text.',
                 'output_modalities': ['audio'],
                 'audio': {
                     'input': {
                         'transcription': {
                             'model': 'whisper-1'
                         },
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         },
                         'turn_detection': {
                             'type': 'server_vad',
                             'threshold': 0.5,
                             'prefix_padding_ms': 300,
                             'silence_duration_ms': 200,
                             'create_response': true
                         }
                     },
                     'output': {
                         'voice': 'alloy',
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         }
                     }
                 }
             };

             realtimeClient.send({ 'type': 'session.update', 'session': sessionConfig });

             while (!isConfigured) {
                 console.log('Waiting for session.updated event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is configured, data can be sent to the session.
             realtimeClient.send({
                 'type': 'conversation.item.create',
                 'item': {
                     'type': 'message',
                     'role': 'user',
                     'content': [{ type: 'input_text', text: 'Please assist the user.' }]
                 }
             });

             realtimeClient.send({ type: 'response.create' });

             // While waiting for the session to finish, the events can be handled in the event handlers.
             // In this example, we just wait for the first response.done event.
             while (!responseDone) {
                 console.log('Waiting for response.done event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             console.log('The sample completed successfully.');
             realtimeClient.close();
         }

         function receiveError(errorEvent: OpenAIRealtimeError): void {
             if (errorEvent instanceof OpenAIRealtimeError) {
                 console.error('Received an error event.');
                 console.error(`Message: ${errorEvent.message}`);
                 console.error(`Stack: ${errorEvent.stack}`); errorEvent
             }

             if (throwOnError) {
                 throw errorEvent;
             }
         }

         function receiveEvent(event: any): void {
             console.log(`Received an event: ${event.type}`);

             switch (event.type) {
                 case 'session.created':
                     console.log(`Session ID: ${event.session.id}`);
                     isCreated = true;
                     break;
                 case 'session.updated':
                     console.log(`Session ID: ${event.session.id}`);
                     isConfigured = true;
                     break;
                 case 'response.output_audio_transcript.delta':
                     console.log(`Transcript delta: ${event.delta}`);
                     break;
                 case 'response.output_audio.delta':
                     let audioBuffer = Buffer.from(event.delta, 'base64');
                     console.log(`Audio delta length: ${audioBuffer.length} bytes`);
                     break;
                 case 'response.done':
                     console.log(`Response ID: ${event.response.id}`);
                     console.log(`The final response is: ${event.response.output[0].content[0].transcript}`);
                     responseDone = true;
                     break;
                 default:
                     console.warn(`Unhandled event type: ${event.type}`);
             }
         }

         main().catch((err) => {
             console.error("The sample encountered an error:", err);
         });

         export { main };
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Install type definitions for Node

         ```bash
         npm i --save-dev @types/node
         ```

      4. Transpile from TypeScript to JavaScript.

         ```bash
         tsc
         ```

      5. Sign in to Azure with the following command:

         ```bash
         az login
         ```

      6. Run the code with the following command:

         ```bash
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import OpenAI from 'openai';
         import { OpenAIRealtimeWS } from 'openai/realtime/ws';
         import { OpenAIRealtimeError } from 'openai/realtime/internal-base';
         import { RealtimeSessionCreateRequest } from 'openai/resources/realtime/realtime';

         let isCreated = false;
         let isConfigured = false;
         let responseDone = false;

         // Set this to false, if you want to continue receiving events after an error is received.
         const throwOnError = true;

         async function main(): Promise<void> {
             // The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             // Example: https://{your-resource}.openai.azure.com
             const endpoint = process.env.AZURE_OPENAI_ENDPOINT || 'AZURE_OPENAI_ENDPOINT';
             const baseUrl = endpoint.replace(/\/$/, "") + '/openai/v1';

             // The deployment name of your Azure OpenAI model is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             // Example: gpt-realtime
             const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || 'gpt-realtime';

             // API Key of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_API_KEY
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             const token = process.env.AZURE_OPENAI_API_KEY || '<Your API Key>';

             // The APIs are compatible with the OpenAI client library.
             // You can use the OpenAI client library to access the Azure OpenAI APIs.
             // Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             const openAIClient = new OpenAI({
                 baseURL: baseUrl,
                 apiKey: token,
             });

             // Due to the current SDK limitation we need to explicitly
             // pass API key as Header
             const realtimeClient = await OpenAIRealtimeWS.create(
                 openAIClient, {
                 model: deploymentName,
                 options: {
                     headers: {
                         "api-key": token
                     }
                 }
             });

             realtimeClient.on('error', (receivedError) => receiveError(receivedError));
             realtimeClient.on('session.created', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('session.updated', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio_transcript.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.done', (receivedEvent) => receiveEvent(receivedEvent));

             console.log('Waiting for events...');
             while (!isCreated) {
                 console.log('Waiting for session.created event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is created, configure it to enable audio input and output.
             const sessionConfig: RealtimeSessionCreateRequest = {
                 'type': 'realtime',
                 'instructions': 'You are a helpful assistant. You respond by voice and text.',
                 'output_modalities': ['audio'],
                 'audio': {
                     'input': {
                         'transcription': {
                             'model': 'whisper-1'
                         },
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         },
                         'turn_detection': {
                             'type': 'server_vad',
                             'threshold': 0.5,
                             'prefix_padding_ms': 300,
                             'silence_duration_ms': 200,
                             'create_response': true
                         }
                     },
                     'output': {
                         'voice': 'alloy',
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         }
                     }
                 }
             };

             realtimeClient.send({
                 'type': 'session.update',
                 'session': sessionConfig
             });
             while (!isConfigured) {
                 console.log('Waiting for session.updated event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is configured, data can be sent to the session.
             realtimeClient.send({
                 'type': 'conversation.item.create',
                 'item': {
                     'type': 'message',
                     'role': 'user',
                     'content': [{
                         type: 'input_text',
                         text: 'Please assist the user.'
                     }
                     ]
                 }
             });

             realtimeClient.send({
                 type: 'response.create'
             });

             // While waiting for the session to finish, the events can be handled in the event handlers.
             // In this example, we just wait for the first response.done event.
             while (!responseDone) {
                 console.log('Waiting for response.done event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             console.log('The sample completed successfully.');
             realtimeClient.close();
         }

         function receiveError(errorEvent: OpenAIRealtimeError): void {
             if (errorEvent instanceof OpenAIRealtimeError) {
                 console.error('Received an error event.');
                 console.error(`Message: ${errorEvent.message}`);
                 console.error(`Stack: ${errorEvent.stack}`);
                 errorEvent
             }

             if (throwOnError) {
                 throw errorEvent;
             }
         }

         function receiveEvent(event: any): void {
             console.log(`Received an event: ${event.type}`);

             switch (event.type) {
                 case 'session.created':
                     console.log(`Session ID: ${event.session.id}`);
                     isCreated = true;
                     break;
                 case 'session.updated':
                     console.log(`Session ID: ${event.session.id}`);
                     isConfigured = true;
                     break;
                 case 'response.output_audio_transcript.delta':
                     console.log(`Transcript delta: ${event.delta}`);
                     break;
                 case 'response.output_audio.delta':
                     let audioBuffer = Buffer.from(event.delta, 'base64');
                     console.log(`Audio delta length: ${audioBuffer.length} bytes`);
                     break;
                 case 'response.done':
                     console.log(`Response ID: ${event.response.id}`);
                     console.log(`The final response is: ${event.response.output[0].content[0].transcript}`);
                     responseDone = true;
                     break;
                 default:
                     console.warn(`Unhandled event type: ${event.type}`);
             }
         }

         main().catch((err) => {
             console.error("The sample encountered an error:", err);
         });

         export {
             main
         };
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Install type definitions for Node

         ```bash
         npm i --save-dev @types/node
         ```

      4. Transpile from TypeScript to JavaScript.

         ```bash
         tsc
         ```

      5. Run the code with the following command:

         ```bash
         node index.js
         ```
    </Tab>
  </Tabs>

  Wait a few moments to get the response.

  ## Output

  The script gets a response from the model and prints the transcript and audio data received.

  The output will look similar to the following:

  ```console
  Waiting for events...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Received an event: session.created
  Session ID: sess_CWQkREiv3jlU3gk48bm0a
  Waiting for session.updated event...
  Waiting for session.updated event...
  Received an event: session.updated
  Session ID: sess_CWQkREiv3jlU3gk48bm0a
  Waiting for response.done event...
  Waiting for response.done event...
  Waiting for response.done event...
  Waiting for response.done event...
  Waiting for response.done event...
  Received an event: response.output_audio_transcript.delta
  Transcript delta: Sure
  Received an event: response.output_audio_transcript.delta
  Transcript delta: ,
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  I'm
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  here
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 4800 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 7200 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  to
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  help
  Received an event: response.output_audio_transcript.delta
  Transcript delta: .
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  What
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  would
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  you
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  like
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  to
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  do
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  or
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  know
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  about
  Received an event: response.output_audio_transcript.delta
  Transcript delta: ?
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 24000 bytes
  Received an event: response.done
  Response ID: resp_CWQkRBrCcCjtHgIEapA92
  The final response is: Sure, I'm here to help. What would you like to do or know about?
  The sample completed successfully.
  ```
</ZonePivot>

<ZonePivot pivot="ai-foundry-portal">
  ## Deploy a model for real-time audio

  To deploy the `gpt-realtime` model in the Microsoft Foundry portal:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and create or select your project.

  2. Select your model deployments:

     1. For Azure OpenAI resource, select **Deployments** from **Shared resources** section in the left pane.
     2. For Foundry resource, select **Models + endpoints** from under **My assets** in the left pane.

  3. Select **+ Deploy model** > **Deploy base model** to open the deployment window.

  4. Search for and select the `gpt-realtime` model and then select **Confirm**.

  5. Review the deployment details and select **Deploy**.

  6. Follow the wizard to finish deploying the model.

  Now that you have a deployment of the `gpt-realtime` model, you can interact with it in the Foundry portal **Audio** playground or Realtime API.

  ## Use the GPT real-time audio

  To chat with your deployed `gpt-realtime` model in the [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs) **Real-time audio** playground, follow these steps:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and select your project that has your deployed `gpt-realtime` model.

  2. Select **Playgrounds** from the left pane.

  3. Select **Audio playground** > **Try the Audio playground**.

     <Callout type="note">
       The **Chat playground** doesn't support the `gpt-realtime` model. Use the **Audio playground** as described in this section.
     </Callout>

  4. Select your deployed `gpt-realtime` model from the **Deployment** dropdown.

  5. Optionally, you can edit contents in the **Give the model instructions and context** text box. Give the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality, tell it what it should and shouldn't answer, and tell it how to format responses.

  6. Optionally, change settings such as threshold, prefix padding, and silence duration.

  7. Select **Start listening** to start the session. You can speak into the microphone to start a chat.

  8. You can interrupt the chat at any time by speaking. You can end the chat by selecting the **Stop listening** button.
</ZonePivot>

## Troubleshooting

### Authentication errors

If you're using keyless authentication (Microsoft Entra ID) and receive authentication errors:

* Verify the `AZURE_OPENAI_API_KEY` environment variable is **not set**. Keyless authentication fails if this variable exists.
* Confirm you've run `az login` to authenticate with Azure CLI.
* Check that your account has the `Cognitive Services OpenAI User` role assigned to the Azure OpenAI resource.

### WebSocket connection failures

If the WebSocket connection fails to establish:

* Verify your endpoint URL format matches the GA format: `{endpoint}/openai/v1` (without the `api-version` parameter).
* Check that your Azure OpenAI resource has a deployed `gpt-realtime` model.
* Ensure your network allows WebSocket connections on port 443.

### Rate limit exceeded

If you receive rate limit errors:

* The Realtime API has specific quotas separate from chat completions.
* Check your current usage in the Azure portal under your Azure OpenAI resource.
* Implement exponential backoff for retry logic in your application.

For more information about quotas, see [Azure OpenAI quotas and limits](quotas-limits).

## Related content

* Learn more about [How to use the Realtime API](how-to/realtime-audio)
* See the [Realtime API reference](realtime-audio-reference)
* Learn more about Azure OpenAI [quotas and limits](quotas-limits)
* Learn more about [Language and voice support for the Speech service](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Azure OpenAI GPT Realtime API for speech and audio is part of the GPT-4o model family that supports low-latency, "speech in, speech out" conversational interactions.

You can use the Realtime API via WebRTC or WebSocket to send audio input to the model and receive audio responses in real time.

Follow the instructions in this article to get started with the Realtime API via WebSockets. Use the Realtime API via WebSockets in server-to-server scenarios where low latency isn't a requirement.

<Callout type="tip">
  In most cases, use the [Realtime API via WebRTC](how-to/realtime-audio-webrtc) for real-time audio streaming in client-side applications such as a web application or mobile app. WebRTC is designed for low-latency, real-time audio streaming and is the best choice for most scenarios.
</Callout>

## Supported models

The GPT real-time models are available for global deployments.

* `gpt-4o-realtime-preview` (version `2024-12-17`)
* `gpt-4o-mini-realtime-preview` (version `2024-12-17`)
* `gpt-realtime` (version `2025-08-28`)
* `gpt-realtime-mini` (version `2025-10-06`)
* `gpt-realtime-mini-2025-12-15` (version `2025-12-15`)

For more information, see the [models and versions documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?tabs=global-standard-aoai%2Cstandard-chat-completions%2Cglobal-standard\&pivots=azure-openai#audio-models).

## API support

Support for the Realtime API was first added in API version `2024-10-01-preview` (retired). Use version `2025-08-28` to access the latest Realtime API features. We recommend you select the generally available API version (without '-preview' suffix) when possible.

<Callout type="caution">
  You need to use **different** endpoint formats for Preview and Generally Available (GA) models. All samples in this article use GA models and GA endpoint format, and don't use `api-version` parameter, which is required for Preview endpoint format only. See detailed information on the endpoint format [in this article](how-to/realtime-audio-websockets#connection-and-authentication).
</Callout>

<Callout type="note">
  The Realtime API has specific rate limits for audio tokens and concurrent sessions. Before deploying to production, review [Azure OpenAI quotas and limits](quotas-limits) for your deployment type.
</Callout>

<ZonePivot pivot="programming-language-javascript">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [Node.js LTS or ESM support.](https://nodejs.org/)
  * An Azure OpenAI resource created in one of the supported regions. For more information about region availability, see the [models and versions documentation](../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).
  * Then, you need to deploy a `gpt-realtime` model with your Azure OpenAI resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services OpenAI User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Deploy a model for real-time audio

  To deploy the `gpt-realtime` model in the Microsoft Foundry portal:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and create or select your project.

  2. Select your model deployments:

     1. For Azure OpenAI resource, select **Deployments** from **Shared resources** section in the left pane.
     2. For Foundry resource, select **Models + endpoints** from under **My assets** in the left pane.

  3. Select **+ Deploy model** > **Deploy base model** to open the deployment window.

  4. Search for and select the `gpt-realtime` model and then select **Confirm**.

  5. Review the deployment details and select **Deploy**.

  6. Follow the wizard to finish deploying the model.

  Now that you have a deployment of the `gpt-realtime` model, you can interact with it in the Foundry portal **Audio** playground or Realtime API.

  ## Set up

  1. Create a new folder `realtime-audio-quickstart-js` and go to the quickstart folder with the following command:

     ```bash
     mkdir realtime-audio-quickstart-js && cd realtime-audio-quickstart-js
     ```

  2. Create the `package.json` with the following command:

     ```bash
     npm init -y
     ```

  3. Update the `type` to `module` in `package.json` with the following command:

     ```bash
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```bash
     npm install openai
     ```

  5. Install the dependent packages used by the OpenAI client library for JavaScript with:

     ```bash
     npm install ws
     ```

  6. For the **recommended** keyless authentication with Microsoft Entra ID, install the `@azure/identity` package with:

     ```bash
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Send text, receive audio response

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         import OpenAI from 'openai';
         import { OpenAIRealtimeWS } from 'openai/realtime/ws';
         import { DefaultAzureCredential, getBearerTokenProvider } from '@azure/identity';
         import { OpenAIRealtimeError } from 'openai/realtime/internal-base';

         let isCreated = false;
         let isConfigured = false;
         let responseDone = false;

         // Set this to false, if you want to continue receiving events after an error is received.
         const throwOnError = true;

         async function main() {
             // The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             // environment variable or replace the default value below.
             // You can find it in the Microsoft Foundry portal in the Overview page of your Azure OpenAI resource.
             // Example: https://{your-resource}.openai.azure.com
             const endpoint = process.env.AZURE_OPENAI_ENDPOINT || 'AZURE_OPENAI_ENDPOINT';
             const baseUrl = endpoint.replace(/\/$/, "") + '/openai/v1';

             // The deployment name of your Azure OpenAI model is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             // Example: gpt-realtime
             const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || 'gpt-realtime';

             // Keyless authentication
             const credential = new DefaultAzureCredential();
             const scope = 'https://cognitiveservices.azure.com/.default';
             const azureADTokenProvider = getBearerTokenProvider(credential, scope);
             const token = await azureADTokenProvider();

             // The APIs are compatible with the OpenAI client library.
             // You can use the OpenAI client library to access the Azure OpenAI APIs.
             // Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             const openAIClient = new OpenAI({
                 baseURL: baseUrl,
                 apiKey: token,
             });
             const realtimeClient = await OpenAIRealtimeWS.create(openAIClient, {
                 model: deploymentName
             });

             realtimeClient.on('error', (receivedError) => receiveError(receivedError));
             realtimeClient.on('session.created', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('session.updated', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio_transcript.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.done', (receivedEvent) => receiveEvent(receivedEvent));

             console.log('Waiting for events...');
             while (!isCreated) {
                 console.log('Waiting for session.created event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is created, configure it to enable audio input and output.
             const sessionConfig = {
                 'type': 'realtime',
                 'instructions': 'You are a helpful assistant. You respond by voice and text.',
                 'output_modalities': ['audio'],
                 'audio': {
                     'input': {
                         'transcription': {
                             'model': 'whisper-1'
                         },
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         },
                         'turn_detection': {
                             'type': 'server_vad',
                             'threshold': 0.5,
                             'prefix_padding_ms': 300,
                             'silence_duration_ms': 200,
                             'create_response': true
                         }
                     },
                     'output': {
                         'voice': 'alloy',
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         }
                     }
                 }
             };

             realtimeClient.send({
                 'type': 'session.update',
                 'session': sessionConfig
             });
             while (!isConfigured) {
                 console.log('Waiting for session.updated event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is configured, data can be sent to the session.
             realtimeClient.send({
                 'type': 'conversation.item.create',
                 'item': {
                     'type': 'message',
                     'role': 'user',
                     'content': [{
                         type: 'input_text',
                         text: 'Please assist the user.'
                     }
                     ]
                 }
             });

             realtimeClient.send({
                 type: 'response.create'
             });



             // While waiting for the session to finish, the events can be handled in the event handlers.
             // In this example, we just wait for the first response.done event.
             while (!responseDone) {
                 console.log('Waiting for response.done event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             console.log('The sample completed successfully.');
             realtimeClient.close();
         }

         function receiveError(err) {
             if (err instanceof OpenAIRealtimeError) {
                 console.error('Received an error event.');
                 console.error(`Message: ${err.cause.message}`);
                 console.error(`Stack: ${err.cause.stack}`);
             }

             if (throwOnError) {
                 throw err;
             }
         }

         function receiveEvent(event) {
             console.log(`Received an event: ${event.type}`);

             switch (event.type) {
                 case 'session.created':
                     console.log(`Session ID: ${event.session.id}`);
                     isCreated = true;
                     break;
                 case 'session.updated':
                     console.log(`Session ID: ${event.session.id}`);
                     isConfigured = true;
                     break;
                 case 'response.output_audio_transcript.delta':
                     console.log(`Transcript delta: ${event.delta}`);
                     break;
                 case 'response.output_audio.delta':
                     let audioBuffer = Buffer.from(event.delta, 'base64');
                     console.log(`Audio delta length: ${audioBuffer.length} bytes`);
                     break;
                 case 'response.done':
                     console.log(`Response ID: ${event.response.id}`);
                     console.log(`The final response is: ${event.response.output[0].content[0].transcript}`);
                     responseDone = true;
                     break;
                 default:
                     console.warn(`Unhandled event type: ${event.type}`);
             }
         }

         main().catch((err) => {
             console.error('The sample encountered an error:', err);
         });
         export {
             main
         };
         ```

      2. Sign in to Azure with the following command:

         ```bash
         az login
         ```

      3. Run the JavaScript file.

         ```bash
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         import OpenAI from 'openai';
         import { OpenAIRealtimeWS } from 'openai/realtime/ws';
         import { OpenAIRealtimeError } from 'openai/realtime/internal-base';

         let isCreated = false;
         let isConfigured = false;
         let responseDone = false;

         // Set this to false, if you want to continue receiving events after an error is received.
         const throwOnError = true;

         async function main() {
             // The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             // Example: https://{your-resource}.openai.azure.com
             const endpoint = process.env.AZURE_OPENAI_ENDPOINT || 'AZURE_OPENAI_ENDPOINT';
             const baseUrl = endpoint.replace(/\/$/, "") + '/openai/v1';

             // The deployment name of your Azure OpenAI model is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             // Example: gpt-realtime
             const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || 'gpt-realtime';

             // API Key of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_API_KEY
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             const token = process.env.AZURE_OPENAI_API_KEY || '<Your API Key>';

             // The APIs are compatible with the OpenAI client library.
             // You can use the OpenAI client library to access the Azure OpenAI APIs.
             // Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             const openAIClient = new OpenAI({
                 baseURL: baseUrl,
                 apiKey: token,
             });

             // Due to the current SDK limitation we need to explicitly
             // pass API key as Header
             const realtimeClient = await OpenAIRealtimeWS.create(
                 openAIClient, {
                 model: deploymentName,
                 options: {
                     headers: {
                         "api-key": token
                     }
                 }
             });

             realtimeClient.on('error', (receivedError) => receiveError(receivedError));
             realtimeClient.on('session.created', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('session.updated', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio_transcript.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.done', (receivedEvent) => receiveEvent(receivedEvent));

             console.log('Waiting for events...');
             while (!isCreated) {
                 console.log('Waiting for session.created event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is created, configure it to enable audio input and output.
             const sessionConfig = {
                 'type': 'realtime',
                 'instructions': 'You are a helpful assistant. You respond by voice and text.',
                 'output_modalities': ['audio'],
                 'audio': {
                     'input': {
                         'transcription': {
                             'model': 'whisper-1'
                         },
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         },
                         'turn_detection': {
                             'type': 'server_vad',
                             'threshold': 0.5,
                             'prefix_padding_ms': 300,
                             'silence_duration_ms': 200,
                             'create_response': true
                         }
                     },
                     'output': {
                         'voice': 'alloy',
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         }
                     }
                 }
             };

             realtimeClient.send({
                 'type': 'session.update',
                 'session': sessionConfig
             });
             while (!isConfigured) {
                 console.log('Waiting for session.updated event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is configured, data can be sent to the session.
             realtimeClient.send({
                 'type': 'conversation.item.create',
                 'item': {
                     'type': 'message',
                     'role': 'user',
                     'content': [{
                         type: 'input_text',
                         text: 'Please assist the user.'
                     }
                     ]
                 }
             });

             realtimeClient.send({
                 type: 'response.create'
             });

             // While waiting for the session to finish, the events can be handled in the event handlers.
             // In this example, we just wait for the first response.done event.
             while (!responseDone) {
                 console.log('Waiting for response.done event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             console.log('The sample completed successfully.');
             realtimeClient.close();
         }

         function receiveError(err) {
             if (err instanceof OpenAIRealtimeError) {
                 console.error('Received an error event.');
                 console.error(`Message: ${err.cause.message}`);
                 console.error(`Stack: ${err.cause.stack}`);
             }

             if (throwOnError) {
                 throw err;
             }
         }

         function receiveEvent(event) {
             console.log(`Received an event: ${event.type}`);

             switch (event.type) {
                 case 'session.created':
                     console.log(`Session ID: ${event.session.id}`);
                     isCreated = true;
                     break;
                 case 'session.updated':
                     console.log(`Session ID: ${event.session.id}`);
                     isConfigured = true;
                     break;
                 case 'response.output_audio_transcript.delta':
                     console.log(`Transcript delta: ${event.delta}`);
                     break;
                 case 'response.output_audio.delta':
                     let audioBuffer = Buffer.from(event.delta, 'base64');
                     console.log(`Audio delta length: ${audioBuffer.length} bytes`);
                     break;
                 case 'response.done':
                     console.log(`Response ID: ${event.response.id}`);
                     console.log(`The final response is: ${event.response.output[0].content[0].transcript}`);
                     responseDone = true;
                     break;
                 default:
                     console.warn(`Unhandled event type: ${event.type}`);
             }
         }

         main().catch((err) => {
             console.error('The sample encountered an error:', err);
         });
         export {
             main
         };
         ```

      2. Run the JavaScript file.

         ```bash
         node index.js
         ```
    </Tab>
  </Tabs>

  Wait a few moments to get the response.

  ## Output

  The script gets a response from the model and prints the transcript and audio data received.

  The output will look similar to the following:

  ```console
  Waiting for events...
  Waiting for session.created event...
  Received an event: session.created
  Session ID: sess_CQx8YO3vKxD9FaPxrbQ9R
  Waiting for session.updated event...
  Received an event: session.updated
  Session ID: sess_CQx8YO3vKxD9FaPxrbQ9R
  Waiting for response.done event...
  Waiting for response.done event...
  Waiting for response.done event...
  Received an event: response.output_audio_transcript.delta
  Transcript delta: Sure
  Received an event: response.output_audio_transcript.delta
  Transcript delta: ,
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  I
  Waiting for response.done event...
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 4800 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 7200 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta: 'm
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  here
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  to
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  help
  Received an event: response.output_audio_transcript.delta
  Transcript delta: .
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  What
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  do
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  you
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  need
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  assistance
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  with
  Received an event: response.output_audio_transcript.delta
  Transcript delta: ?
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 28800 bytes
  Received an event: response.done
  Response ID: resp_CQx8YwQCszDqSUXRutxP9
  The final response is: Sure, I'm here to help. What do you need assistance with?
  The sample completed successfully.
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/). We recommend using Python 3.10 or later, but having at least Python 3.8 is required. If you don't have a suitable version of Python installed, you can follow the instructions in the [VS Code Python Tutorial](https://code.visualstudio.com/docs/python/python-tutorial#_install-a-python-interpreter) for the easiest way of installing Python on your operating system.
  * An Azure OpenAI resource created in one of the supported regions. For more information about region availability, see the [models and versions documentation](../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).
  * Then, you need to deploy a `gpt-realtime`, `gpt-realtime-mini`, or `gpt-realtime-mini-2025-12-15` model with your Azure OpenAI resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services OpenAI User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Deploy a model for real-time audio

  To deploy the `gpt-realtime` model in the Microsoft Foundry portal:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and create or select your project.

  2. Select your model deployments:

     1. For Azure OpenAI resource, select **Deployments** from **Shared resources** section in the left pane.
     2. For Foundry resource, select **Models + endpoints** from under **My assets** in the left pane.

  3. Select **+ Deploy model** > **Deploy base model** to open the deployment window.

  4. Search for and select the `gpt-realtime` model and then select **Confirm**.

  5. Review the deployment details and select **Deploy**.

  6. Follow the wizard to finish deploying the model.

  Now that you have a deployment of the `gpt-realtime` model, you can interact with it in the Foundry portal **Audio** playground or Realtime API.

  ## Set up

  1. Create a new folder `realtime-audio-quickstart-py` and go to the quickstart folder with the following command:

     ```bash
     mkdir realtime-audio-quickstart-py && cd realtime-audio-quickstart-py
     ```

  2. Create a virtual environment. If you already have Python 3.10 or higher installed, you can create a virtual environment using the following commands:

     <Tabs>
       <Tab title="Windows">
         ```shell
         py -3 -m venv .venv
         .venv\scripts\activate
         ```
       </Tab>

       <Tab title="Linux">
         ```bash
         python3 -m venv .venv
         source .venv/bin/activate
         ```
       </Tab>

       <Tab title="macOS">
         ```bash
         python3 -m venv .venv
         source .venv/bin/activate
         ```
       </Tab>
     </Tabs>

     Activating the Python environment means that when you run `python` or `pip` from the command line, you then use the Python interpreter contained in the `.venv` folder of your application. You can use the `deactivate` command to exit the python virtual environment, and can later reactivate it when needed.

     <Callout type="tip">
       We recommend that you create and activate a new Python environment to use to install the packages you need for this tutorial. Don't install packages into your global python installation. You should always use a virtual or conda environment when installing python packages, otherwise you can break your global installation of Python.
     </Callout>

  3. Install the OpenAI Python client library with:

     ```shell
     pip install openai[realtime]
     ```

     <Callout type="note">
       This library is maintained by OpenAI. Refer to the [release history](https://github.com/openai/openai-python/releases) to track the latest updates to the library.
     </Callout>

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the `azure-identity` package with:

     ```shell
     pip install azure-identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Send text, receive audio response

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `text-in-audio-out.py` file with the following code:

         ```python
         import os
         import base64
         import asyncio
         from openai import AsyncOpenAI
         from azure.identity import DefaultAzureCredential, get_bearer_token_provider

         async def main() -> None:
             """
             When prompted for user input, type a message and hit enter to send it to the model.
             Enter "q" to quit the conversation.
             """

             credential = DefaultAzureCredential()
             token_provider = get_bearer_token_provider(credential, "https://cognitiveservices.azure.com/.default")
             token = token_provider()

             # The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             # environment variable.
             # You can find it in the Microsoft Foundry portal in the Overview page of your Azure OpenAI resource.
             # Example: https://{your-resource}.openai.azure.com
             endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]

             # The deployment name of the model you want to use is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             # environment variable.
             # You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             # Example: gpt-realtime
             deployment_name = os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]

             base_url = endpoint.replace("https://", "wss://").rstrip("/") + "/openai/v1"

             # The APIs are compatible with the OpenAI client library.
             # You can use the OpenAI client library to access the Azure OpenAI APIs.
             # Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             client = AsyncOpenAI(
                 websocket_base_url=base_url,
                 api_key=token
             )
             async with client.realtime.connect(
                 model=deployment_name,
             ) as connection:
                 # after the connection is created, configure the session.
                 await connection.session.update(session={
                     "type": "realtime",
                     "instructions": "You are a helpful assistant. You respond by voice and text.",
                     "output_modalities": ["audio"],
                     "audio": {
                         "input": {
                             "transcription": {
                                 "model": "whisper-1",
                             },
                             "format": {
                                 "type": "audio/pcm",
                                 "rate": 24000,
                             },
                             "turn_detection": {
                                 "type": "server_vad",
                                 "threshold": 0.5,
                                 "prefix_padding_ms": 300,
                                 "silence_duration_ms": 200,
                                 "create_response": True,
                             }
                         },
                         "output": {
                             "voice": "alloy",
                             "format": {
                                 "type": "audio/pcm",
                                 "rate": 24000,
                             }
                         }
                     }
                 })

                 # After the session is configured, data can be sent to the session.
                 while True:
                     user_input = input("Enter a message: ")
                     if user_input == "q":
                         print("Stopping the conversation.")
                         break

                     await connection.conversation.item.create(
                         item={
                             "type": "message",
                             "role": "user",
                             "content": [{"type": "input_text", "text": user_input}],
                         }
                     )
                     await connection.response.create()
                     async for event in connection:
                         if event.type == "response.output_text.delta":
                             print(event.delta, flush=True, end="")
                         elif event.type == "session.created":
                             print(f"Session ID: {event.session.id}")
                         elif event.type == "response.output_audio.delta":
                             audio_data = base64.b64decode(event.delta)
                             print(f"Received {len(audio_data)} bytes of audio data.")
                         elif event.type == "response.output_audio_transcript.delta":
                             print(f"Received text delta: {event.delta}")
                         elif event.type == "response.output_text.done":
                             print()
                         elif event.type == "error":
                             print("Received an error event.")
                             print(f"Error code: {event.error.code}")
                             print(f"Error Event ID: {event.error.event_id}")
                             print(f"Error message: {event.error.message}")
                         elif event.type == "response.done":
                             break

             print("Conversation ended.")
             credential.close()

         asyncio.run(main())
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the Python file.

         ```shell
         python text-in-audio-out.py
         ```

      4. When prompted for user input, type a message and hit enter to send it to the model. Enter "q" to quit the conversation.
    </Tab>

    <Tab title="API key">
      1. Create the `text-in-audio-out.py` file with the following code:

         ```python
         import os
         import base64
         import asyncio
         from openai import AsyncOpenAI

         async def main() -> None:
             """
             When prompted for user input, type a message and hit enter to send it to the model.
             Enter "q" to quit the conversation.
             """

             # The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             # environment variable.
             # You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             # Example: https://{your-resource}.openai.azure.com
             endpoint = os.environ["AZURE_OPENAI_ENDPOINT"]
             base_url = endpoint.replace("https://", "wss://").rstrip("/") + "/openai/v1"

             # The deployment name of the model you want to use is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             # environment variable.
             # You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             # Example: gpt-realtime
             deployment_name = os.environ["AZURE_OPENAI_DEPLOYMENT_NAME"]

             # API Key of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_API_KEY
             # environment variable.
             # You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             token=os.environ["AZURE_OPENAI_API_KEY"]

             # The APIs are compatible with the OpenAI client library.
             # You can use the OpenAI client library to access the Azure OpenAI APIs.
             # Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             client = AsyncOpenAI(
                 websocket_base_url=base_url,
                 api_key=token
             )
             async with client.realtime.connect(
                 model=deployment_name,
             ) as connection:
                 # after the connection is created, configure the session.
                 await connection.session.update(session={
                     "type": "realtime",
                     "instructions": "You are a helpful assistant. You respond by voice and text.",
                     "output_modalities": ["audio"],
                     "audio": {
                         "input": {
                             "transcription": {
                                 "model": "whisper-1",
                             },
                             "format": {
                                 "type": "audio/pcm",
                                 "rate": 24000,
                             },
                             "turn_detection": {
                                 "type": "server_vad",
                                 "threshold": 0.5,
                                 "prefix_padding_ms": 300,
                                 "silence_duration_ms": 200,
                                 "create_response": True,
                             }
                         },
                         "output": {
                             "voice": "alloy",
                             "format": {
                                 "type": "audio/pcm",
                                 "rate": 24000,
                             }
                         }
                     }
                 })

                 # After the session is configured, data can be sent to the session.
                 while True:
                     user_input = input("Enter a message: ")
                     if user_input == "q":
                         print("Stopping the conversation.")
                         break

                     await connection.conversation.item.create(
                         item={
                             "type": "message",
                             "role": "user",
                             "content": [{"type": "input_text", "text": user_input}],
                         }
                     )
                     await connection.response.create()
                     async for event in connection:
                         if event.type == "response.output_text.delta":
                             print(event.delta, flush=True, end="")
                         elif event.type == "session.created":
                             print(f"Session ID: {event.session.id}")
                         elif event.type == "response.output_audio.delta":
                             audio_data = base64.b64decode(event.delta)
                             print(f"Received {len(audio_data)} bytes of audio data.")
                         elif event.type == "response.output_audio_transcript.delta":
                             print(f"Received text delta: {event.delta}")
                         elif event.type == "response.output_text.done":
                             print()
                         elif event.type == "error":
                             print("Received an error event.")
                             print(f"Error code: {event.error.code}")
                             print(f"Error Event ID: {event.error.event_id}")
                             print(f"Error message: {event.error.message}")
                         elif event.type == "response.done":
                             break

             print("Conversation ended.")

         asyncio.run(main())
         ```

      2. Run the Python file.

         ```shell
         python text-in-audio-out.py
         ```

      3. When prompted for user input, type a message and hit enter to send it to the model. Enter "q" to quit the conversation.
    </Tab>
  </Tabs>

  Wait a few moments to get the response.

  ## Output

  The script gets a response from the model and prints the transcript and audio data received.

  The output looks similar to the following:

  ```console
  Enter a message: How are you today?
  Session ID: sess_CgAuonaqdlSNNDTdqBagI
  Received text delta: I'm
  Received text delta:  doing
  Received text delta:  well
  Received text delta: ,
  Received 4800 bytes of audio data.
  Received 7200 bytes of audio data.
  Received 12000 bytes of audio data.
  Received text delta:  thank
  Received text delta:  you
  Received text delta:  for
  Received text delta:  asking
  Received text delta: !
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received text delta:  How
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received text delta:  about
  Received text delta:  you
  Received text delta: —
  Received text delta: how
  Received text delta:  are
  Received text delta:  you
  Received text delta:  feeling
  Received text delta:  today
  Received text delta: ?
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 12000 bytes of audio data.
  Received 24000 bytes of audio data.
  Enter a message: q
  Stopping the conversation.
  Conversation ended.
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [Node.js LTS or ESM support.](https://nodejs.org/)
  * [TypeScript](https://www.typescriptlang.org/download/) installed globally.
  * An Azure OpenAI resource created in one of the supported regions. For more information about region availability, see the [models and versions documentation](../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).
  * Then, you need to deploy a `gpt-realtime` model with your Azure OpenAI resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services OpenAI User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Deploy a model for real-time audio

  To deploy the `gpt-realtime` model in the Microsoft Foundry portal:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and create or select your project.

  2. Select your model deployments:

     1. For Azure OpenAI resource, select **Deployments** from **Shared resources** section in the left pane.
     2. For Foundry resource, select **Models + endpoints** from under **My assets** in the left pane.

  3. Select **+ Deploy model** > **Deploy base model** to open the deployment window.

  4. Search for and select the `gpt-realtime` model and then select **Confirm**.

  5. Review the deployment details and select **Deploy**.

  6. Follow the wizard to finish deploying the model.

  Now that you have a deployment of the `gpt-realtime` model, you can interact with it in the Foundry portal **Audio** playground or Realtime API.

  ## Set up

  1. Create a new folder `realtime-audio-quickstart-ts` and go to the quickstart folder with the following command:

     ```bash
     mkdir realtime-audio-quickstart-ts && cd realtime-audio-quickstart-ts
     ```

  2. Create the `package.json` with the following command:

     ```bash
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```bash
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```bash
     npm install openai
     ```

  5. Install the dependent packages used by the OpenAI client library for JavaScript with:

     ```bash
     npm install ws
     ```

  6. For the **recommended** keyless authentication with Microsoft Entra ID, install the `@azure/identity` package with:

     ```bash
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Send text, receive audio response

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import OpenAI from 'openai';
         import { OpenAIRealtimeWS } from 'openai/realtime/ws';
         import { OpenAIRealtimeError } from 'openai/realtime/internal-base';
         import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
         import { RealtimeSessionCreateRequest } from 'openai/resources/realtime/realtime';

         let isCreated = false;
         let isConfigured = false;
         let responseDone = false;

         // Set this to false, if you want to continue receiving events after an error is received.
         const throwOnError = true;

         async function main(): Promise<void> {
             // The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             // environment variable or replace the default value below.
             // You can find it in the Microsoft Foundry portal in the Overview page of your Azure OpenAI resource.
             // Example: https://{your-resource}.openai.azure.com
             const endpoint = process.env.AZURE_OPENAI_ENDPOINT || 'AZURE_OPENAI_ENDPOINT';
             const baseUrl = endpoint.replace(/\/$/, "") + '/openai/v1';

             // The deployment name of your Azure OpenAI model is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             // Example: gpt-realtime
             const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || 'gpt-realtime';

             // Keyless authentication
             const credential = new DefaultAzureCredential();
             const scope = "https://cognitiveservices.azure.com/.default";
             const azureADTokenProvider = getBearerTokenProvider(credential, scope);
             const token = await azureADTokenProvider();

             // The APIs are compatible with the OpenAI client library.
             // You can use the OpenAI client library to access the Azure OpenAI APIs.
             // Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             const openAIClient = new OpenAI({
                 baseURL: baseUrl,
                 apiKey: token,
             });
             const realtimeClient = await OpenAIRealtimeWS.create(openAIClient, { model: deploymentName });

             realtimeClient.on('error', (receivedError) => receiveError(receivedError));
             realtimeClient.on('session.created', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('session.updated', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio_transcript.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.done', (receivedEvent) => receiveEvent(receivedEvent));

             console.log('Waiting for events...');
             while (!isCreated) {
                 console.log('Waiting for session.created event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is created, configure it to enable audio input and output.
             const sessionConfig: RealtimeSessionCreateRequest = {
                 'type': 'realtime',
                 'instructions': 'You are a helpful assistant. You respond by voice and text.',
                 'output_modalities': ['audio'],
                 'audio': {
                     'input': {
                         'transcription': {
                             'model': 'whisper-1'
                         },
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         },
                         'turn_detection': {
                             'type': 'server_vad',
                             'threshold': 0.5,
                             'prefix_padding_ms': 300,
                             'silence_duration_ms': 200,
                             'create_response': true
                         }
                     },
                     'output': {
                         'voice': 'alloy',
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         }
                     }
                 }
             };

             realtimeClient.send({ 'type': 'session.update', 'session': sessionConfig });

             while (!isConfigured) {
                 console.log('Waiting for session.updated event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is configured, data can be sent to the session.
             realtimeClient.send({
                 'type': 'conversation.item.create',
                 'item': {
                     'type': 'message',
                     'role': 'user',
                     'content': [{ type: 'input_text', text: 'Please assist the user.' }]
                 }
             });

             realtimeClient.send({ type: 'response.create' });

             // While waiting for the session to finish, the events can be handled in the event handlers.
             // In this example, we just wait for the first response.done event.
             while (!responseDone) {
                 console.log('Waiting for response.done event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             console.log('The sample completed successfully.');
             realtimeClient.close();
         }

         function receiveError(errorEvent: OpenAIRealtimeError): void {
             if (errorEvent instanceof OpenAIRealtimeError) {
                 console.error('Received an error event.');
                 console.error(`Message: ${errorEvent.message}`);
                 console.error(`Stack: ${errorEvent.stack}`); errorEvent
             }

             if (throwOnError) {
                 throw errorEvent;
             }
         }

         function receiveEvent(event: any): void {
             console.log(`Received an event: ${event.type}`);

             switch (event.type) {
                 case 'session.created':
                     console.log(`Session ID: ${event.session.id}`);
                     isCreated = true;
                     break;
                 case 'session.updated':
                     console.log(`Session ID: ${event.session.id}`);
                     isConfigured = true;
                     break;
                 case 'response.output_audio_transcript.delta':
                     console.log(`Transcript delta: ${event.delta}`);
                     break;
                 case 'response.output_audio.delta':
                     let audioBuffer = Buffer.from(event.delta, 'base64');
                     console.log(`Audio delta length: ${audioBuffer.length} bytes`);
                     break;
                 case 'response.done':
                     console.log(`Response ID: ${event.response.id}`);
                     console.log(`The final response is: ${event.response.output[0].content[0].transcript}`);
                     responseDone = true;
                     break;
                 default:
                     console.warn(`Unhandled event type: ${event.type}`);
             }
         }

         main().catch((err) => {
             console.error("The sample encountered an error:", err);
         });

         export { main };
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Install type definitions for Node

         ```bash
         npm i --save-dev @types/node
         ```

      4. Transpile from TypeScript to JavaScript.

         ```bash
         tsc
         ```

      5. Sign in to Azure with the following command:

         ```bash
         az login
         ```

      6. Run the code with the following command:

         ```bash
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import OpenAI from 'openai';
         import { OpenAIRealtimeWS } from 'openai/realtime/ws';
         import { OpenAIRealtimeError } from 'openai/realtime/internal-base';
         import { RealtimeSessionCreateRequest } from 'openai/resources/realtime/realtime';

         let isCreated = false;
         let isConfigured = false;
         let responseDone = false;

         // Set this to false, if you want to continue receiving events after an error is received.
         const throwOnError = true;

         async function main(): Promise<void> {
             // The endpoint of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_ENDPOINT
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             // Example: https://{your-resource}.openai.azure.com
             const endpoint = process.env.AZURE_OPENAI_ENDPOINT || 'AZURE_OPENAI_ENDPOINT';
             const baseUrl = endpoint.replace(/\/$/, "") + '/openai/v1';

             // The deployment name of your Azure OpenAI model is required. You can set it in the AZURE_OPENAI_DEPLOYMENT_NAME
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the "Models + endpoints" page of your Azure OpenAI resource.
             // Example: gpt-realtime
             const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || 'gpt-realtime';

             // API Key of your Azure OpenAI resource is required. You can set it in the AZURE_OPENAI_API_KEY
             // environment variable or replace the default value below.
             // You can find it in the Foundry portal in the Overview page of your Azure OpenAI resource.
             const token = process.env.AZURE_OPENAI_API_KEY || '<Your API Key>';

             // The APIs are compatible with the OpenAI client library.
             // You can use the OpenAI client library to access the Azure OpenAI APIs.
             // Make sure to set the baseURL and apiKey to use the Azure OpenAI endpoint and token.
             const openAIClient = new OpenAI({
                 baseURL: baseUrl,
                 apiKey: token,
             });

             // Due to the current SDK limitation we need to explicitly
             // pass API key as Header
             const realtimeClient = await OpenAIRealtimeWS.create(
                 openAIClient, {
                 model: deploymentName,
                 options: {
                     headers: {
                         "api-key": token
                     }
                 }
             });

             realtimeClient.on('error', (receivedError) => receiveError(receivedError));
             realtimeClient.on('session.created', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('session.updated', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.output_audio_transcript.delta', (receivedEvent) => receiveEvent(receivedEvent));
             realtimeClient.on('response.done', (receivedEvent) => receiveEvent(receivedEvent));

             console.log('Waiting for events...');
             while (!isCreated) {
                 console.log('Waiting for session.created event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is created, configure it to enable audio input and output.
             const sessionConfig: RealtimeSessionCreateRequest = {
                 'type': 'realtime',
                 'instructions': 'You are a helpful assistant. You respond by voice and text.',
                 'output_modalities': ['audio'],
                 'audio': {
                     'input': {
                         'transcription': {
                             'model': 'whisper-1'
                         },
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         },
                         'turn_detection': {
                             'type': 'server_vad',
                             'threshold': 0.5,
                             'prefix_padding_ms': 300,
                             'silence_duration_ms': 200,
                             'create_response': true
                         }
                     },
                     'output': {
                         'voice': 'alloy',
                         'format': {
                             'type': 'audio/pcm',
                             'rate': 24000,
                         }
                     }
                 }
             };

             realtimeClient.send({
                 'type': 'session.update',
                 'session': sessionConfig
             });
             while (!isConfigured) {
                 console.log('Waiting for session.updated event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             // After the session is configured, data can be sent to the session.
             realtimeClient.send({
                 'type': 'conversation.item.create',
                 'item': {
                     'type': 'message',
                     'role': 'user',
                     'content': [{
                         type: 'input_text',
                         text: 'Please assist the user.'
                     }
                     ]
                 }
             });

             realtimeClient.send({
                 type: 'response.create'
             });

             // While waiting for the session to finish, the events can be handled in the event handlers.
             // In this example, we just wait for the first response.done event.
             while (!responseDone) {
                 console.log('Waiting for response.done event...');
                 await new Promise((resolve) => setTimeout(resolve, 100));
             }

             console.log('The sample completed successfully.');
             realtimeClient.close();
         }

         function receiveError(errorEvent: OpenAIRealtimeError): void {
             if (errorEvent instanceof OpenAIRealtimeError) {
                 console.error('Received an error event.');
                 console.error(`Message: ${errorEvent.message}`);
                 console.error(`Stack: ${errorEvent.stack}`);
                 errorEvent
             }

             if (throwOnError) {
                 throw errorEvent;
             }
         }

         function receiveEvent(event: any): void {
             console.log(`Received an event: ${event.type}`);

             switch (event.type) {
                 case 'session.created':
                     console.log(`Session ID: ${event.session.id}`);
                     isCreated = true;
                     break;
                 case 'session.updated':
                     console.log(`Session ID: ${event.session.id}`);
                     isConfigured = true;
                     break;
                 case 'response.output_audio_transcript.delta':
                     console.log(`Transcript delta: ${event.delta}`);
                     break;
                 case 'response.output_audio.delta':
                     let audioBuffer = Buffer.from(event.delta, 'base64');
                     console.log(`Audio delta length: ${audioBuffer.length} bytes`);
                     break;
                 case 'response.done':
                     console.log(`Response ID: ${event.response.id}`);
                     console.log(`The final response is: ${event.response.output[0].content[0].transcript}`);
                     responseDone = true;
                     break;
                 default:
                     console.warn(`Unhandled event type: ${event.type}`);
             }
         }

         main().catch((err) => {
             console.error("The sample encountered an error:", err);
         });

         export {
             main
         };
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Install type definitions for Node

         ```bash
         npm i --save-dev @types/node
         ```

      4. Transpile from TypeScript to JavaScript.

         ```bash
         tsc
         ```

      5. Run the code with the following command:

         ```bash
         node index.js
         ```
    </Tab>
  </Tabs>

  Wait a few moments to get the response.

  ## Output

  The script gets a response from the model and prints the transcript and audio data received.

  The output will look similar to the following:

  ```console
  Waiting for events...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Waiting for session.created event...
  Received an event: session.created
  Session ID: sess_CWQkREiv3jlU3gk48bm0a
  Waiting for session.updated event...
  Waiting for session.updated event...
  Received an event: session.updated
  Session ID: sess_CWQkREiv3jlU3gk48bm0a
  Waiting for response.done event...
  Waiting for response.done event...
  Waiting for response.done event...
  Waiting for response.done event...
  Waiting for response.done event...
  Received an event: response.output_audio_transcript.delta
  Transcript delta: Sure
  Received an event: response.output_audio_transcript.delta
  Transcript delta: ,
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  I'm
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  here
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 4800 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 7200 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  to
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  help
  Received an event: response.output_audio_transcript.delta
  Transcript delta: .
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  What
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  would
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  you
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  like
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  to
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  do
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  or
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  know
  Received an event: response.output_audio_transcript.delta
  Transcript delta:  about
  Received an event: response.output_audio_transcript.delta
  Transcript delta: ?
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Waiting for response.done event...
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 12000 bytes
  Received an event: response.output_audio.delta
  Audio delta length: 24000 bytes
  Received an event: response.done
  Response ID: resp_CWQkRBrCcCjtHgIEapA92
  The final response is: Sure, I'm here to help. What would you like to do or know about?
  The sample completed successfully.
  ```
</ZonePivot>

<ZonePivot pivot="ai-foundry-portal">
  ## Deploy a model for real-time audio

  To deploy the `gpt-realtime` model in the Microsoft Foundry portal:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and create or select your project.

  2. Select your model deployments:

     1. For Azure OpenAI resource, select **Deployments** from **Shared resources** section in the left pane.
     2. For Foundry resource, select **Models + endpoints** from under **My assets** in the left pane.

  3. Select **+ Deploy model** > **Deploy base model** to open the deployment window.

  4. Search for and select the `gpt-realtime` model and then select **Confirm**.

  5. Review the deployment details and select **Deploy**.

  6. Follow the wizard to finish deploying the model.

  Now that you have a deployment of the `gpt-realtime` model, you can interact with it in the Foundry portal **Audio** playground or Realtime API.

  ## Use the GPT real-time audio

  To chat with your deployed `gpt-realtime` model in the [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs) **Real-time audio** playground, follow these steps:

  1. Go to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and select your project that has your deployed `gpt-realtime` model.

  2. Select **Playgrounds** from the left pane.

  3. Select **Audio playground** > **Try the Audio playground**.

     <Callout type="note">
       The **Chat playground** doesn't support the `gpt-realtime` model. Use the **Audio playground** as described in this section.
     </Callout>

  4. Select your deployed `gpt-realtime` model from the **Deployment** dropdown.

  5. Optionally, you can edit contents in the **Give the model instructions and context** text box. Give the model instructions about how it should behave and any context it should reference when generating a response. You can describe the assistant's personality, tell it what it should and shouldn't answer, and tell it how to format responses.

  6. Optionally, change settings such as threshold, prefix padding, and silence duration.

  7. Select **Start listening** to start the session. You can speak into the microphone to start a chat.

  8. You can interrupt the chat at any time by speaking. You can end the chat by selecting the **Stop listening** button.
</ZonePivot>

## Troubleshooting

### Authentication errors

If you're using keyless authentication (Microsoft Entra ID) and receive authentication errors:

* Verify the `AZURE_OPENAI_API_KEY` environment variable is **not set**. Keyless authentication fails if this variable exists.
* Confirm you've run `az login` to authenticate with Azure CLI.
* Check that your account has the `Cognitive Services OpenAI User` role assigned to the Azure OpenAI resource.

### WebSocket connection failures

If the WebSocket connection fails to establish:

* Verify your endpoint URL format matches the GA format: `{endpoint}/openai/v1` (without the `api-version` parameter).
* Check that your Azure OpenAI resource has a deployed `gpt-realtime` model.
* Ensure your network allows WebSocket connections on port 443.

### Rate limit exceeded

If you receive rate limit errors:

* The Realtime API has specific quotas separate from chat completions.
* Check your current usage in the Azure portal under your Azure OpenAI resource.
* Implement exponential backoff for retry logic in your application.

For more information about quotas, see [Azure OpenAI quotas and limits](quotas-limits).

## Related content

* Learn more about [How to use the Realtime API](how-to/realtime-audio)
* See the [Realtime API reference](realtime-audio-reference)
* Learn more about Azure OpenAI [quotas and limits](quotas-limits)
* Learn more about [Language and voice support for the Speech service](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Azure OpenAI GPT Realtime API for speech and audio is part of the GPT-4o model family that supports low-latency, "speech in, speech out" conversational interactions.

You can use the Realtime API via WebRTC, SIP, or WebSocket to send audio input to the model and receive audio responses in real time. Follow the instructions in this article to get started with the Realtime API via WebRTC.

In most cases, use the WebRTC API for real-time audio streaming. The WebRTC API is a web standard that enables real-time communication (RTC) between browsers and mobile applications. Here are some reasons why WebRTC is preferred for real-time audio streaming:

* **Lower latency**: WebRTC is designed to minimize delay, making it more suitable for audio and video communication where low latency is critical for maintaining quality and synchronization.
* **Media handling**: WebRTC has built-in support for audio and video codecs, providing optimized handling of media streams.
* **Error correction**: WebRTC includes mechanisms for handling packet loss and jitter, which are essential for maintaining the quality of audio streams over unpredictable networks.
* **Peer-to-peer communication**: WebRTC allows direct communication between clients, reducing the need for a central server to relay audio data, which can further reduce latency.

Use the [Realtime API via WebSockets](realtime-audio-websockets) if you need to:

* Stream audio data from a server to a client.
* Send and receive data in real time between a client and server.

WebSockets aren't recommended for real-time audio streaming because they have higher latency than WebRTC.

## Supported models

You can access the GPT real-time models for global deployments in the [East US 2 and Sweden Central regions](../../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).

* `gpt-4o-mini-realtime-preview` (2024-12-17)
* `gpt-4o-realtime-preview` (2024-12-17)
* `gpt-realtime` (version 2025-08-28)
* `gpt-realtime-mini` (version 2025-10-06)
* `gpt-realtime-mini-2025-12-15` (version 2025-12-15)

You should use API version `2025-08-28` in the URL for the Realtime API. The API version is included in the sessions URL.

For more information about supported models, see the [models and versions documentation](../../foundry-models/concepts/models-sold-directly-by-azure#audio-models).

<Callout type="important">
  Use the GA protocol for WebRTC.

  You can still use the beta protocol, but we recommend that you start with the GA Protocol. If you're a current customer, plan to migrate to the GA Protocol.

  This article describes how to use WebRTC with the GA Protocol. We preserve the legacy protocol documentation [here](https://learn.microsoft.com/en-us/previous-versions/azure/foundry-models/realtime-audio-webrtc-legacy).
</Callout>

## Prerequisites

Before you can use GPT real-time audio, you need:

* An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* A Microsoft Foundry resource - [Create a Microsoft Foundry resource](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal) in one of the [supported regions](#supported-models).
* A deployment of the `gpt-4o-realtime-preview`, `gpt-4o-mini-realtime-preview`, `gpt-realtime`, `gpt-realtime-mini`, or `gpt-realtime-mini-2025-12-15` model in a supported region as described in the [supported models](#supported-models) section in this article.
  * In the Foundry portal, load your project. Select **Build** in the upper-right menu, then select the **Models** tab on the left pane, and select **Deploy a base model**. Search for the model you want, and select **Deploy** on the model page.

## Set up WebRTC

To use WebRTC, you need two pieces of code:

1. A web browser application.
2. A service where your web browser can retrieve an ephemeral token.

Other options:

* Proxy the web browser's session negotiation via Session Description Protocol through the same service retrieving the ephemeral token. This scenario is more secure because the web browser doesn't have access to the ephemeral token.
* Filter the messages going to the web browser by using a query parameter.
* Create an observer WebSocket connection to listen to or record the session.

## Steps

### Step 1: Set up service to procure ephemeral token

The key to generating an ephemeral token is the REST API using

```
url = https://{your azure resource}.openai.azure.com/openai/v1/realtime/client_secrets
```

You use this URL with either an api-key or Microsoft Entra ID token. This request retrieves an ephemeral token and sets up the session configuration you want the web browser to use, including the prompt instructions and output voice.

Here's some sample python code for a token service. The web browser application can call this service by using the /token endpoint to retrieve an ephemeral token. This sample code uses the DefaultAzureCredential to authenticate to the RealtimeAPI generating ephemeral tokens.

```
from flask import Flask, jsonify

import os
import requests
import time
import threading

from azure.identity import DefaultAzureCredential

app = Flask(__name__)

# Session configuration
session_config = {
    "session": {
        "type": "realtime",
        "model": "<your model deployment name>",
        "instructions": "You are a helpful assistant.",
        "audio": {
            "output": {
                "voice": "marin",
            },
        },
    },
}

# Get configuration from environment variables
azure_resource = os.getenv('AZURE_RESOURCE')  # e.g., 'your-azure-resource'

# Token caching variables
cached_token = None
token_expiry = 0
token_lock = threading.Lock()

def get_bearer_token(resource_scope: str) -> str:
    """Get a bearer token using DefaultAzureCredential with caching."""
    global cached_token, token_expiry

    current_time = time.time()

    # Check if we have a valid cached token (with 5 minute buffer before expiry)
    with token_lock:
        if cached_token and current_time < (token_expiry - 300):
            return cached_token

    # Get a new token
    try:
        credential = DefaultAzureCredential()
        token = credential.get_token(resource_scope)

        with token_lock:
            cached_token = token.token
            token_expiry = token.expires_on

        print(f"Acquired new bearer token, expires at: {time.ctime(token_expiry)}")
        return cached_token

    except Exception as e:
        print(f"Failed to acquire bearer token: {e}")
        raise

@app.route('/token', methods=['GET'])
def get_token():
    """
    An endpoint which returns the contents of a REST API request to the protected endpoint.
    Uses DefaultAzureCredential for authentication with token caching.
    """
    try:
        # Get bearer token using DefaultAzureCredential
        bearer_token = get_bearer_token("https://cognitiveservices.azure.com/.default")

        # Construct the Azure OpenAI endpoint URL
        url = f"https://{azure_resource}.openai.azure.com/openai/v1/realtime/client_secrets"

        headers = {
            "Authorization": f"Bearer {bearer_token}",
            "Content-Type": "application/json",
        }

        # Make the request to Azure OpenAI
        response = requests.post(
            url,
            headers=headers,
            json=session_config,
            timeout=30
        )

        # Check if the request was successful
        if response.status_code != 200:
            print(f"Request failed with status {response.status_code}: {response.reason}")
            print(f"Response headers: {dict(response.headers)}")
            print(f"Response content: {response.text}")

        response.raise_for_status()

        # Parse the JSON response and extract the ephemeral token
        data = response.json()
        ephemeral_token = data.get('value', '')

        if not ephemeral_token:
            print(f"No ephemeral token found in response: {data}")
            return jsonify({"error": "No ephemeral token available"}), 500

        # Return the ephemeral token as JSON
        return jsonify({"token": ephemeral_token})

    except requests.exceptions.RequestException as e:
        print(f"Token generation error: {e}")
        if hasattr(e, 'response') and e.response is not None:
            print(f"Response status: {e.response.status_code}")
            print(f"Response reason: {e.response.reason}")
            print(f"Response content: {e.response.text}")
        return jsonify({"error": "Failed to generate token"}), 500
    except Exception as e:
        print(f"Unexpected error: {e}")
        return jsonify({"error": "Failed to generate token"}), 500

if __name__ == '__main__':
    if not azure_resource:
        print("Error: AZURE_RESOURCE environment variable is required")
        exit(1)

    print(f"Starting token service for Azure resource: {azure_resource}")
    print("Using DefaultAzureCredential for authentication")
    print("Production mode - use gunicorn to run this service:")

    port = int(os.getenv('PORT', 5000))
    print(f"  gunicorn -w 4 -b 0.0.0.0:{port} --timeout 30 token-service:app")
```

### Step 2: Set up your browser application

Your browser application calls your token service to get the token and then initiates a webRTC connection with the RealtimeAPI. To initiate the webRTC connection, use the following URL with the ephemeral token for authentication.

```
 https://<your azure resource>.openai.azure.com/openai/v1/realtime/calls
```

Once connected, the browser application sends text over the data channel and audio over the media channel. Here's a sample HTML document to get you started.

```
html
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Azure OpenAI Realtime Session</title>
    </head>
    <body>
        <h1>Azure OpenAI Realtime Session</h1>
        <button onclick="StartSession()">Start Session</button>

        <!-- Log container for API messages -->
        <div id="logContainer"></div>

        <script>

        const AZURE_RESOURCE = "<your azure resource>"
        const WEBRTC_URL= `https://${AZURE_RESOURCE}.openai.azure.com/openai/v1/realtime/calls?webrtcfilter=on`

        async function StartSession() {
            try {

                // Call our token service to get the ephemeral key
                const tokenResponse = await fetch("/token");

                if (!tokenResponse.ok) {
                    throw new Error(`Token service request failed: ${tokenResponse.status}`);
                }

                const tokenData = await tokenResponse.json();
                const ephemeralKey = tokenData.token;
                console.log("Ephemeral key received from token service");

                // Mask the ephemeral key in the log message.
                logMessage("Ephemeral Key Received from Token Service: " + "***");

                // Set up the WebRTC connection using the ephemeral key.
                init(ephemeralKey);

            } catch (error) {
                console.error("Error fetching ephemeral key:", error);
                logMessage("Error fetching ephemeral key: " + error.message);
            }
        }

        async function init(ephemeralKey) {
            logMessage("🚀 Starting WebRTC initialization...");

            let peerConnection = new RTCPeerConnection();
            logMessage("✅ RTCPeerConnection created");

            // Set up to play remote audio from the model.
            const audioElement = document.createElement('audio');
            audioElement.autoplay = true;
            document.body.appendChild(audioElement);
            logMessage("🔊 Audio element created and added to page");

            peerConnection.ontrack = (event) => {
                logMessage("🎵 Remote track received! Type: " + event.track.kind);
                logMessage("📊 Number of streams: " + event.streams.length);

                if (event.streams.length > 0) {
                    audioElement.srcObject = event.streams[0];
                    logMessage("✅ Audio stream assigned to audio element");

                    // Add event listeners to audio element for debugging
                    audioElement.onloadstart = () => logMessage("🔄 Audio loading started");
                    audioElement.oncanplay = () => logMessage("▶️ Audio can start playing");
                    audioElement.onplay = () => logMessage("🎵 Audio playback started");
                    audioElement.onerror = (e) => logMessage("❌ Audio error: " + e.message);
                } else {
                    logMessage("⚠️ No streams in track event");
                }
            };

                // Set up data channel for sending and receiving events
            logMessage("🎤 Requesting microphone access...");
            try {
                const clientMedia = await navigator.mediaDevices.getUserMedia({ audio: true });
                logMessage("✅ Microphone access granted");

                const audioTrack = clientMedia.getAudioTracks()[0];
                logMessage("🎤 Audio track obtained: " + audioTrack.label);

                peerConnection.addTrack(audioTrack);
                logMessage("✅ Audio track added to peer connection");
            } catch (error) {
                logMessage("❌ Failed to get microphone access: " + error.message);
                return;
            }

            const dataChannel = peerConnection.createDataChannel('realtime-channel');
            logMessage("📡 Data channel created");

            dataChannel.addEventListener('open', () => {
                logMessage('✅ Data channel is open - ready to send messages');

                // Send client events to start the conversation
                logMessage("📝 Preparing to send text input message...");
                const event = {
                    type: "conversation.item.create",
                    item: {
                        type: "message",
                        role: "user",
                        content: [
                            {
                                type: "input_text",
                                text: "hello there! Can you give me some vacation options?",
                            },
                        ],
                    },
                };

                logMessage("📤 Sending conversation.item.create event...");
                logMessage("💬 Text content: " + event.item.content[0].text);

                try {
                    dataChannel.send(JSON.stringify(event));
                    logMessage("✅ Text input sent successfully!");

                    // Now send response.create to trigger the AI response
                    const responseEvent = {
                        type: "response.create"
                    };

                    logMessage("📤 Sending response.create event to trigger AI response...");
                    dataChannel.send(JSON.stringify(responseEvent));
                    logMessage("✅ Response.create sent successfully!");

                } catch (error) {
                    logMessage("❌ Failed to send text input: " + error.message);
                }
            });                dataChannel.addEventListener('message', (event) => {
                    const realtimeEvent = JSON.parse(event.data);
                    console.log(realtimeEvent);
                    logMessage("Received server event: " + JSON.stringify(realtimeEvent, null, 2));
                    if (realtimeEvent.type === "session.update") {
                        const instructions = realtimeEvent.session.instructions;
                        logMessage("Instructions: " + instructions);
                    } else if (realtimeEvent.type === "session.error") {
                        logMessage("Error: " + realtimeEvent.error.message);
                    } else if (realtimeEvent.type === "session.end") {
                        logMessage("Session ended.");
                    }
                });

                dataChannel.addEventListener('close', () => {
                    logMessage('Data channel is closed');
                });

            // Start the session using the Session Description Protocol (SDP)
            logMessage("🤝 Creating WebRTC offer...");
            const offer = await peerConnection.createOffer();
            await peerConnection.setLocalDescription(offer);
            logMessage("✅ Local description set");

            logMessage("📡 Sending SDP offer to: " + WEBRTC_URL);
            const sdpResponse = await fetch(`${WEBRTC_URL}`, {
                method: "POST",
                body: offer.sdp,
                headers: {
                    Authorization: `Bearer ${ephemeralKey}`,
                    "Content-Type": "application/sdp",
                },
            });

            logMessage("📥 Received SDP response, status: " + sdpResponse.status);
            if (!sdpResponse.ok) {
                logMessage("❌ SDP exchange failed: " + sdpResponse.statusText);
                return;
            }

            const answerSdp = await sdpResponse.text();
            logMessage("✅ Got SDP answer, length: " + answerSdp.length + " chars");

            const answer = { type: "answer", sdp: answerSdp };
            await peerConnection.setRemoteDescription(answer);
            logMessage("✅ Remote description set - WebRTC connection should be establishing...");

            // Add connection state logging
            peerConnection.onconnectionstatechange = () => {
                logMessage("🔗 Connection state: " + peerConnection.connectionState);
            };

            peerConnection.oniceconnectionstatechange = () => {
                logMessage("🧊 ICE connection state: " + peerConnection.iceConnectionState);
            };                const button = document.createElement('button');
                button.innerText = 'Close Session';
                button.onclick = stopSession;
                document.body.appendChild(button);



                function stopSession() {
                    if (dataChannel) dataChannel.close();
                    if (peerConnection) peerConnection.close();
                    peerConnection = null;
                    logMessage("Session closed.");
                }

            }

            function logMessage(message) {
                const logContainer = document.getElementById("logContainer");
                const p = document.createElement("p");
                p.textContent = message;
                logContainer.appendChild(p);
            }
        </script>
    </body>
    </html>
```

In the sample, we use the query parameter webrtcfilter=on. This query parameter limits the data channel messages sent to the browser to keep your prompt instructions private. When the filter is turned on, only the following messages are returned to the browser on the data channel:

* input\_audio\_buffer.speech\_started
* input\_audio\_buffer.speech\_stopped
* output\_audio\_buffer.started
* output\_audio\_buffer.stopped
* conversation.item.input\_audio\_transcription.completed
* conversation.item.added
* conversation.item.created
* response.output\_text.delta
* response.output\_text.done
* response.output\_audio\_transcript.delta
* response.output\_audio\_transcript.done

### Step 3 (optional): Create a websocket observer/controller

If you proxy the session negotiation through your service application, you can parse the Location header that's returned and use it to create a websocket connection to the WebRTC call. This connection can record the WebRTC call and even control it by issuing session.update events and other commands directly.

Here's an updated version of the token\_service shown earlier, now with a /connect endpoint that you can use to both get the ephemeral token and negotiate the session initiation. It also includes a websocket connection that listens to the WebRTC session.

```
from flask import Flask, jsonify, request
#from flask_cors import CORS

import os
import requests
import time
import threading
import asyncio
import json
import websockets

from azure.identity import DefaultAzureCredential

app = Flask(__name__)
# CORS(app)  # Enable CORS for all routes when running locally for testing

# Session configuration
session_config = {
    "session": {
        "type": "realtime",
        "model": "<YOUR MODEL DEPLOYMENT NAME>",
        "instructions": "You are a helpful assistant.",
        "audio": {
            "output": {
                "voice": "marin",
            },
        },
    },
}

# Get configuration from environment variables
azure_resource = os.getenv('AZURE_RESOURCE')  # e.g., 'your-azure-resource'

# Token caching variables
cached_token = None
token_expiry = 0
token_lock = threading.Lock()

def get_bearer_token(resource_scope: str) -> str:
    """Get a bearer token using DefaultAzureCredential with caching."""
    global cached_token, token_expiry

    current_time = time.time()

    # Check if we have a valid cached token (with 5 minute buffer before expiry)
    with token_lock:
        if cached_token and current_time < (token_expiry - 300):
            return cached_token

    # Get a new token
    try:
        credential = DefaultAzureCredential()
        token = credential.get_token(resource_scope)

        with token_lock:
            cached_token = token.token
            token_expiry = token.expires_on

        print(f"Acquired new bearer token, expires at: {time.ctime(token_expiry)}")
        return cached_token

    except Exception as e:
        print(f"Failed to acquire bearer token: {e}")
        raise


def get_ephemeral_token():
    """
    Generate an ephemeral token from Azure OpenAI.

    Returns:
        str: The ephemeral token

    Raises:
        Exception: If token generation fails
    """
    # Get bearer token using DefaultAzureCredential
    bearer_token = get_bearer_token("https://cognitiveservices.azure.com/.default")

    # Construct the Azure OpenAI endpoint URL
    url = f"https://{azure_resource}.openai.azure.com/openai/v1/realtime/client_secrets"

    headers = {
        "Authorization": f"Bearer {bearer_token}",
        "Content-Type": "application/json",
    }

    # Make the request to Azure OpenAI
    response = requests.post(
        url,
        headers=headers,
        json=session_config,
        timeout=30
    )

    # Check if the request was successful
    if response.status_code != 200:
        print(f"Request failed with status {response.status_code}: {response.reason}")
        print(f"Response headers: {dict(response.headers)}")
        print(f"Response content: {response.text}")

    response.raise_for_status()

    # Parse the JSON response and extract the ephemeral token
    data = response.json()
    ephemeral_token = data.get('value', '')

    if not ephemeral_token:
        print(f"No ephemeral token found in response: {data}")
        raise Exception("No ephemeral token available")

    return ephemeral_token


def perform_sdp_negotiation(ephemeral_token, sdp_offer):
    """
    Perform SDP negotiation with the Azure OpenAI Realtime API.

    Args:
        ephemeral_token (str): The ephemeral token for authentication
        sdp_offer (str): The SDP offer to send

    Returns:
        tuple: (sdp_answer, location_header) - The SDP answer from the server and Location header for WebSocket

    Raises:
        Exception: If SDP negotiation fails
    """
    # Construct the realtime endpoint URL - matching the v1transceiver_test pattern
    realtime_url = f"https://{azure_resource}.openai.azure.com/openai/v1/realtime/calls"

    headers = {
        'Authorization': f'Bearer {ephemeral_token}',
        'Content-Type': 'application/sdp'  # Azure OpenAI expects application/sdp, not form data
    }

    print(f"Sending SDP offer to: {realtime_url}")

    # Send the SDP offer as raw body data (not form data)
    response = requests.post(realtime_url, data=sdp_offer, headers=headers, timeout=30)

    if response.status_code == 201:  # Changed from 200 to 201 to match the test expectation
        sdp_answer = response.text
        location_header = response.headers.get('Location', '')
        print(f"Received SDP answer: {sdp_answer[:100]}...")
        if location_header:
            print(f"Captured Location header: {location_header}")
        else:
            print("Warning: No Location header found in response")
        return sdp_answer, location_header
    else:
        error_msg = f"SDP negotiation failed: {response.status_code} - {response.text}"
        print(error_msg)
        raise Exception(error_msg)


@app.route('/token', methods=['GET'])
def get_token():
    """
    An endpoint which returns an ephemeral token for Azure OpenAI Realtime API.
    Uses DefaultAzureCredential for authentication with token caching.
    """
    try:
        ephemeral_token = get_ephemeral_token()

        return jsonify({
            "token": ephemeral_token,
            "endpoint": f"https://{azure_resource}.openai.azure.com",
            "deployment": "gpt-4o-realtime-preview"
        })

    except requests.exceptions.RequestException as e:
        print(f"Token generation error: {e}")
        if hasattr(e, 'response') and e.response is not None:
            print(f"Response status: {e.response.status_code}")
            print(f"Response reason: {e.response.reason}")
            print(f"Response content: {e.response.text}")
        return jsonify({"error": "Failed to generate token"}), 500
    except Exception as e:
        print(f"Unexpected error: {e}")
        return jsonify({"error": "Failed to generate token"}), 500


async def connect_websocket(location_header, bearer_token=None, api_key=None):
    """
    Connect to the WebSocket endpoint using the Location header.
    Similar to the _connect_websocket function in run_v1transceiver_test.py

    Args:
        location_header (str): The Location header from the SDP negotiation response
        bearer_token (str, optional): Bearer token for authentication
        api_key (str, optional): API key for authentication (fallback)

    Returns:
        None: Just logs messages, doesn't store them
    """

    # Extract call_id from location header
    # Example: /v1/realtime/calls/rtc_abc123 -> rtc_abc123
    call_id = location_header.split('/')[-1]
    print(f"Extracted call_id: {call_id}")

    # Construct WebSocket URL: wss://<resource>.openai.azure.com/openai/v1/realtime?call_id=<call_id>
    ws_url = f"wss://{azure_resource}.openai.azure.com/openai/v1/realtime?call_id={call_id}"
    print(f"Connecting to WebSocket: {ws_url}")

    message_count = 0

    try:
        # WebSocket headers - use proper authentication
        headers = {}

        if bearer_token is not None:
            print("Using Bearer token for WebSocket authentication")
            headers["Authorization"] = f"Bearer {bearer_token}"
        elif api_key is not None:
            print("Using API key for WebSocket authentication")
            headers["api-key"] = api_key
        else:
            print("Warning: No authentication provided for WebSocket")

        async with websockets.connect(ws_url, additional_headers=headers) as websocket:
            print("WebSocket connection established")

            # Listen for messages
            try:
                async for message in websocket:
                    try:
                        # Parse JSON message
                        json_data = json.loads(message)
                        msg_type = json_data.get('type', 'unknown')
                        message_count += 1
                        print(f"WebSocket [{message_count}]: {msg_type}")

                        # Handle specific message types with additional details
                        if msg_type == 'response.done':
                            session_status = json_data['response'].get('status', 'unknown')
                            session_details = json_data['response'].get('details', 'No details provided')
                            print(f"  -> Response status: {session_status}, Details: {session_details}")
                            # Continue listening instead of breaking
                        elif msg_type == 'session.created':
                            session_id = json_data.get('session', {}).get('id', 'unknown')
                            print(f"  -> Session created: {session_id}")
                        elif msg_type == 'error':
                            error_message = json_data.get('error', {}).get('message', 'No error message')
                            print(f"  -> Error: {error_message}")

                    except json.JSONDecodeError:
                        message_count += 1
                        print(f"WebSocket [{message_count}]: Non-JSON message: {message[:100]}...")
                    except Exception as e:
                        print(f"Error processing WebSocket message: {e}")

            except websockets.exceptions.ConnectionClosed:
                print(f"WebSocket connection closed by remote (processed {message_count} messages)")
            except Exception as e:
                print(f"WebSocket message loop error: {e}")

    except Exception as e:
        print(f"WebSocket connection error: {e}")

    print(f"WebSocket monitoring completed. Total messages processed: {message_count}")



def start_websocket_background(location_header, bearer_token):
    """
    Start WebSocket connection in background thread to monitor/record the call.
    """
    def run_websocket():
        try:
            print(f"Starting background WebSocket monitoring for: {location_header}")

            # Create new event loop for this thread
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                # Run the WebSocket connection (now just logs, doesn't return messages)
                loop.run_until_complete(
                    connect_websocket(location_header, bearer_token)
                )
                print("Background WebSocket monitoring completed.")

            except Exception as e:
                print(f"Background WebSocket error: {e}")
            finally:
                loop.close()

        except Exception as e:
            print(f"Failed to start background WebSocket: {e}")

    # Start the WebSocket in a background thread
    websocket_thread = threading.Thread(target=run_websocket, daemon=True)
    websocket_thread.start()
    print("Background WebSocket thread started")


@app.route('/connect', methods=['POST'])
def connect_and_negotiate():
    """
    Get token and perform SDP negotiation.
    Expects multipart form data with 'sdp' field containing the SDP offer.
    Returns SDP answer as plain text (matching the v1transceiver_test behavior).
    Automatically starts WebSocket connection in background to monitor/record the call.
    """
    try:
        # Get the SDP offer from multipart form data
        if 'sdp' not in request.form:
            return jsonify({"error": "Missing 'sdp' field in multipart form data"}), 400

        sdp_offer = request.form['sdp']
        print(f"Received SDP offer: {sdp_offer[:100]}...")

        # Get ephemeral token using shared function
        ephemeral_token = get_ephemeral_token()
        print(f"Got ephemeral token for SDP negotiation: {ephemeral_token[:20]}...")

        # Perform SDP negotiation using shared function
        sdp_answer, location_header = perform_sdp_negotiation(ephemeral_token, sdp_offer)

        # Create response headers
        response_headers = {'Content-Type': 'application/sdp'}

        # If we have a location header, start WebSocket connection in background to monitor/record the call
        if location_header:
            try:
                # Get a bearer token for WebSocket authentication
                bearer_token = get_bearer_token("https://cognitiveservices.azure.com/.default")
                start_websocket_background(location_header, bearer_token)
            except Exception as e:
                print(f"Failed to start background WebSocket monitoring: {e}")
                # Don't fail the main request if WebSocket setup fails

        # Return SDP answer as plain text, just like the v1transceiver_test expects
        return sdp_answer, 201, response_headers

    except Exception as e:
        error_msg = f"Error in SDP negotiation: {e}"
        print(error_msg)
        return jsonify({"error": error_msg}), 500


if __name__ == '__main__':
    if not azure_resource:
        print("Error: AZURE_RESOURCE environment variable is required")
        exit(1)

    print(f"Starting token service for Azure resource: {azure_resource}")
    print("Using DefaultAzureCredential for authentication")

    port = int(os.getenv('PORT', 5000))
    print(f"  gunicorn -w 4 -b 0.0.0.0:{port} --timeout 30 token-service:app")
```

The associated browser changes are shown here.

```
html
    <!DOCTYPE html>
    <html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Azure OpenAI Realtime Session - Connect Endpoint</title>
    </head>
    <body>
        <h1>Azure OpenAI Realtime Session - Using /connect Endpoint</h1>
        <button onclick="StartSession()">Start Session</button>

        <!-- Log container for API messages -->
        <div id="logContainer"></div>

        <script>

        const AZURE_RESOURCE = "YOUR AZURE RESOURCE NAME"

        async function StartSession() {
            try {
                logMessage("🚀 Starting session with /connect endpoint...");

                // Set up the WebRTC connection first
                const peerConnection = new RTCPeerConnection();
                logMessage("✅ RTCPeerConnection created");

                // Get microphone access and add audio track BEFORE creating offer
                logMessage("🎤 Requesting microphone access...");
                try {
                    const clientMedia = await navigator.mediaDevices.getUserMedia({ audio: true });
                    logMessage("✅ Microphone access granted");

                    const audioTrack = clientMedia.getAudioTracks()[0];
                    logMessage("🎤 Audio track obtained: " + audioTrack.label);

                    peerConnection.addTrack(audioTrack);
                    logMessage("✅ Audio track added to peer connection");
                } catch (error) {
                    logMessage("❌ Failed to get microphone access: " + error.message);
                    return;
                }

                // Set up audio playback
                const audioElement = document.createElement('audio');
                audioElement.autoplay = true;
                document.body.appendChild(audioElement);
                logMessage("🔊 Audio element created and added to page");

                peerConnection.ontrack = (event) => {
                    logMessage("🎵 Remote track received! Type: " + event.track.kind);
                    logMessage("📊 Number of streams: " + event.streams.length);

                    if (event.streams.length > 0) {
                        audioElement.srcObject = event.streams[0];
                        logMessage("✅ Audio stream assigned to audio element");

                        // Add event listeners to audio element for debugging
                        audioElement.onloadstart = () => logMessage("🔄 Audio loading started");
                        audioElement.oncanplay = () => logMessage("▶️ Audio can start playing");
                        audioElement.onplay = () => logMessage("🎵 Audio playback started");
                        audioElement.onerror = (e) => logMessage("❌ Audio error: " + e.message);
                    } else {
                        logMessage("⚠️ No streams in track event");
                    }
                };

                // Set up data channel BEFORE SDP exchange
                const dataChannel = peerConnection.createDataChannel('realtime-channel');
                logMessage("📡 Data channel created");

                dataChannel.addEventListener('open', () => {
                    logMessage('✅ Data channel is open - ready to send messages');

                    // Send client events to start the conversation
                    logMessage("📝 Preparing to send text input message...");
                    const event = {
                        type: "conversation.item.create",
                        item: {
                            type: "message",
                            role: "user",
                            content: [
                                {
                                    type: "input_text",
                                    text: "hello there! Can you give me some vacation options?",
                                },
                            ],
                        },
                    };

                    logMessage("📤 Sending conversation.item.create event...");
                    logMessage("💬 Text content: " + event.item.content[0].text);

                    try {
                        dataChannel.send(JSON.stringify(event));
                        logMessage("✅ Text input sent successfully!");

                        // Now send response.create to trigger the AI response
                        const responseEvent = {
                            type: "response.create"
                        };

                        logMessage("📤 Sending response.create event to trigger AI response...");
                        dataChannel.send(JSON.stringify(responseEvent));
                        logMessage("✅ Response.create sent successfully!");

                    } catch (error) {
                        logMessage("❌ Failed to send text input: " + error.message);
                    }
                });

                dataChannel.addEventListener('message', (event) => {
                    const realtimeEvent = JSON.parse(event.data);
                    console.log(realtimeEvent);
                    logMessage("📥 Received server event: " + realtimeEvent.type);

                    // Log more detail for important events
                    if (realtimeEvent.type === "error") {
                        logMessage("❌ Error: " + realtimeEvent.error.message);
                    } else if (realtimeEvent.type === "session.created") {
                        logMessage("🎉 Session created successfully");
                    } else if (realtimeEvent.type === "response.output_audio_transcript.done") {
                        logMessage("📝 AI transcript complete: " + (realtimeEvent.transcript || ""));
                    } else if (realtimeEvent.type === "response.done") {
                        logMessage("✅ Response completed");
                    }
                });

                dataChannel.addEventListener('close', () => {
                    logMessage('❌ Data channel is closed');
                });

                dataChannel.addEventListener('error', (error) => {
                    logMessage('❌ Data channel error: ' + error);
                });

                // Add connection state logging
                peerConnection.onconnectionstatechange = () => {
                    logMessage("🔗 Connection state: " + peerConnection.connectionState);
                };

                peerConnection.oniceconnectionstatechange = () => {
                    logMessage("🧊 ICE connection state: " + peerConnection.iceConnectionState);
                };

                // Create offer AFTER setting up data channel
                const offer = await peerConnection.createOffer();
                await peerConnection.setLocalDescription(offer);
                logMessage("🤝 WebRTC offer created with audio track");

                // Prepare multipart form data for /connect endpoint
                const formData = new FormData();
                formData.append('sdp', offer.sdp);

                logMessage("📤 Sending SDP via multipart form to /connect endpoint...");

                // Call our /connect endpoint with multipart form data
                const connectResponse = await fetch("/connect", {
                    method: "POST",
                    body: formData  // FormData automatically sets correct Content-Type
                });

                if (!connectResponse.ok) {
                    throw new Error(`Connect service request failed: ${connectResponse.status}`);
                }

                // Get the SDP answer directly as text (not JSON)
                const answerSdp = await connectResponse.text();
                logMessage("✅ Got SDP answer from /connect endpoint, length: " + answerSdp.length + " chars");

                // Set up the WebRTC connection using the SDP answer
                const answer = { type: "answer", sdp: answerSdp };
                await peerConnection.setRemoteDescription(answer);
                logMessage("✅ Remote description set");

                // Add close session button
                const button = document.createElement('button');
                button.innerText = 'Close Session';
                button.onclick = () => stopSession(dataChannel, peerConnection);
                document.body.appendChild(button);
                logMessage("🔴 Close session button added");

                function stopSession(dataChannel, peerConnection) {
                    if (dataChannel) dataChannel.close();
                    if (peerConnection) peerConnection.close();
                    logMessage("Session closed.");
                }

            } catch (error) {
                console.error("Error in StartSession:", error);
                logMessage("Error in StartSession: " + error.message);
            }
        }

        function logMessage(message) {
            const logContainer = document.getElementById("logContainer");
            const p = document.createElement("p");
            p.textContent = message;
            logContainer.appendChild(p);
        }

        async function init(peerConnection) {
            logMessage("� Continuing WebRTC setup with existing peer connection...");

            // Set up to play remote audio from the model.
            const audioElement = document.createElement('audio');
            audioElement.autoplay = true;
            document.body.appendChild(audioElement);
            logMessage("🔊 Audio element created and added to page");

            peerConnection.ontrack = (event) => {
                logMessage("🎵 Remote track received! Type: " + event.track.kind);
                logMessage("📊 Number of streams: " + event.streams.length);

                if (event.streams.length > 0) {
                    audioElement.srcObject = event.streams[0];
                    logMessage("✅ Audio stream assigned to audio element");

                    // Add event listeners to audio element for debugging
                    audioElement.onloadstart = () => logMessage("🔄 Audio loading started");
                    audioElement.oncanplay = () => logMessage("▶️ Audio can start playing");
                    audioElement.onplay = () => logMessage("🎵 Audio playback started");
                    audioElement.onerror = (e) => logMessage("❌ Audio error: " + e.message);
                } else {
                    logMessage("⚠️ No streams in track event");
                }
            };

            const dataChannel = peerConnection.createDataChannel('realtime-channel');
            logMessage("📡 Data channel created");

            dataChannel.addEventListener('open', () => {
                logMessage('✅ Data channel is open - ready to send messages');

                // Send client events to start the conversation
                logMessage("📝 Preparing to send text input message...");
                const event = {
                    type: "conversation.item.create",
                    item: {
                        type: "message",
                        role: "user",
                        content: [
                            {
                                type: "input_text",
                                text: "hello there! Can you give me some vacation options?",
                            },
                        ],
                    },
                };

                logMessage("📤 Sending conversation.item.create event...");
                logMessage("💬 Text content: " + event.item.content[0].text);

                try {
                    dataChannel.send(JSON.stringify(event));
                    logMessage("✅ Text input sent successfully!");

                    // Now send response.create to trigger the AI response
                    const responseEvent = {
                        type: "response.create"
                    };

                    logMessage("📤 Sending response.create event to trigger AI response...");
                    dataChannel.send(JSON.stringify(responseEvent));
                    logMessage("✅ Response.create sent successfully!");

                } catch (error) {
                    logMessage("❌ Failed to send text input: " + error.message);
                }
            });

            dataChannel.addEventListener('close', () => {
                logMessage('❌ Data channel is closed');
            });

            dataChannel.addEventListener('error', (error) => {
                logMessage('❌ Data channel error: ' + error);
            });            // Add connection state logging
            peerConnection.onconnectionstatechange = () => {
                logMessage("� Connection state: " + peerConnection.connectionState);
            };

            peerConnection.oniceconnectionstatechange = () => {
                logMessage("🧊 ICE connection state: " + peerConnection.iceConnectionState);
            };

            // Add close session button
            const button = document.createElement('button');
            button.innerText = 'Close Session';
            button.onclick = () => stopSession(dataChannel, peerConnection);
            document.body.appendChild(button);
            logMessage("🔴 Close session button added");

            function stopSession(dataChannel, peerConnection) {
                if (dataChannel) dataChannel.close();
                if (peerConnection) peerConnection.close();
                logMessage("Session closed.");
            }
        }

            function logMessage(message) {
                const logContainer = document.getElementById("logContainer");
                const p = document.createElement("p");
                p.textContent = message;
                logContainer.appendChild(p);
            }
        </script>
    </body>
</html>
```

## Related content

* Try the [real-time audio quickstart](../realtime-audio-quickstart)
* See the [Realtime API reference](../realtime-audio-reference)
* Learn more about Azure OpenAI [quotas and limits](../quotas-limits)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Azure OpenAI GPT Realtime API for speech and audio is part of the GPT-4o model family that supports low-latency, "speech in, speech out" conversational interactions.

You can use the Realtime API via WebRTC, SIP, or WebSocket to send audio input to the model and receive audio responses in real time.

Follow the instructions in this article to get started with the Realtime API via WebSockets. Use the Realtime API via WebSockets in server-to-server scenarios where low latency isn't a requirement.

<Callout type="tip">
  In most cases, use the [Realtime API via WebRTC](realtime-audio-webrtc) for real-time audio streaming in client-side applications such as a web application or mobile app. WebRTC is designed for low-latency, real-time audio streaming and is the best choice for most scenarios.
</Callout>

## Prerequisites

Before you can use GPT real-time audio, you need:

* An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* A Microsoft Foundry resource. Create the resource in one of the [supported regions](#supported-models). For setup steps, see [Create a Microsoft Foundry resource](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal).
* A deployment of the `gpt-4o-realtime-preview`, `gpt-4o-mini-realtime-preview`, `gpt-realtime`, `gpt-realtime-mini`, or `gpt-realtime-mini-2025-12-15` model in a supported region as described in the [supported models](#supported-models) section in this article.
  * In the Foundry portal, load your project. Select **Build** in the upper-right menu, then select the **Models** tab on the left pane, and select **Deploy a base model**. Search for the model you want, and select **Deploy** on the model page.

## Supported models

The GPT real-time models are available for global deployments in the [East US 2 and Sweden Central regions](../../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).

* `gpt-4o-mini-realtime-preview` (`2024-12-17`)
* `gpt-4o-realtime-preview` (`2024-12-17`)
* `gpt-realtime` (`2025-08-28`)
* `gpt-realtime-mini` (`2025-10-06`)
* `gpt-realtime-mini-2025-12-15` (`2025-12-15`)

For more information about supported models, see the [models and versions documentation](../../foundry-models/concepts/models-sold-directly-by-azure#audio-models).

## Connection and authentication

The Realtime API (via `/realtime`) is built on [the WebSockets API](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) to facilitate fully asynchronous streaming communication between the end user and model.

The Realtime API is accessed via a secure WebSocket connection to the `/realtime` endpoint of your Azure OpenAI resource.

You can construct a full request URI by concatenating:

* The secure WebSocket (`wss://`) protocol.
* Your Azure OpenAI resource endpoint hostname, for example, `my-aoai-resource.openai.azure.com`
* The `openai/realtime` API path.
* A `deployment` query string parameter with the name of your `gpt-4o-realtime-preview`, `gpt-4o-mini-realtime-preview`, or `gpt-realtime` model deployment.
* **(Preview version only)** An `api-version` query string parameter for a supported API version such as `2025-04-01-preview`.

The following example is a well-constructed `/realtime` request URI:

<Tabs>
  <Tab title="GA version">
    ```http
    wss://my-eastus2-openai-resource.openai.azure.com/openai/v1/realtime?model=gpt-realtime-deployment-name
    ```
  </Tab>

  <Tab title="Preview version">
    ```http
    wss://my-eastus2-openai-resource.openai.azure.com/openai/realtime?api-version=2025-04-01-preview&deployment=gpt-4o-mini-realtime-preview-deployment-name
    ```
  </Tab>
</Tabs>

To authenticate:

* **Microsoft Entra** (recommended): Use token-based authentication with the `/realtime` API for an Azure OpenAI resource with managed identity enabled. Apply a retrieved authentication token using a `Bearer` token with the `Authorization` header.

* **API key**: An `api-key` can be provided in one of two ways:

  * Using an `api-key` connection header on the pre-handshake connection. This option isn't available in a browser environment.
  * Using an `api-key` query string parameter on the request URI. Query string parameters are encrypted when using HTTPS/WSS.

## Realtime API via WebSockets architecture

Once the WebSocket connection session to `/realtime` is established and authenticated, the functional interaction takes place via events for sending and receiving WebSocket messages. These events each take the form of a JSON object.

![Diagram of the Realtime API authentication and connection sequence.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/real-time/realtime-api-sequence.png)

Events can be sent and received in parallel and applications should generally handle them both concurrently and asynchronously.

* A client-side caller establishes a connection to `/realtime`, which starts a new [`session`](../realtime-audio-reference#realtimerequestsession).
* A `session` automatically creates a default `conversation`. Multiple concurrent conversations aren't supported.
* The `conversation` accumulates input signals until a `response` is started, either via a direct event by the caller or automatically by voice activity detection (VAD).
* Each `response` consists of one or more `items`, which can encapsulate messages, function calls, and other information.
* Each message `item` has `content_part`, allowing multiple modalities (text and audio) to be represented across a single item.
* The `session` manages configuration of caller input handling (for example, user audio) and common output generation handling.
* Each caller-initiated [`response.create`](../realtime-audio-reference#realtimeclienteventresponsecreate) can override some of the output [`response`](../realtime-audio-reference#realtimeresponse) behavior, if desired.
* Server-created `item` and the `content_part` in messages can be populated asynchronously and in parallel. For example, receiving audio, text, and function information concurrently in a round robin fashion.

## Try the quickstart

Now that you've done the above steps, you can follow the instructions in the [Realtime API quickstart](../realtime-audio-quickstart) to get started with the Realtime API via WebSockets.

## Related content

* Try the [real-time audio quickstart](../realtime-audio-quickstart)
* See the [Realtime API reference](../realtime-audio-reference)
* Learn more about Azure OpenAI [quotas and limits](../quotas-limits)

<Callout type="note">
  This document refers to the [Microsoft Foundry (classic)](../../what-is-foundry#microsoft-foundry-portals) portal.

  🔍 [View the Microsoft Foundry (new) documentation](../../what-is-foundry?view=foundry\&preserve-view=true) to learn about the new portal.
</Callout>

Azure OpenAI GPT Realtime API for speech and audio is part of the GPT-4o model family that supports low-latency, "speech in, speech out" conversational interactions.

You can use the Realtime API via WebRTC, SIP, or WebSocket to send audio input to the model and receive audio responses in real time. Follow the instructions in this article to get started with the Realtime API via SIP.

SIP is a protocol used to make phone calls over the internet. With SIP and the Realtime API you can direct incoming phone calls to the API.

## Supported models

The GPT real-time models are available for global deployments in [East US 2 and Sweden Central regions](../../foundry-models/concepts/models-sold-directly-by-azure#global-standard-model-availability).

* `gpt-4o-mini-realtime-preview` (2024-12-17)
* `gpt-4o-realtime-preview` (2024-12-17)
* `gpt-realtime` (version 2025-08-28)
* `gpt-realtime-mini` (version 2025-10-06)
* `gpt-realtime-mini-2025-12-15` (version 2025-12-15)

## Prerequisites

Before you can use GPT real-time audio, you need:

* An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* A Microsoft Foundry resource - [Create a Microsoft Foundry resource](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal) in one of the [supported regions](#supported-models).
* A deployment of the `gpt-4o-realtime-preview`, `gpt-4o-mini-realtime-preview`, `gpt-realtime`, `gpt-realtime-mini`, or `gpt-realtime-mini-2025-12-15` model in a supported region as described in the [supported models](#supported-models) section in this article.
  * In the Microsoft Foundry portal, load your project. Select **Build** in the upper right menu, then select the **Models** tab on the left pane, and **Deploy a base model**. Search for the model you want, and select **Deploy** on the model page.

## Connecting to SIP

If you want to connect a phone number to the Realtime API, use a SIP trunking provider (for example, Twilio). A trunking provider is a service that converts your phone call to IP traffic. After you purchase a phone number from your SIP trunking provider, follow the instructions shown here.

Start by creating a webhook for incoming calls with the Azure OpenAI Webhook Service. We have a [REST API](webhooks) that allows you to create, update, view and delete webhook endpoints.

Then, point your SIP trunk at the Azure OpenAI SIP endpoint, using the internal ID of your Azure Resource. Example:

* Get internal ID of your Azure Open AI Resource. You can find the internal ID by clicking on the `JSON View` of your resource.
* Your project ID = `"proj_<internalId>"` This might look like `"proj_88c4a88817034471a0ba0fcae24ceb1b"`

Your sip invites use this project ID as the user: for example, `sip:proj_88c4a88817034471a0ba0fcae24ceb1b@<region>.sip.ai.azure.com;transport=tls`.

The currently supported regions are swedencentral and eastus2.

## Handling incoming calls

When Azure OpenAI receives SIP traffic associated with your project, your webhook endpoint receives an incoming event message. The event fired for sip calls is type = `realtime.call.incoming` like the example shown here.

```
POST https://my_website.com/webhook_endpoint
user-agent: OpenAI/1.0 (+https://platform.openai.com/docs/webhooks)
content-type: application/json
webhook-id: wh_685342e6c53c8190a1be43f081506c52 # unique ID for idempotency
webhook-timestamp: 1750287078 # timestamp of delivery attempt
webhook-signature: v1,Signature # signature to verify authenticity from OpenAI

{
  "object": "event",
  "id": "evt_685343a1381c819085d44c354e1b330e",
  "type": "realtime.call.incoming",
  "created_at": 1750287018, // Unix timestamp
  "data": {
    "call_id": "some_unique_id",
    "sip_headers": [
      { "name": "From", "value": "sip:+142555512112@sip.example.com" },
      { "name": "To", "value": "sip:+18005551212@sip.example.com" },
      { "name": "Call-ID", "value": "rtc_xyz"}
    ]
  }
}
```

From your webhook endpoint, you can accept, reject, or refer this call, using the call\_id value from the webhook event. When accepting the call, you provide the needed configuration (instructions, voice, etc.) for the Realtime API session. Once established, you can set up a WebSocket and monitor the session as usual. The APIs to accept, reject, monitor, refer, and hang up the call are documented in the following sections.

### Accept the call

Use the Accept call endpoint to approve the inbound call and configure the real-time session that answers it. Send the same parameters you would send in to a create client secret. You can include any values you would use in a session.update message, but type, model and instructions are required.

<Callout type="note">
  For authorization, you can either use the api-key header or the Bearer token as shown here. Remember the model name is actually the name of your deployment.
</Callout>

```
curl -X POST "https://<your azure resource name>.openai.azure.com/openai/v1/realtime/calls/$CALL_ID/accept" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
        "type": "realtime",
        "model": "gpt-realtime",
        "instructions": "You are Alex, a friendly concierge for Example Corp."
      }'
```

The request path must include

* The call\_id from the realtime.call.incoming webhook event
* Authorization (or api-key) header

The endpoint returns 200 OK once the SIP leg is ringing and the real-time session is being established.

### Reject the call

Use the Reject call endpoint to decline an invite when you don't want to handle the incoming call (for example, from an unsupported country/region code.) To control the response sent back to the carrier, supply an optional SIP status code along with the required call\_id path parameter. The example here shows a request sending 486, which indicates the system is too busy to take the call.

```
curl -X POST "https://<your azure resource name>.openai.azure.com/openai/v1/realtime/calls/$CALL_ID/reject" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"status_code": 486}'
```

If no status code is supplied, the sip server sends a status code of 603 to the customer as part of the Decline message. A successful request responds with 200 OK after OpenAI delivers the SIP response.

### Redirect the call

Transfer an active call using the Refer call endpoint. Provide the call\_id and the target\_uri that should be placed in the SIP Refer-To header (for example [+14155550123](tel:+14155550123) or sip:agent\@example.com).

```
curl -X POST "https://<your azure resource name>.openai.azure.com/openai/v1/realtime/calls/$CALL_ID/refer" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"target_uri": "tel:+14155550123"}'
```

OpenAI returns 200 OK once the REFER is relayed to your SIP provider. The downstream system handles the rest of the call flow for the caller.

### Monitor call events and issue session commands and updates

After you accept a call, open a WebSocket connection to the same session to stream events and issue Realtime API commands. To create a websocket to an existing call, you must use the call\_id parameter. The model argument isn't used because it's configured as part of the json when accepting the call. The example here shows a common scenario, issuing a "response.create" message to instruct the Realtime API system to "answer the phone and say hello."

Here's a sample of a WebSocket request to a specific SIP call.

```
GET wss://<your azure resource name>.openai.azure.com/openai/v1/realtime?call_id={call_id}
```

**Query parameters**

| Parameter | Type   | Description                                         |
| --------- | ------ | --------------------------------------------------- |
| call\_id  | string | Identifier from the realtime.call.incoming webhook. |

**Headers**

Authorization: Bearer $TOKEN (or api-key: your API key)

The WebSocket behaves exactly like any other Realtime API connection.

You can send messages like 'response.create' or 'session.update' to control the call, and listen for server events being returned to track progress.

The following code snippet illustrates how a websocket connection is made.

```
import WebSocket from "ws";

const callId = "rtc_u1_9c6574da8b8a41a18da9308f4ad974ce";
const ws = new WebSocket(`wss://<your azure resource name>.openai.azure.com/openai/v1/realtime?call_id=${callId}`, {
    headers: {
        api-key: `${process.env.OPENAI_API_KEY}`,
    },
});

ws.on("open", () => {
    ws.send(
        JSON.stringify({
            type: "response.create",
        })
    );
});
```

### Hang up the call

End the session with the Hang up endpoint when your application should disconnect the caller. This endpoint can be used to terminate both SIP and WebRTC real-time sessions.

```
curl -X POST "https://<your azure resource name>.openai.azure.com/openai/v1/realtime/calls/$CALL_ID/hangup" \
  -H "Authorization: Bearer $TOKEN"
The API responds with 200 OK when it starts tearing down the call.
```

## Sample webhook endpoint

The following code is a python example of a realtime.call.incoming handler. It accepts the call and then logs all the events from the Realtime API.

```
from flask import Flask, request, Response, jsonify, make_response
from openai import OpenAI, InvalidWebhookSignatureError
import asyncio
import json
import os
import requests
import time
import threading
import websockets

app = Flask(__name__)
client = OpenAI(
    webhook_secret=os.environ["OPENAI_WEBHOOK_SECRET"]
)

AUTH_HEADER = {
    "api-key": os.getenv("OPENAI_API_KEY")
}

call_accept = {
    "type": "realtime",
    "instructions": "You are a support agent.",
    "model": "gpt-realtime",
}

response_create = {
    "type": "response.create",
    "response": {
        "instructions": (
            "Say to the user 'Thank you for calling, how can I help you'"
        )
    },
}


async def websocket_task(call_id):
    try:
        async with websockets.connect(
            "wss://<your azure resource>.openai.azure.com/openai/v1/realtime?call_id=" + call_id,
            additional_headers=AUTH_HEADER,
        ) as websocket:
            await websocket.send(json.dumps(response_create))

            while True:
                response = await websocket.recv()
                print(f"Received from WebSocket: {response}")
    except Exception as e:
        print(f"WebSocket error: {e}")


@app.route("/", methods=["POST"])
def webhook():
    try:
        event = client.webhooks.unwrap(request.data, request.headers)

        if event.type == "realtime.call.incoming":
            requests.post(
                "https://<your azure resource name>.openai.azure.com/openai/v1/realtime/calls/"
                + event.data.call_id
                + "/accept",
                headers={**AUTH_HEADER, "Content-Type": "application/json"},
                json=call_accept,
            )
            threading.Thread(
                target=lambda: asyncio.run(
                    websocket_task(event.data.call_id)
                ),
                daemon=True,
            ).start()
            return Response(status=200)
    except InvalidWebhookSignatureError as e:
        print("Invalid signature", e)
        return Response("Invalid signature", status=400)


if __name__ == "__main__":
    app.run(port=8000)
```

## Next steps

You now know how to get a call connected over SIP. The next step is building your real-time application prompts to server your customers.

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this quickstart, you use the [Azure OpenAI Whisper model](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/whisper-overview) for speech to text conversion. The Whisper model can transcribe human speech in numerous languages, and it can also translate other languages into English.

<ZonePivot pivot="rest-api">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * Be sure that you are assigned at least the [Cognitive Services Contributor](how-to/role-based-access-control#cognitive-services-contributor) role for the Azure OpenAI resource.
  * A sample audio file. You can get sample audio, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Setup

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  ## Create a REST API request and response

  In a bash shell, run the following command. You need to replace `YourDeploymentName` with the deployment name you chose when you deployed the Whisper model. The deployment name isn't necessarily the same as the model name. Entering the model name results in an error unless you chose a deployment name that's identical to the underlying model name.

  ```bash
  curl $AZURE_OPENAI_ENDPOINT/openai/deployments/YourDeploymentName/audio/transcriptions?api-version=2024-02-01 \
   -H "api-key: $AZURE_OPENAI_API_KEY" \
   -H "Content-Type: multipart/form-data" \
   -F file="@./wikipediaOcelot.wav"
  ```

  The first line of the preceding command with an example endpoint would appear as follows:

  ```bash
  curl https://aoai-docs.openai.azure.com/openai/deployments/{YourDeploymentName}/audio/transcriptions?api-version=2024-02-01 \
  ```

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```bash
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  ## Prerequisites

  * An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * [Python 3.8 or later](https://www.python.org)
  * The [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli).
  * A sample audio file. You can get sample audio, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Setup

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  <Callout type="important">
    **Passwordless authentication is recommended**

    For passwordless authentication, you need to:

    1. Use the `azure-identity` package (`pip install azure-identity`).
    2. Assign the `Cognitive Services User` role to your user account. This can be done in the Azure portal under **Access control (IAM)** > **Add role assignment**.
    3. Sign in with the Azure CLI such as `az login`.
  </Callout>

  ## Create a Python environment

  Create a new directory for your project and navigate to it from a terminal or command prompt.

  ```Bash
  mkdir whisper-quickstart; cd whisper-quickstart
  ```

  Create and activate a virtual environment for this project.

  <Tabs>
    <Tab title="Windows">
      ```powershell
      python -m venv .venv
      .venv\Scripts\activate
      ```
    </Tab>

    <Tab title="macOS/Linux">
      ```Bash
      python3 -m venv .venv
      source .venv/bin/activate
      ```
    </Tab>
  </Tabs>

  Install the OpenAI Python client library with:

  <Tabs>
    <Tab title="OpenAI Python 1.x">
      ```console
      pip install openai
      ```
    </Tab>

    <Tab title="OpenAI Python 0.28.1">
      <Callout type="note">
        The OpenAI Python library version `0.28.1` is deprecated. We recommend using `1.x`. Consult our [migration guide](how-to/migration) for information on moving from `0.28.1` to `1.x`.
      </Callout>

      ```console
      pip install openai==0.28.1
      ```
    </Tab>
  </Tabs>

  ## Create the Python app

  1. Create a new Python file called *quickstart.py*. Then open it up in your preferred editor or IDE.

  2. Replace the contents of *quickstart.py* with the following code. Modify the code to add your deployment name:

  <Tabs>
    <Tab title="OpenAI Python 1.x">
      ```python
          import os
          from openai import AzureOpenAI

          client = AzureOpenAI(
              api_key=os.getenv("AZURE_OPENAI_API_KEY"),
              api_version="2024-02-01",
              azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
          )

          deployment_id = "YOUR-DEPLOYMENT-NAME-HERE" #This will correspond to the custom name you chose for your deployment when you deployed a model."
          audio_test_file = "./wikipediaOcelot.wav"

          result = client.audio.transcriptions.create(
              file=open(audio_test_file, "rb"),
              model=deployment_id
          )

          print(result)
      ```
    </Tab>

    <Tab title="OpenAI Python 0.28.1">
      ```python
          import openai
          import time
          import os

          openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")
          openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")  # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
          openai.api_type = "azure"
          openai.api_version = "2024-02-01"

          model_name = "whisper"
          deployment_id = "YOUR-DEPLOYMENT-NAME-HERE" #This will correspond to the custom name you chose for your deployment when you deployed a model."
          audio_language="en"

          audio_test_file = "./wikipediaOcelot.wav"

          result = openai.Audio.transcribe(
                      file=open(audio_test_file, "rb"),
                      model=model_name,
                      deployment_id=deployment_id
                  )

          print(result)
      ```
    </Tab>
  </Tabs>

  Run the application using the `python` command on your quickstart file:

  ```python
  python quickstart.py
  ```

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```python
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-dotnet">
  ## Prerequisites

  * An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * [The .NET 8.0 SDK](https://dotnet.microsoft.com/en-us/download)

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Setup

  1. Create a new folder `whisper-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir whisper-quickstart && cd whisper-quickstart
     ```

  2. Create a new console application with the following command:

     ```shell
     dotnet new console
     ```

  3. Install the [OpenAI .NET client library](https://www.nuget.org/packages/Azure.AI.OpenAI/) with the [dotnet add package](https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-add-package) command:

     ```console
     dotnet add package Azure.AI.OpenAI
     ```

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the [Azure.Identity](https://www.nuget.org/packages/Azure.Identity) package with:

     ```console
     dotnet add package Azure.Identity
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
      ```
    </Tab>

    <Tab title="API key">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));
      ```
    </Tab>
  </Tabs>

  <Callout type="note">
    You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).
  </Callout>

  To run the quickstart, follow these steps:

  1. Replace the contents of `Program.cs` with the following code and update the placeholder values with your own.

     ```csharp
     using Azure;
     using Azure.AI.OpenAI;
     using Azure.Identity; // Required for Passwordless auth


     string deploymentName = "whisper";

     string endpoint = Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT") ?? "https://<your-resource-name>.openai.azure.com/";
     string key = Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY") ?? "<your-key>";

     // Use the recommended keyless credential instead of the AzureKeyCredential credential.
     AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
     //AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));

     var audioFilePath = "<audio file path>"

     var audioClient = openAIClient.GetAudioClient(deploymentName);

     var result = await audioClient.TranscribeAudioAsync(audioFilePath);

     Console.WriteLine("Transcribed text:");
     foreach (var item in result.Value.Text)
     {
         Console.Write(item);
     }
     ```

  2. Run the application using the `dotnet run` command or the run button at the top of Visual Studio:

     ```dotnetcli
     dotnet run
     ```

  ## Output

  If you are using the sample audio file, you should see the following text printed out in the console:

  ```text
  The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States,
  Mexico, and Central and South America. This medium-sized cat is characterized by solid
  black spots and streaks on its coat, round ears...
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Setup

  1. Create a new folder `synthesis-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir synthesis-quickstart && cd synthesis-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  4. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Create a sample application

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { createReadStream } = require("fs");
         const { AzureOpenAI } = require("openai");
         const { DefaultAzureCredential, getBearerTokenProvider } = require("@azure/identity");

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient() {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = "2024-08-01-preview";
         const deploymentName = "whisper";

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Output

  ```json
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [TypeScript](https://www.typescriptlang.org/download/)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `whisper-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir whisper-quickstart && cd whisper-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```shell
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  5. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Create a sample application

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";
         import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      5. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  ## Output

  ```json
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-powershell">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [You can use either the latest version, PowerShell 7, or Windows PowerShell 5.1.](https://aka.ms/installpowershell)
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Set up

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  ## Create a PowerShell app

  Run the following command. You need to replace `YourDeploymentName` with the deployment name you chose when you deployed the Whisper model. The deployment name isn't necessarily the same as the model name. Entering the model name results in an error unless you chose a deployment name that is identical to the underlying model name.

  ```powershell
  # Azure OpenAI metadata variables
  $openai = @{
      api_key     = $Env:AZURE_OPENAI_API_KEY
      api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
      api_version = '2024-02-01' # this may change in the future
      name        = 'YourDeploymentName' #This will correspond to the custom name you chose for your deployment when you deployed a model.
  }

  # Header for authentication
  $headers = [ordered]@{
      'api-key' = $openai.api_key
  }

  $form = @{ file = get-item -path './wikipediaOcelot.wav' }

  # Send a completion call to generate an answer
  $url = "$($openai.api_base)/openai/deployments/$($openai.name)/audio/transcriptions?api-version=$($openai.api_version)"

  $response = Invoke-RestMethod -Uri $url -Headers $headers -Form $form -Method Post -ContentType 'multipart/form-data'
  return $response.text
  ```

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [The PowerShell Secret Management with Azure Key Vault](https://learn.microsoft.com/en-us/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```text
  The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs.
  ```
</ZonePivot>

<Callout type="note">
  For information about other audio models that you can use with Azure OpenAI, see [Audio models](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio).
</Callout>

<Callout type="tip">
  The file size limit for the Whisper model is 25 MB. If you need to transcribe a file larger than 25 MB, you can use the Azure Speech in Foundry Tools [batch transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-create#use-a-whisper-model) API.
</Callout>

## Clean up resources

If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

* [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
* [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

## Next steps

* To learn how to convert audio data to text in batches, see [Create a batch transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-create).
* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this quickstart, you use the [Azure OpenAI Whisper model](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/whisper-overview) for speech to text conversion. The Whisper model can transcribe human speech in numerous languages, and it can also translate other languages into English.

<ZonePivot pivot="rest-api">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * Be sure that you are assigned at least the [Cognitive Services Contributor](how-to/role-based-access-control#cognitive-services-contributor) role for the Azure OpenAI resource.
  * A sample audio file. You can get sample audio, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Setup

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  ## Create a REST API request and response

  In a bash shell, run the following command. You need to replace `YourDeploymentName` with the deployment name you chose when you deployed the Whisper model. The deployment name isn't necessarily the same as the model name. Entering the model name results in an error unless you chose a deployment name that's identical to the underlying model name.

  ```bash
  curl $AZURE_OPENAI_ENDPOINT/openai/deployments/YourDeploymentName/audio/transcriptions?api-version=2024-02-01 \
   -H "api-key: $AZURE_OPENAI_API_KEY" \
   -H "Content-Type: multipart/form-data" \
   -F file="@./wikipediaOcelot.wav"
  ```

  The first line of the preceding command with an example endpoint would appear as follows:

  ```bash
  curl https://aoai-docs.openai.azure.com/openai/deployments/{YourDeploymentName}/audio/transcriptions?api-version=2024-02-01 \
  ```

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```bash
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  ## Prerequisites

  * An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * [Python 3.8 or later](https://www.python.org)
  * The [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli).
  * A sample audio file. You can get sample audio, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Setup

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  <Callout type="important">
    **Passwordless authentication is recommended**

    For passwordless authentication, you need to:

    1. Use the `azure-identity` package (`pip install azure-identity`).
    2. Assign the `Cognitive Services User` role to your user account. This can be done in the Azure portal under **Access control (IAM)** > **Add role assignment**.
    3. Sign in with the Azure CLI such as `az login`.
  </Callout>

  ## Create a Python environment

  Create a new directory for your project and navigate to it from a terminal or command prompt.

  ```Bash
  mkdir whisper-quickstart; cd whisper-quickstart
  ```

  Create and activate a virtual environment for this project.

  <Tabs>
    <Tab title="Windows">
      ```powershell
      python -m venv .venv
      .venv\Scripts\activate
      ```
    </Tab>

    <Tab title="macOS/Linux">
      ```Bash
      python3 -m venv .venv
      source .venv/bin/activate
      ```
    </Tab>
  </Tabs>

  Install the OpenAI Python client library with:

  <Tabs>
    <Tab title="OpenAI Python 1.x">
      ```console
      pip install openai
      ```
    </Tab>

    <Tab title="OpenAI Python 0.28.1">
      <Callout type="note">
        The OpenAI Python library version `0.28.1` is deprecated. We recommend using `1.x`. Consult our [migration guide](how-to/migration) for information on moving from `0.28.1` to `1.x`.
      </Callout>

      ```console
      pip install openai==0.28.1
      ```
    </Tab>
  </Tabs>

  ## Create the Python app

  1. Create a new Python file called *quickstart.py*. Then open it up in your preferred editor or IDE.

  2. Replace the contents of *quickstart.py* with the following code. Modify the code to add your deployment name:

  <Tabs>
    <Tab title="OpenAI Python 1.x">
      ```python
          import os
          from openai import AzureOpenAI

          client = AzureOpenAI(
              api_key=os.getenv("AZURE_OPENAI_API_KEY"),
              api_version="2024-02-01",
              azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
          )

          deployment_id = "YOUR-DEPLOYMENT-NAME-HERE" #This will correspond to the custom name you chose for your deployment when you deployed a model."
          audio_test_file = "./wikipediaOcelot.wav"

          result = client.audio.transcriptions.create(
              file=open(audio_test_file, "rb"),
              model=deployment_id
          )

          print(result)
      ```
    </Tab>

    <Tab title="OpenAI Python 0.28.1">
      ```python
          import openai
          import time
          import os

          openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")
          openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")  # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
          openai.api_type = "azure"
          openai.api_version = "2024-02-01"

          model_name = "whisper"
          deployment_id = "YOUR-DEPLOYMENT-NAME-HERE" #This will correspond to the custom name you chose for your deployment when you deployed a model."
          audio_language="en"

          audio_test_file = "./wikipediaOcelot.wav"

          result = openai.Audio.transcribe(
                      file=open(audio_test_file, "rb"),
                      model=model_name,
                      deployment_id=deployment_id
                  )

          print(result)
      ```
    </Tab>
  </Tabs>

  Run the application using the `python` command on your quickstart file:

  ```python
  python quickstart.py
  ```

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```python
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-dotnet">
  ## Prerequisites

  * An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * [The .NET 8.0 SDK](https://dotnet.microsoft.com/en-us/download)

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Setup

  1. Create a new folder `whisper-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir whisper-quickstart && cd whisper-quickstart
     ```

  2. Create a new console application with the following command:

     ```shell
     dotnet new console
     ```

  3. Install the [OpenAI .NET client library](https://www.nuget.org/packages/Azure.AI.OpenAI/) with the [dotnet add package](https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-add-package) command:

     ```console
     dotnet add package Azure.AI.OpenAI
     ```

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the [Azure.Identity](https://www.nuget.org/packages/Azure.Identity) package with:

     ```console
     dotnet add package Azure.Identity
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
      ```
    </Tab>

    <Tab title="API key">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));
      ```
    </Tab>
  </Tabs>

  <Callout type="note">
    You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).
  </Callout>

  To run the quickstart, follow these steps:

  1. Replace the contents of `Program.cs` with the following code and update the placeholder values with your own.

     ```csharp
     using Azure;
     using Azure.AI.OpenAI;
     using Azure.Identity; // Required for Passwordless auth


     string deploymentName = "whisper";

     string endpoint = Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT") ?? "https://<your-resource-name>.openai.azure.com/";
     string key = Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY") ?? "<your-key>";

     // Use the recommended keyless credential instead of the AzureKeyCredential credential.
     AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
     //AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));

     var audioFilePath = "<audio file path>"

     var audioClient = openAIClient.GetAudioClient(deploymentName);

     var result = await audioClient.TranscribeAudioAsync(audioFilePath);

     Console.WriteLine("Transcribed text:");
     foreach (var item in result.Value.Text)
     {
         Console.Write(item);
     }
     ```

  2. Run the application using the `dotnet run` command or the run button at the top of Visual Studio:

     ```dotnetcli
     dotnet run
     ```

  ## Output

  If you are using the sample audio file, you should see the following text printed out in the console:

  ```text
  The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States,
  Mexico, and Central and South America. This medium-sized cat is characterized by solid
  black spots and streaks on its coat, round ears...
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Setup

  1. Create a new folder `synthesis-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir synthesis-quickstart && cd synthesis-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  4. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Create a sample application

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { createReadStream } = require("fs");
         const { AzureOpenAI } = require("openai");
         const { DefaultAzureCredential, getBearerTokenProvider } = require("@azure/identity");

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient() {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = "2024-08-01-preview";
         const deploymentName = "whisper";

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Output

  ```json
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [TypeScript](https://www.typescriptlang.org/download/)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `whisper-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir whisper-quickstart && cd whisper-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```shell
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  5. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Create a sample application

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";
         import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      5. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  ## Output

  ```json
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-powershell">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [You can use either the latest version, PowerShell 7, or Windows PowerShell 5.1.](https://aka.ms/installpowershell)
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Set up

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  ## Create a PowerShell app

  Run the following command. You need to replace `YourDeploymentName` with the deployment name you chose when you deployed the Whisper model. The deployment name isn't necessarily the same as the model name. Entering the model name results in an error unless you chose a deployment name that is identical to the underlying model name.

  ```powershell
  # Azure OpenAI metadata variables
  $openai = @{
      api_key     = $Env:AZURE_OPENAI_API_KEY
      api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
      api_version = '2024-02-01' # this may change in the future
      name        = 'YourDeploymentName' #This will correspond to the custom name you chose for your deployment when you deployed a model.
  }

  # Header for authentication
  $headers = [ordered]@{
      'api-key' = $openai.api_key
  }

  $form = @{ file = get-item -path './wikipediaOcelot.wav' }

  # Send a completion call to generate an answer
  $url = "$($openai.api_base)/openai/deployments/$($openai.name)/audio/transcriptions?api-version=$($openai.api_version)"

  $response = Invoke-RestMethod -Uri $url -Headers $headers -Form $form -Method Post -ContentType 'multipart/form-data'
  return $response.text
  ```

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [The PowerShell Secret Management with Azure Key Vault](https://learn.microsoft.com/en-us/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```text
  The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs.
  ```
</ZonePivot>

<Callout type="note">
  For information about other audio models that you can use with Azure OpenAI, see [Audio models](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio).
</Callout>

<Callout type="tip">
  The file size limit for the Whisper model is 25 MB. If you need to transcribe a file larger than 25 MB, you can use the Azure Speech in Foundry Tools [batch transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-create#use-a-whisper-model) API.
</Callout>

## Clean up resources

If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

* [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
* [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

## Next steps

* To learn how to convert audio data to text in batches, see [Create a batch transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-create).
* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).


<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this quickstart, you use the [Azure OpenAI Whisper model](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/whisper-overview) for speech to text conversion. The Whisper model can transcribe human speech in numerous languages, and it can also translate other languages into English.

<ZonePivot pivot="rest-api">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * Be sure that you are assigned at least the [Cognitive Services Contributor](how-to/role-based-access-control#cognitive-services-contributor) role for the Azure OpenAI resource.
  * A sample audio file. You can get sample audio, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Setup

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  ## Create a REST API request and response

  In a bash shell, run the following command. You need to replace `YourDeploymentName` with the deployment name you chose when you deployed the Whisper model. The deployment name isn't necessarily the same as the model name. Entering the model name results in an error unless you chose a deployment name that's identical to the underlying model name.

  ```bash
  curl $AZURE_OPENAI_ENDPOINT/openai/deployments/YourDeploymentName/audio/transcriptions?api-version=2024-02-01 \
   -H "api-key: $AZURE_OPENAI_API_KEY" \
   -H "Content-Type: multipart/form-data" \
   -F file="@./wikipediaOcelot.wav"
  ```

  The first line of the preceding command with an example endpoint would appear as follows:

  ```bash
  curl https://aoai-docs.openai.azure.com/openai/deployments/{YourDeploymentName}/audio/transcriptions?api-version=2024-02-01 \
  ```

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```bash
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  ## Prerequisites

  * An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * [Python 3.8 or later](https://www.python.org)
  * The [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli).
  * A sample audio file. You can get sample audio, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Setup

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  <Callout type="important">
    **Passwordless authentication is recommended**

    For passwordless authentication, you need to:

    1. Use the `azure-identity` package (`pip install azure-identity`).
    2. Assign the `Cognitive Services User` role to your user account. This can be done in the Azure portal under **Access control (IAM)** > **Add role assignment**.
    3. Sign in with the Azure CLI such as `az login`.
  </Callout>

  ## Create a Python environment

  Create a new directory for your project and navigate to it from a terminal or command prompt.

  ```Bash
  mkdir whisper-quickstart; cd whisper-quickstart
  ```

  Create and activate a virtual environment for this project.

  <Tabs>
    <Tab title="Windows">
      ```powershell
      python -m venv .venv
      .venv\Scripts\activate
      ```
    </Tab>

    <Tab title="macOS/Linux">
      ```Bash
      python3 -m venv .venv
      source .venv/bin/activate
      ```
    </Tab>
  </Tabs>

  Install the OpenAI Python client library with:

  <Tabs>
    <Tab title="OpenAI Python 1.x">
      ```console
      pip install openai
      ```
    </Tab>

    <Tab title="OpenAI Python 0.28.1">
      <Callout type="note">
        The OpenAI Python library version `0.28.1` is deprecated. We recommend using `1.x`. Consult our [migration guide](how-to/migration) for information on moving from `0.28.1` to `1.x`.
      </Callout>

      ```console
      pip install openai==0.28.1
      ```
    </Tab>
  </Tabs>

  ## Create the Python app

  1. Create a new Python file called *quickstart.py*. Then open it up in your preferred editor or IDE.

  2. Replace the contents of *quickstart.py* with the following code. Modify the code to add your deployment name:

  <Tabs>
    <Tab title="OpenAI Python 1.x">
      ```python
          import os
          from openai import AzureOpenAI

          client = AzureOpenAI(
              api_key=os.getenv("AZURE_OPENAI_API_KEY"),
              api_version="2024-02-01",
              azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
          )

          deployment_id = "YOUR-DEPLOYMENT-NAME-HERE" #This will correspond to the custom name you chose for your deployment when you deployed a model."
          audio_test_file = "./wikipediaOcelot.wav"

          result = client.audio.transcriptions.create(
              file=open(audio_test_file, "rb"),
              model=deployment_id
          )

          print(result)
      ```
    </Tab>

    <Tab title="OpenAI Python 0.28.1">
      ```python
          import openai
          import time
          import os

          openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")
          openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")  # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
          openai.api_type = "azure"
          openai.api_version = "2024-02-01"

          model_name = "whisper"
          deployment_id = "YOUR-DEPLOYMENT-NAME-HERE" #This will correspond to the custom name you chose for your deployment when you deployed a model."
          audio_language="en"

          audio_test_file = "./wikipediaOcelot.wav"

          result = openai.Audio.transcribe(
                      file=open(audio_test_file, "rb"),
                      model=model_name,
                      deployment_id=deployment_id
                  )

          print(result)
      ```
    </Tab>
  </Tabs>

  Run the application using the `python` command on your quickstart file:

  ```python
  python quickstart.py
  ```

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```python
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-dotnet">
  ## Prerequisites

  * An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * [The .NET 8.0 SDK](https://dotnet.microsoft.com/en-us/download)

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Setup

  1. Create a new folder `whisper-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir whisper-quickstart && cd whisper-quickstart
     ```

  2. Create a new console application with the following command:

     ```shell
     dotnet new console
     ```

  3. Install the [OpenAI .NET client library](https://www.nuget.org/packages/Azure.AI.OpenAI/) with the [dotnet add package](https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-add-package) command:

     ```console
     dotnet add package Azure.AI.OpenAI
     ```

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the [Azure.Identity](https://www.nuget.org/packages/Azure.Identity) package with:

     ```console
     dotnet add package Azure.Identity
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
      ```
    </Tab>

    <Tab title="API key">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));
      ```
    </Tab>
  </Tabs>

  <Callout type="note">
    You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).
  </Callout>

  To run the quickstart, follow these steps:

  1. Replace the contents of `Program.cs` with the following code and update the placeholder values with your own.

     ```csharp
     using Azure;
     using Azure.AI.OpenAI;
     using Azure.Identity; // Required for Passwordless auth


     string deploymentName = "whisper";

     string endpoint = Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT") ?? "https://<your-resource-name>.openai.azure.com/";
     string key = Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY") ?? "<your-key>";

     // Use the recommended keyless credential instead of the AzureKeyCredential credential.
     AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
     //AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));

     var audioFilePath = "<audio file path>"

     var audioClient = openAIClient.GetAudioClient(deploymentName);

     var result = await audioClient.TranscribeAudioAsync(audioFilePath);

     Console.WriteLine("Transcribed text:");
     foreach (var item in result.Value.Text)
     {
         Console.Write(item);
     }
     ```

  2. Run the application using the `dotnet run` command or the run button at the top of Visual Studio:

     ```dotnetcli
     dotnet run
     ```

  ## Output

  If you are using the sample audio file, you should see the following text printed out in the console:

  ```text
  The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States,
  Mexico, and Central and South America. This medium-sized cat is characterized by solid
  black spots and streaks on its coat, round ears...
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Setup

  1. Create a new folder `synthesis-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir synthesis-quickstart && cd synthesis-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  4. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Create a sample application

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { createReadStream } = require("fs");
         const { AzureOpenAI } = require("openai");
         const { DefaultAzureCredential, getBearerTokenProvider } = require("@azure/identity");

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient() {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = "2024-08-01-preview";
         const deploymentName = "whisper";

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Output

  ```json
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [TypeScript](https://www.typescriptlang.org/download/)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `whisper-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir whisper-quickstart && cd whisper-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```shell
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  5. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Create a sample application

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";
         import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      5. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  ## Output

  ```json
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-powershell">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [You can use either the latest version, PowerShell 7, or Windows PowerShell 5.1.](https://aka.ms/installpowershell)
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Set up

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  ## Create a PowerShell app

  Run the following command. You need to replace `YourDeploymentName` with the deployment name you chose when you deployed the Whisper model. The deployment name isn't necessarily the same as the model name. Entering the model name results in an error unless you chose a deployment name that is identical to the underlying model name.

  ```powershell
  # Azure OpenAI metadata variables
  $openai = @{
      api_key     = $Env:AZURE_OPENAI_API_KEY
      api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
      api_version = '2024-02-01' # this may change in the future
      name        = 'YourDeploymentName' #This will correspond to the custom name you chose for your deployment when you deployed a model.
  }

  # Header for authentication
  $headers = [ordered]@{
      'api-key' = $openai.api_key
  }

  $form = @{ file = get-item -path './wikipediaOcelot.wav' }

  # Send a completion call to generate an answer
  $url = "$($openai.api_base)/openai/deployments/$($openai.name)/audio/transcriptions?api-version=$($openai.api_version)"

  $response = Invoke-RestMethod -Uri $url -Headers $headers -Form $form -Method Post -ContentType 'multipart/form-data'
  return $response.text
  ```

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [The PowerShell Secret Management with Azure Key Vault](https://learn.microsoft.com/en-us/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```text
  The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs.
  ```
</ZonePivot>

<Callout type="note">
  For information about other audio models that you can use with Azure OpenAI, see [Audio models](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio).
</Callout>

<Callout type="tip">
  The file size limit for the Whisper model is 25 MB. If you need to transcribe a file larger than 25 MB, you can use the Azure Speech in Foundry Tools [batch transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-create#use-a-whisper-model) API.
</Callout>

## Clean up resources

If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

* [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
* [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

## Next steps

* To learn how to convert audio data to text in batches, see [Create a batch transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-create).
* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).


<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this quickstart, you use the [Azure OpenAI Whisper model](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/whisper-overview) for speech to text conversion. The Whisper model can transcribe human speech in numerous languages, and it can also translate other languages into English.

<ZonePivot pivot="rest-api">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * Be sure that you are assigned at least the [Cognitive Services Contributor](how-to/role-based-access-control#cognitive-services-contributor) role for the Azure OpenAI resource.
  * A sample audio file. You can get sample audio, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Setup

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  ## Create a REST API request and response

  In a bash shell, run the following command. You need to replace `YourDeploymentName` with the deployment name you chose when you deployed the Whisper model. The deployment name isn't necessarily the same as the model name. Entering the model name results in an error unless you chose a deployment name that's identical to the underlying model name.

  ```bash
  curl $AZURE_OPENAI_ENDPOINT/openai/deployments/YourDeploymentName/audio/transcriptions?api-version=2024-02-01 \
   -H "api-key: $AZURE_OPENAI_API_KEY" \
   -H "Content-Type: multipart/form-data" \
   -F file="@./wikipediaOcelot.wav"
  ```

  The first line of the preceding command with an example endpoint would appear as follows:

  ```bash
  curl https://aoai-docs.openai.azure.com/openai/deployments/{YourDeploymentName}/audio/transcriptions?api-version=2024-02-01 \
  ```

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```bash
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  ## Prerequisites

  * An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * [Python 3.8 or later](https://www.python.org)
  * The [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli).
  * A sample audio file. You can get sample audio, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Setup

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  <Callout type="important">
    **Passwordless authentication is recommended**

    For passwordless authentication, you need to:

    1. Use the `azure-identity` package (`pip install azure-identity`).
    2. Assign the `Cognitive Services User` role to your user account. This can be done in the Azure portal under **Access control (IAM)** > **Add role assignment**.
    3. Sign in with the Azure CLI such as `az login`.
  </Callout>

  ## Create a Python environment

  Create a new directory for your project and navigate to it from a terminal or command prompt.

  ```Bash
  mkdir whisper-quickstart; cd whisper-quickstart
  ```

  Create and activate a virtual environment for this project.

  <Tabs>
    <Tab title="Windows">
      ```powershell
      python -m venv .venv
      .venv\Scripts\activate
      ```
    </Tab>

    <Tab title="macOS/Linux">
      ```Bash
      python3 -m venv .venv
      source .venv/bin/activate
      ```
    </Tab>
  </Tabs>

  Install the OpenAI Python client library with:

  <Tabs>
    <Tab title="OpenAI Python 1.x">
      ```console
      pip install openai
      ```
    </Tab>

    <Tab title="OpenAI Python 0.28.1">
      <Callout type="note">
        The OpenAI Python library version `0.28.1` is deprecated. We recommend using `1.x`. Consult our [migration guide](how-to/migration) for information on moving from `0.28.1` to `1.x`.
      </Callout>

      ```console
      pip install openai==0.28.1
      ```
    </Tab>
  </Tabs>

  ## Create the Python app

  1. Create a new Python file called *quickstart.py*. Then open it up in your preferred editor or IDE.

  2. Replace the contents of *quickstart.py* with the following code. Modify the code to add your deployment name:

  <Tabs>
    <Tab title="OpenAI Python 1.x">
      ```python
          import os
          from openai import AzureOpenAI

          client = AzureOpenAI(
              api_key=os.getenv("AZURE_OPENAI_API_KEY"),
              api_version="2024-02-01",
              azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
          )

          deployment_id = "YOUR-DEPLOYMENT-NAME-HERE" #This will correspond to the custom name you chose for your deployment when you deployed a model."
          audio_test_file = "./wikipediaOcelot.wav"

          result = client.audio.transcriptions.create(
              file=open(audio_test_file, "rb"),
              model=deployment_id
          )

          print(result)
      ```
    </Tab>

    <Tab title="OpenAI Python 0.28.1">
      ```python
          import openai
          import time
          import os

          openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")
          openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")  # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
          openai.api_type = "azure"
          openai.api_version = "2024-02-01"

          model_name = "whisper"
          deployment_id = "YOUR-DEPLOYMENT-NAME-HERE" #This will correspond to the custom name you chose for your deployment when you deployed a model."
          audio_language="en"

          audio_test_file = "./wikipediaOcelot.wav"

          result = openai.Audio.transcribe(
                      file=open(audio_test_file, "rb"),
                      model=model_name,
                      deployment_id=deployment_id
                  )

          print(result)
      ```
    </Tab>
  </Tabs>

  Run the application using the `python` command on your quickstart file:

  ```python
  python quickstart.py
  ```

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```python
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-dotnet">
  ## Prerequisites

  * An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).
  * [The .NET 8.0 SDK](https://dotnet.microsoft.com/en-us/download)

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Setup

  1. Create a new folder `whisper-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir whisper-quickstart && cd whisper-quickstart
     ```

  2. Create a new console application with the following command:

     ```shell
     dotnet new console
     ```

  3. Install the [OpenAI .NET client library](https://www.nuget.org/packages/Azure.AI.OpenAI/) with the [dotnet add package](https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-add-package) command:

     ```console
     dotnet add package Azure.AI.OpenAI
     ```

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the [Azure.Identity](https://www.nuget.org/packages/Azure.Identity) package with:

     ```console
     dotnet add package Azure.Identity
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
      ```
    </Tab>

    <Tab title="API key">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));
      ```
    </Tab>
  </Tabs>

  <Callout type="note">
    You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).
  </Callout>

  To run the quickstart, follow these steps:

  1. Replace the contents of `Program.cs` with the following code and update the placeholder values with your own.

     ```csharp
     using Azure;
     using Azure.AI.OpenAI;
     using Azure.Identity; // Required for Passwordless auth


     string deploymentName = "whisper";

     string endpoint = Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT") ?? "https://<your-resource-name>.openai.azure.com/";
     string key = Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY") ?? "<your-key>";

     // Use the recommended keyless credential instead of the AzureKeyCredential credential.
     AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
     //AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));

     var audioFilePath = "<audio file path>"

     var audioClient = openAIClient.GetAudioClient(deploymentName);

     var result = await audioClient.TranscribeAudioAsync(audioFilePath);

     Console.WriteLine("Transcribed text:");
     foreach (var item in result.Value.Text)
     {
         Console.Write(item);
     }
     ```

  2. Run the application using the `dotnet run` command or the run button at the top of Visual Studio:

     ```dotnetcli
     dotnet run
     ```

  ## Output

  If you are using the sample audio file, you should see the following text printed out in the console:

  ```text
  The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States,
  Mexico, and Central and South America. This medium-sized cat is characterized by solid
  black spots and streaks on its coat, round ears...
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Setup

  1. Create a new folder `synthesis-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir synthesis-quickstart && cd synthesis-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  4. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Create a sample application

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { createReadStream } = require("fs");
         const { AzureOpenAI } = require("openai");
         const { DefaultAzureCredential, getBearerTokenProvider } = require("@azure/identity");

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient() {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = "2024-08-01-preview";
         const deploymentName = "whisper";

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  ## Output

  ```json
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [TypeScript](https://www.typescriptlang.org/download/)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `whisper-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir whisper-quickstart && cd whisper-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```shell
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  5. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Create a sample application

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";
         import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      5. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { createReadStream } from "fs";
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const audioFilePath = "<audio file path>";
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-08-01-preview";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "whisper";

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }

         export async function main() {
           console.log("== Transcribe Audio Sample ==");

           const client = getClient();
           const result = await client.audio.transcriptions.create({
             model: "",
             file: createReadStream(audioFilePath),
           });

           console.log(`Transcription: ${result.text}`);
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  ## Output

  ```json
  {"text":"The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs."}
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-powershell">
  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [You can use either the latest version, PowerShell 7, or Windows PowerShell 5.1.](https://aka.ms/installpowershell)
  * An Azure OpenAI resource with a speech to text model deployed in a [supported region](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Set up

  ### Retrieve key and endpoint

  To successfully make a call against Azure OpenAI, you need an *endpoint* and a *key*.

  | Variable name           | Value                                                                                                                                                                                                                                                                                          |
  | ----------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `AZURE_OPENAI_ENDPOINT` | The service endpoint can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. Alternatively, you can find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | `AZURE_OPENAI_API_KEY`  | This value can be found in the **Keys & Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                                                                                                            |

  Go to your resource in the Azure portal. The **Endpoint and Keys** can be found in the **Resource Management** section. Copy your endpoint and access key as you'll need both for authenticating your API calls. You can use either `KEY1` or `KEY2`. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot of the overview UI for an Azure OpenAI resource in the Azure portal with the endpoint & access keys location circled in red.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```CMD
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      ```

      ```CMD
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      ```

      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```Bash
      echo export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE" >> /etc/environment && source /etc/environment
      ```

      ```Bash
      echo export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE" >> /etc/environment && source /etc/environment
      ```
    </Tab>
  </Tabs>

  ## Create a PowerShell app

  Run the following command. You need to replace `YourDeploymentName` with the deployment name you chose when you deployed the Whisper model. The deployment name isn't necessarily the same as the model name. Entering the model name results in an error unless you chose a deployment name that is identical to the underlying model name.

  ```powershell
  # Azure OpenAI metadata variables
  $openai = @{
      api_key     = $Env:AZURE_OPENAI_API_KEY
      api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
      api_version = '2024-02-01' # this may change in the future
      name        = 'YourDeploymentName' #This will correspond to the custom name you chose for your deployment when you deployed a model.
  }

  # Header for authentication
  $headers = [ordered]@{
      'api-key' = $openai.api_key
  }

  $form = @{ file = get-item -path './wikipediaOcelot.wav' }

  # Send a completion call to generate an answer
  $url = "$($openai.api_base)/openai/deployments/$($openai.name)/audio/transcriptions?api-version=$($openai.api_version)"

  $response = Invoke-RestMethod -Uri $url -Headers $headers -Form $form -Method Post -ContentType 'multipart/form-data'
  return $response.text
  ```

  You can get sample audio files, such as *wikipediaOcelot.wav*, from the [Azure Speech in Foundry Tools SDK repository at GitHub](https://github.com/Azure-Samples/cognitive-services-speech-sdk/tree/master/sampledata/audiofiles).

  <Callout type="important">
    For production, store and access your credentials using a secure method, such as [The PowerShell Secret Management with Azure Key Vault](https://learn.microsoft.com/en-us/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information, see [credential security](https://learn.microsoft.com/en-us/azure/ai-services/security-features).
  </Callout>

  ## Output

  ```text
  The ocelot, Lepardus paradalis, is a small wild cat native to the southwestern United States, Mexico, and Central and South America. This medium-sized cat is characterized by solid black spots and streaks on its coat, round ears, and white neck and undersides. It weighs between 8 and 15.5 kilograms, 18 and 34 pounds, and reaches 40 to 50 centimeters 16 to 20 inches at the shoulders. It was first described by Carl Linnaeus in 1758. Two subspecies are recognized, L. p. paradalis and L. p. mitis. Typically active during twilight and at night, the ocelot tends to be solitary and territorial. It is efficient at climbing, leaping, and swimming. It preys on small terrestrial mammals such as armadillo, opossum, and lagomorphs.
  ```
</ZonePivot>

<Callout type="note">
  For information about other audio models that you can use with Azure OpenAI, see [Audio models](../foundry-models/concepts/models-sold-directly-by-azure?tabs=standard-audio).
</Callout>

<Callout type="tip">
  The file size limit for the Whisper model is 25 MB. If you need to transcribe a file larger than 25 MB, you can use the Azure Speech in Foundry Tools [batch transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-create#use-a-whisper-model) API.
</Callout>

## Clean up resources

If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

* [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
* [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

## Next steps

* To learn how to convert audio data to text in batches, see [Create a batch transcription](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/batch-transcription-create).
* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).


<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this article, you learn how to use the Foundry portal to deploy a Foundry Model in a Foundry resource for inference. Foundry Models include models such as Azure OpenAI models, Meta Llama models, and more. After you deploy a Foundry Model, you can interact with it in the Foundry Playground and use it from code.

This article uses a Foundry Model from partners and community `Llama-3.2-90B-Vision-Instruct` for illustration. Models from partners and community require that you subscribe to Azure Marketplace before deployment. On the other hand, Foundry Models sold directly by Azure, such as Azure OpenAI in Foundry Models, don't have this requirement. For more information about Foundry Models, including the regions where they're available for deployment, see [Foundry Models sold directly by Azure](../concepts/models-sold-directly-by-azure) and [Foundry Models from partners and community](../concepts/models-from-partners).

## Prerequisites

To complete this article, you need:

* An Azure subscription with a valid payment method. If you don't have an Azure subscription, create a [paid Azure account](https://azure.microsoft.com/pricing/purchase-options/pay-as-you-go) to begin. If you're using [GitHub Models](https://docs.github.com/en/github-models/), you can [upgrade to Foundry Models](quickstart-github-models) and create an Azure subscription in the process.

* The **Cognitive Services Contributor** role or equivalent permissions on the Foundry resource to create and manage deployments. For more information, see [Azure RBAC roles](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles).

* A [Microsoft Foundry project](../../how-to/create-projects). This kind of project is managed under a Foundry resource.

* [Foundry Models from partners and community](../concepts/models-from-partners) require access to **Azure Marketplace** to create subscriptions. Ensure you have the [permissions required to subscribe to model offerings](configure-marketplace). [Foundry Models sold directly by Azure](../concepts/models-sold-directly-by-azure) don't have this requirement.

## Deploy a model

Deploy a model by following these steps in the Foundry portal:

1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.

   ![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/new-foundry.png)

2. From the Foundry portal homepage, select **Discover** in the upper-right navigation, then **Models** in the left pane.

3. Select a model and review its details in the model card. This article uses `Llama-3.2-90B-Vision-Instruct` for illustration.

4. Select **Deploy** > **Custom settings** to customize your deployment. Alternatively, you can use the default deployment settings by selecting **Deploy** > **Default settings**.

5. For [Foundry Models from partners and community](../concepts/models-from-partners), you need to subscribe to Azure Marketplace. This requirement applies to `Llama-3.2-90B-Vision-Instruct`, for example. Read the terms of use and select **Agree and Proceed** to accept the terms.

   <Callout type="note">
     For [Foundry Models sold directly by Azure](../concepts/models-sold-directly-by-azure), such as the Azure OpenAI model `gpt-4o-mini`, you don't subscribe to Azure Marketplace.
   </Callout>

6. Configure the deployment settings:

   * By default, the deployment uses the model name. You can modify this name before deploying.
   * During inference, the deployment name is used in the `model` parameter to route requests to this particular deployment.

   Select **Deploy** to create your deployment.

   <Callout type="tip">
     Each model supports different deployment types, providing different data residency or throughput guarantees. See [deployment types](../concepts/deployment-types) for more details. In this example, the model supports the Global Standard deployment type.
   </Callout>

7. When the deployment completes, you land on the [Foundry Playgrounds](../../concepts/concept-playgrounds) where you can interactively test the model. Your project and resource must be in one of the supported regions of deployment for the model. Verify that the deployment status shows **Succeeded** in your deployment list.

## Manage models

You can manage the existing model deployments in the resource by using the Foundry portal.

1. Select **Build** in the upper-right navigation.

2. Select **Models** in the left pane to see the list of deployments in the resource.

From a deployment's detail page, you can view endpoint details and keys, adjust deployment settings, or delete a deployment that you no longer need.

## Test the deployment in the playground

You can interact with the new model in the Foundry portal by using the playground. The playground is a web-based interface that lets you interact with the model in real-time. Use the playground to test the model with different prompts and see the model's responses.

1. From the list of deployments, select the **Llama-3.2-90B-Vision-Instruct** deployment to open up the playground page.

2. Type your prompt and see the outputs.

3. Select the **Code** tab to see details about how to access the model deployment programmatically.

## Use the model with code

To run inference on the deployed model, see the following examples:

* To use the **Responses API with Foundry Models sold directly by Azure**, such as Microsoft AI, DeepSeek, and Grok models, see [How to generate text responses with Microsoft Foundry Models](generate-responses).

* To use the **Responses API with OpenAI models**, see [Getting started with the responses API](../../openai/how-to/responses).

* To use the **Chat completions API with models sold by partners**, such as the Llama model deployed in this article, see [Model support for chat completions](../../openai/api-version-lifecycle#model-support).

## Regional availability and quota limits of a model

For Foundry Models, the default quota varies by model and region. Certain models might only be available in some regions. For more information on availability and quota limits, see [Azure OpenAI in Microsoft Foundry Models quotas and limits](../../openai/quotas-limits) and [Microsoft Foundry Models quotas and limits](../quotas-limits).

## Quota for deploying and running inference on a model

For Foundry Models, deploying and running inference consume quota that Azure assigns to your subscription on a per-region, per-model basis in units of Tokens-per-Minute (TPM). When you sign up for Foundry, you receive default quota for most of the available models. Then, you assign TPM to each deployment as you create it, which reduces the available quota for that model. You can continue to create deployments and assign them TPMs until you reach your quota limit.

When you reach your quota limit, you can only create new deployments of that model if you:

* Request more quota by submitting a [quota increase form](https://aka.ms/oai/stuquotarequest).
* Adjust the allocated quota on other model deployments in the Foundry portal, to free up tokens for new deployments.

For more information about quota, see [Microsoft Foundry Models quotas and limits](../quotas-limits) and [Manage Azure OpenAI quota](../../openai/how-to/quota?tabs=rest).

## Troubleshooting

| Issue                              | Resolution                                                                                                     |
| ---------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| Quota exceeded                     | [Request more quota](https://aka.ms/oai/stuquotarequest) or reallocate TPM from existing deployments.          |
| Region not supported               | Check [regional availability](../quotas-limits) and deploy in a supported region.                              |
| Marketplace subscription error     | Verify you have the [required permissions](configure-marketplace) to subscribe to Azure Marketplace offerings. |
| Deployment status shows **Failed** | Confirm that the model is available in your selected region and that you have sufficient quota.                |

## Related content

* [How to generate text responses with Microsoft Foundry Models](generate-responses)
* [Deployment types for Foundry Models](../concepts/deployment-types)
* [Deploy models using Azure CLI and Bicep](create-model-deployments)
* [Azure OpenAI supported programming languages](../../openai/supported-languages)

[Microsoft Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-foundry) provides a unified platform for enterprise AI operations, model builders, and application development. This foundation combines production-grade infrastructure with friendly interfaces, ensuring organizations can build and operate AI applications with confidence.

With Foundry, you can:

* Deploy the latest language models from Microsoft, OpenAI, Meta, DeepSeek, and more using the robust model catalog

* Test the deployed models in a model playground

* Quickly get started with developing generative AI applications using a collection of Azure curated code templates

* Configure and deploy agents with Foundry Agent Service

With the Foundry for Visual Studio Code extension, you can accomplish much of this workflow directly from Visual Studio Code. It also comes with other features, such as code templates, playgrounds, and integration with other VS Code extensions and features.

This article shows you how to quickly get started using the features of the Foundry for Visual Studio Code extension.

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

## Prerequisites

Before using the Foundry for Visual Studio Code extension, you must:

* Download, install, and configure Visual Studio Code. More information: [Download Visual Studio Code](https://code.visualstudio.com/Download)

* Your subscription needs to be below your [quota limit](../quota) to [deploy a new model in this quickstart](#deploy-a-model-from-the-model-catalog). Otherwise you already need to have a [deployed chat model](../../foundry-models/how-to/deploy-foundry-models).

* Set the appropriate RBAC permissions to create and manage Foundry resources with the Visual Studio Code extension. For more information, see [Role-based access control for Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/rbac-foundry).

## Installation

After you install Visual Studio Code, you need to install the Foundry for Visual Studio Code extension.

To install the Foundry for Visual Studio Code extension, either use the Visual Studio Code Marketplace or install it directly from within Visual Studio Code.

### Install from the Visual Studio Code Marketplace

To install the Foundry for Visual Studio Code extension from the Visual Studio Code Marketplace, follow these steps:

1. Open the [Foundry for Visual Studio Code extension page](https://marketplace.visualstudio.com/items?itemName=TeamsDevApp.vscode-ai-foundry).
2. Select the **Install** button.
3. Follow the prompts to install the extension in Visual Studio Code.
4. After installation, open Visual Studio Code and verify the extension is installed successfully from the status messages.
5. The extension should now be visible in the primary navigation bar on the left side of Visual Studio Code.

### Install from within Visual Studio Code

To install the Foundry for Visual Studio Code extension in Visual Studio Code, follow these steps:

1. Open Visual Studio Code.

2. Select **Extensions** from the left pane.

3. Select the **Settings** icon from the top-right on the extensions pane.

4. Search for and select **Foundry**.

5. Select **Install**.

6. Verify the extension is installed successfully from the status messages.

## Get started

Get started with the Foundry extension by using the following steps.

<Callout type="note">
  For a full list of features available in the extension, use the Command Palette. Select `F1` to open the command palette and search **Foundry**. The following screenshot shows some of the actions for Foundry. ![A screenshot of the Visual Studio Code command palette for Foundry.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/visual-studio-command-palette-small.png)
</Callout>

### Sign in to your resources

Sign in to your Azure subscription to access your resources with the following steps:

1. Select the Azure Icon on the VS Code Navbar.

2. Sign in by selecting the `Sign in to Azure...` item in the **Azure Resources** view.

3. Under the "Resources" section, select your Azure Subscription and Resource Group.

4. Select **Foundry** and right-click your project.

5. Select **Open in Foundry Extension**.

### Explore the Foundry Extension

The Foundry Extension opens in its own view, with the Foundry Icon now displayed on the VS Code Navbar. The extension has three main sections: **Resources**, **Tools**, and **Help and Feedback**.

![A screenshot of the Foundry Extension with highlighted sections.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/initial-view.png)

* **Resources**: This section contains the resources you have access to in your Foundry project. The **Resources** section is the main view for interacting with your Foundry Services. It contains the following subsections:

  * **Models**: This section contains the models you can use to build and deploy your AI applications. The **Models** view is where you can find your deployed models in your Foundry project.

  * **Declarative Agents**: This section contains your deployed declarative agents in your Foundry project.

  * **Hosted Agents (Preview)**: This section contains your deployed hosted agents in your Foundry project.

  * **Assets**: This section contains the assets you have in your Foundry project.

    * Connections: This subsection contains the connections you have in your Foundry project. for example, Bing Grounding connections.
    * Vector Stores: This subsection contains the vector stores you have in your Foundry project.

  * **Classic**: This section contains the agents built in your classic Foundry projects.

* **Tools**: This section contains the tools you can use to build and deploy your AI applications. The **Tools** view is where you can find the tools available to deploy and then work with your deployed models and agents. It contains the following subsections:

  * **Model Catalog**: The link to the model catalog you can use to discover and deploy models.
  * **Model Playground**: The link to the model playground for interacting with your deployed models in your Foundry project.
  * **Remote Agent Playground**: The link to the agent playground for interacting with your deployed remote agents in your Foundry project.
  * **Local Agent Playground**: The link to the agent playground for interacting with your deployed local agents in your Foundry project.
  * **Local Visualizer**: The link to the local visualizer to visualize the interactions between agents and how they collaborate in your Foundry project.
  * **Deploy Hosted Agents**: The link to deploy a new hosted agent using a dockerfile in your Foundry project.

* **Help and Feedback**: This section contains links to the Foundry documentation, feedback, support, and the Microsoft Privacy Statement. It contains the following subsections:

  * **Documentation**: The link to the Foundry Extension documentation.
  * **GitHub**: The link to the Foundry extension GitHub repository.
  * **Microsoft Privacy Statement**: The link to the Microsoft Privacy Statement.
  * **Join the Foundry Community: Discord + Forum**: The link to the Foundry community Discord server.

<Callout type="note">
  To learn more about working with Agents in the Foundry Extension, see the [Work with Agent Service in Visual Studio Code](vs-code-agents) article.
</Callout>

## Create a project

You can create a new Foundry project from the Foundry Extension view with the following steps:

1. Select the **plus** icon next to **Resources** in the **Resources** section of the Foundry Extension view.

   You can either create a new resource group or select an existing one.

2. To create a new resource group:

   1. In the top center, select **Create new resource group** and press Enter.

   2. In the top center, enter the Azure Resource Group name to use in the **Enter new resource group** textbox and press Enter.

   3. In the top center, select the location you want to use from the list of available locations and press Enter.

3. To use an existing resource group:

   * In the top center, select the resource group you want to use from the list of available resource groups and press Enter.

4. In the top center, enter the Foundry Project name to use in the **Enter project name** textbox and press Enter.

After project deployment, a popup appears with the message **Project deployed successfully**.

To deploy a model to the newly created project, select the **Deploy a model** button in the popup. This action opens the **Model Catalog** page in the Foundry Extension view to select the desired model to [deploy.](#deploy-a-model-from-the-model-catalog)

### The default Foundry Project

When you open a project in the Foundry Extension, that project is set as your default project.

Switch your default project by following these steps:

1. Right-click on the Foundry Project and select the **Switch Default Project in Azure Extension** option.

2. In the top center, select the Foundry Project you want to use from the list of available projects and press Enter.

Your selected project will now display **Default** after the project name.

<Callout type="tip">
  Right-click on your project name to access the project endpoint and/or the project API key.
</Callout>

## Work with models

The Foundry for Visual Studio Code extension enables you to create, interact with, and deploy Large Language Models from within Visual Studio Code.

### Explore all models with the model catalog

The [model catalog](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/model-catalog-overview) in Foundry portal is the hub to discover and use a wide range of models for building generative AI applications.

Access the model catalog from several different ways:

* The **Foundry: Open Model Catalog** command palette command.
* Select the **plus** icon next to **Models** in the **Resources** section of the Foundry Extension view.
* Select the **Model Catalog** link in the **Tools** section of the Foundry Extension view.

#### Open the model catalog from the command palette

Access the model catalog from the command palette to explore and deploy a curated selection of models available in Foundry, right from inside VS Code.

1. Select `F1` to open the command palette.

2. Enter **Foundry: Open Model Catalog** and press Enter.

3. The **Model Catalog** page is displayed.

   ![Screenshot of the \*\*Model Catalog\*\* page in VS Code.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/display-model-catalog.png)

4. Filter the **Model Catalog** by `Hosted by`,`Publisher`, `Feature` and/or `Model type` using the dropdowns at the top-left of the page. Select `Fine-Tuning Support` using the toggle button to filter models that support fine-tuning.

5. Search for a specific model using the search bar at the top-center of the page.

#### Open the model catalog from the Resources section

The **Model Catalog** is also available in the **Resources** section of the Foundry Extension view.

In the Foundry Extension view, select the **plus** icon next to **Models** to open the Model Catalog.

<Callout type="tip">
  You can also right-click on **Models** and select the **Deploy new AI model** option to open the Model Catalog to start the deployment process.
</Callout>

#### Open the model catalog from the Tools section

The **Model Catalog** is also available in the **Tools** section of the Foundry Extension view. Double-click on the **Model Catalog** link to open the Model Catalog.

### Deploy a model from the model catalog

Deploy a selected model in the model catalog using the following steps:

1. Select the **Deploy** button immediately following the selected model name.

2. The **Model deployment** page is displayed.

3. Enter the model deployment name to use in the **Enter deployment name** textbox and press Enter.

4. Select the deployment type to use in the **Deployment type** dropdown and press Enter.

5. Select the model version to use in the **Model version** dropdown and press Enter.

6. (Optional) Select the tokens per minute to use in the **Tokens per minute** slider and press Enter.

7. In the bottom-left corner, select the **Deploy in Foundry** button to deploy the model.

8. A confirmation dialog box appears. Select the **Deploy** button to deploy the model to your project.

9. After a successful deployment, your model will be listed by deployment name with your other deployed models under the **Models** section in your project.

### View deployed models

In the Azure Resources Extension view, select the **caret** icon in front of the **Models** section to view the list of deployed models.

The expanded **Models** section displays the list of deployed models.

#### View model card information

Selecting a deployed model opens up a panel that provides some basic information:

* Deployment Info: This section contains the information about the model:

  * Name
  * Provisioning state
  * Deployment type
  * Rate limit information
  * Version info
  * Model name
  * Model version

* Endpoint info: This section contains the Target URI link, authentication type, and key.

* Useful links: This section contains the code sample repository and tutorial links to get started with AI application development.

#### Update a model

To update the model card information, select the **Edit** button on the top-right of the model card.

Update the desired editable fields such as rate limit directly within VS Code.

To save the changes, select the **Save** button on the top-right of the model card.

### Explore the model sample code

Create a sample code file using the following steps.

1. Right-click on your deployed model and select the **Open code file** option.

2. In the top center, select your preferred SDK to use in the **Choose preferred SDK** dropdown and press Enter.

3. In the top center, select your preferred language to use in the **Choose language** dropdown and press Enter.

4. In the top center, select your preferred authentication method to use in the **Choose authentication method** dropdown and press Enter.

5. A sample code file is generated and opened in a new tab in VS Code.

#### Sample code file

This Python sample code file demonstrates a basic call to the responses API. The call is synchronous:

![Screenshot of generated sample code file.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/sample-code-file.png)

### Interact with models using a model playground

Chat interactively with the model, change settings, and system instructions using the **Model Playground**.

The **Model Playground** is available in the **Tools** section of the Foundry Extension view. Double-click on the **Model Playground** link to open the Model Playground.

You can also open the model playground using the following steps:

1. Right-click on your deployed model and select the **Open in playground** option.

2. The **Playground** page is displayed.

3. Type your prompt and see the outputs.

4. Additionally, you can use **View code** in the top-right corner to see details about how to access the model deployment programmatically.

5. Select the **History** link at the top-left of the playground to view the chat history.

## Cleanup resources

The Azure resources that you created in this article are billed to your Azure subscription. If you don't expect to need these resources in the future, delete them to avoid incurring more charges.

### Delete your models

1. In the VS Code navbar, refresh the **Foundry Extension**. In the **Resources** section, expand the **Models** subsection to display the list of deployed models.

2. Right-click on your deployed model to delete and select the **Delete** option.

### Delete your tools

Delete the connected tool with the following steps:

1. Open the Azure portal
2. Select the Azure Resource Group containing the tool.
3. Select the **Delete** button.

## Next steps

* Learn about [working with the Agent Service](vs-code-agents) using this Visual Studio Code extension.

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Find, explore, and deploy AI solution templates from the Foundry portal.

Streamline your code-first development with prebuilt, task-specific Azure AI templates. These ready-to-use, customizable templates help you skip setup, reduce friction, and deliver value faster with trusted, scalable infrastructure. Built on insights from over 2,000 customer engagements, AI solution templates significantly reduce time from concept to production at scale.

AI solution templates include customizable code samples, pre-integrated Azure services, and GitHub-hosted quick-start guides. Development teams can focus on outcomes instead of setup, building solutions for popular use cases like live voice agents, release management, and data unification.

These templates harness the power of multi-agent, agentic AI, enabling you to automate workflows, optimize operations, reduce costs, and make faster, data-driven decisions.

<Callout type="important">
  Starter templates, manifests, code samples, and other resources made available by Microsoft or its partners ("samples") are designed to assist in accelerating development of agents and AI solutions for specific scenarios. Review all provided resources and carefully test output behavior in the context of your use case. AI responses might be inaccurate and AI actions should be monitored with human oversight. Learn more in the transparency documents for [Agent Service](../../responsible-ai/agents/transparency-note) and [Agent Framework](https://github.com/microsoft/agent-framework/blob/main/TRANSPARENCY_FAQ.md).

  Agents and AI solutions you create might be subject to legal and regulatory requirements, might require licenses, or might not be suitable for all industries, scenarios, or use cases. By using any sample, you acknowledge that Agents, AI solutions, or other output created using those samples are solely your responsibility, and that you will comply with all applicable laws, regulations, and relevant safety standards, terms of service, and codes of conduct.
</Callout>

Available templates:

| Template                                                                                                                                         | Popular customer use cases                                                                                                                                                                                                             |
| ------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Get started with AI chat](https://github.com/Azure-Samples/get-started-with-ai-chat)                                                            | Build interactive chat applications [Baseline Microsoft Foundry chat reference architecture](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/architecture/baseline-azure-ai-foundry-chat)                                   |
| [Get started with AI agents](https://github.com/Azure-Samples/get-started-with-ai-agents)                                                        | Create autonomous AI agents                                                                                                                                                                                                            |
| [Build agentic apps to unify data](https://github.com/microsoft/agentic-applications-for-unified-data-foundation-solution-accelerator/tree/main) | Embed analytics across applications with instant data visualization for: • Sales performance analysis • Customer insights and reporting • Natural language analysis on structured data                                                 |
| [Create a multi-agent release manager assistant](https://github.com/Azure-Samples/openai/tree/main/Agent_Based_Samples/release_manager)          | Enable AI-powered release planning for: • Cross-system release coordination • Real-time dependency mapping and release health assessment • Advanced visualization for retrieved insights • Safe update mechanisms built into AI agents |
| [Create a call center voice agent](https://github.com/Azure-Samples/call-center-voice-agent-accelerator)                                         | Develop interactive voice agents for: • Customer support • Product catalog navigation • Self-service solutions                                                                                                                         |
| [Unlock insights from conversational data](https://github.com/microsoft/Conversation-Knowledge-Mining-Solution-Accelerator)                      | Extract knowledge from conversations [Build a conversation knowledge mining solution](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/idea/unlock-insights-from-conversational-data)                                        |
| [Multi-agent workflow automation](https://github.com/microsoft/Multi-Agent-Custom-Automation-Engine-Solution-Accelerator)                        | Automate complex workflows [Build a multiple-agent workflow automation solution](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/idea/multiple-agent-workflow-automation)                                                   |
| [Multi-modal content processing](https://github.com/microsoft/content-processing-solution-accelerator)                                           | Process diverse content types [Extract and map information from unstructured content](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/idea/multi-modal-content-processing)                                                  |
| [Generate documents from your data](https://github.com/microsoft/document-generation-solution-accelerator)                                       | Create documents automatically [Build a document generation system](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/idea/generate-documents-from-your-data)                                                                 |
| [Improve client meetings with agents](https://github.com/microsoft/Build-your-own-copilot-Solution-Accelerator)                                  | Enhance meeting productivity                                                                                                                                                                                                           |
| [Modernize your code with agents](https://github.com/microsoft/Modernize-your-code-solution-accelerator)                                         | Update legacy code                                                                                                                                                                                                                     |
| [Build your conversational agent](https://github.com/Azure-Samples/Azure-Language-OpenAI-Conversational-Agent-Accelerator)                       | Create conversational experiences                                                                                                                                                                                                      |
| [Retrieve and summarize SharePoint data](https://github.com/microsoft/app-with-sharepoint-knowledge)                                             | Retrieve content from SharePoint sites                                                                                                                                                                                                 |

<Callout type="tip">
  Each template includes a GitHub README with setup, deployment, and customization instructions. Start there for the fastest path forward.
</Callout>

## Prerequisites

* An Azure account with an active subscription. If you don't have one, create a [free Azure account, which includes a free trial subscription](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* Appropriate RBAC role to create Foundry resources. For details, see [Role-based access control](../../concepts/rbac-foundry).

- A [Foundry project](../create-projects).

## Benefits of AI solution templates

AI templates in Microsoft Foundry provide:

* **Faster time-to-value**: Skip boilerplate code and infrastructure setup to move from concept to production quickly.
* **Reduced engineering overhead**: Preintegrated Azure services eliminate deployment friction.
* **Trusted infrastructure**: Build with confidence on Microsoft's secure, scalable AI platform.
* **Modular and interoperable foundation**: Scale solutions efficiently across your organization.
* **Best practices built-in**: Leverage proven patterns and frameworks from thousands of customer engagements.

## Start with a sample application

1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/new-foundry.png)
2. Select **Discover** from the upper-right navigation.
3. Select **Solution templates** from the left pane.
4. Select **Open in Github** to view the entire sample application.
5. In some cases, you can also view a step-by-step tutorial that explains the AI code.

## Explore the sample application

When you view the GitHub repository for your sample, check the README for more instructions and information on how to deploy your own version of the application.

Instructions vary by sample, but most include how to:

* Open the solution in the location of your choice:

  * GitHub Codespaces
  * VS Code Dev Containers
  * Your local IDE

* Deploy the application to Azure

* Test the application

The README also includes information about the application, such as the use case, architecture, and pricing information.

## Deploy templates quickly

By using the quick-deploy options, you can launch any Foundry template in minutes. These architectures and implementations are customizable while staying Well-Architected Framework (WAF) aligned by using Azure Verified Modules. Use tools such as PSRule and TFlint to test that your modified implementation is production-ready.

## Next step

* [Quickstart: Get started with Foundry](../../quickstarts/get-started-code)

After you [get started with the Microsoft Foundry for Visual Studio Code extension](get-started-projects-vs-code), use [Foundry Agent Service](https://learn.microsoft.com/en-us/azure/ai-services/agents/overview) to build agents. Agents are microservices that:

* Answer questions by using their training data or search other sources with retrieval-augmented generation (RAG).
* Perform specific actions.
* Automate complete workflows.

Agents combine AI models with tools to access and interact with your data.

Foundry developers can stay productive by developing, testing, and deploying agents in the familiar environment of Visual Studio Code (VS Code).

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

## Create and edit an Azure AI agent within the designer view

Follow these steps to create an Azure AI agent:

1. [Sign in to your Azure resources](get-started-projects-vs-code#sign-in-to-your-resources).

2. [Set your default project](get-started-projects-vs-code#create-a-project).

3. [Deploy a model](get-started-projects-vs-code#deploy-a-model-from-the-model-catalog) to use with your agent.

4. In the **Foundry Extension** view, find the **Resources** section.

5. Select the plus (**+**) icon next to the **Agents** subsection to create a new AI agent.

   ![Screenshot of the plus sign next to the Agents subsection.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/select-agent-plus.png)

### Interact with your agent in the designer

After you choose your save location, both the agent .yaml file and the designer view open so that you can edit your AI agent. Perform the following tasks in the agent designer:

1. In the prompt, enter a name for your agent.

2. In the dropdown list, select the name of your model deployment. The deployment name is what you chose when you deployed an existing model.

3. The extension generates the **Id** value. Configure the following fields:

   * Add a description for your agent.
   * Set system instructions.
   * Configure tools for agent use.

   ![Screenshot of the agent designer for editing and interacting with an AI agent.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-designer.png)

4. To save the .yaml file, select **File** > **Save** on the VS Code menu bar.

### Explore the Azure AI agent's .yaml definition

Your AI agent's .yaml file was opened at the same time that the designer was. This file contains the details and setup information for your agent. It's similar to the following .yaml file example:

```yaml
# yaml-language-server: $schema=https://aka.ms/ai-foundry-vsc/agent/1.0.0
version: 1.0.0
name: my-agent
description: Description of the agent
id: ''
metadata:
  authors:
    - author1
    - author2
  tags:
    - tag1
    - tag2
model:
  id: 'gpt-4o-1'
  options:
    temperature: 1
    top_p: 1
instructions: Instructions for the agent
tools: []
```

## Add tools to the Azure AI agent

Agent Service has the following set of tools that you can use to interact with your data sources. These tools are available in the Foundry for Visual Studio Code extension.

* [Grounding with Bing search](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/bing-grounding)
* [File search](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/file-search?tabs=python\&pivots=overview)
* [Code interpreter](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/code-interpreter)
* [OpenAPI specified tools](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/openapi-spec)
* [Model Context Protocol (MCP)](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/model-context-protocol)

For more information about using MCP tools, see [Work with Agent Service and MCP server tools in Visual Studio Code (preview)](vs-code-agents-mcp).

### Add a tool to the AI agent

1. In the designer, in the upper-right corner of the **TOOL** section, select **Add tool**. In the dropdown list, select the tool that you want to add.

   ![Screenshot of selections in the agent designer for adding a tool.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-tool-plus.png)

2. The designer displays the appropriate pane to configure the tool, as shown in the following images:

   * Grounding with Bing search:

     ![Screenshot of the pane for the Grounding with Bing search tool.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-bing-tool-dialog.png)

   * File search:

     ![Screenshot of the pane for the file upload tool.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-file-search-tool-dialog.png)

   * Code interpreter:

     ![Screenshot of the pane for the code interpreter tool.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-ci-tool-dialog.png)

   * OpenAPI 3.0 specified tools:

     ![Screenshot of the pane for the OpenAPI 3.0 specified tools.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-openapi-tool-dialog.png)

3. After you enter the required information, select **Create and connect**, **Upload and save**, or **Create Tool**. The button varies according to the pane.

When you add a tool, you can also add any new assets that it needs. For example, if you add a file search tool, you can use an existing vector store asset or make a new asset for your vector store to host your uploaded files.

## Create an Azure AI agent on Foundry

Create your agent directly on Foundry by using the following steps:

1. In the designer, select the **Create Agent on Foundry** button.

   ![Screenshot of the agent designer with the button for creating an agent on Foundry highlighted.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-deploy.png)

2. In VS Code, refresh the **Azure Resources** view. The deployed agent appears in the **Agents** subsection.

   ![Screenshot of a deployed agent in the Azure Resources view.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-deployed.png)

## View the details of the deployed AI agent

Selecting the deployed agent opens the **AGENT PREFERENCES** pane in a view-only mode. You can:

* Select the **Edit Agent** button to view the agent designer and the .yaml definition of the agent for editing.
* Select the **Open Code File** button to create a sample code file that uses the agent.
* Select the **Open Playground** button to open the agent playground.

![Screenshot of the pane for agent preferences, with the Edit Agent, Open Code File, and Open Playground buttons highlighted.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/deployed-agent-view.png)

## Edit and update the deployed AI agent

1. On the **AGENT PREFERENCES** pane, select the **Edit Agent** button. The agent designer opens with the agent's .yaml file.

2. Edit the agent's configuration, such as the model, tools, and instructions.

3. After you finish editing, select the **Update Agent on Foundry** button to save your changes.

![Screenshot of the pane for agent preferences, with the Update Agent on Foundry button highlighted.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/update-agent.png)

## Create a sample code file

1. Right-click your deployed agent, and then select the **Open Code File** option. Or, on the **AGENT PREFERENCES** pane, select the **Open Code File** button.

   ![Screenshot of the agent shortcut menu with the Open Code File option highlighted.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/open-agent-code-file.png)

2. In the **Choose your preferred SDK** dropdown list, select your preferred SDK for the agent code file, and then select the `Enter` key.

   ![Screenshot of the dropdown list for selecting an SDK as part of agent code file selection.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/choose-agent-preferred-sdk.png)

3. In the **Choose a language** dropdown list, select your preferred language for the agent code file, and then select the `Enter` key.

   ![Screenshot of the dropdown list for choosing a language as part of agent code file selection.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/choose-agent-language.png)

4. In the **Choose an auth method** dropdown list, select your preferred authentication method for the agent code file, and then select the `Enter` key.

   ![Screenshot of the dropdown list for choosing an authentication method as part of agent code file selection.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/choose-agent-authn-method.png)

### Explore the sample code file

The following Python sample code file demonstrates a basic call to interact with the agent through the Foundry Projects API.

![Screenshot of a generated agent sample code file.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/sample-agent-code-file.png)

## Interact with agents by using the agent playground

1. Right-click your deployed agent, and then select the **Open Playground** option.

   Alternatively, select the **Agent Playground** link in the **Tools** subsection, and then select your agent from the dropdown list.

   This step opens the **Agent Playground** pane and starts a thread with your agent so that you can send messages.

   ![Screenshot of the agent playground in Visual Studio Code.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-display-playground.png)

2. Enter your prompt and view the outputs.

   This example uses **Bing Grounding** to illustrate a web search for information. The agent uses the model and tools that you configured in the agent designer. The source of the information appears in the section for agent annotations.

   ![Screenshot of the Agent Playground pane with agent annotations highlighted in VS Code.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-playground-run.png)

## Explore threads

The **Threads** subsection displays the threads created during a run with your agent. In the **Azure Resources** view, expand the **Threads** subsection to view the list.

![Screenshot of the threads in the Threads subsection.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/thread-list.png)

Keep these terms in mind as you explore threads:

* A *thread* is a conversation session between an agent and a user. Threads store messages and automatically handle truncation to fit content into a model's context.

* A *message* is a single interaction between the agent and the user. Messages can include text, images, and other files. Messages are stored as a list on the thread.

* A *run* is a single execution of an agent. Each run can have multiple threads, and each thread can have multiple messages. The agent uses its configuration and a thread's messages to perform tasks by calling models and tools. As part of a run, the agent appends messages to the thread.

### View thread details

To view the **THREAD DETAILS** pane, select a thread.

![Screenshot of the pane for thread details.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/thread-view.png)

### View run details

To view run information in a JSON file, select the **View run info** button on the **THREAD DETAILS** pane. The following screenshot shows an example JSON file.

![Screenshot of an example JSON file of run details.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/run-file.png)

## Clean up resources

The Azure resources that you created in this article are billed to your Azure subscription. If you don't expect to need these resources in the future, delete them to avoid incurring more charges.

### Delete your agents

<Callout type="tip">
  Because you can [customize the left pane](../../what-is-foundry#customize-the-left-pane) in the Microsoft Foundry portal, you might see different items than shown in these steps. If you don't see what you're looking for, select **... More** at the bottom of the left pane.
</Callout>

1. In VS Code, refresh the **Azure Resources** view. Expand the **Agents** subsection to display the list of deployed agents.
2. Right-click the deployed agent that you want to delete, and then select **Delete**.

### Delete your models

1. In VS Code, refresh the **Azure Resources** view. Expand the **Models** subsection to display the list of deployed models.

2. Right-click the deployed model that you want to delete, and then select **Delete**.

   ![Screenshot of the shortcut menu with the Delete command for a selected model.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/delete-model.png)

### Delete your connected tools

1. Open the Azure portal.

2. Select the Azure resource group that contains the tool.

3. Select the **Delete** button.

## Related content

* Learn about the tools that you can use with Azure AI agents, such as [file search](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/file-search?tabs=python\&pivots=overview) or [code interpreter](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/code-interpreter?tabs=python\&pivots=overview).

In this article, you learn how to add and use [Model Context Protocol (MCP)](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/model-context-protocol) tools with Azure AI agents by using the [Microsoft Foundry for Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=TeamsDevApp.vscode-ai-foundry).

After you [build an agent in Foundry Agent Service](vs-code-agents) by using this Visual Studio Code (VS Code) extension, you can add MCP tools to your agent.

Using or building an MCP server allows your agent to:

* Access up-to-date information from your APIs and services.
* Retrieve relevant context to enhance the quality of responses from your AI models.

Agents combine AI models with tools to access and interact with your data.

Foundry developers can stay productive by developing, testing, and deploying MCP tool-calling agents in the familiar environment of VS Code.

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

## Create an Azure AI agent within the designer view

To create an Azure AI agent, follow the steps in [Create and edit Azure AI agents within the designer view](vs-code-agents#create-and-edit-an-azure-ai-agent-within-the-designer-view).

## Add an existing MCP server tool to the AI agent

After you create your agent, you can add tools to it, including MCP tools. For more information about available tools, see [Tools for Azure AI agents](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/overview).

You can bring multiple remote MCP servers by adding them as tools. For each tool, you need to provide a unique **Server Label** value within the same agent and a **Server URL** value that points to the remote MCP server.

<Callout type="warning">
  The remote MCP servers that you use with the MCP tool in this article are not from Microsoft. Microsoft doesn't test or verify these servers. For details, see [Considerations for using non-Microsoft services and servers](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/model-context-protocol?branch=pr-en-us-6966#considerations-for-using-non-microsoft-services-and-servers).
</Callout>

To add an existing MCP server tool to your AI agent, follow these steps:

1. In the designer, in the upper-right corner of the **TOOL** section, select the **Add tool** button. In the dropdown list, select the **MCP Server** tool.

2. Find the remote MCP server that you want to connect to, such as the GitHub MCP server. Create or update a Foundry agent with an MCP tool by using the following information:

   1. **Server URL**: The URL of the MCP server; for example, `https://gitmcp.io/Azure/azure-rest-api-specs`.

   2. **Server Label**: A unique identifier of this MCP server to the agent; for example, `fetch_rest_api_docs`.

   ![Screenshot of the dialog for creating an MCP tool.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-mcp-tool-dialog.png)

3. In the **Allowed tools** dropdown list, choose which tools the MCP server can use.

   ![Screenshot of the list of allowed tools in the dialog for creating an MCP tool.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-select-mcp-tool.png)

4. After you enter the required information, select the **Create tool** button.

### Create an Azure AI agent on Foundry

After you add an MCP tool, you can create an agent directly on Foundry by using the following steps:

1. In the designer, select the **Create Agent on Foundry** button.

   ![Screenshot of the agent designer with the button for creating an agent on Foundry highlighted.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-deploy.png)

2. In VS Code, refresh the **Azure Resources** view. The deployed agent appears in the **Agents** subsection.

   ![Screenshot of a deployed agent in the Azure Resources view.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/agent-deployed.png)

## View the details of a deployed AI agent

Selecting the deployed agent opens the **AGENT PREFERENCES** pane in a view-only mode. You can:

* Select the **Edit Agent** button to view the agent designer and the .yaml definition of the agent for editing.
* Select the **Open Code File** button to create a sample code file that uses the agent.
* Select the **Open Playground** button to open the agent playground.

![Screenshot of the pane for agent preferences with the Edit Agent, Open Code File, and Open Playground buttons highlighted.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/deployed-agent-view.png)

### Edit and update the deployed AI agent

1. On the **AGENT PREFERENCES** pane, select the **Edit Agent** button. The agent designer opens with the agent's .yaml file.

2. Edit the agent's configuration, such as the model, tools, and instructions.

3. After you finish editing, select the **Update Agent on Foundry** button to save your changes.

![Screenshot of the pane for agent preferences with the Update Agent on Foundry button highlighted.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/update-agent.png)

### Interact with agents by using the MCP server tool in the agent playground

1. Right-click your deployed agent that has an **MCP Server** tool, and then select the **Open Playground** option. This action starts a thread with your agent so that you can send messages.

2. On the **Agent Playground** pane, enter a prompt such as **Give me an example for creating a container app** and send it.

3. Select the authentication method for the MCP server tool and proceed.

   ![Screenshot of the MCP server tool's authentication prompt in the agent playground.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/mcp-tool-authentication.png)

4. Select the approval preference for the MCP server tool and proceed.

   ![Screenshot of the MCP server tool's approval preference in the agent playground.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/mcp-tool-approval-preference.png)

5. If you chose **Ask every time** for your approval preference, you need to approve or reject the tool call.

   ![Screenshot of the MCP server tool's approval prompt in the agent playground.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/mcp-approve-tool.png)

6. The agent uses the model and the MCP server tools that you configured in the agent designer to retrieve the information. The source of the information appears in the section for agent annotations.

   ![Screenshot of the agent playground with agent annotations highlighted in Visual Studio Code after use of the MCP server tool.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/mcp-tool-response.png)

## Clean up resources

The Azure resources that you created in this article are billed to your Azure subscription. If you don't expect to need these resources in the future, delete them to avoid incurring more charges.

### Delete your agents

<Callout type="tip">
  Because you can [customize the left pane](../../what-is-foundry#customize-the-left-pane) in the Microsoft Foundry portal, you might see different items than shown in these steps. If you don't see what you're looking for, select **... More** at the bottom of the left pane.
</Callout>

1. In the [Foundry portal](https://ai.azure.com/?cid=learnDocs), on the left menu, select **Agents**.

2. Select the agent that you want to delete, and then select **Delete**.

   ![Screenshot of the Foundry portal with the Delete command for a selected agent.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/delete-agent.png)

### Delete your models

1. In VS Code, refresh the **Azure Resources** view. Expand the **Models** subsection to display the list of deployed models.

2. Right-click the deployed model that you want to delete, and then select **Delete**.

   ![Screenshot of the shortcut menu with the Delete command for a selected model.](https://learn.microsoft.com/azure/ai-foundry/media/how-to/get-started-projects-vs-code/delete-model.png)

### Delete your connected tools

1. Open the Azure portal.

2. Select the Azure resource group that contains the tool.

3. Select the **Delete** button.

## Related content

* Learn about the tools that you can use with Azure AI agents, such as [file search](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/file-search?tabs=python\&pivots=overview) or [code interpreter](https://learn.microsoft.com/en-us/azure/ai-services/agents/how-to/tools/code-interpreter?tabs=python\&pivots=overview).

In this article, you learn how to add and use [Foundry Agent workflows](../concepts/workflow) with Azure AI agents by using the [Microsoft Foundry for Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=TeamsDevApp.vscode-ai-foundry).

After you [build an agent in Foundry Agent Service](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/vs-code-agents?view=foundry\&tabs=windows-powershell\&pivots=python\&preserve-view=true) by using this Visual Studio Code (VS Code) extension, you can add workflows to your agent.

Foundry Workflows is a UI-based tool in Foundry that creates declarative, predefined sequences of actions including agents, as in Microsoft Agent Framework Workflows.

Workflows let you build intelligent automation systems that blend AI agents with business processes in a visual way. Traditional single-agent systems struggle to handle complex tasks with many parts. When you orchestrate multiple agents, each with specialized skills or roles, you create systems that are more robust, adaptive, and capable of solving real-world problems together.

## What is a declarative agent?

A declarative agent is an AI agent that operates based on predefined rules, workflows, or configurations instead of explicit programming logic. This approach lets users define what the agent should do and how it should behave through high-level specifications. Declarative agents make it easier to create and manage complex interactions without deep coding knowledge.

## View a declarative agent workflow

For declarative agent workflows start by creating a workflow in [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). The following sections guide you through the steps to view and test a simple workflow that uses an agent to process user input.

1. In Foundry, navigate to your project that contains the workflow you want to work with.
2. Select the **Workflows** tab from the left-hand menu.
3. Select the workflow you want to open in VS Code.

### Open in VS Code - YAML based workflow playground

1. Select the **Build** tab and then select the **YAML** button on the right-hand side.
2. Open the YAML directly from Foundry by selecting the **Open in VS Code for Web** button.
3. This action opens the workflow YAML file in the VS Code for Web editor.
4. View both the YAML definition and the visual representation of the workflow in the editor.
5. Edit the YAML as needed to modify the workflow and see the changes reflected in the visual editor.
6. When you're done, save your changes directly back to Foundry from the VS Code for Web editor by selecting Deploy from the ellipsis menu (...) in the upper right corner.

### Open in VS Code from Foundry

1. In Foundry, navigate to your project that contains the workflow you want to work with.
2. Select the **Workflows** tab from the left-hand menu.
3. Select the workflow you want to open in VS Code.
4. Select the **Build** tab and then select the **Code** button on the right-hand side.
5. Select the **Open in VS Code for the Web** button.
6. This action opens the workflow code file in the VS Code for Web editor.
7. View both the code definition and the visual representation of the workflow in the editor.
8. Edit the YAML as needed to modify the workflow and see the changes reflected in the visual editor.
9. When you're done, save your changes directly back to Foundry from the VS Code for Web editor by selecting Deploy from the ellipsis menu (...) in the upper right corner.

## Test workflow in playground

To test the workflow in the VS Code extension playground, follow these steps:

1. In the "My Resources" section of the VS Code extension, locate and select your Foundry project.
2. Select **Declarative Agents**.
3. Select the version of the workflow you want to test.
4. Right-click the workflow and select **Open in Local Agent Playground**. Alternatively, select the **Local Agent Playground** link in the **Tools** subsection, and then select your agent from the dropdown list. This step opens the **Local Agent Playground** pane and starts a conversation with your agent so that you can send messages.
5. In the input box at the bottom of the **Local Agent Playground** pane, type a message to send to the agent and press **Enter**.
6. Review the agent's response in the conversation.

## Convert YAML based workflow to Agent Framework code

To customize your YAML-based workflows, convert it to Agent Framework code using GitHub Copilot using the following steps:

1. Open the workflow YAML file in VS Code.
2. Select **Generate Code** button on the top-right of the YAML editor.
3. Select **Generate with Copilot** from the dropdown menu.
4. Select the programming language you want to generate the code in (Python, C#, or YAML).
5. In the GitHub Copilot pane that opens, the extension creates the code generation prompts for the selected language.
6. GitHub Copilot generates Agent Framework code based on the YAML definition.
7. GitHub Copilot will ask you if you'd like to run the generated code locally in the VS Code extension's local agent playground.
8. The local visualizer opens, and you can see each step as the agent executes.
9. Review and modify the generated code as needed to fit your requirements.
10. Right-click the generated code file and select **Deploy to Foundry** to deploy the code back to your Foundry project.
11. In Foundry, navigate to your project and verify that the code has been successfully deployed.

In this article, you learn how to add and use [hosted Foundry Agent workflows](../concepts/hosted-agents) with Azure AI agents by using the [Microsoft Foundry for Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=TeamsDevApp.vscode-ai-foundry).

After you [build an agent in Foundry Agent Service](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/develop/vs-code-agents?view=foundry\&tabs=windows-powershell\&pivots=python\&preserve-view=true) by using this Visual Studio Code (VS Code) extension, you can add hosted agent workflows to your agent.

Foundry developers can stay productive by developing, testing, and deploying hosted agent workflows in the familiar environment of VS Code.

## Create a hosted agent workflow

You can use the Foundry for Visual Studio Code extension to create hosted agent workflows. A hosted agent workflow is a sequence of agents that work together to accomplish a task. Each agent in the workflow can have its own model, tools, and instructions.

1. Open the command palette (`Ctrl`+`Shift`+`P`).

2. Run this command: `>Microsoft Foundry: Create a New Hosted Agent`.

3. Select a programming language.

4. Select a folder where you want to save your new workflow.

5. Enter a name for your workflow project.

A new folder is created with the necessary files for your hosted agent project, including a sample code file to get you started.

### Install dependencies

Install the required dependencies for your hosted agent project. The dependencies vary based on the programming language that you selected when you created the project.

<ZonePivot pivot="python">
  #### Prerequisites

  To run the sample hosted agent Python project, make sure you install Python 3.10 or higher. You also need a Foundry project with a deployed model, or an Azure OpenAI resource.

  Give the project's managed identity the required permissions. Assign the built-in [Azure AI User](https://aka.ms/foundry-ext-project-role) and [AcrPull](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/containers#acrpull) roles. Ensure the `acrPull` role is also assigned to the managed identity of the Foundry project where you plan to deploy the hosted agent.

  <Callout type="important">
    Check to make sure that your region supports hosted agents. For the latest information on supported regions, see [Region Availability](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/concepts/hosted-agents?view=foundry\&preserve-view=true#region-availability).
  </Callout>

  #### Setup and installation

  1. Create virtual environment.

     ```bash
      python -m venv .venv
     ```

  2. Activate the virtual environment.

     ```bash
     # PowerShell
     ./.venv/Scripts/Activate.ps1

     # Windows cmd
     .venv\Scripts\activate.bat

     # Unix/MacOS
     source .venv/bin/activate
     ```

  3. Install the following package:

     ```bash
     pip install azure-ai-agentserver-agentframework
     ```
</ZonePivot>

<ZonePivot pivot="csharp">
  #### Prerequisites

  To run the sample hosted agent C# project, make sure you have a Foundry project with a deployed model, or an Azure OpenAI resource.

  Give the project's managed identity the required permissions. Assign the built-in [Azure AI User](https://aka.ms/foundry-ext-project-role) and [AcrPull](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/containers#acrpull) roles. Ensure the `acrPull` role is also assigned to the managed identity of the Foundry project where you plan to deploy the hosted agent.

  <Callout type="important">
    Check to make sure that your region supports hosted agents. For the latest information on supported regions, see [Region Availability](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/concepts/hosted-agents?view=foundry\&preserve-view=true#region-availability).
  </Callout>

  #### Setup and installation

  1. Download and install the .NET 9 SDK from the [official .NET website](https://dotnet.microsoft.com/download).

  2. Go to your project directory and run this command to get the necessary NuGet packages:

     ```bash
     dotnet restore
     ```
</ZonePivot>

## Run your hosted workflow locally

<ZonePivot pivot="python">
  The sample workflow project creates an .env file with the necessary environment variables. Create or update the .env file with your Foundry credentials:

  ```
  AZURE_AI_PROJECT_ENDPOINT=https://<your-resource-name>.services.ai.azure.com/api/projects/<your-project-name>

  AZURE_AI_MODEL_DEPLOYMENT_NAME=<your-model-deployment-name>
  ```

  <Callout type="important">
    Never commit the `.env` file to version control. Add it to your `.gitignore` file.
  </Callout>

  ### Authenticate your hosted agent

  The hosted agent sample authenticates using [DefaultAzureCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential?view=azure-python\&preserve-view=true). Configure your development environment to provide credentials via one of the supported sources, for example:

  * Azure CLI (`az login`)
  * Visual Studio Code account sign-in
  * Visual Studio account sign-in
  * Environment variables for a service principal (AZURE\_TENANT\_ID, AZURE\_CLIENT\_ID, AZURE\_CLIENT\_SECRET)

  Confirm authentication locally by running either the Azure CLI `az account show` or `az account get-access-token` commands before running the sample.

  You can run the hosted agent in interactive mode or container mode.

  ### Run your hosted agent in interactive mode

  Run the hosted agent directly for development and testing:

  ```bash
  python interactive.py
  ```

  ### Run your hosted agent in container mode

  <Callout type="tip">
    Open the local playground before starting the container agent to ensure the visualization functions correctly.
  </Callout>

  To run the hosted agent in container mode:

  1. Open the Visual Studio Code Command Palette and execute the `Microsoft Foundry: Open Container Agent Playground Locally` command.
  2. Execute `container.py` to initialize the containerized hosted agent.
  3. Submit a request to the agent through the playground interface. For example, enter a prompt such as: "Create a slogan for a new electric SUV that's affordable and fun to drive."
  4. Review the agent's response in the playground interface.
</ZonePivot>

<ZonePivot pivot="csharp">
  The sample workflow project creates an .env file with the necessary environment variables. Create or update the .env file with your Foundry credentials:

  1. Set up your environment variables based on your operating system:

     <Tabs>
       <Tab title="Windows (PowerShell)">
         ```powershell
         $env:AZURE_AI_PROJECT_ENDPOINT="https://<your-resource-name>.services.ai.azure.com/api/projects/<your-project-name>"
         $env:AZURE_AI_MODEL_DEPLOYMENT_NAME="your-deployment-name"
         ```
       </Tab>

       <Tab title="Windows (command prompt)">
         ```dos
         set AZURE_AI_PROJECT_ENDPOINT=https://your-resource-name.openai.azure.com/
         set AZURE_AI_MODEL_DEPLOYMENT_NAME=your-deployment-name
         ```
       </Tab>

       <Tab title="macOS/Linux (Bash)">
         ```bash
         export AZURE_AI_PROJECT_ENDPOINT="https://your-resource-name.openai.azure.com/"
         export AZURE_AI_MODEL_DEPLOYMENT_NAME="your-deployment-name"
         ```
       </Tab>
     </Tabs>

  #### Authenticate your hosted agent

  The hosted agent sample authenticates using [DefaultAzureCredential](https://learn.microsoft.com/en-us/dotnet/azure/sdk/authentication/credential-chains?tabs=dac#defaultazurecredential-overview). Configure your development environment to provide credentials via one of the supported sources, for example:

  * Azure CLI (`az login`)
  * Visual Studio Code account sign-in
  * Visual Studio account sign-in
  * Environment variables for a service principal (AZURE\_TENANT\_ID, AZURE\_CLIENT\_ID, AZURE\_CLIENT\_SECRET)

  Confirm authentication locally by running either the Azure CLI `az account show` or `az account get-access-token` commands before running the sample.

  You can run the hosted agent in interactive mode or container mode.

  ### Run your hosted agent in interactive mode

  Run the hosted agent directly for development and testing:

  ```bash
  dotnet build
  dotnet run --interactive
  ```

  ### Run your hosted agent in container mode

  <Callout type="tip">
    Open the local playground before starting the container agent to ensure the visualization functions correctly.
  </Callout>

  To run the agent in container mode:

  1. Open the Visual Studio Code Command Palette and execute the `Microsoft Foundry: Open Container Agent Playground Locally` command.
  2. Use the following command to initialize the containerized hosted agent.
     ```bash
     dotnet build
     dotnet run
     ```
  3. Submit a request to the agent through the playground interface. For example, enter a prompt such as: "Create a slogan for a new electric SUV that's affordable and fun to drive."
  4. Review the agent's response in the playground interface.
</ZonePivot>

## Visualize hosted agent workflow execution

By using the Foundry for Visual Studio Code extension, you can visualize the interactions between agents and how they collaborate to achieve your desired outcome.

<ZonePivot pivot="python">
  Enable visualization in your workflows by adding the following code snippet:

  ```python
  from agent_framework.observability import setup_observability
  setup_observability(vs_code_extension_port=4319) # Default port is 4319
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  Add the following reference to your csproj file:

  ```xml
  <ItemGroup>
      <PackageReference Include="OpenTelemetry" Version="1.12.0" />
      <PackageReference Include="OpenTelemetry.Exporter.Console" Version="1.12.0" />
      <PackageReference Include="OpenTelemetry.Exporter.OpenTelemetryProtocol" Version="1.12.0" />
      <PackageReference Include="System.Diagnostics.DiagnosticSource" Version="9.0.10" />
  </ItemGroup>
  ```

  Update your program to include the following code snippet:

  ```CSharp
  using System.Diagnostics;
  using OpenTelemetry;
  using OpenTelemetry.Logs;
  using OpenTelemetry.Metrics;
  using OpenTelemetry.Resources;
  using OpenTelemetry.Trace;

  var otlpEndpoint =
      Environment.GetEnvironmentVariable("OTLP_ENDPOINT") ?? "http://localhost:4319";

  var resourceBuilder = OpenTelemetry
      .Resources.ResourceBuilder.CreateDefault()
      .AddService("WorkflowSample");

  var s_tracerProvider = OpenTelemetry
      .Sdk.CreateTracerProviderBuilder()
      .SetResourceBuilder(resourceBuilder)
      .AddSource("Microsoft.Agents.AI.*") // All agent framework sources
      .SetSampler(new AlwaysOnSampler()) // Ensure all traces are sampled
      .AddOtlpExporter(options =>
      {
          options.Endpoint = new Uri(otlpEndpoint);
          options.Protocol = OpenTelemetry.Exporter.OtlpExportProtocol.Grpc;
      })
      .Build();
  ```
</ZonePivot>

### Monitor and visualize your hosted agent workflow

To monitor and visualize your hosted agent workflow execution in real time:

1. Open the command palette (`Ctrl`+`Shift`+`P`).

2. Run this command: `>Microsoft Foundry: Open Visualizer for Hosted Agents`.

A new tab opens in VS Code to display the execution graph. The visualization updates itself automatically as your workflow progresses, to show the flow between agents and their interactions.

#### Port conflicts

For port conflicts, you can change the visualization port by setting it in the Foundry extension settings. To do that, follow these steps:

1. In the left sidebar of VS Code, select the gear icon to open the settings menu.
2. Select `Extensions` > `Microsoft Foundry Configuration`.
3. Locate the `Hosted Agent Visualization Port` setting and change it to an available port number.
4. Restart VS Code to apply the changes.

#### Change port in code

<ZonePivot pivot="python">
  Change the visualization port by setting the `FOUNDRY_OTLP_PORT` environment variable. Update the observability port in the `workflow.py` file accordingly.

  For example, to change the port to 4318, use this command:

  ```bash
    export FOUNDRY_OTLP_PORT=4318
  ```

  In `workflow.py`, update the port number in the observability configuration:

  ```python
    setup_observability(vs_code_extension_port=4318)
  ```

  <Callout type="tip">
    To enable more debugging information, add the `enable_sensitive_data=True` parameter to the `setup_observability` function.
  </Callout>
</ZonePivot>

<ZonePivot pivot="csharp">
  For any port conflicts, change the visualization port by setting the `FOUNDRY_OTLP_PORT` environment variable. Update the OTLP endpoint in your program accordingly.

  For example, to change the port to 4318, use this command:

  ```powershell
    $env:FOUNDRY_OTLP_PORT="4318"
  ```

  In your program, update the OTLP endpoint to use the new port number:

  ```CSharp
  var otlpEndpoint =
      Environment.GetEnvironmentVariable("OTLP_ENDPOINT") ?? "http://localhost:4318";
  ```
</ZonePivot>

## Deploy the hosted agent

To deploy the hosted agent:

<Callout type="important">
  Make sure you give the necessary permissions to deploy hosted agents in your Foundry workspace, as stated in the [Prerequisites](#prerequisites). You might need to work with your Azure administrator to get the required role assignments.
</Callout>

<ZonePivot pivot="python">
  1. Open the Visual Studio Code Command Palette and run the `Microsoft Foundry: Deploy Hosted Agent` command.
  2. Configure the deployment settings by selecting your target workspace, specifying the container agent file (`container.py`), and defining any other deployment parameters as needed.
  3. Upon successful deployment, the hosted agent appears in the `Hosted Agents (Preview)` section of the Microsoft Foundry extension tree view.
  4. Select the deployed agent to access detailed information and test functionality using the integrated playground interface.
</ZonePivot>

<ZonePivot pivot="csharp">
  1. Open the Visual Studio Code Command Palette and run the `Microsoft Foundry: Deploy Hosted Agent` command.
  2. Configure the deployment settings by selecting your target workspace, specifying the container agent file (`<your-project-name>.csproj`), and defining any other deployment parameters as needed.
  3. Upon successful deployment, the hosted agent appears in the `Hosted Agents (Preview)` section of the Microsoft Foundry extension tree view.
  4. Select the deployed agent to access detailed information and test functionality using the integrated playground interface.
</ZonePivot>

For more information about publishing hosted agents, see [Publish and share agents in Microsoft Foundry](publish-agent).

Foundry MCP Server (preview) is a Microsoft-managed, cloud-hosted implementation of the Model Context Protocol (MCP). It exposes curated tools that let your agents perform read and write operations against Foundry services without calling backend APIs directly.

Use an MCP-compliant client such as Visual Studio Code to connect to the public endpoint, authenticate with Entra ID, and let LLMs access the tools. After you connect, you can build agents that invoke these tools with natural language prompts.

In this article, you learn how to:

* Connect to Foundry MCP Server with GitHub Copilot in Visual Studio Code
* Run prompts to test Foundry MCP Server tools and interact with Azure resources

<Callout type="note">
  This feature is currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

## Prerequisites

* Azure account with an active subscription. If you don't have one, [create a free Azure account](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* Foundry project. If you don't have a project, create one with the [Microsoft Foundry SDK Quickstart](https://learn.microsoft.com/en-us/azure/ai-foundry/quickstarts/get-started-code?tabs=python#first-run-experience).
* [Visual Studio Code](https://code.visualstudio.com/download).
* [GitHub Copilot](https://marketplace.visualstudio.com/items?itemName=GitHub.copilot) Visual Studio Code extension.

## Benefits of Foundry MCP Server

* **Cloud-hosted interface for AI tool orchestration**: Foundry MCP Server (preview) provides a secure, scalable endpoint for MCP-compliant clients. You don't need to deploy infrastructure, enabling seamless integration and multi-agent scenarios.
* **Identity and access control**: The server enforces authentication and authorization with Microsoft Entra ID. It performs all operations within the authenticated user's permissions (On-Behalf-Of flow).
* **Scenario-focused, extensible tools**: The MCP Server exposes a growing set of tools for read and write operations on models, deployments, evaluations, and agents in Foundry. The tools are extensible, letting developers and agents interact with services without knowing backend APIs or data schemas.
* **Accelerated agent and developer productivity**: Natural language workflows (via MCP clients and large language models) enable rapid tool discovery and invocation, streamlining development and multi-agent orchestration.

## Install and start Foundry MCP Server

Select an option to install Foundry MCP Server in Visual Studio Code.

<Tabs>
  <Tab title="User profile">
    Install Foundry MCP Server in your user profile so it's available to all workspaces in Visual Studio Code.

    1. Open the **Command Palette** (`Ctrl`+`Shift`+`P`).

    2. Search for **MCP:Add Server**.

    3. Select the **HTTP (Http or Server-Sent Events)** option.

    4. Enter `https://mcp.ai.azure.com` as the URL.

    5. Enter a friendly name such as *foundry-mcp-remote*, then press `Enter`. Visual Studio Code adds the following server entry under your user profile:

       ```json
       {
         "servers": {
           "foundry-mcp-remote": {
             "type": "http",
             "url": "https://mcp.ai.azure.com"
           }
         }
       }
       ```

    6. Open the **Command Palette** (`Ctrl`+`Shift`+`P`).

    7. Search for and select **MCP:List Servers**.

    8. Select Foundry MCP Server you added and choose **Start Server**.

    9. When prompted, sign in to Azure so the MCP server can interact with services in your subscription.

    10. Open GitHub Copilot and select **Agent Mode**.

    11. Select the tools icon, search for *Foundry* to filter the list, and confirm the server appears.

        ![Screenshot of GitHub Copilot Agent Mode tools list showing Foundry MCP Server tool.](https://learn.microsoft.com/azure/ai-foundry/default/media/mcp/foundry-mcp-server-tools.png)

        Learn more about Agent Mode in the [Visual Studio Code documentation](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode).
  </Tab>

  <Tab title="Workspace install">
    Install Foundry MCP Server for a specific workspace to scope it to that folder:

    1. Open an empty folder or an existing project folder in Visual Studio Code.

    2. In the folder root, create a `.vscode` folder if it doesn't exist.

    3. Inside the `.vscode` folder, create a file named `mcp.json`, and add the following JSON.

       ```json
       {
         "servers": {
           "azure-ai-foundry-mcp-preview": {
             "type": "http",
             "url": "https://mcp.ai.azure.com"
           }
         }
       }
       ```

    4. Save your changes to `mcp.json`.

    5. Select the **Start** button above the new server entry.

    6. When prompted, sign in so the MCP server can interact with services in your subscription.

    7. Open GitHub Copilot and select Agent Mode.

    8. Select the tools icon, search for *Foundry* to filter the results, and confirm the server appears.

       ![A screenshot showing Foundry MCP Server as GitHub Copilot tool.](https://learn.microsoft.com/azure/ai-foundry/default/media/mcp/foundry-mcp-server-tools.png)

       To learn more about Agent Mode, visit the [Visual Studio Code Documentation](https://code.visualstudio.com/docs/copilot/chat/chat-agent-mode).
  </Tab>
</Tabs>

## Use prompts to test Foundry MCP Server

1. Open the GitHub Copilot chat panel and confirm **Agent Mode** is selected.

2. Enter a prompt that uses Foundry MCP Server tools—for example *Tell me about the latest models on Foundry*.

3. Copilot requests permission to run the required Foundry MCP Server operation. Select **Continue** or use the arrow to choose a more specific behavior:

   * **Current session** always runs the operation in the current GitHub Copilot Agent Mode session.
   * **Current workspace** always runs the command for the current Visual Studio Code workspace.
   * **Always allow** sets the operation to always run for any GitHub Copilot Agent Mode session or any Visual Studio Code workspace.

   ![Screenshot of options to run Foundry MCP Server operations.](https://learn.microsoft.com/azure/ai-foundry/default/media/mcp/foundry-mcp-server-run-tool.png)

   The response resembles the following shortened output:

   ```text
   Latest / Notable Foundry Models (Preview Snapshot)
   Curated from the model catalog and benchmark data you requested. I've grouped by category and highlighted truly recent arrivals (2025 releases or late 2024 previews), plus why you'd pick them. Where available, I note cost, performance, or capability signals (e.g., throughput_gtps, reasoning focus, modality).

   1. Frontier & Reasoning Models
   gpt-5-pro (2025-10-06) – Latest flagship conversational / reasoning model from OpenAI; expect top-tier multi-turn coherence and complex tool orchestration.
   gpt-5 (2025-08-07), gpt-5-mini, gpt-5-nano – New performance tiers; mini/nano are cost-optimized for high-volume requests.
   o3-pro (2025-06-10) – High reasoning accuracy (multiple >0.95 accuracy slices) but very high latency (p50 ~102s) indicating chain-of-thought style deliberation. Use only for tasks requiring deep reasoning (complex math, logic).
   o3 (2025-04-16) – Balanced reasoning; much faster than o3-pro; good accuracy/quality trade-off.
   o4-mini (2025-04-16) – Successor in "o" line; strong quality with better latency than o3-pro.
   Phi-4 (versions through 7) – Microsoft small frontier open model; competitive quality at radically lower token cost (input $0.125 / 1M tokens). Strong for cost-sensitive general tasks.

   // Further output omitted
   ```

4. Explore and test Foundry MCP Server operations with other prompts, such as:

   ```text
   What tools can I use from Foundry MCP Server (preview)?
   Tell me about the latest models on Foundry
   Show me details about the GPT-5-mini model on Foundry
   ```

## Related content

* [Foundry MCP Server security and best practices](security-best-practices)
* [Foundry MCP Server tools and example prompts](available-tools)

Use the following sections to find available tools and example prompts for Foundry MCP Server (preview). Foundry MCP Server lets you use conversational prompts instead of API calls to interact with Foundry services.

<Callout type="note">
  This feature is currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

## Dataset management

**evaluation\_dataset\_create (write)**

Create or update a dataset version in a Foundry project.

Example prompts include:

* "Upload my customer support Q\&A dataset from this Azure Blob Storage URL."
* "Create a new dataset version 2.0 for my training data located at `<blob-storage-account-url>`."
* "Register a new evaluation dataset called `product-reviews-v1` from my blob storage."

**evaluation\_dataset\_get (read)**

Get a dataset by name and version, or list all datasets in the project.

Example prompts include:

* "Show me all datasets in my Foundry project"
* "Get details for the 'customer-support-qa' dataset version 2"
* "List all available datasets I can use for evaluation"

## Evaluation operations

**evaluation\_create (write)**

Create an evaluation run for a dataset using one or more evaluators.

Example prompts include:

* "Create an evaluation run for my customer service dataset using Relevance, Groundedness, and Coherence evaluators."
* "Run an evaluation on dataset-456 with Violence, HateUnfairness, and ContentSafety evaluators for my chatbot model."
* "Evaluate my QA model using the F1Score, BleuScore, and RougeScore metrics on the test dataset."

**evaluation\_get (read)**

List evaluation runs in the Azure AI Project.

Example prompts include:

* "Show me all evaluation runs in my Foundry project"
* "List the recent evaluations I've run this week"
* "Get the status of all my model evaluations"

**evaluation\_comparison\_create (write)**

Create comparison results of evaluations within a group.

Example prompts include:

* "Compare the performance of my baseline model against the two new fine-tuned versions."
* "Create a comparison between run-baseline-123 and treatment runs run-124, run-125 for evaluation eval-456."
* "I want to compare Model A vs Model B performance on the same evaluation metrics."

**evaluation\_comparison\_get (read)**

Get or list comparison results of evaluations within a group.

Example prompts include:

* "Get the results of comparison insight-789."
* "Show me the comparison results I created yesterday."
* "Retrieve all evaluation comparison insights from my project."

## Model catalog and details

**model\_catalog\_list (read)**

List models from the Foundry model catalog.

Example prompts include:

* "Show me all GPT-4 models available in the catalog."
* "List all Microsoft-published models with MIT license."
* "Find models I can use for free in the playground."
* "What models are available for text generation from OpenAI?"

**model\_details\_get (read)**

Get full model details and code sample from Foundry.

Example prompts include:

* "Get detailed information and code samples for GPT-5-mini."
* "Show me the specifications and usage examples for the Llama-2-70b model."
* "I need the documentation and sample code for the text-embedding-ada-002 model."

## Model deployment Management

**model\_deploy (write)**

Create or update a model deployment in the specified Foundry account.

Example prompts include:

* "Deploy GPT-5-mini as 'production-chatbot' with 20 capacity units"
* "Create a new deployment called 'content-generator' using the GPT-4o model."
* "Deploy the latest version of text-davinci-003 for my application."

**model\_deployment\_get (read)**

Get one or more model deployments from a Foundry account.

Example prompts include:

* "Show me all my current model deployments."
* "Get details for my 'production-chatbot' deployment."
* "List all deployments in my Foundry account."

**model\_deployment\_delete (write)**

Delete a specific model deployment by name.

Example prompts include:

* "Delete the 'old-test-deployment' that I'm no longer using."
* "Remove my staging deployment to free up quota."
* "Clean up that deprecated model deployment from my Foundry account `<account-name>`."

## Model analytics and recommendations

**model\_benchmark\_get (read)**

Fetch benchmark data for Foundry models.

Example prompts include:

* "Show me benchmark data for all available models."
* "Get performance comparisons across different model families."
* "I want to see accuracy and cost metrics for various models."

**model\_benchmark\_subset\_get (read)**

Get benchmark data for specific model name and version pairs.

Example prompts include:

* "Compare benchmark performance between GPT-4 and GPT-3.5-turbo."
* "Get benchmark data for Claude-3 vs Llama-2-70b models."
* "Show me performance metrics for the specific model versions I'm considering."

**model\_similar\_models\_get (read)**

Returns a list of similar models based on deployment or model details.

Example prompts include:

* "Find models similar to my current GPT-4 deployment."
* "What alternatives are there to the model I'm currently using?"
* "Show me models with similar capabilities to my production deployment."

**model\_switch\_recommendations\_get (read)**

Get model switch recommendations based on benchmark data.

Example prompts include:

* "Recommend better models based on my current deployment's performance."
* "Should I switch from my current model to something more cost-effective?"
* "Get optimization recommendations for my production model deployment."
* "What models would give me better quality/cost ratio than what I'm using now?"

## Model monitoring and operations

**model\_monitoring\_metrics\_get (read)**

Get monitoring metrics for a model deployment.

Example prompts include:

* "Show me the request metrics for my production-chatbot deployment."
* "Get latency statistics for my GPT-4o deployment over the last week."
* "Check the quota usage for my text-embedding deployment."
* "What are the error rates for my content-generator model?"

**model\_deprecation\_info\_get (read)**

Get deployment info enriched with deprecation data.

Example prompts include:

* "Check if my production deployment is using a deprecated model version."
* "Get deprecation information for my legacy-chatbot deployment."
* "Are any of my current deployments scheduled for retirement?"

**model\_quota\_list (read)**

List available deployment quota and usage for a subscription in a region.

Example prompts include:

* "Check my available quota in East US region."
* "How much capacity do I have left for new deployments in West Europe?"
* "Show me quota usage across all regions for my subscription."

## Example conversation examples

**Complete Model Evaluation Workflow:**

* "Upload my evaluation dataset from this blob storage URL."
* "Run an evaluation using Relevance, Groundedness, and Safety evaluators."
* "Compare my baseline model against the new fine-tuned version."
* "Show me the comparison results with statistical significance."

**Model Deployment & Optimization:**

* "Show me all GPT-4 models available in the catalog."
* "Deploy GPT-4o as 'customer-service-bot' with 15 capacity units."
* "Monitor the request latency for my new deployment."
* "Recommend more cost-effective alternatives based on current usage."

**Resource Management & Cleanup:**

* "List all my current deployments and their usage."
* "Check which deployments are using deprecated model versions."
* "Show me my quota usage across all regions."
* "Delete unused test deployments to free up capacity."

## Related content

* Get started with [Foundry MCP Server](get-started)
* Learn how to [build your own MCP server](build-your-own-mcp-server)
* Try the example workflows above in your own Microsoft Foundry project

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

A Foundry resource provides unified access to models, agents, and tools. This article explains which SDK and endpoint to use for your scenario.

| SDK                    | What it's for                                                                                                                                               | Endpoint                                                                    |
| ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |
| **Foundry SDK**        | Foundry-specific capabilities with OpenAI-compatible interfaces. Includes access to Foundry direct models through the Responses API (not Chat Completions). | `https://<resource-name>.services.ai.azure.com/api/projects/<project-name>` |
| **OpenAI SDK**         | Latest OpenAI SDK models and features with the full OpenAI API surface. Foundry direct models available through Chat Completions API (not Responses).       | `https://<resource-name>.openai.azure.com/openai/v1`                        |
| **Foundry Tools SDKs** | Prebuilt solutions (Vision, Speech, Content Safety, and more).                                                                                              | Tool-specific endpoints (varies by service).                                |
| **Agent Framework**    | Multi-agent orchestration in code. Cloud-agnostic.                                                                                                          | Uses the project endpoint via the Foundry SDK.                              |

**Choose your SDK**:

* Use **Foundry SDK** when building apps with agents, evaluations, or Foundry-specific features
* Use **OpenAI SDK** when maximum OpenAI compatibility is required, or using Foundry direct models via Chat Completions
* Use **Foundry Tools SDKs** when working with specific AI services (Vision, Speech, Language, etc.)
* Use **Agent Framework** when building multi-agent systems in code (local orchestration)

<Callout type="note">
  **Resource types:** A Foundry resource provides all endpoints previously listed. An Azure OpenAI resource provides only the `/openai/v1` endpoint.

  **Authentication:** Samples here use Microsoft Entra ID (`DefaultAzureCredential`). API keys work on `/openai/v1`. Pass the key as `api_key` instead of a token provider.
</Callout>

## Prerequisites

* An Azure account with an active subscription. If you don't have one, create a [free Azure account, which includes a free trial subscription](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).

* Have one of the following Azure RBAC roles to create and manage Foundry resources:

  * **Azure AI User** (least-privilege role for development)
  * **Azure AI Project Manager** (for managing Foundry projects)
  * **Contributor** or **Owner** (for subscription-level permissions)

  For details on each role's permissions, see [Role-based access control for Microsoft Foundry](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/rbac-azure-ai-foundry).

* Install the required language runtimes, global tools, and VS Code extensions as described in [Prepare your development environment](install-cli-sdk).

<Callout type="important">
  Before starting, make sure your development environment is ready.\
  This article focuses on **scenario-specific steps** like SDK installation, authentication, and running sample code.
</Callout>

### Verify prerequisites

Before proceeding, confirm:

* \[ ] Azure subscription is active: `az account show`

* \[ ] You have the required RBAC role: Check Azure portal → Foundry resource → Access control (IAM)

* \[ ] Language runtime installed:

  * Python: `python --version` (≥3.8)
  * Node.js: `node --version` (≥18)
  * .NET: `dotnet --version` (≥6.0)
  * Java: `java --version` (≥11)

## Foundry SDK

The Foundry SDK connects to a single project endpoint that provides access to the most popular Foundry capabilities:

```
https://<resource-name>.services.ai.azure.com/api/projects/<project-name>
```

<Callout type="note">
  If your organization uses a custom subdomain, replace `<resource-name>` with `<your-custom-subdomain>` in the endpoint URL.
</Callout>

This approach simplifies application configuration. Instead of managing multiple endpoints, you configure one.

### Install the SDK

<Callout type="note">
  **SDK versions:** The 2.x preview SDK targets the new Foundry portal and API. The 1.x GA SDK targets Foundry classic. Make sure the samples you follow match your installed package.
</Callout>

<ZonePivot pivot="programming-language-python">
  | SDK Version   | Portal Version  | Status  | Python Package                     |
  | ------------- | --------------- | ------- | ---------------------------------- |
  | 2.x (preview) | Foundry (new)   | Preview | `azure-ai-projects>=2.0.0b1 --pre` |
  | 1.x (GA)      | Foundry classic | Stable  | `azure-ai-projects==1.0.0`         |

  The [Azure AI Projects client library for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-projects-readme?view=azure-python-preview\&preserve-view=true) is a unified library that enables you to use multiple client libraries together by connecting to a single project endpoint.

  Run these commands to install the preview packages for Foundry projects.

  ```bash
  pip install --pre azure-ai-projects
  pip install azure-identity openai
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-java">
  | SDK Version               | Portal Version | Status  | Java Package                          |
  | ------------------------- | -------------- | ------- | ------------------------------------- |
  | 1.0.0-beta.3 1.0.0-beta.1 | Foundry (new)  | Preview | `azure-ai-projects` `azure-ai-agents` |
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  | SDK Version            | Portal Version  | Status  | JavaScript Package                |
  | ---------------------- | --------------- | ------- | --------------------------------- |
  | 2.0.0-beta.4 (preview) | Foundry (new)   | Preview | `@azure/ai-projects 'prerelease'` |
  | 1.0.1                  | Foundry classic | Stable  | `@azure/ai-projects`              |
</ZonePivot>

<ZonePivot pivot="programming-language-csharp">
  | SDK Version            | Portal Version  | Status  | .NET Package                                   |
  | ---------------------- | --------------- | ------- | ---------------------------------------------- |
  | 1.2.0-beta.5 (preview) | Foundry (new)   | Preview | `Azure.AI.Projects` `Azure.AI.Projects.Openai` |
  | 1.x (GA)               | Foundry classic | Stable  | `Azure.AI.Projects`                            |
</ZonePivot>

<ZonePivot pivot="programming-language-java">
  The [Azure AI Projects client library for Java (preview)](https://learn.microsoft.com/en-us/java/api/overview/azure/ai-projects-readme) is a unified library that enables you to use multiple client libraries together by connecting to a single project endpoint.

  <Callout type="important">
    Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
  </Callout>

  Add these packages to your installation for Foundry projects.

  ```java
  package com.azure.ai.agents;

  import com.azure.core.util.Configuration;
  import com.azure.identity.DefaultAzureCredentialBuilder;
  import com.openai.models.responses.Response;
  import com.openai.models.responses.ResponseCreateParams;
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  The [Azure AI Projects client library for JavaScript](https://learn.microsoft.com/en-us/javascript/api/overview/azure/ai-projects-readme) is a unified library that enables you to use multiple client libraries together by connecting to a single project endpoint.

  Run this command to install the preview JavaScript packages for Foundry projects.

  ```bash
  npm install @azure/ai-projects@beta @azure/identity dotenv
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-csharp">
  The [Azure AI Projects client library for .NET](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.projects-readme) is a unified library that enables you to use multiple client libraries together by connecting to a single project endpoint.

  Run this command to add the Azure.AI.Projects package to your .NET project.

  ```bash
  dotnet add package Azure.AI.Projects --prerelease
  dotnet add package Azure.AI.Projects.OpenAI --prerelease
  dotnet add package Azure.Identity
  ```
</ZonePivot>

### Using the Foundry SDK

The SDK exposes two client types because Foundry and OpenAI have different API shapes:

* **Project client** – Use for Foundry-native operations where OpenAI has no equivalent. Examples: listing connections, retrieving project properties, enabling tracing.
* **OpenAI-compatible client** – Use for Foundry functionality that builds on OpenAI concepts. The Responses API, agents, evaluations, and fine-tuning all use OpenAI-style request/response patterns. This client also gives you access to Foundry direct models (non-Azure-OpenAI models hosted in Foundry). The project endpoint serves this traffic on the `/openai` route.

Most apps use both clients. Use the project client for setup and configuration, then use the OpenAI-compatible client for running agents, evaluations, and calling models (including Foundry direct models).

<ZonePivot pivot="programming-language-python">
  **Create a project client:**

  ```python
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient

  project_client = AIProjectClient(
    endpoint="https://<resource-name>.services.ai.azure.com/api/projects/<project-name>",
    credential=DefaultAzureCredential())
  ```

  **Create an OpenAI-compatible client from your project:**

  ```python
  with project_client.get_openai_client() as openai_client:
      response = openai_client.responses.create(
          model="gpt-5.2",
          input="What is the size of France in square miles?",
      )
      print(f"Response output: {response.output_text}")
  ```

  **Expected output**:

  ```
  Response output: France has an area of approximately 213,011 square miles (551,695 square kilometers).
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-java">
  **Create a project client:**

  ```java
  import com.azure.ai.projects.ProjectsClient;
  import com.azure.ai.projects.ProjectsClientBuilder;
  import com.azure.identity.DefaultAzureCredentialBuilder;

  String endpoint = "https://<resource-name>.services.ai.azure.com/api/projects/<project-name>";

  ProjectsClient projectClient = new ProjectsClientBuilder()
      .credential(new DefaultAzureCredentialBuilder().build())
      .endpoint(endpoint)
      .buildClient();
  ```

  **Create and use an OpenAI-compatible client from your project:**

  ```java
  OpenAIClient openAIClient = projectClient.getOpenAIClient();
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  **Create a project client:**

  ```javascript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = "https://<resource-name>.services.ai.azure.com/api/projects/<project-name>";
  const deploymentName = "gpt-5.2";
  const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
  ```

  **Create an OpenAI-compatible client from your project:**

  ```javascript
  const openAIClient = await project.getOpenAIClient();
  const response = await openAIClient.responses.create({
      model: deploymentName,
      input: "What is the size of France in square miles?",
  });
  console.log(`Response output: ${response.output_text}`);
  ```
</ZonePivot>

<ZonePivot pivot="programming-language-csharp">
  **Create a project client:**

  ```csharp
  using Azure.AI.Projects.OpenAI;
  using Azure.Identity;
  using OpenAI.Responses;

  string endpoint = "https://<resource-name>.services.ai.azure.com/api/projects/<project-name>";

  AIProjectClient projectClient = new(
      endpoint: new Uri(endpoint),
      tokenProvider: new DefaultAzureCredential());
  ```

  **Create an OpenAI-compatible client from your project:**

  ```csharp
  #pragma warning disable OPENAI001
  OpenAIResponseClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForModel("gpt-5.2");
  OpenAIResponse response = responseClient.CreateResponse("What is the speed of light?");
  Console.WriteLine(response.GetOutputText());
  #pragma warning restore OPENAI001
  ```
</ZonePivot>

### What you can do with the Foundry SDK

* [Access Foundry Models](../../quickstarts/get-started-code), including Azure OpenAI
* [Use the Foundry Agent Service](../../../ai-services/agents/quickstart?context=/azure/ai-foundry/context/context)
* [Run cloud evaluations](cloud-evaluation)
* [Enable app tracing](trace-application)
* [Fine-tune a model](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/fine-tuning?view=foundry\&tabs=azure-openai\&pivots=programming-language-python\&preserve-view=true)
* Get endpoints and keys for Foundry Tools, local orchestration, and more

## Troubleshooting

### Authentication errors

If you see `DefaultAzureCredential failed to retrieve a token`:

1. **Verify Azure CLI is authenticated**:

   ```bash
   az account show
   az login  # if not logged in
   ```

2. **Check RBAC role assignment**:

   * Confirm you have at least the Azure AI User role on the Foundry project
   * See [Assign Azure roles](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal)

3. **For managed identity in production**:

   * Ensure the managed identity has the appropriate role assigned
   * See [Configure managed identities](../../concepts/authentication-authorization-foundry#identity-types)

### Endpoint configuration errors

If you see `Connection refused` or `404 Not Found`:

* **Verify resource and project names** match your actual deployment
* **Check endpoint URL format**: Should be `https://<resource-name>.services.ai.azure.com/api/projects/<project-name>`
* **For custom subdomains**: Replace `<resource-name>` with your custom subdomain

### SDK version mismatches

If code samples fail with `AttributeError` or `ModuleNotFoundError`:

* **Check SDK version**:
  ```bash
  pip show azure-ai-projects  # Python
  npm list @azure/ai-projects  # JavaScript
  dotnet list package  # .NET
  ```
* **Verify moniker alignment**: 2.x SDK requires Foundry portal, 1.x SDK requires Foundry classic
* **Reinstall with correct version flags**: See installation commands in each language section above

## OpenAI SDK

Use the OpenAI SDK when you want the full OpenAI API surface and maximum client compatibility. This endpoint provides access to Azure OpenAI models and Foundry direct models (via Responses API). It doesn't provide access to Foundry-specific features like agents and evaluations.

The following snippet shows how to use the Azure OpenAI `/openai/v1` endpoint directly.

<ZonePivot pivot="programming-language-python">
  ```python
  from openai import OpenAI
  from azure.identity import DefaultAzureCredential, get_bearer_token_provider

  token_provider = get_bearer_token_provider(
      DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
  )

  client = OpenAI(
    base_url = "https://<resource-name>.openai.azure.com/openai/v1/",
    api_key=token_provider,
  )

  response = client.responses.create(
      model="model_deployment_name",
      input= "What is the size of France in square miles?"
  )

  print(response.model_dump_json(indent=2))
  ```

  **Expected output**:

  ```json
  {
    "id": "resp_abc123",
    "object": "response",
    "created": 1234567890,
    "model": "gpt-5.2",
    "output_text": "France has an area of approximately 213,011 square miles (551,695 square kilometers)."
  }
  ```

  For more information, see [Azure OpenAI supported programming languages](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/supported-languages?view=foundry\&tabs=dotnet-secure%2Csecure%2Cpython-entra\&pivots=programming-language-python\&preserve-view=true)
</ZonePivot>

<ZonePivot pivot="programming-language-java">
  <Callout type="important">
    Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
  </Callout>

  The following snippet shows how to use the Azure OpenAI `/openai/v1` endpoint directly.

  ```java
  import com.azure.identity.AuthenticationUtil;
  import com.azure.identity.DefaultAzureCredential;
  import com.azure.identity.DefaultAzureCredentialBuilder;
  import com.openai.client.OpenAIClient;
  import com.openai.client.okhttp.OpenAIOkHttpClient;
  import com.openai.credential.BearerTokenCredential;

  import java.util.function.Supplier;

  DefaultAzureCredential tokenCredential = new DefaultAzureCredentialBuilder().build();
  String endpoint = "https://<resource-name>.openai.azure.com/openai/v1";
  String deploymentName = "gpt-5.2";
  Supplier<String> bearerTokenSupplier = AuthenticationUtil.getBearerTokenSupplier(
          tokenCredential, "https://cognitiveservices.azure.com/.default");
  OpenAIClient openAIClient = OpenAIOkHttpClient.builder()
          .baseUrl(endpoint)
          .credential(BearerTokenCredential.create(bearerTokenSupplier))
          .build();

  ResponseCreateParams responseCreateParams = ResponseCreateParams.builder()
          .input("What is the speed of light?")
          .model(deploymentName)
          .build();

  Response response = openAIClient.responses().create(responseCreateParams);

  System.out.println("Response output: " + response.getOutputText());
  ```

  For more information on using the OpenAI SDK, see [Azure OpenAI supported programming languages](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/supported-languages?view=foundry\&tabs=dotnet-secure%2Csecure%2Cpython-entra\&pivots=programming-language-java\&preserve-view=true)
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  ```javascript
  const endpoint = "https://<resource-name>.openai.azure.com/openai/v1";
  const scope = "https://cognitiveservices.azure.com/.default";
  const azureADTokenProvider = getBearerTokenProvider(new DefaultAzureCredential(), scope);
  const client = new OpenAI({ baseURL: endpoint, apiKey: azureADTokenProvider });
  const response = await client.responses.create({
          model: deploymentName,
          input: "What is the size of France in square miles?",
      });
  console.log(`Response output: ${response.output_text}`);
  ```

  For more information on using the OpenAI SDK, see [Azure OpenAI supported programming languages](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/supported-languages?view=foundry\&tabs=dotnet-secure%2Csecure%2Cpython-entra\&pivots=programming-language-javascript\&preserve-view=true)
</ZonePivot>

<ZonePivot pivot="programming-language-csharp">
  1. Install the OpenAI package:

  Run this command to add the OpenAI client library to your .NET project.

  ```bash
  dotnet add package OpenAI
  ```

  When it succeeds, the .NET CLI confirms that it installed the `OpenAI` package.

  This snippet configures `DefaultAzureCredential`, builds `OpenAIClientOptions`, and creates a `ResponseClient` for the Azure OpenAI v1 endpoint.

  ```csharp
  using Azure.Identity;
  using Azure.Core;
  using OpenAI;
  using System;
  using System.ClientModel.Primitives;

  #pragma warning disable OPENAI001

  const string directModelEndpoint  = "https://<resource-name>.openai.azure.com/openai/v1/";
  const string deploymentName = "gpt-5.2";

  BearerTokenPolicy tokenPolicy = new(
       new DefaultAzureCredential(),
       "https://cognitiveservices.azure.com/.default");

  OpenAIResponseClient client = new(
       model: deploymentName,
       authenticationPolicy: tokenPolicy, // To use Entra
    // credential: new ApiKeyCredential("<YOUR-AZURE-OPENAI-API-KEY>") // To use APIKEY
       options: new OpenAIClientOptions()
       {
           Endpoint = new($"{directModelEndpoint}"),
       });
  ResponseCreationOptions options = new ResponseCreationOptions
   {
       Temperature = (float)0.7,
   };

  OpenAIResponse modelDirectResponse = client.CreateResponse(
        [
           ResponseItem.CreateUserMessageItem("What is the size of France in square miles?"),
        ], options);

  Console.WriteLine($"[ASSISTANT]: {modelDirectResponse.GetOutputText()}");
  #pragma warning restore OPENAI001
  // The ResponseClient lets you interact with models and services in your project.
  ```

  For more information on using the OpenAI SDK, see [Azure OpenAI supported programming languages](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/supported-languages?view=foundry\&tabs=dotnet-secure%2Csecure%2Cpython-entra\&pivots=programming-language-programming-language-dotnet\&preserve-view=true)
</ZonePivot>

## Using the Agent Framework for local orchestration

Microsoft Agent Framework is an open-source SDK for building multi-agent systems in code (for example, .NET and Python) with a cloud-provider-agnostic interface.

Use Agent Framework when you want to define and orchestrate agents locally. Pair it with the Foundry SDK when you want those agents to run against Foundry models or when you want Agent Framework to orchestrate agents hosted in Foundry.

For more information, see the [Microsoft Agent Framework overview](https://learn.microsoft.com/en-us/agent-framework/overview/agent-framework-overview).

## Foundry Tools SDKs

Foundry Tools (formerly Azure AI Services) are prebuilt point solutions with dedicated SDKs. Use the following endpoints to work with Foundry Tools.

### Which endpoint should you use?

Choose an endpoint based on your needs:

Use the Azure AI Services endpoint to access Computer Vision, Content Safety, Document Intelligence, Language, Translation, and Token Foundry Tools.

Foundry Tools endpoint: `https://<your-resource-name>.cognitiveservices.azure.com/`

<Callout type="note">
  Endpoints use either your resource name or a custom subdomain. If your organization set up a custom subdomain, replace `your-resource-name` with `your-custom-subdomain` in all endpoint examples.
</Callout>

For Speech and Translation Foundry Tools, use the endpoints in the following tables. Replace placeholders with your resource information.

#### Speech Endpoints

| Foundry Tool              | Endpoint                                                    |
| ------------------------- | ----------------------------------------------------------- |
| Speech to Text (Standard) | `https://<YOUR-RESOURCE-REGION>.stt.speech.microsoft.com`   |
| Text to Speech (Neural)   | `https://<YOUR-RESOURCE-REGION>.tts.speech.microsoft.com`   |
| Custom Voice              | `https://<YOUR-RESOURCE-NAME>.cognitiveservices.azure.com/` |

#### Translation Endpoints

| Foundry Tool         | Endpoint                                                    |
| -------------------- | ----------------------------------------------------------- |
| Text Translation     | `https://api.cognitive.microsofttranslator.com/`            |
| Document Translation | `https://<YOUR-RESOURCE-NAME>.cognitiveservices.azure.com/` |

The following sections include quickstart links for the Foundry Tools SDKs and reference information.

<ZonePivot pivot="programming-language-csharp">
  ### C# supported Foundry Tools

  | Foundry Tool                                                                                                                                                                                                                                                           | Description                                                                                                              | Quickstarts and reference documentation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
  | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | ![Speech icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/speech.svg) [Speech](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/overview)                                                     | Add speech to text, text to speech, translation, and speaker recognition capabilities to applications.                   | • [Speech to text quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text?tabs=windows\&pivots=programming-language-csharp) • [Text to speech quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-text-to-speech?tabs=windows\&pivots=programming-language-csharp) • [Speech translation quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-translation?tabs=windows\&pivots=programming-language-csharp) • [Speech SDK for .NET](https://learn.microsoft.com/en-us/dotnet/api/microsoft.cognitiveservices.speech?view=azure-dotnet\&branch=main\&preserve-view=true) • [Speech NuGet package (Speech CLI)](https://www.nuget.org/packages/Microsoft.CognitiveServices.Speech.CLI)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
  | ![Language icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/language.svg) [Language](https://learn.microsoft.com/en-us/azure/ai-services/language-service/overview)                                             | Build applications with natural language understanding capabilities.                                                     | • [Custom question answering (CQA) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/question-answering/quickstart/sdk?tabs=windows\&pivots=programming-language-csharp) • [Entity linking quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/entity-linking/quickstart?tabs=windows\&pivots=programming-language-csharp) • [Language detection quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/language-detection/quickstart?tabs=windows\&pivots=programming-language-csharp) • [Key Phrase extraction quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/key-phrase-extraction/quickstart?tabs=windows\&pivots=programming-language-csharp) • [Detecting named entities (NER) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/named-entity-recognition/quickstart?\&tabs=windows%2Cga-api\&pivots=programming-language-csharp) • [Detect Personally Identifiable Information (PII) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/quickstart?\&tabs=windows\&pivots=programming-language-csharp) • [Sentiment analysis and opinion mining quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/sentiment-opinion-mining/quickstart?tabs=windows\&pivots=programming-language-csharp) • [Using text, document and conversation summarization quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/summarization/quickstart?tabs=text-summarization%2Cwindows\&pivots=programming-language-csharp) • [Using Text Analytics for health quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/text-analytics-for-health/quickstart?tabs=windows\&pivots=programming-language-csharp) • [Language SDK for .NET (text analysis)](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.textanalytics-readme?view=azure-dotnet\&preserve-view=true) • [Language NuGet package (text analysis)](https://www.nuget.org/packages/Azure.AI.TextAnalytics) • [Language SDK for .NET (Question Answering)](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.language.questionanswering-readme?view=azure-dotnet\&preserve-view=true) • [Language NuGet package (question answering)](https://www.nuget.org/packages/Azure.AI.Language.QuestionAnswering) |
  | ![Translator icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/translator.svg) [Translator](https://learn.microsoft.com/en-us/azure/ai-services/translator/overview)                                             | Use AI-powered translation technology to translate more than 100 in-use, at-risk, and endangered languages and dialects. | • [Translator SDK for .NET (text)](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.translation.text-readme?view=azure-dotnet-preview\&preserve-view=true) • [Translator NuGet package (text)](https://www.nuget.org/packages/Azure.AI.Translation.Text/1.0.0-beta.1) • [Translator SDK for .NET (batch)](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/AI.Translation.Document-readme?view=azure-dotnet\&preserve-view=true) • [Translator NuGet package (batch)](https://www.nuget.org/packages/Azure.AI.Translation.Document)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
  | ![Azure AI Search icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/search.svg) [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)                                    | Bring AI-powered cloud search to your mobile and web apps.                                                               | • [Use agentic retrieval quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-agentic-retrieval?tabs=search-perms%2Csearch-endpoint\&pivots=programming-language-csharp) • [Vector search quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector?tabs=keyless\&pivots=csharp) • [Classic generative search (RAG) using grounding data quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-rag?pivots=csharp) • [Full-text search quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-text?tabs=keyless%2Cwindows\&pivots=csharp) • [Semantic ranking quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-semantic?pivots=csharp) • [Chat with Azure OpenAI models using your own data quickstart](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/use-your-data-quickstart?view=foundry-classic\&viewFallbackFrom=foundry\&context=%2Fazure%2Fsearch%2Fcontext%2Fcontext\&tabs=keyless%2Ctypescript-keyless%2Cpython-new\&pivots=programming-language-csharp\&preserve-view=true) • [Azure AI Search SDK for .NET](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/search.documents-readme?view=azure-dotnet\&preserve-view=true) • [Azure AI Search NuGet package](https://www.nuget.org/packages/Azure.Search.Documents/11.6.0-beta.2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
  | ![Content Safety icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/content-safety.svg) [Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview)                             | Detect harmful content in applications and services.                                                                     | • [Analyze text content quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-text?tabs=visual-studio%2Cwindows\&pivots=programming-language-csharp) • [Use a text blocklist quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-blocklist?tabs=visual-studio%2Cwindows\&pivots=programming-language-csharp) • [Analyze image content quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-image?tabs=visual-studio%2Cwindows\&pivots=programming-language-csharp) • [Content Safety SDK for .NET](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.contentsafety-readme?view=azure-dotnet\&preserve-view=true) • [Content Safety NuGet package](https://www.nuget.org/packages/Azure.AI.ContentSafety/1.0.0)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
  | ![Document Intelligence icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/document-intelligence.svg) [Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview) | Turn documents into intelligent data-driven solutions.                                                                   | • [Document Intelligence quickstart](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?view=doc-intel-4.0.0\&pivots=programming-language-csharp\&preserve-view=true) • [Document Intelligence SDK for .NET](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/ai.documentintelligence-readme?view=azure-dotnet-preview\&preserve-view=true) • [Document Intelligence NuGet package](https://www.nuget.org/packages/Azure.AI.DocumentIntelligence/1.0.0-beta.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
  | ![Vision icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/vision.svg) [Vision](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview)                                                    | Analyze content in digital images and rich media assets.                                                                 | • [Azure Vision in Foundry Tools v3.2 GA Read quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/client-library?tabs=windows%2Cvisual-studio\&programming-language-csharp) • [Image Analysis quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/image-analysis-client-library-40?tabs=windows%2Cvisual-studio\&programming-language-csharp) • [Use the Face service quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/identity-client-library?tabs=windows%2Cvisual-studio\&pivots=programming-language-csharp) • [Vision SDK for .NET](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/AI.Vision.ImageAnalysis-readme?view=azure-dotnet-preview\&preserve-view=true) • [Vision NuGet package](https://www.nuget.org/packages/Azure.AI.Vision.ImageAnalysis)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
</ZonePivot>

<ZonePivot pivot="programming-language-java">
  ### Java supported Foundry Tools

  | Foundry Tool                                                                                                                                                                                                                                                           | Description                                                                                                              | Quickstarts and reference documentation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
  | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | ![Speech icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/speech.svg) [Speech](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/overview)                                                     | Add speech to text, text to speech, translation, and speaker recognition capabilities to applications.                   | • [Speech to text quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text?tabs=windows\&pivots=programming-language-java) • [Text to speech quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-text-to-speech?tabs=windows\&pivots=programming-language-java) • [Speech translation quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-translation?tabs=windows\&pivots=programming-language-java) • [Speech SDK for Java](https://learn.microsoft.com/en-us/java/api/com.microsoft.cognitiveservices.speech?view=azure-java-stable\&branch=main\&preserve-view=true) • [Speech Maven package](https://central.sonatype.com/artifact/com.microsoft.cognitiveservices.speech/client-sdk/1.34.0?smo=true)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
  | ![Language icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/language.svg) [Language](https://learn.microsoft.com/en-us/azure/ai-services/language-service/overview)                                             | Build applications with natural language understanding capabilities.                                                     | • [Entity linking quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/entity-linking/quickstart?tabs=windows\&pivots=programming-language-java) • [Language detection quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/language-detection/quickstart?tabs=windows\&pivots=programming-language-java) • [Key Phrase extraction quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/key-phrase-extraction/quickstart?tabs=windows\&pivots=programming-language-java) • [Detecting named entities (NER) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/named-entity-recognition/quickstart?\&tabs=windows%2Cga-api\&pivots=programming-language-java) • [Detect Personally Identifiable Information (PII) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/quickstart?\&tabs=windows\&pivots=programming-language-java) • [Sentiment analysis and opinion mining quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/sentiment-opinion-mining/quickstart?tabs=windows\&pivots=programming-language-java) • [Using text, document and conversation summarization quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/summarization/quickstart?tabs=text-summarization%2Cwindows\&pivots=programming-language-java) • [Using Text Analytics for health quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/text-analytics-for-health/quickstart?tabs=windows\&pivots=programming-language-java) • [Language SDK for Java (text analysis)](https://learn.microsoft.com/en-us/java/api/overview/azure/ai-textanalytics-readme?view=azure-java-stable\&preserve-view=true) • [Language Maven package](https://central.sonatype.com/artifact/com.microsoft.azure.cognitiveservices/azure-cognitiveservices-language) |
  | ![Translator icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/translator.svg) [Translator](https://learn.microsoft.com/en-us/azure/ai-services/translator/overview)                                             | Use AI-powered translation technology to translate more than 100 in-use, at-risk, and endangered languages and dialects. | • [Translator SDK for Java (text)](https://learn.microsoft.com/en-us/java/api/overview/azure/ai-translation-text-readme?view=azure-java-preview\&preserve-view=true) • [Translator Maven package (text)](https://central.sonatype.com/artifact/com.azure/azure-ai-translation-text)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
  | ![Azure AI Search icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/search.svg) [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)                                    | Bring AI-powered cloud search to your mobile and web apps.                                                               | • [Use agentic retrieval quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-agentic-retrieval?tabs=search-perms%2Csearch-endpoint\&pivots=programming-language-java) • [Vector search quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector?tabs=keyless\&pivots=java) • [Classic generative search (RAG) using grounding data quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-rag?pivots=java) • [Full-text search quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-text?tabs=keyless%2Cwindows\&pivots=java) • [Semantic ranking quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-semantic?pivots=java) • [Chat with Azure OpenAI models using your own data quickstart](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/use-your-data-quickstart?view=foundry-classic\&viewFallbackFrom=foundry\&context=%2Fazure%2Fsearch%2Fcontext%2Fcontext\&tabs=keyless%2Ctypescript-keyless%2Cpython-new\&pivots=programming-language-java\&preserve-view=true) • [Azure AI Search SDK for Java](https://learn.microsoft.com/en-us/java/api/overview/azure/search-documents-readme?view=azure-java-stable\&preserve-view=true) • [Azure AI Search Maven package](https://central.sonatype.com/artifact/com.azure/azure-search-documents/11.7.0-beta.1?smo=true)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
  | ![Content Safety icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/content-safety.svg) [Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview)                             | Detect harmful content in applications and services.                                                                     | • [Analyze text content quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-text?tabs=visual-studio%2Cwindows\&pivots=programming-language-java) • [Use a text blocklist quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-blocklist?tabs=visual-studio%2Cwindows\&pivots=programming-language-java) • [Analyze image content quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-image?tabs=visual-studio%2Cwindows\&pivots=programming-language-java) • [Content Safety SDK for Java](https://learn.microsoft.com/en-us/java/api/overview/azure/ai-contentsafety-readme?view=azure-java-stable\&preserve-view=true) • [Content Safety Maven package](https://central.sonatype.com/artifact/com.azure/azure-ai-contentsafety)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
  | ![Document Intelligence icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/document-intelligence.svg) [Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview) | Turn documents into intelligent data-driven solutions.                                                                   | • [Document Intelligence quickstart](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?view=doc-intel-4.0.0\&pivots=programming-language-java\&preserve-view=true) • [Document Intelligence SDK for Java](https://learn.microsoft.com/en-us/java/api/overview/azure/ai-documentintelligence-readme?view=azure-java-preview\&preserve-view=true) • [Document Intelligence Maven package](https://mvnrepository.com/artifact/com.azure/azure-ai-documentintelligence/1.0.0-beta.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
  | ![Vision icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/vision.svg) [Vision](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview)                                                    | Analyze content in digital images and rich media assets.                                                                 | • [Image Analysis quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/image-analysis-client-library-40?tabs=windows%2Cvisual-studio\&pivots=programming-language-java) • [Use the Face service quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/identity-client-library?tabs=windows%2Cvisual-studio\&pivots=programming-language-java) • [Vision SDK for Java](https://learn.microsoft.com/en-us/java/api/overview/azure/ai-vision-imageanalysis-readme?view=azure-java-preview\&preserve-view=true) • [Vision Maven package](https://central.sonatype.com/artifact/com.azure/azure-ai-vision-imageanalysis)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  ### JavaScript supported Foundry Tools

  | Foundry Tool                                                                                                                                                                                                                                                           | Description                                                                                                              | Quickstarts and reference documentation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
  | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | ![Speech icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/speech.svg) [Speech](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/overview)                                                     | Add speech to text, text to speech, translation, and speaker recognition capabilities to applications.                   | • [Speech to text quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text?tabs=windows\&pivots=programming-language-javascript) • [Text to speech quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-text-to-speech?tabs=windows\&pivots=programming-language-javascript) • [Speech translation quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-translation?tabs=windows\&pivots=programming-language-javascript) • [Speech SDK for JavaScript](https://learn.microsoft.com/en-us/javascript/api/microsoft-cognitiveservices-speech-sdk/?view=azure-node-latest\&branch=main\&preserve-view=true) • [Speech npm package](https://www.npmjs.com/package/microsoft-cognitiveservices-speech-sdk)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
  | ![Language icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/language.svg) [Language](https://learn.microsoft.com/en-us/azure/ai-services/language-service/overview)                                             | Build applications with natural language understanding capabilities.                                                     | • [Entity linking quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/entity-linking/quickstart?tabs=windows\&pivots=programming-language-javascript) • [Language detection quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/language-detection/quickstart?tabs=windows\&pivots=programming-language-javascript) • [Key Phrase extraction quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/key-phrase-extraction/quickstart?tabs=windows\&pivots=programming-language-javascript) • [Detecting named entities (NER) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/named-entity-recognition/quickstart?\&tabs=windows%2Cga-api\&pivots=programming-language-javascript) • [Detect Personally Identifiable Information (PII) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/quickstart?\&tabs=windows\&pivots=programming-language-javascript) • [Sentiment analysis and opinion mining quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/sentiment-opinion-mining/quickstart?tabs=windows\&pivots=programming-language-javascript) • [Using text, document and conversation summarization quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/summarization/quickstart?tabs=text-summarization%2Cwindows\&pivots=programming-language-javascript) • [Using Text Analytics for health quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/text-analytics-for-health/quickstart?tabs=windows\&pivots=programming-language-javascript) • [Language SDK for JavaScript (text analysis)](https://learn.microsoft.com/en-us/javascript/api/overview/azure/ai-language-text-readme?view=azure-node-latest\&preserve-view=true) • [Language npm package](https://www.npmjs.com/package/@azure/ai-language-text) |
  | ![Translator icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/translator.svg) [Translator](https://learn.microsoft.com/en-us/azure/ai-services/translator/overview)                                             | Use AI-powered translation technology to translate more than 100 in-use, at-risk, and endangered languages and dialects. | • [Translator SDK for JavaScript (text)](https://learn.microsoft.com/en-us/javascript/api/overview/azure/text-translation?view=azure-node-preview\&preserve-view=true) • [Translator npm package (text)](https://www.npmjs.com/package/@azure-rest/ai-translation-text/v/1.0.0-beta.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
  | ![Azure AI Search icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/search.svg) [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)                                    | Bring AI-powered cloud search to your mobile and web apps.                                                               | • [Use agentic retrieval quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-agentic-retrieval?tabs=search-perms%2Csearch-endpoint\&pivots=programming-language-javascript) • [Vector search quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector?tabs=keyless\&pivots=javascript) • [Classic generative search (RAG) using grounding data quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-rag?pivots=javascript) • [Full-text search quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-text?tabs=keyless%2Cwindows\&pivots=javascript) • [Semantic ranking quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-semantic?pivots=javascript) • [Chat with Azure OpenAI models using your own data quickstart](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/use-your-data-quickstart?view=foundry-classic\&viewFallbackFrom=foundry\&context=%2Fazure%2Fsearch%2Fcontext%2Fcontext\&tabs=keyless%2Ctypescript-keyless%2Cpython-new\&pivots=programming-language-javascript\&preserve-view=true) • [Azure AI Search SDK for JavaScript](https://learn.microsoft.com/en-us/javascript/api/overview/azure/search-documents-readme?view=azure-node-latest\&preserve-view=true) • [Azure AI Search npm package](https://www.npmjs.com/package/@azure/search-documents/v/12.0.0?activeTab=readme)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
  | ![Content Safety icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/content-safety.svg) [Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview)                             | Detect harmful content in applications and services.                                                                     | • [Analyze text content quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-text?tabs=visual-studio%2Cwindows\&pivots=programming-language-javascript) • [Use a text blocklist quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-blocklist?tabs=visual-studio%2Cwindows\&pivots=programming-language-javascript) • [Analyze image content quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-image?tabs=visual-studio%2Cwindows\&pivots=programming-language-javascript) • [Content Safety npm package](https://www.npmjs.com/package/@azure-rest/ai-content-safety/v/1.0.0-beta.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
  | ![Document Intelligence icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/document-intelligence.svg) [Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview) | Turn documents into intelligent data-driven solutions.                                                                   | • [Document Intelligence quickstart](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?view=doc-intel-4.0.0\&pivots=programming-language-javascript\&preserve-view=true) • [Document Intelligence SDK for JavaScript](https://learn.microsoft.com/en-us/javascript/api/overview/azure/ai-document-intelligence-rest-readme?view=azure-node-preview\&preserve-view=true) • [Document Intelligence npm package](https://www.npmjs.com/package/@azure-rest/ai-document-intelligence/v/1.0.0-beta.1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
  | ![Vision icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/vision.svg) [Vision](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview)                                                    | Analyze content in digital images and rich media assets.                                                                 | • [Azure Vision in Foundry Tools v3.2 GA Read quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/client-library?tabs=windows%2Cvisual-studio\&pivots=programming-language-javascript) • [Image Analysis quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/image-analysis-client-library-40?tabs=windows%2Cvisual-studio\&pivots=programming-language-javascript) • [Use the Face service quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/identity-client-library?tabs=windows%2Cvisual-studio\&pivots=programming-language-javascript) • [Vision SDK for JavaScript](https://learn.microsoft.com/en-us/javascript/api/overview/azure/ai-vision-image-analysis-rest-readme?view=azure-node-preview\&preserve-view=true) • [Vision npm package](https://www.npmjs.com/package/@azure-rest/ai-vision-image-analysis/v/1.0.0-beta.2)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  ### Python supported Foundry Tools

  | Foundry Tool                                                                                                                                                                                                                                                           | Description                                                                                                              | Quickstarts and reference documentation                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
  | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | ![Speech icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/speech.svg) [Speech](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/overview)                                                     | Add speech to text, text to speech, translation, and speaker recognition capabilities to applications.                   | • [Speech to text quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text?tabs=windows\&pivots=programming-language-python) • [Text to speech quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-text-to-speech?tabs=windows\&pivots=programming-language-python) • [Speech translation quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-translation?tabs=windows\&pivots=programming-language-python) • [Speech SDK for Python](https://learn.microsoft.com/en-us/python/api/azure-cognitiveservices-speech/?view=azure-python\&branch=main\&preserve-view=true) • [Speech PyPi package](https://pypi.org/project/azure-cognitiveservices-speech/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
  | ![Language icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/language.svg) [Language](https://learn.microsoft.com/en-us/azure/ai-services/language-service/overview)                                             | Build applications with natural language understanding capabilities.                                                     | • [Custom question answering (CQA) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/question-answering/quickstart/sdk?tabs=windows\&pivots=programming-language-python) • [Entity linking quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/entity-linking/quickstart?tabs=windows\&pivots=programming-language-python) • [Language detection quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/language-detection/quickstart?tabs=windows\&pivots=programming-language-python) • [Key Phrase extraction quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/key-phrase-extraction/quickstart?tabs=windows\&pivots=programming-language-python) • [Detect named entities (NER) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/named-entity-recognition/quickstart?\&tabs=windows%2Cga-api\&pivots=programming-language-python) • [Detect Personally Identifiable Information (PII) quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/quickstart?\&tabs=windows\&pivots=programming-language-python) • [Sentiment analysis and opinion mining quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/sentiment-opinion-mining/quickstart?tabs=windows\&pivots=programming-language-python) • [Using text, document and conversation summarization quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/summarization/quickstart?tabs=text-summarization%2Cwindows\&pivots=programming-language-python) • [Using Text Analytics for health quickstart](https://learn.microsoft.com/en-us/azure/ai-services/language-service/text-analytics-for-health/quickstart?tabs=windows\&pivots=programming-language-python) • [Language SDK for Python (text analysis)](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-textanalytics-readme?view=azure-python\&preserve-view=true) • [Language PyPi package (text analysis)](https://pypi.org/project/azure-cognitiveservices-language-textanalytics/) • [Language SDK for Python (question answering)](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-language-questionanswering-readme?view=azure-python\&preserve-view=true) • [Language PyPi package (question answering)](https://pypi.org/project/azure-ai-language-questionanswering/) • [Language SDK for Python (language conversations)](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-language-conversations-readme?view=azure-python\&preserve-view=true) • [Language PyPi package (language conversations)](https://pypi.org/project/azure-ai-language-conversations/) |
  | ![Translator icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/translator.svg) [Translator](https://learn.microsoft.com/en-us/azure/ai-services/translator/overview)                                             | Use AI-powered translation technology to translate more than 100 in-use, at-risk, and endangered languages and dialects. | • [Translator SDK for Python (text)](https://learn.microsoft.com/en-us/python/api/azure-ai-translation-text/azure.ai.translation.text?view=azure-python-preview\&preserve-view=true) • [Translator PyPi package (text)](https://pypi.org/project/azure-ai-translation-text/1.0.0b1/) • [Translator SDK for Python (batch)](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-translation-document-readme?view=azure-python\&preserve-view=true) • [Translator PyPi package (batch)](https://pypi.org/project/azure-ai-translation-document/1.0.0/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
  | ![Azure AI Search icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/search.svg) [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search)                                    | Bring AI-powered cloud search to your mobile and web apps.                                                               | • [Connect to a search service quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-rbac?pivots=python) • [Use agentic retrieval quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-agentic-retrieval?tabs=search-perms%2Csearch-endpoint\&pivots=programming-language-python) • [Vector search quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector?tabs=keyless\&pivots=python) • [Classic generative search (RAG) using grounding data quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-rag?pivots=python) • [Full-text search quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-text?tabs=keyless%2Cwindows\&pivots=python) • [Semantic ranking quickstart](https://learn.microsoft.com/en-us/azure/search/search-get-started-semantic?pivots=python) • [Chat with Azure OpenAI models using your own data quickstart](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/use-your-data-quickstart?view=foundry-classic\&viewFallbackFrom=foundry\&context=%2Fazure%2Fsearch%2Fcontext%2Fcontext\&tabs=keyless%2Ctypescript-keyless%2Cpython-new\&pivots=programming-language-python\&preserve-view=true) • [Azure AI Search SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/search-documents-readme?view=azure-python\&preserve-view=true) • [Azure AI Search PyPi package](https://pypi.org/project/azure-search-documents/11.6.0b1/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
  | ![Content Safety icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/content-safety.svg) [Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview)                             | Detect harmful content in applications and services.                                                                     | • [Analyze text content quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-text?tabs=visual-studio%2Cwindows\&pivots=programming-language-python) • [Use a text blocklist quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-blocklist?tabs=visual-studio%2Cwindows\&pivots=programming-language-python) • [Analyze image content quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-image?tabs=visual-studio%2Cwindows\&pivots=programming-language-python) • [Content Safety SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-contentsafety-readme?view=azure-python\&preserve-view=true) • [Content Safety PyPi package](https://pypi.org/project/azure-ai-contentsafety/1.0.0/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
  | ![Document Intelligence icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/document-intelligence.svg) [Document Intelligence](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview) | Turn documents into intelligent data-driven solutions.                                                                   | • [Document Intelligence quickstart](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?view=doc-intel-4.0.0\&pivots=programming-language-python\&preserve-view=true) • [Document Intelligence SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-documentintelligence-readme?view=azure-python-preview\&preserve-view=true) • [Document Intelligence PyPi package](https://pypi.org/project/azure-ai-documentintelligence/1.0.0b1/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
  | ![Vision icon](https://learn.microsoft.com/azure/ai-foundry/reusable-content/ce-skilling/azure/media/ai-services/vision.svg) [Vision](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/overview)                                                    | Analyze content in digital images and rich media assets.                                                                 | • [Azure Vision in Foundry Tools v3.2 GA Read quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/client-library?tabs=windows%2Cvisual-studio\&pivots=programming-language-python) • [Image Analysis quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/image-analysis-client-library-40?tabs=windows%2Cvisual-studio\&pivots=programming-language-python) • [Use the Face service quickstart](https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/quickstarts-sdk/identity-client-library?tabs=windows%2Cvisual-studio\&pivots=programming-language-python) • [Vision SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-vision-imageanalysis-readme?view=azure-python-preview\&preserve-view=true) • [Vision PyPi package](https://pypi.org/project/azure-ai-vision-imageanalysis/)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
</ZonePivot>

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Microsoft Foundry Models enables you to access the most powerful models from leading model providers through a single endpoint and set of credentials. This capability lets you switch between models and use them in your application without changing any code.

This article explains how the Foundry services organize models and how to use the inference endpoint to access them.

<Callout type="important">
  If you're currently using an Azure AI Inference beta SDK with Microsoft Foundry Models or Azure OpenAI service, we strongly recommend that you transition to the generally available [OpenAI/v1 API](https://aka.ms/openai/v1), which uses an OpenAI stable SDK.

  For more information on how to migrate to the OpenAI/v1 API by using an SDK in your programming language of choice, see [Migrate from Azure AI Inference SDK to OpenAI SDK](../../how-to/model-inference-to-openai-migration).
</Callout>

## Deployments

Foundry uses **deployments** to make models available. **Deployments** give a model a name and set specific configurations. You can access a model by using its deployment name in your requests.

A deployment includes:

* A model name
* A model version
* A provisioning or capacity type1
* A content filtering configuration1
* A rate limiting configuration1

1 These configurations can change depending on the selected model.

A Foundry resource can have many model deployments. You only pay for inference performed on model deployments. Deployments are Azure resources, so they're subject to Azure policies.

For more information about creating deployments, see [Add and configure model deployments](../how-to/create-model-deployments).

## Azure OpenAI inference endpoint

The **Azure OpenAI API** exposes the full capabilities of OpenAI models and supports more features like assistants, threads, files, and batch inference. You might also access non-OpenAI models through this route.

Azure OpenAI endpoints, usually of the form `https://<resource-name>.openai.azure.com`, work at the deployment level and each deployment has its own associated URL. However, you can use the same authentication mechanism to consume the deployments. For more information, see the reference page for [Azure OpenAI API](../../openai/reference).

![An illustration showing how Azure OpenAI deployments contain a single URL for each deployment.](https://learn.microsoft.com/azure/ai-foundry/media/endpoint/endpoint-openai.png)

Each deployment has a URL that's formed by concatenating the **Azure OpenAI** base URL and the route `/deployments/<model-deployment-name>`.

<Tabs>
  <Tab title="Python">
    Install the package `openai` using your package manager, like pip:

    ```bash
    pip install openai --upgrade
    ```

    Then, you can use the package to consume the model. The following example shows how to create a client to consume chat completions:

    ```python
    import os
    from openai import AzureOpenAI

    client = AzureOpenAI(
        azure_endpoint = "https://<resource>.services.ai.azure.com"
        api_key=os.getenv("AZURE_INFERENCE_CREDENTIAL"),
        api_version="2024-10-21",
    )
    ```
  </Tab>

  <Tab title="JavaScript">
    Install the package `openai` using npm:

    ```bash
    npm install openai
    ```

    Then, you can use the package to consume the model. The following example shows how to create a client to consume chat completions:

    ```javascript
    import { AzureKeyCredential } from "@azure/openai";

    const endpoint = "https://<resource>.services.ai.azure.com";
    const apiKey = new AzureKeyCredential(process.env.AZURE_INFERENCE_CREDENTIAL);
    const apiVersion = "2024-10-21"

    const client = new AzureOpenAI({
        endpoint,
        apiKey,
        apiVersion,
        "deepseek-v3-0324"
    });
    ```

    Here, `deepseek-v3-0324` is the name of a model deployment in the Microsoft Foundry resource.
  </Tab>

  <Tab title="C#">
    Install the OpenAI library with the following command:

    ```dotnetcli
    dotnet add package Azure.AI.OpenAI --prerelease
    ```

    You can use the package to consume the model. The following example shows how to create a client to consume chat completions:

    ```csharp
    AzureOpenAIClient client = new(
        new Uri("https://<resource>.services.ai.azure.com"),
        new ApiKeyCredential(Environment.GetEnvironmentVariable("AZURE_INFERENCE_CREDENTIAL"))
    );
    ```
  </Tab>

  <Tab title="Java">
    Add the package to your project:

    ```xml
    <dependency>
        <groupId>com.azure</groupId>
        <artifactId>azure-ai-openai</artifactId>
        <version>1.0.0-beta.16</version>
    </dependency>
    ```

    Then, you can use the package to consume the model. The following example shows how to create a client to consume chat completions:

    ```java
    OpenAIClient client = new OpenAIClientBuilder()
        .credential(new AzureKeyCredential("{key}"))
        .endpoint("https://<resource>.services.ai.azure.com")
        .buildClient();
    ```
  </Tab>

  <Tab title="REST">
    Use the reference section to explore the API design and which parameters are available. For example, the reference section for Chat completions details how to use the route `/chat/completions` to generate predictions based on chat-formatted instructions:

    **Request**

    ```HTTP/1.1
    POST https://<resource>.services.ai.azure.com/openai/deployments/deepseek-v3-0324/chat/completions?api-version=2024-10-21
    api-key: <api-key>
    Content-Type: application/json
    ```

    Here, `deepseek-v3-0324` is the name of a model deployment in the Foundry resource.
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    ```python
    response = client.chat.completions.create(
        model="deepseek-v3-0324", # Replace with your model deployment name.
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Explain Riemann's conjecture in 1 paragraph"}
        ]
    )

    print(response.model_dump_json(indent=2)
    ```
  </Tab>

  <Tab title="JavaScript">
    ```javascript
    var messages = [
        { role: "system", content: "You are a helpful assistant" },
        { role: "user", content: "Explain Riemann's conjecture in 1 paragraph" },
    ];

    const response = await client.chat.completions.create({ messages, model: "deepseek-v3-0324" });

    console.log(response.choices[0].message.content)
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    ChatCompletion response = chatClient.CompleteChat(
        [
            new SystemChatMessage("You are a helpful assistant."),
            new UserChatMessage("Explain Riemann's conjecture in 1 paragraph"),
        ]);

    Console.WriteLine($"{response.Role}: {response.Content[0].Text}");
    ```
  </Tab>

  <Tab title="Java">
    ```java
    List<ChatRequestMessage> chatMessages = new ArrayList<>();
    chatMessages.add(new ChatRequestSystemMessage("You are a helpful assistant"));
    chatMessages.add(new ChatRequestUserMessage("Explain Riemann's conjecture in 1 paragraph"));

    ChatCompletions chatCompletions = client.getChatCompletions("deepseek-v3-0324",
        new ChatCompletionsOptions(chatMessages));

    System.out.printf("Model ID=%s is created at %s.%n", chatCompletions.getId(), chatCompletions.getCreatedAt());
    for (ChatChoice choice : chatCompletions.getChoices()) {
        ChatResponseMessage message = choice.getMessage();
        System.out.printf("Index: %d, Chat Role: %s.%n", choice.getIndex(), message.getRole());
        System.out.println("Message:");
        System.out.println(message.getContent());
    }
    ```

    Here, `deepseek-v3-0324` is the name of a model deployment in the Microsoft Foundry resource.
  </Tab>

  <Tab title="REST">
    **Request**

    ```HTTP/1.1
    POST https://<resource>.services.ai.azure.com/openai/deployments/deepseek-v3-0324/chat/completions?api-version=2024-10-21
    api-key: <api-key>
    Content-Type: application/json
    ```

    ```JSON
    {
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant"
            },
            {
                "role": "user",
                "content": "Explain Riemann's conjecture in 1 paragraph"
            }
        ]
    }
    ```

    Here, `deepseek-v3-0324` is the name of a model deployment in the Foundry resource.
  </Tab>
</Tabs>

For more information about how to use the **Azure OpenAI endpoint**, see [Azure OpenAI in Foundry Models documentation](models-sold-directly-by-azure).

## Keyless authentication

Models deployed to Foundry Models in Foundry Tools support keyless authorization by using Microsoft Entra ID. Keyless authorization enhances security, simplifies the user experience, reduces operational complexity, and provides robust compliance support for modern development. It makes keyless authorization a strong choice for organizations adopting secure and scalable identity management solutions.

To use keyless authentication, [configure your resource and grant access to users](../how-to/configure-entra-id) to perform inference. After you configure the resource and grant access, authenticate as follows:

<Tabs>
  <Tab title="Python">
    Install the OpenAI SDK using a package manager like pip:

    ```bash
    pip install openai
    ```

    For Microsoft Entra ID authentication, also install:

    ```bash
    pip install azure-identity
    ```

    Use the package to consume the model. The following example shows how to create a client to consume chat completions with Microsoft Entra ID and make a test call to the chat completions endpoint with your model deployment.

    Replace `<resource>` with your Foundry resource name. Find it in the Azure portal or by running `az cognitiveservices account list`. Replace `DeepSeek-V3.1` with your actual deployment name.

    ```python
    from openai import OpenAI
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider

    token_provider = get_bearer_token_provider(
        DefaultAzureCredential(),
        "https://cognitiveservices.azure.com/.default"
    )

    client = OpenAI(
        base_url="https://<resource>.openai.azure.com/openai/v1/",
        api_key=token_provider,
    )

    completion = client.chat.completions.create(
        model="DeepSeek-V3.1",  # Required: your deployment name
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "What is Azure AI?"}
        ]
    )

    print(completion.choices[0].message.content)
    ```

    Expected output

    ```output
    Azure AI is a comprehensive suite of artificial intelligence services and tools from Microsoft that enables developers to build intelligent applications. It includes services for natural language processing, computer vision, speech recognition, and machine learning capabilities.
    ```

    Reference: [OpenAI Python SDK](https://github.com/openai/openai-python) and [DefaultAzureCredential class](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential).
  </Tab>

  <Tab title="C#">
    Install the OpenAI SDK:

    ```dotnetcli
    dotnet add package OpenAI
    ```

    For Microsoft Entra ID authentication, also install the `Azure.Identity` package:

    ```dotnetcli
    dotnet add package Azure.Identity
    ```

    Import the following namespaces:

    ```csharp
    using Azure.Identity;
    using OpenAI;
    using OpenAI.Chat;
    using System.ClientModel.Primitives;
    ```

    Then, use the package to consume the model. The following example shows how to create a client to consume chat completions with Microsoft Entra ID, and then make a test call to the chat completions endpoint with your model deployment.

    Replace `<resource>` with your Foundry resource name (find it in the Azure portal). Replace `gpt-4o-mini` with your actual deployment name.

    ```csharp
    #pragma warning disable OPENAI001

    BearerTokenPolicy tokenPolicy = new(
        new DefaultAzureCredential(),
        "https://cognitiveservices.azure.com/.default"
    );

    ChatClient client = new(
        model: "gpt-4o-mini", // Your deployment name
        authenticationPolicy: tokenPolicy,
        options: new OpenAIClientOptions() {
            Endpoint = new Uri("https://<resource>.openai.azure.com/openai/v1/")
        }
    );

    ChatCompletion completion = client.CompleteChat(
        new SystemChatMessage("You are a helpful assistant."),
        new UserChatMessage("What is Azure AI?")
    );

    Console.WriteLine(completion.Content[0].Text);
    ```

    Expected output:

    ```output
    Azure AI is a comprehensive suite of artificial intelligence services and tools from Microsoft that enables developers to build intelligent applications. It includes services for natural language processing, computer vision, speech recognition, and machine learning capabilities.
    ```

    Reference: [OpenAI .NET SDK](https://github.com/openai/openai-dotnet) and [DefaultAzureCredential class](https://learn.microsoft.com/en-us/dotnet/api/azure.identity.defaultazurecredential).
  </Tab>

  <Tab title="JavaScript">
    Install the OpenAI SDK with npm:

    ```bash
    npm install openai
    ```

    For Microsoft Entra ID authentication, also install:

    ```bash
    npm install @azure/identity
    ```

    Then, use the package to consume the model. The following example shows how to create a client to consume chat completions with Microsoft Entra ID, and then make a test call to the chat completions endpoint with your model deployment.

    Replace `<resource>` with your Foundry resource name (find it in the Azure portal or by running `az cognitiveservices account list`). Replace `DeepSeek-V3.1` with your actual deployment name.

    ```javascript
    import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
    import { OpenAI } from "openai";

    const tokenProvider = getBearerTokenProvider(
        new DefaultAzureCredential(),
        'https://cognitiveservices.azure.com/.default'
    );

    const client = new OpenAI({
        baseURL: "https://<resource>.openai.azure.com/openai/v1/",
        apiKey: tokenProvider
    });

    const completion = await client.chat.completions.create({
        model: "DeepSeek-V3.1", // Required: your deployment name
        messages: [
            { role: "system", content: "You are a helpful assistant." },
            { role: "user", content: "What is Azure AI?" }
        ]
    });

    console.log(completion.choices[0].message.content);
    ```

    Expected output:

    ```output
    Azure AI is a comprehensive suite of artificial intelligence services and tools from Microsoft that enables developers to build intelligent applications. It includes services for natural language processing, computer vision, speech recognition, and machine learning capabilities.
    ```

    Reference: [OpenAI Node.js SDK](https://github.com/openai/openai-node) and [DefaultAzureCredential class](https://learn.microsoft.com/en-us/javascript/api/@azure/identity/defaultazurecredential).
  </Tab>

  <Tab title="Java">
    Add the OpenAI SDK to your project. Check the [OpenAI Java GitHub repository](https://github.com/openai/openai-java) for the latest version and installation instructions.

    For Microsoft Entra ID authentication, also add:

    ```xml
    <dependency>
        <groupId>com.azure</groupId>
        <artifactId>azure-identity</artifactId>
        <version>1.18.0</version>
    </dependency>
    ```

    Then, use the package to consume the model. The following example shows how to create a client to consume chat completions with Microsoft Entra ID, and then make a test call to the chat completions endpoint with your model deployment.

    Replace `<resource>` with your Foundry resource name (find it in the Azure portal). Replace `DeepSeek-V3.1` with your actual deployment name.

    ```java
    import com.openai.client.OpenAIClient;
    import com.openai.client.okhttp.OpenAIOkHttpClient;
    import com.azure.identity.DefaultAzureCredential;
    import com.azure.identity.DefaultAzureCredentialBuilder;
    import com.openai.models.chat.completions.*;

    DefaultAzureCredential tokenCredential = new DefaultAzureCredentialBuilder().build();

    OpenAIClient client = OpenAIOkHttpClient.builder()
        .baseUrl("https://<resource>.openai.azure.com/openai/v1/")
        .credential(BearerTokenCredential.create(
            AuthenticationUtil.getBearerTokenSupplier(
                tokenCredential,
                "https://cognitiveservices.azure.com/.default"
            )
        ))
        .build();

    ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
        .addSystemMessage("You are a helpful assistant.")
        .addUserMessage("What is Azure AI?")
        .model("DeepSeek-V3.1") // Required: your deployment name
        .build();

    ChatCompletion completion = client.chat().completions().create(params);
    System.out.println(completion.choices().get(0).message().content());
    ```

    Expected output:

    ```output
    Azure AI is a comprehensive suite of artificial intelligence services and tools from Microsoft that enables developers to build intelligent applications. It includes services for natural language processing, computer vision, speech recognition, and machine learning capabilities.
    ```

    Reference: [OpenAI Java SDK](https://github.com/openai/openai-java) and [DefaultAzureCredential class](https://learn.microsoft.com/en-us/java/api/com.azure.identity.defaultazurecredential).
  </Tab>

  <Tab title="REST">
    Explore the API design in the reference section to see which parameters are available. Indicate the authentication token in the header `Authorization`. For example, the [Chat completion](../../openai/latest#create-chat-completion) reference section details how to use the `/chat/completions` route to generate predictions based on chat-formatted instructions. The path `/models` is included in the root of the URL:

    **Request**

    Replace `<resource>` with your Foundry resource name (find it in the Azure portal or by running `az cognitiveservices account list`). Replace `MAI-DS-R1` with your actual deployment name.

    The base\_url will accept both `https://<resource>.openai.azure.com/openai/v1/` and `https://<resource>.services.ai.azure.com/openai/v1/` formats.

    ```bash
    curl -X POST https://<resource>.openai.azure.com/openai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_OPENAI_AUTH_TOKEN" \
      -d '{
          "model": "MAI-DS-R1",
          "messages": [
          {
            "role": "system",
            "content": "You are a helpful assistant."
          },
          {
            "role": "user",
            "content": "Explain what the bitter lesson is?"
          }
        ]
      }'
    ```

    **Response**

    If authentication is successful, you receive a `200 OK` response with chat completion results in the response body:

    ```json
    {
      "id": "chatcmpl-...",
      "object": "chat.completion",
      "created": 1738368234,
      "model": "MAI-DS-R1",
      "choices": [
        {
          "index": 0,
          "message": {
            "role": "assistant",
            "content": "The bitter lesson refers to a key insight in AI research that emphasizes the importance of general-purpose learning methods that leverage computation, rather than human-designed domain-specific approaches. It suggests that methods which scale with increased computation tend to be more effective in the long run."
          },
          "finish_reason": "stop"
        }
      ],
      "usage": {
        "prompt_tokens": 28,
        "completion_tokens": 52,
        "total_tokens": 80
      }
    }
    ```

    Tokens must be issued with scope `https://cognitiveservices.azure.com/.default`.

    For testing purposes, the easiest way to get a valid token for your user account is to use the Azure CLI. In a console, run the following Azure CLI command:

    ```azurecli
    az account get-access-token --resource https://cognitiveservices.azure.com --query "accessToken" --output tsv
    ```

    This command outputs an access token that you can store in the `$AZURE_OPENAI_AUTH_TOKEN` environment variable.

    Reference: [Chat Completions API](../../openai/latest#create-chat-completion)
  </Tab>
</Tabs>

## Related content

* [Foundry Models and capabilities](models-sold-directly-by-azure)
* [Deployment types in Foundry Models](deployment-types)
* [Model and region availability for Foundry Models](../../how-to/deploy-models-serverless-availability)
* [What is Azure OpenAI in Foundry Models?](models-sold-directly-by-azure)

https://learn.microsoft.com/en-us/javascript/api/overview/azure/ai-projects-readme?view=azure-node-latest&preserve-view=true

This article shows you how to use the v1 Azure OpenAI API. The v1 API simplifies authentication, removes the need for dated `api-version` parameters, and supports cross-provider model calls.

<Callout type="note">
  New API response objects might be added to the API response at any time. We recommend you only parse the response objects you require.
</Callout>

## Prerequisites

* An Azure subscription - [Create one for free](https://azure.microsoft.com/free/cognitive-services)
* A [Foundry resource](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource) or [Azure OpenAI resource](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/create-resource) deployed in a [supported region](https://azure.microsoft.com/explore/global-infrastructure/products-by-region/?products=cognitive-services)
* At least one [model deployment](../foundry-models/how-to/deploy-foundry-models)
* For Microsoft Entra ID authentication: the `Cognitive Services OpenAI User` role assigned to your identity. For more information, see [Role-based access control for Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/role-based-access-control)

## API evolution

Previously, Azure OpenAI received monthly updates of new API versions. Taking advantage of new features required constantly updating code and environment variables with each new API release. Azure OpenAI also required the extra step of using Azure specific clients which created overhead when migrating code between OpenAI and Azure OpenAI.

Starting in August 2025, you can opt in to the next generation v1 Azure OpenAI APIs which add support for:

* Ongoing access to the latest features with no need to specify new `api-version`'s each month.
* Faster API release cycle with new features launching more frequently.
* OpenAI client support with minimal code changes to swap between OpenAI and Azure OpenAI when using key-based authentication.
* OpenAI client support for token based authentication and automatic token refresh without the need to take a dependency on a separate Azure OpenAI client.
* Make chat completions calls with models from other providers like DeepSeek and Grok which support the v1 chat completions syntax.

Access to new API calls that are still in preview will be controlled by passing feature specific preview headers allowing you to opt in to the features you want, without having to swap API versions. Alternatively, some features will indicate preview status through their API path and don't require an additional header.

Examples:

* `/openai/v1/evals` is in preview and requires passing an `"aoai-evals":"preview"` header.
* `/openai/v1/fine_tuning/alpha/graders/` is in preview and requires no custom header due to the presence of `alpha` in the API path.

For the initial v1 Generally Available (GA) API launch, only a subset of the inference and authoring API capabilities are supported. All GA features are supported for use in production. Support for more capabilities is being added rapidly.

## Code changes

<Tabs>
  <Tab title="Python">
    ### v1 API

    [Python v1 examples](supported-languages)

    **API Key**:

    ```python
    import os
    from openai import OpenAI

    client = OpenAI(
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        base_url="https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/"
    )

    response = client.responses.create(
      model="gpt-4.1-nano", # Replace with your model deployment name
      input="This is a test.",
    )

    print(response.model_dump_json(indent=2))
    ```

    Key differences from the previous API:

    * `OpenAI()` client is used instead of `AzureOpenAI()`.
    * `base_url` passes the Azure OpenAI endpoint and `/openai/v1` is appended to the endpoint address.
    * `api-version` is no longer a required parameter with the v1 GA API.

    **API Key** with environment variables:

    Set the following environment variables before running the code:

    | Variable          | Value                                                    |
    | ----------------- | -------------------------------------------------------- |
    | `OPENAI_BASE_URL` | `https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/` |
    | `OPENAI_API_KEY`  | Your Azure OpenAI API key                                |

    Then create the client without parameters:

    ```python
    client = OpenAI()
    ```

    **Microsoft Entra ID**:

    <Callout type="important">
      Handling automatic token refresh was previously handled through use of the `AzureOpenAI()` client. The v1 API removes this dependency, by adding automatic token refresh support to the `OpenAI()` client.
    </Callout>

    ```python
    from openai import OpenAI
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider

    token_provider = get_bearer_token_provider(
        DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
    )

    client = OpenAI(
      base_url = "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/",
      api_key = token_provider
    )

    response = client.responses.create(
        model="gpt-4.1-nano",
        input= "This is a test"
    )

    print(response.model_dump_json(indent=2))
    ```

    * `base_url` passes the Azure OpenAI endpoint and `/openai/v1` is appended to the endpoint address.
    * `api_key` parameter is set to `token_provider`, enabling automatic retrieval and refresh of an authentication token instead of using a static API key.
  </Tab>

  <Tab title="C#">
    ### v1 API

    [C# v1 examples](supported-languages)

    **API Key**:

    ```csharp
    OpenAIClient client = new(
        new ApiKeyCredential("{your-api-key}"),
        new OpenAIClientOptions()
        {
            Endpoint = new("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/"),
        })
    ```

    **Microsoft Entra ID**:

    ```csharp
    #pragma warning disable OPENAI001

    BearerTokenPolicy tokenPolicy = new(
        new DefaultAzureCredential(),
        "https://cognitiveservices.azure.com/.default");
    OpenAIClient client = new(
        authenticationPolicy: tokenPolicy,
        options: new OpenAIClientOptions()
        {
            Endpoint = new("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/"),
        })
    ```
  </Tab>

  <Tab title="JavaScript">
    ### v1 API

    [JavaScript v1 examples](supported-languages)

    **API Key**:

    ```javascript
    const client = new OpenAI({
        baseURL: "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/",
        apiKey: "{your-api-key}"
    });
    ```

    **API Key** with environment variables set for `OPENAI_BASE_URL` and `OPENAI_API_KEY`:

    ```javascript
    const client = new OpenAI();
    ```

    **Microsoft Entra ID**:

    ```javascript
    const tokenProvider = getBearerTokenProvider(
        new DefaultAzureCredential(),
        'https://cognitiveservices.azure.com/.default');
    const client = new OpenAI({
        baseURL: "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/",
        apiKey: tokenProvider
    });
    ```
  </Tab>

  <Tab title="Go">
    ### v1 API

    [Go v1 examples](supported-languages)

    **API Key**:

    ```go
    client := openai.NewClient(
        option.WithBaseURL("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/"),
        option.WithAPIKey("{your-api-key}")
    )
    ```

    **API Key** with environment variables set for `OPENAI_BASE_URL` and `OPENAI_API_KEY`:

    ```go
    client := openai.NewClient()
    ```

    **Microsoft Entra ID**:

    ```go
    tokenCredential, err := azidentity.NewDefaultAzureCredential(nil)

    client := openai.NewClient(
        option.WithBaseURL("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/"),
        azure.WithTokenCredential(tokenCredential)
    )
    ```
  </Tab>

  <Tab title="Java">
    [Java v1 examples](supported-languages)

    ### v1 API

    **API Key**:

    ```java

    OpenAIClient client = OpenAIOkHttpClient.builder()
                    .baseUrl("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/")
                    .apiKey(apiKey)
                    .build();
    ```

    **API Key** with environment variables set for `OPENAI_BASE_URL` and `OPENAI_API_KEY`:

    ```java
    OpenAIClient client = OpenAIOkHttpClient.builder()
                    .fromEnv()
                    .build();
    ```

    **Microsoft Entra ID**:

    ```java
    Credential tokenCredential = BearerTokenCredential.create(
            AuthenticationUtil.getBearerTokenSupplier(
                    new DefaultAzureCredentialBuilder().build(),
                    "https://cognitiveservices.azure.com/.default"));
    OpenAIClient client = OpenAIOkHttpClient.builder()
            .baseUrl("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/")
            .credential(tokenCredential)
            .build();
    ```
  </Tab>

  <Tab title="REST">
    ### v1 API

    **API Key**:

    ```bash
    curl -X POST https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/responses \
      -H "Content-Type: application/json" \
      -H "api-key: $AZURE_OPENAI_API_KEY" \
      -d '{
         "model": "gpt-4.1-nano",
         "input": "This is a test"
        }'
    ```

    **Microsoft Entra ID**:

    ```bash
    curl -X POST https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/responses \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_OPENAI_AUTH_TOKEN" \
      -d '{
         "model": "gpt-4o",
         "input": "This is a test"
        }'
    ```
  </Tab>
</Tabs>

## Model support

For Azure OpenAI models we recommend using the [Responses API](supported-languages), however, the v1 API also allows you to make chat completions calls with models from other providers like DeepSeek and Grok which support the OpenAI v1 chat completions syntax.

`base_url` will accept both `https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/` and `https://YOUR-RESOURCE-NAME.services.ai.azure.com/openai/v1/` formats.

<Callout type="note">
  Responses API also works with Foundry Models sold directly by Azure, such as Microsoft AI, DeepSeek, and Grok models. To learn how to use the Responses API with these models, see [How to generate text responses with Microsoft Foundry Models](../foundry-models/how-to/generate-responses).
</Callout>

<Tabs>
  <Tab title="Python">
    ```python
    from openai import OpenAI
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider

    token_provider = get_bearer_token_provider(
        DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
    )

    client = OpenAI(
      base_url = "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/",
      api_key=token_provider,
    )
    completion = client.chat.completions.create(
      model="MAI-DS-R1", # Replace with your model deployment name.
      messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Tell me about the attention is all you need paper"}
      ]
    )

    #print(completion.choices[0].message)
    print(completion.model_dump_json(indent=2))
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    using Azure.Identity;
    using OpenAI;
    using OpenAI.Chat;
    using System.ClientModel.Primitives;

    #pragma warning disable OPENAI001

    BearerTokenPolicy tokenPolicy = new(
        new DefaultAzureCredential(),
        "https://cognitiveservices.azure.com/.default");

    ChatClient client = new(
        model: "MAI-DS-R1", // Replace with your model deployment name.
        authenticationPolicy: tokenPolicy,
        options: new OpenAIClientOptions() {

            Endpoint = new Uri("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1")
       }
    );

    ChatCompletion completion = client.CompleteChat("Tell me about the attention is all you need paper");

    Console.WriteLine($"[ASSISTANT]: {completion.Content[0].Text}");
    ```
  </Tab>

  <Tab title="JavaScript">
    ```javascript
    import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
    import { OpenAI } from "openai";

    const tokenProvider = getBearerTokenProvider(
        new DefaultAzureCredential(),
        'https://cognitiveservices.azure.com/.default');
    const client = new OpenAI({
        baseURL: "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/",
        apiKey: tokenProvider
    });

    const messages = [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: 'Tell me about the attention is all you need paper' }
    ];

    // Make the API request with top-level await
    const result = await client.chat.completions.create({
        messages,
        model: 'MAI-DS-R1', // model deployment name
        max_tokens: 100
    });

    // Print the full response
    console.log('Full response:', result);

    // Print just the message content from the response
    console.log('Response content:', result.choices[0].message.content);
    ```
  </Tab>

  <Tab title="Go">
    ```go

    package main

    import (
    	"context"
    	"fmt"
    	"log"

    	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
    	"github.com/openai/openai-go/v3"
    	"github.com/openai/openai-go/v3/azure"
    	"github.com/openai/openai-go/v3/option"
    )

    func main() {
    	// Create an Azure credential
    	tokenCredential, err := azidentity.NewDefaultAzureCredential(nil)
    	if err != nil {
    		log.Fatalf("Failed to create credential: %s", err)
    	}

    	// Create a client with Azure OpenAI endpoint and token credential
    	client := openai.NewClient(
    		option.WithBaseURL("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/"),
    		azure.WithTokenCredential(tokenCredential),
    	)

    	// Make a completion request
    	chatCompletion, err := client.Chat.Completions.New(context.TODO(), openai.ChatCompletionNewParams{
    		Messages: []openai.ChatCompletionMessageParamUnion{
    			openai.UserMessage("Explain what the bitter lesson is?"),
    		},
    		Model: "MAI-DS-R1", // Use your deployed model name on Azure
    	})
    	if err != nil {
    		log.Fatalf("Failed to get chat completions: %s", err)
    	}

    	fmt.Println(chatCompletion.Choices[0].Message.Content)
    }
    ```
  </Tab>

  <Tab title="Java">
    ```java
    package com.example;

    import com.openai.client.OpenAIClient;
    import com.openai.client.okhttp.OpenAIOkHttpClient;
    import com.openai.models.ChatModel;
    import com.openai.models.chat.completions.ChatCompletion;
    import com.openai.models.chat.completions.ChatCompletionCreateParams;

    public class OpenAITest {
        public static void main(String[] args) {
            // Get API key from environment variable for security
            String apiKey = System.getenv("OPENAI_API_KEY");
            String resourceName = "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1";
            String modelDeploymentName = "MAI-DS-R1"; //replace with your model deployment name

            try {
                OpenAIClient client = OpenAIOkHttpClient.builder()
                        .baseUrl(resourceName)
                        .apiKey(apiKey)
                        .build();

               ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
                  .addUserMessage("Explain what the bitter lesson is?")
                  .model(modelDeploymentName)
                  .build();
               ChatCompletion chatCompletion = client.chat().completions().create(params);
            }
        }
    }
    ```
  </Tab>

  <Tab title="REST">
    ```bash
    curl -X POST https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_OPENAI_AUTH_TOKEN" \
      -d '{
          "model": "MAI-DS-R1",
          "messages": [
          {
            "role": "developer",
            "content": "You are a helpful assistant."
          },
          {
            "role": "user",
            "content": "Explain what the bitter lesson is?"
          }
        ]
      }'
    ```
  </Tab>
</Tabs>

## v1 API support

* [v1 OpenAPI 3.0 spec](https://github.com/Azure/azure-rest-api-specs/blob/main/specification/ai/data-plane/OpenAI.v1/azure-v1-v1-generated.json)

### Status

Generally Available features are supported for use in production.

| API Path                                                                                        | Status              |
| ----------------------------------------------------------------------------------------------- | ------------------- |
| `/openai/v1/chat/completions`                                                                   | Generally Available |
| `/openai/v1/embeddings`                                                                         | Generally Available |
| `/openai/v1/evals`                                                                              | Preview             |
| `/openai/v1/files`                                                                              | Generally Available |
| `/openai/v1/fine_tuning/jobs/{fine_tuning_job_id}/checkpoints/{fine_tuning_checkpoint_id}/copy` | Preview             |
| `/openai/v1/fine_tuning/alpha/graders/`                                                         | Preview             |
| `/openai/v1/fine_tuning/`                                                                       | Generally Available |
| `/openai/v1/models`                                                                             | Generally Available |
| `/openai/v1/responses`                                                                          | Generally Available |
| `/openai/v1/vector_stores`                                                                      | Generally Available |

### Preview headers

| API Path                                                                                        | Header                                   |
| ----------------------------------------------------------------------------------------------- | :--------------------------------------- |
| `/openai/v1/evals`                                                                              | `"aoai-evals":"preview"`                 |
| `/openai/v1/fine_tuning/jobs/{fine_tuning_job_id}/checkpoints/{fine_tuning_checkpoint_id}/copy` | `"aoai-copy-ft-checkpoints" : "preview"` |

## API version changelog

The following sections summarize changes between API versions.

### Changes between v1 preview release and 2025-04-01-preview

* [v1 preview API](#api-evolution)

* [Video generation support](concepts/video-generation)

* **NEW** Responses API features:

  * Remote Model Context Protocol (MCP) servers tool integration
  * Support for asynchronous background tasks
  * Encrypted reasoning items
  * Image generation

### Changes between 2025-04-01-preview and 2025-03-01-preview

* [`GPT-image-1` support](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/dall-e)
* [Reasoning summary for `o3` and `o4-mini`](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning)
* [Evaluation API](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/authoring-reference-preview#evaluation---create)

### Changes between 2025-03-01-preview and 2025-02-01-preview

* [Responses API](how-to/responses)
* Computer use

### Changes between 2025-02-01-preview and 2025-01-01-preview

* Stored completions (distillation API support).

### Changes between 2025-01-01-preview and 2024-12-01-preview

* `prediction` parameter added for [predicted outputs](how-to/predicted-outputs) support.
* `gpt-4o-audio-preview` [model support](audio-completions-quickstart).

### Changes between 2024-12-01-preview and 2024-10-01-preview

* `store`, and `metadata` parameters added for stored completions support.
* `reasoning_effort` added for latest [reasoning models](how-to/reasoning).
* `user_security_context` added for [Microsoft Defender for Cloud integration](https://aka.ms/TP4AI/Documentation/EndUserContext).

### Changes between 2024-09-01-preview and 2024-08-01-preview

* `max_completion_tokens` added to support `o1-preview` and `o1-mini` models. `max_tokens` doesn't work with the **o1 series** models.
* `parallel_tool_calls` added.
* `completion_tokens_details` & `reasoning_tokens` added.
* `stream_options` & `include_usage` added.

### Changes between 2024-07-01-preview and 2024-08-01-preview API specification

* [Structured outputs support](how-to/structured-outputs).

* Large file upload API added.

* On your data changes:

  * Mongo DB integration.
  * `role_information` parameter removed.
  * [`rerank_score`](https://github.com/Azure/azure-rest-api-specs/blob/2b700e5e84d4a95880d373e6a4bce5d16882e4b5/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview/2024-08-01-preview/inference.json#L5532) added to citation object.
  * AML datasource removed.
  * AI Search vectorization integration improvements.

### Changes between 2024-05-01-preview and 2024-07-01-preview API specification

* [Batch API support added](how-to/batch)
* [Vector store chunking strategy parameters](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview?#request-body-17)
* `max_num_results` that the file search tool should output.

### Changes between 2024-04-01-preview and 2024-05-01-preview API specification

* Assistants v2 support - [File search tool and vector storage](https://go.microsoft.com/fwlink/?linkid=2272425)
* Fine-tuning [checkpoints](https://github.com/Azure/azure-rest-api-specs/blob/9583ed6c26ce1f10bbea92346e28a46394a784b4/specification/cognitiveservices/data-plane/AzureOpenAI/authoring/preview/2024-05-01-preview/azureopenai.json#L586), [seed](https://github.com/Azure/azure-rest-api-specs/blob/9583ed6c26ce1f10bbea92346e28a46394a784b4/specification/cognitiveservices/data-plane/AzureOpenAI/authoring/preview/2024-05-01-preview/azureopenai.json#L1574), [events](https://github.com/Azure/azure-rest-api-specs/blob/9583ed6c26ce1f10bbea92346e28a46394a784b4/specification/cognitiveservices/data-plane/AzureOpenAI/authoring/preview/2024-05-01-preview/azureopenai.json#L529)
* On your data updates
* DALL-E 2 now supports model deployment and can be used with the latest preview API.
* Content filtering updates

### Changes between 2024-03-01-preview and 2024-04-01-preview API specification

* **Breaking Change**: Enhancements parameters removed. This impacts the `gpt-4` **Version:** `vision-preview` model.
* [timestamp\_granularities](https://github.com/Azure/azure-rest-api-specs/blob/fbc90d63f236986f7eddfffe3dca6d9d734da0b2/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview/2024-04-01-preview/inference.json#L5217) parameter added.
* [`audioWord`](https://github.com/Azure/azure-rest-api-specs/blob/fbc90d63f236986f7eddfffe3dca6d9d734da0b2/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview/2024-04-01-preview/inference.json#L5286) object added.
* Additional TTS [`response_formats: wav & pcm`](https://github.com/Azure/azure-rest-api-specs/blob/fbc90d63f236986f7eddfffe3dca6d9d734da0b2/specification/cognitiveservices/data-plane/AzureOpenAI/inference/preview/2024-04-01-preview/inference.json#L5333).

## Troubleshooting

| Issue                                       | Cause                                | Solution                                                                                                                                                                           |
| ------------------------------------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `404 Not Found` when calling the v1 API     | Incorrect `base_url` format          | Verify the URL ends with `/openai/v1/`. Both `https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/` and `https://YOUR-RESOURCE-NAME.services.ai.azure.com/openai/v1/` are valid. |
| `401 Unauthorized` with Entra ID            | Missing or incorrect role assignment | Assign the `Cognitive Services OpenAI User` role to your identity. Role assignments can take up to 5 minutes to propagate.                                                         |
| `AzureOpenAI()` client doesn't work with v1 | v1 API uses the `OpenAI()` client    | Replace `AzureOpenAI()` with `OpenAI()` and set `base_url` to your Azure endpoint with `/openai/v1/` appended.                                                                     |
| `api-version` parameter rejected            | v1 API doesn't use `api-version`     | Remove any `api-version` query parameters from your requests. The v1 API doesn't require or accept them.                                                                           |
| Preview features not available              | Missing preview header               | For preview APIs like `/openai/v1/evals`, pass the required preview header (for example, `"aoai-evals":"preview"`). See [Preview headers](#preview-headers).                       |

## Known issues

* The `2025-04-01-preview` Azure OpenAI spec uses OpenAPI 3.1. It's a known issue that this version isn't fully supported by [Azure API Management](https://learn.microsoft.com/en-us/azure/api-management/api-management-key-concepts).

## Next steps

* [Supported programming languages for the v1 API](supported-languages)
* [Foundry Models sold directly by Azure](../foundry-models/concepts/models-sold-directly-by-azure)
* [Working with Azure OpenAI models](how-to/working-with-models)
* [Azure OpenAI quotas and limits](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/quotas-limits)
* [v1 OpenAPI 3.0 spec](https://github.com/Azure/azure-rest-api-specs/blob/main/specification/ai/data-plane/OpenAI.v1/azure-v1-v1-generated.json)

This article contains all the monitoring reference information for this service.

See [Monitor Azure OpenAI](how-to/monitor-openai) for details on the data you can collect for Azure OpenAI in Microsoft Foundry Models and how to use it.

## Metrics

This section lists all the automatically collected platform metrics for this service. These metrics are also part of the global list of [all platform metrics supported in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/supported-metrics/metrics-index#supported-metrics-per-resource-type).

For information on metric retention, see [Azure Monitor Metrics overview](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-platform-metrics#retention-of-metrics).

### Supported metrics for Microsoft.CognitiveServices/accounts

Here are the most important metrics we think you should monitor for Azure OpenAI. Later in this article is a longer list of all available metrics for this namespace which contains more details on metrics in this shorter list. *Please see below list for most up to date information. We're working on refreshing the tables in the following sections.*

* Azure OpenAI Requests
* Active Tokens
* Generated Completion Tokens
* Processed FineTuned Training Hours
* Processed Inference Tokens
* Processed Prompt Tokens
* Provisioned-managed Utilization V2
* Prompt Token Cache Match Rate
* Time to Response
* Time Between Tokens
* Time to Last Byte
* Normalized Time to First Byte
* Tokens per Second

You can also monitor Content Safety metrics that are used by other related services.

* Blocked Volume
* Harmful Volume Detected
* Potential Abusive User Count
* Safety System Event
* Total Volume Sent for Safety Check

<Callout type="note">
  The **Provisioned-managed Utilization** metric is now deprecated and is no longer recommended. This metric has been replaced by the **Provisioned-managed Utilization V2** metric. Tokens per Second, Time to Response, Time Between Tokens are currently not available for Standard deployments.
</Callout>

Cognitive Services metrics have the category **Cognitive Services - HTTP Requests** in the following table. These metrics are legacy metrics that are common to all resources of this type. Microsoft no longer recommends that you use these metrics with Azure OpenAI.

The following table lists the metrics available for the Microsoft.CognitiveServices/accounts resource type.

* All columns might not be present in every table.
* Some columns might be beyond the viewing area of the page. Select **Expand table** to view all available columns.

**Table headings**

* **Category** - The metrics group or classification.
* **Metric** - The metric display name as it appears in the Azure portal.
* **Name in REST API** - The metric name as referred to in the [REST API](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/rest-api-walkthrough).
* **Unit** - Unit of measure.
* **Aggregation** - The default [aggregation](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/metrics-aggregation-explained) type. Valid values: Average (Avg), Minimum (Min), Maximum (Max), Total (Sum), Count.
* **Dimensions** - [Dimensions](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/metrics-aggregation-explained#dimensions-splitting-and-filtering) available for the metric.
* **Time Grains** - [Intervals](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/metrics-aggregation-explained#granularity) at which the metric is sampled. For example, `PT1M` indicates that the metric is sampled every minute, `PT30M` every 30 minutes, `PT1H` every hour, and so on.
* **DS Export**- Whether the metric is exportable to Azure Monitor Logs via diagnostic settings. For information on exporting metrics, see [Create diagnostic settings in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/create-diagnostic-settings?tabs=portal).

### Category: Actions

| Metric                                                     | Name in REST API      | Unit  | Aggregation | Dimensions                  | Time Grains | DS Export |
| ---------------------------------------------------------- | --------------------- | ----- | ----------- | --------------------------- | ----------- | --------- |
| **Action Occurences** Number of times each action appears. | `ActionIdOccurrences` | Count | Total (Sum) | `ActionId`, `Mode`, `RunId` | PT1M        | Yes       |
| **Actions Per Event** Number of actions per event.         | `ActionsPerEvent`     | Count | Average     | `Mode`, `RunId`             | PT1M        | Yes       |

### Category: Azure OpenAI - HTTP Requests

| Metric                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | Name in REST API              | Unit    | Aggregation               | Dimensions                                                                                                                                                                       | Time Grains | DS Export |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | ------- | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | --------- |
| **Azure OpenAI AvailabilityRate** Availability percentage with the following calculation: (Total Calls - Server Errors)/Total Calls. Server Errors include any HTTP responses >=500.                                                                                                                                                                                                                                                                                             | `AzureOpenAIAvailabilityRate` | Percent | Minimum, Maximum, Average | `ApiName`, `OperationName`, `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                                                                           | PT1M        | No        |
| **Azure OpenAI Requests** Number of calls made to the Azure OpenAI API over a period of time. Applies to PTU, PTU-Managed and Pay-as-you-go deployments. To breakdown API requests, you can add a filter or apply splitting by the following dimensions: ModelDeploymentName, ModelName, ModelVersion, StatusCode (successful, clienterrors, server errors), IsSpillover for spillover information, ServiceTier, StreamType (Streaming vs non-streaming requests) and operation. | `AzureOpenAIRequests`         | Count   | Total (Sum)               | `ApiName`, `OperationName`, `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `StatusCode`, `IsSpillover`, `ServiceTierRequest`, `ServiceTierResponse` | PT1M        | Yes       |

### Category: Azure OpenAI - Latency

| Metric                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | Name in REST API                | Unit         | Aggregation               | Dimensions                                                                                                           | Time Grains | DS Export |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------- | ------------ | ------------------------- | -------------------------------------------------------------------------------------------------------------------- | ----------- | --------- |
| **Time Between Token** For streaming requests; model token generation rate, measured in milliseconds. Applies to PTU, PTU-managed and Pay-as-you-go deployments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | `AzureOpenAINormalizedTBTInMS`  | MilliSeconds | Maximum, Minimum, Average | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                                                         | PT1M        | Yes       |
| **Normalized Time to First Byte** For streaming and non-streaming requests; time it takes for first byte of response data to be received after request is made by model, normalized by token. Applies to PTU, PTU-managed, and Pay-as-you-go deployments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `AzureOpenAINormalizedTTFTInMS` | MilliSeconds | Maximum, Minimum, Average | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                                                         | PT1M        | Yes       |
| **Time to Response** Recommended latency (responsiveness) measure for streaming requests. Applies to PTU, PTU-managed and Pay-as-you-go deployments. Calculated as time taken for the first response to appear after a user sends a prompt, as measured by the API gateway. This number increases as the prompt size increases and/or cache hit size reduces. To breakdown time to response metric, you can add a filter or apply splitting by the following dimensions: ModelDeploymentName, ModelName, and ModelVersion. Note: this metric is an approximation as measured latency is heavily dependent on multiple factors, including concurrent calls and overall workload pattern. In addition, it does not account for any client-side latency that may exist between your client and the API endpoint. Please refer to your own logging for optimal latency tracking. | `AzureOpenAITimeToResponse`     | MilliSeconds | Minimum, Maximum, Average | `ApiName`, `OperationName`, `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `StatusCode` | PT1M        | Yes       |
| **Tokens Per Second** Enumerates the generation speed for a given Azure OpenAI model response. The total tokens generated is divided by the time to generate the tokens, in seconds. Applies to PTU, PTU-managed and Pay-as-you-go deployments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | `AzureOpenAITokenPerSecond`     | Count        | Maximum, Minimum, Average | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                                                         | PT1M        | Yes       |
| **Time to Last Byte** For streaming and non-streaming requests; time it takes for last byte of response data to be received after request is made by model. Applies to PTU, PTU-managed, and Pay-as-you-go deployments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | `AzureOpenAITTLTInMS`           | MilliSeconds | Maximum, Minimum, Average | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                                                         | PT1M        | Yes       |

### Category: Azure OpenAI - Usage

| Metric                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Name in REST API                             | Unit    | Aggregation                            | Dimensions                                                                                | Time Grains | DS Export |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------- | ------- | -------------------------------------- | ----------------------------------------------------------------------------------------- | ----------- | --------- |
| **Active Tokens** Total tokens minus cached tokens over a period of time. Applies to PTU and PTU-managed deployments. Use this metric to understand your TPS or TPM based utilization for PTUs and compare to your benchmarks for target TPS or TPM for your scenarios. To breakdown API requests, you can add a filter or apply splitting by the following dimensions: ModelDeploymentName, ModelName, and ModelVersion.                                     | `ActiveTokens`                               | Count   | Minimum, Maximum, Average, Total (Sum) | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                              | PT1M        | Yes       |
| **Audio Completion Tokens** Number of audio prompt tokens generated (output) on an OpenAI model. Applies to PTU-managed and Pay-as-you-go model deployments.                                                                                                                                                                                                                                                                                                  | `AudioCompletionTokens`                      | Count   | Total (Sum)                            | `ModelDeploymentName`, `ModelName`, `ModelVersion`, `Region`                              | PT1M        | Yes       |
| **Audio Prompt Tokens** Number of audio prompt tokens processed (input) on an OpenAI model. Applies to PTU-managed and Pay-as-you-go model deployments.                                                                                                                                                                                                                                                                                                       | `AudioPromptTokens`                          | Count   | Total (Sum)                            | `ModelDeploymentName`, `ModelName`, `ModelVersion`, `Region`                              | PT1M        | Yes       |
| **Prompt Token Cache Match Rate** Percentage of prompt tokens that hit the cache. Applies to PTU and PTU-managed deployments.                                                                                                                                                                                                                                                                                                                                 | `AzureOpenAIContextTokensCacheMatchRate`     | Percent | Minimum, Maximum, Average              | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                              | PT1M        | No        |
| **Provisioned-managed Utilization (deprecated)** Utilization % for a provisoned-managed deployment, calculated as (PTUs consumed / PTUs deployed) x 100. When utilization is greater than or equal to 100%, calls are throttled and error code 429 returned. To breakdown this metric, you can add a filter or apply splitting by the following dimensions: ModelDeploymentName, ModelName, ModelVersion and StreamType (Streaming vs non-streaming requests) | `AzureOpenAIProvisionedManagedUtilization`   | Percent | Minimum, Maximum, Average              | `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                | PT1M        | No        |
| **Provisioned-managed Utilization V2** Utilization % for a provisoned-managed deployment, calculated as (PTUs consumed / PTUs deployed) x 100. When utilization is greater than or equal to 100%, calls are throttled and error code 429 returned. To breakdown this metric, you can add a filter or apply splitting by the following dimensions: ModelDeploymentName, ModelName, ModelVersion and StreamType (Streaming vs non-streaming requests)           | `AzureOpenAIProvisionedManagedUtilizationV2` | Percent | Minimum, Maximum, Average              | `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                | PT1M        | No        |
| **Processed FineTuned Training Hours** Number of Training Hours Processed on an OpenAI FineTuned Model                                                                                                                                                                                                                                                                                                                                                        | `FineTunedTrainingHours`                     | Count   | Total (Sum)                            | `ApiName`, `ModelDeploymentName`, `FeatureName`, `UsageChannel`, `Region`                 | PT1M        | Yes       |
| **Generated Completion Tokens** Number of tokens generated (output) from an OpenAI model. Applies to PTU, PTU-managed and Pay-as-you-go deployments. To breakdown this metric, you can add a filter or apply splitting by the following dimensions: ModelDeploymentName and ModelName.                                                                                                                                                                        | `GeneratedTokens`                            | Count   | Total (Sum)                            | `ApiName`, `ModelDeploymentName`, `FeatureName`, `UsageChannel`, `Region`, `ModelVersion` | PT1M        | Yes       |
| **Processed Prompt Tokens** Number of prompt tokens processed (input) on an OpenAI model. Applies to PTU, PTU-managed and Pay-as-you-go deployments. To breakdown this metric, you can add a filter or apply splitting by the following dimensions: ModelDeploymentName and ModelName.                                                                                                                                                                        | `ProcessedPromptTokens`                      | Count   | Total (Sum)                            | `ApiName`, `ModelDeploymentName`, `FeatureName`, `UsageChannel`, `Region`, `ModelVersion` | PT1M        | Yes       |
| **Realtime API Seconds Used** RealtimeAPI number of seconds used                                                                                                                                                                                                                                                                                                                                                                                              | `RealtimeUsageTime`                          | Count   | Total (Sum)                            | `Region`, `ModelDeploymentName`                                                           | PT1M        | Yes       |
| **Processed Inference Tokens** Number of inference tokens processed on an OpenAI model. Calculated as prompt tokens (input) plus generated tokens (output). Applies to PTU, PTU-managed and Pay-as-you-go deployments. To breakdown this metric, you can add a filter or apply splitting by the following dimensions: ModelDeploymentName and ModelName.                                                                                                      | `TokenTransaction`                           | Count   | Total (Sum)                            | `ApiName`, `ModelDeploymentName`, `FeatureName`, `UsageChannel`, `Region`, `ModelVersion` | PT1M        | Yes       |

### Category: Cognitive Services - HTTP Requests

| Metric                                                                                                                           | Name in REST API  | Unit         | Aggregation | Dimensions                                           | Time Grains | DS Export |
| -------------------------------------------------------------------------------------------------------------------------------- | ----------------- | ------------ | ----------- | ---------------------------------------------------- | ----------- | --------- |
| **Blocked Calls** Number of calls that exceeded rate or quota limit. Do not use for Azure OpenAI service.                        | `BlockedCalls`    | Count        | Total (Sum) | `ApiName`, `OperationName`, `Region`, `RatelimitKey` | PT1M        | Yes       |
| **Client Errors** Number of calls with client side error (HTTP response code 4xx). Do not use for Azure OpenAI service.          | `ClientErrors`    | Count        | Total (Sum) | `ApiName`, `OperationName`, `Region`, `RatelimitKey` | PT1M        | Yes       |
| **Data In** Size of incoming data in bytes. Do not use for Azure OpenAI service.                                                 | `DataIn`          | Bytes        | Total (Sum) | `ApiName`, `OperationName`, `Region`                 | PT1M        | Yes       |
| **Data Out** Size of outgoing data in bytes. Do not use for Azure OpenAI service.                                                | `DataOut`         | Bytes        | Total (Sum) | `ApiName`, `OperationName`, `Region`                 | PT1M        | Yes       |
| **Latency** Latency in milliseconds. Do not use for Azure OpenAI service.                                                        | `Latency`         | MilliSeconds | Average     | `ApiName`, `OperationName`, `Region`, `RatelimitKey` | PT1M        | Yes       |
| **Ratelimit** The current ratelimit of the ratelimit key. Do not use for Azure OpenAI service.                                   | `Ratelimit`       | Count        | Total (Sum) | `Region`, `RatelimitKey`                             | PT1M        | Yes       |
| **Server Errors** Number of calls with service internal error (HTTP response code 5xx). Do not use for Azure OpenAI service.     | `ServerErrors`    | Count        | Total (Sum) | `ApiName`, `OperationName`, `Region`, `RatelimitKey` | PT1M        | Yes       |
| **Successful Calls** Number of successful calls. Do not use for Azure OpenAI service.                                            | `SuccessfulCalls` | Count        | Total (Sum) | `ApiName`, `OperationName`, `Region`, `RatelimitKey` | PT1M        | Yes       |
| **Total Calls** Total number of calls. Do not use for Azure OpenAI service.                                                      | `TotalCalls`      | Count        | Total (Sum) | `ApiName`, `OperationName`, `Region`, `RatelimitKey` | PT1M        | Yes       |
| **Total Errors** Total number of calls with error response (HTTP response code 4xx or 5xx). Do not use for Azure OpenAI service. | `TotalErrors`     | Count        | Total (Sum) | `ApiName`, `OperationName`, `Region`, `RatelimitKey` | PT1M        | Yes       |
| **Total Token Calls** Total number of token calls.                                                                               | `TotalTokenCalls` | Count        | Total (Sum) | `ApiName`, `OperationName`, `Region`                 | PT1M        | Yes       |

### Category: Cognitive Services - SLI

| Metric                                                                                                                                                                                                       | Name in REST API | Unit    | Aggregation               | Dimensions                                           | Time Grains | DS Export |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------- | ------- | ------------------------- | ---------------------------------------------------- | ----------- | --------- |
| **AvailabilityRate** Availability percentage with the following calculation: (Total Calls - Server Errors)/Total Calls. Server Errors include any HTTP responses >=500. Do not use for Azure OpenAI service. | `SuccessRate`    | Percent | Minimum, Maximum, Average | `ApiName`, `OperationName`, `Region`, `RatelimitKey` | PT1M        | No        |

### Category: ContentSafety - Risks\&Safety

| Metric                                                                                                                                                                                                                                                                                              | Name in REST API       | Unit  | Aggregation | Dimensions                                                                                                  | Time Grains | DS Export |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------- | ----- | ----------- | ----------------------------------------------------------------------------------------------------------- | ----------- | --------- |
| **Potentially Abusive User Count** Number of potentially abusive user that detected over a period of time. You can add a filter or apply splitting by the following dimension: ModelDeploymentName.                                                                                                 | `RAIAbusiveUsersCount` | Count | Total (Sum) | `Region`, `ModelDeploymentName`                                                                             | PT1M        | Yes       |
| **Harmful Volume Detected** Number of calls made to Azure OpenAI API and detected as harmful(both block model and annotate mode) by content filter applied over a period of time. You can add a filter or apply splitting by the following dimensions: ModelDeploymentName, ModelName and TextType. | `RAIHarmfulRequests`   | Count | Total (Sum) | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `ApiName`, `TextType`, `Category`, `Severity` | PT1M        | Yes       |
| **Blocked Volume** Number of calls made to Azure OpenAI API and rejected by content filter applied over a period of time. You can add a filter or apply splitting by the following dimensions: ModelDeploymentName, ModelName and TextType.                                                         | `RAIRejectedRequests`  | Count | Total (Sum) | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `ApiName`, `TextType`, `Category`             | PT1M        | Yes       |
| **Safety System Event** System event for risks & safety monitoring. You can add a filter or apply splitting by the following dimension: EventType.                                                                                                                                                  | `RAISystemEvent`       | Count | Average     | `Region`, `EventType`                                                                                       | PT1M        | Yes       |
| **Total Volume Sent For Safety Check** Number of calls made to Azure OpenAI API and detected by content filter applied over a period of time. You can add a filter or apply splitting by the following dimensions: ModelDeploymentName, ModelName.                                                  | `RAITotalRequests`     | Count | Total (Sum) | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `ApiName`                                     | PT1M        | Yes       |

### Category: ContentSafety - Usage

| Metric                                                                    | Name in REST API                        | Unit  | Aggregation | Dimensions   | Time Grains | DS Export |
| ------------------------------------------------------------------------- | --------------------------------------- | ----- | ----------- | ------------ | ----------- | --------- |
| **Call Count for Image Moderation** Number of calls for image moderation. | `ContentSafetyImageAnalyzeRequestCount` | Count | Total (Sum) | `ApiVersion` | PT1M        | Yes       |
| **Call Count for Text Moderation** Number of calls for text moderation.   | `ContentSafetyTextAnalyzeRequestCount`  | Count | Total (Sum) | `ApiVersion` | PT1M        | Yes       |

### Category: Estimations

| Metric                                                                          | Name in REST API           | Unit  | Aggregation | Dimensions      | Time Grains | DS Export |
| ------------------------------------------------------------------------------- | -------------------------- | ----- | ----------- | --------------- | ----------- | --------- |
| **Baseline Random Event count** Estimation for baseline random event count.     | `BaselineRandomEventCount` | Count | Total (Sum) | `Mode`, `RunId` | PT1M        | Yes       |
| **Baseline Random Reward** Estimation for baseline random reward.               | `BaselineRandomReward`     | Count | Total (Sum) | `Mode`, `RunId` | PT1M        | Yes       |
| **Online Event Count** Estimation for online event count.                       | `OnlineEventCount`         | Count | Total (Sum) | `Mode`, `RunId` | PT1M        | Yes       |
| **Online Reward** Estimation for online reward.                                 | `OnlineReward`             | Count | Total (Sum) | `Mode`, `RunId` | PT1M        | Yes       |
| **User Baseline Event Count** Estimation for user defined baseline event count. | `UserBaselineEventCount`   | Count | Total (Sum) | `Mode`, `RunId` | PT1M        | Yes       |
| **User Baseline Reward** Estimation for user defined baseline reward.           | `UserBaselineReward`       | Count | Total (Sum) | `Mode`, `RunId` | PT1M        | Yes       |

### Category: Feature Occurences

| Metric                                                                        | Name in REST API              | Unit  | Aggregation | Dimensions                   | Time Grains | DS Export |
| ----------------------------------------------------------------------------- | ----------------------------- | ----- | ----------- | ---------------------------- | ----------- | --------- |
| **Action Feature Occurrences** Number of times each action feature appears.   | `ActionFeatureIdOccurrences`  | Count | Total (Sum) | `FeatureId`, `Mode`, `RunId` | PT1M        | Yes       |
| **Context Feature Occurrences** Number of times each context feature appears. | `ContextFeatureIdOccurrences` | Count | Total (Sum) | `FeatureId`, `Mode`, `RunId` | PT1M        | Yes       |
| **Slot Feature Occurrences** Number of times each slot feature appears.       | `SlotFeatureIdOccurrences`    | Count | Total (Sum) | `FeatureId`, `Mode`, `RunId` | PT1M        | Yes       |

### Category: FeatureCardinality

| Metric                                                                   | Name in REST API             | Unit  | Aggregation | Dimensions                   | Time Grains | DS Export |
| ------------------------------------------------------------------------ | ---------------------------- | ----- | ----------- | ---------------------------- | ----------- | --------- |
| **Feature Cardinality by Action** Feature Cardinality based on Action.   | `FeatureCardinality_Action`  | Count | Average     | `FeatureId`, `Mode`, `RunId` | PT1M        | Yes       |
| **Feature Cardinality by Context** Feature Cardinality based on Context. | `FeatureCardinality_Context` | Count | Average     | `FeatureId`, `Mode`, `RunId` | PT1M        | Yes       |
| **Feature Cardinality by Slot** Feature Cardinality based on Slot.       | `FeatureCardinality_Slot`    | Count | Average     | `FeatureId`, `Mode`, `RunId` | PT1M        | Yes       |

### Category: Features Per Event

| Metric                                                                     | Name in REST API          | Unit  | Aggregation | Dimensions      | Time Grains | DS Export |
| -------------------------------------------------------------------------- | ------------------------- | ----- | ----------- | --------------- | ----------- | --------- |
| **Action Features Per Event** Average number of action features per event. | `ActionFeaturesPerEvent`  | Count | Average     | `Mode`, `RunId` | PT1M        | Yes       |
| **Context Features Per Event** Number of context features per event.       | `ContextFeaturesPerEvent` | Count | Average     | `Mode`, `RunId` | PT1M        | Yes       |
| **Slot Features Per Event** Average number of slot features per event.     | `SlotFeaturesPerEvent`    | Count | Average     | `Mode`, `RunId` | PT1M        | Yes       |

### Category: Language - Jobs

| Metric                                                                                                                                                                     | Name in REST API | Unit         | Aggregation               | Dimensions             | Time Grains | DS Export |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------- | ------------ | ------------------------- | ---------------------- | ----------- | --------- |
| **Job Duration (Preview)** Note: this value depends heavily on the input size, number of documents and task's complexity. This is an aggregate value across all job tasks. | `JobDuration`    | MilliSeconds | Minimum, Maximum, Average | `JobStatus`, `JobType` | PT1M        | Yes       |

### Category: Models - HTTP Requests

| Metric                                                                                                                                                                         | Name in REST API        | Unit    | Aggregation               | Dimensions                                                                                                                                                                       | Time Grains | DS Export |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------- | ------- | ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------- | --------- |
| **Model Availability Rate** Availability percentage with the following calculation: (Total Calls - Server Errors)/Total Calls. Server Errors include any HTTP responses >=500. | `ModelAvailabilityRate` | Percent | Minimum, Maximum, Average | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                                                                                                                     | PT1M        | No        |
| **Model Requests** Number of calls made to the model API over a period of time. Applies to PTU, PTU-Managed and Pay-as-you-go deployments.                                     | `ModelRequests`         | Count   | Total (Sum)               | `ApiName`, `OperationName`, `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `StatusCode`, `IsSpillover`, `ServiceTierRequest`, `ServiceTierResponse` | PT1M        | Yes       |

### Category: Models - Latency

| Metric                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Name in REST API              | Unit         | Aggregation               | Dimensions                                                                                                           | Time Grains | DS Export |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | ------------ | ------------------------- | -------------------------------------------------------------------------------------------------------------------- | ----------- | --------- |
| **Time Between Tokens** For streaming requests; Model token generation rate, measured in milliseconds. Applies to PTU and PTU-managed deployments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | `NormalizedTimeBetweenTokens` | MilliSeconds | Maximum, Minimum, Average | `ApiName`, `OperationName`, `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`               | PT1M        | Yes       |
| **Normalized Time to First Byte** For streaming and non-streaming requests; time it takes for first byte of response data to be received after request is made by model, normalized by token. Applies to PTU, PTU-managed, and Pay-as-you-go deployments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | `NormalizedTimeToFirstToken`  | MilliSeconds | Maximum, Minimum, Average | `ApiName`, `OperationName`, `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`               | PT1M        | Yes       |
| **Time to Last Byte** For streaming and non-streaming requests; time it takes for last byte of response data to be received after request is made by model. Applies to PTU, PTU-managed, and Pay-as-you-go deployments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | `TimeToLastByte`              | MilliSeconds | Maximum, Minimum, Average | `ApiName`, `OperationName`, `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`               | PT1M        | Yes       |
| **Time to Response** Recommended latency (responsiveness) measure for streaming requests. Applies to PTU and PTU-managed deployments. Calculated as time taken for the first response to appear after a user sends a prompt, as measured by the API gateway. This number increases as the prompt size increases and/or cache hit size reduces. To breakdown time to response metric, you can add a filter or apply splitting by the following dimensions: ModelDeploymentName, ModelName, and ModelVersion. Note: this metric is an approximation as measured latency is heavily dependent on multiple factors, including concurrent calls and overall workload pattern. In addition, it does not account for any client-side latency that may exist between your client and the API endpoint. Please refer to your own logging for optimal latency tracking. | `TimeToResponse`              | MilliSeconds | Minimum, Maximum, Average | `ApiName`, `OperationName`, `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `StatusCode` | PT1M        | Yes       |
| **Tokens Per Second** Enumerates the generation speed for a given model response. The total tokens generated is divided by the time to generate the tokens, in seconds. Applies to PTU and PTU-managed deployments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | `TokensPerSecond`             | Count        | Maximum, Minimum, Average | `ApiName`, `OperationName`, `Region`, `StreamType`, `ModelDeploymentName`, `ModelName`, `ModelVersion`               | PT1M        | Yes       |

### Category: Models - Usage

| Metric                                                                                                                                                                                                                                          | Name in REST API         | Unit    | Aggregation               | Dimensions                                                                               | Time Grains | DS Export |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------ | ------- | ------------------------- | ---------------------------------------------------------------------------------------- | ----------- | --------- |
| **Annotated Pages** Total number of pages processed with annotations. Applies to PTU, PTU-Managed and Pay-as-you-go deployments.                                                                                                                | `AnnotatedPages`         | Count   | Total (Sum)               | `ApiName`, `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                  | PT1M        | Yes       |
| **Audio Input Tokens** Number of audio prompt tokens processed (input) on an OpenAI model. Applies to PTU-managed model deployments.                                                                                                            | `AudioInputTokens`       | Count   | Total (Sum)               | `ModelDeploymentName`, `ModelName`, `ModelVersion`, `Region`                             | PT1M        | Yes       |
| **Audio Output Tokens** Number of audio prompt tokens generated (output) on an OpenAI model. Applies to PTU-managed model deployments.                                                                                                          | `AudioOutputTokens`      | Count   | Total (Sum)               | `ModelDeploymentName`, `ModelName`, `ModelVersion`, `Region`                             | PT1M        | Yes       |
| **Prompt tokens read from cache** Total number of tokens read from the cache. Applies to Anthropic model deployments. Surfaced in response usage section as `cache_read_input_tokens`                                                           | `cacheReadInputTokens`   | Count   | Total (Sum)               | `ApiName`, `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `ContextLength` | PT1M        | Yes       |
| **Prompt tokens written to cache (1 hour TTL)** The number of prompt tokens used to create the 1 hour entry. Applies to Anthropic model deployments. Surfaced in response usage section as `cache_creation.ephemeral_1h_input_tokens`           | `ephemeral1hInputTokens` | Count   | Total (Sum)               | `ApiName`, `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `ContextLength` | PT1M        | Yes       |
| **Prompt tokens written to cache (5 minute TTL)** The number of prompt tokens used to create the 5 minute cache entry. Applies to Anthropic model deployments. Surfaced in response usage section as `cache_creation.ephemeral_5m_input_tokens` | `ephemeral5mInputTokens` | Count   | Total (Sum)               | `ApiName`, `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`, `ContextLength` | PT1M        | Yes       |
| **Generated Images** Total number of images generated. Applies to PTU, PTU-Managed and Pay-as-you-go deployments.                                                                                                                               | `GeneratedImages`        | Count   | Total (Sum)               | `ApiName`, `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                  | PT1M        | Yes       |
| **Input Tokens** Number of prompt tokens processed (input) on a model. Applies to PTU, PTU-Managed and Pay-as-you-go deployments.                                                                                                               | `InputTokens`            | Count   | Total (Sum)               | `ApiName`, `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                  | PT1M        | Yes       |
| **Output Tokens** Number of tokens generated (output) from an OpenAI model. Applies to PTU, PTU-Managed and Pay-as-you-go deployments.                                                                                                          | `OutputTokens`           | Count   | Total (Sum)               | `ApiName`, `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                  | PT1M        | Yes       |
| **Provisioned Utilization** Utilization % for a provisoned-managed deployment, calculated as (PTUs consumed / PTUs deployed) x 100. When utilization is greater than or equal to 100%, calls are throttled and error code 429 returned.         | `ProvisionedUtilization` | Percent | Minimum, Maximum, Average | `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                             | PT1M        | No        |
| **Total Pages** Total number of pages processed. Applies to PTU, PTU-Managed and Pay-as-you-go deployments.                                                                                                                                     | `TotalPages`             | Count   | Total (Sum)               | `ApiName`, `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                  | PT1M        | Yes       |
| **Total Tokens** Number of inference tokens processed on a model. Calculated as prompt tokens (input) plus generated tokens (output). Applies to PTU, PTU-Managed and Pay-as-you-go deployments.                                                | `TotalTokens`            | Count   | Total (Sum)               | `ApiName`, `Region`, `ModelDeploymentName`, `ModelName`, `ModelVersion`                  | PT1M        | Yes       |

### Category: Namespaces Per Event

| Metric                                                                         | Name in REST API            | Unit  | Aggregation | Dimensions      | Time Grains | DS Export |
| ------------------------------------------------------------------------------ | --------------------------- | ----- | ----------- | --------------- | ----------- | --------- |
| **Action Namespaces Per Event** Average number of action namespaces per event. | `ActionNamespacesPerEvent`  | Count | Average     | `Mode`, `RunId` | PT1M        | Yes       |
| **Context Namespaces Per Event** Number of context namespaces per event.       | `ContextNamespacesPerEvent` | Count | Average     | `Mode`, `RunId` | PT1M        | Yes       |
| **Slot Namespaces Per Event** Average number of slot namespaces per event.     | `SlotNamespacesPerEvent`    | Count | Average     | `Mode`, `RunId` | PT1M        | Yes       |

### Category: Rewards

| Metric                                                 | Name in REST API | Unit  | Aggregation | Dimensions                                                                                                          | Time Grains | DS Export |
| ------------------------------------------------------ | ---------------- | ----- | ----------- | ------------------------------------------------------------------------------------------------------------------- | ----------- | --------- |
| **Average Reward Per Event** Average reward per event. | `Reward`         | Count | Average     | `BaselineAction`, `ChosenActionId`, `MatchesBaseline`, `NonDefaultReward`, `Mode`, `RunId`                          | PT1M        | Yes       |
| **Slot Reward** Reward per slot.                       | `SlotReward`     | Count | Average     | `BaselineActionId`, `ChosenActionId`, `MatchesBaseline`, `NonDefaultReward`, `SlotId`, `SlotIndex`, `Mode`, `RunId` | PT1M        | Yes       |

### Category: Slots

| Metric                                                                                 | Name in REST API                       | Unit  | Aggregation | Dimensions                             | Time Grains | DS Export |
| -------------------------------------------------------------------------------------- | -------------------------------------- | ----- | ----------- | -------------------------------------- | ----------- | --------- |
| **Baseline Estimator Overall Reward** Baseline Estimator Overall Reward.               | `BaselineEstimatorOverallReward`       | Count | Average     | `Mode`, `RunId`                        | PT1M        | Yes       |
| **Baseline Estimator Slot Reward** Baseline Estimator Reward by slot.                  | `BaselineEstimatorSlotReward`          | Count | Average     | `SlotId`, `SlotIndex`, `Mode`, `RunId` | PT1M        | Yes       |
| **Baseline Random Estimator Overall Reward** Baseline Random Estimator Overall Reward. | `BaselineRandomEstimatorOverallReward` | Count | Average     | `Mode`, `RunId`                        | PT1M        | Yes       |
| **Baseline Random Estimator Slot Reward** Baseline Random Estimator Reward by slot.    | `BaselineRandomEstimatorSlotReward`    | Count | Average     | `SlotId`, `SlotIndex`, `Mode`, `RunId` | PT1M        | Yes       |
| **Slots** Number of slots per event.                                                   | `NumberOfSlots`                        | Count | Average     | `Mode`, `RunId`                        | PT1M        | Yes       |
| **Online Estimator Overall Reward** Online Estimator Overall Reward.                   | `OnlineEstimatorOverallReward`         | Count | Average     | `Mode`, `RunId`                        | PT1M        | Yes       |
| **Online Estimator Slot Reward** Online Estimator Reward by slot.                      | `OnlineEstimatorSlotReward`            | Count | Average     | `SlotId`, `SlotIndex`, `Mode`, `RunId` | PT1M        | Yes       |
| **Slot Occurrences** Number of times each slot appears.                                | `SlotIdOccurrences`                    | Count | Total (Sum) | `SlotId`, `SlotIndex`, `Mode`, `RunId` | PT1M        | Yes       |

### Category: SpeechServices - Usage

| Metric                                                                                   | Name in REST API                      | Unit  | Aggregation | Dimensions                                         | Time Grains | DS Export |
| ---------------------------------------------------------------------------------------- | ------------------------------------- | ----- | ----------- | -------------------------------------------------- | ----------- | --------- |
| **Audio Seconds Batch Transcribed** Batch number of seconds transcribed                  | `AudioSecondsBatchTranscribed`        | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Audio Seconds Batch Whisper Transcribed** Batch whisper number of seconds transcribed  | `AudioSecondsBatchWhisperTranscribed` | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Audio Seconds Fast Transcribed** Fast number of seconds transcribed                    | `AudioSecondsFastTranscribed`         | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Audio Seconds Fast Whisper Transcribed** Fast whisper number of seconds transcribed    | `AudioSecondsFastWhisperTranscribed`  | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Audio Seconds Transcribed** Number of seconds transcribed                              | `AudioSecondsTranscribed`             | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Audio Seconds Translated** Number of seconds translated                                | `AudioSecondsTranslated`              | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Avatar Model Hosting Seconds** Number of Seconds.                                      | `AvatarModelHostingSeconds`           | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Avatar Model Training Seconds** Number of Seconds.                                     | `AvatarModelTrainingSeconds`          | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Number of Speaker Profiles** Number of speaker profiles enrolled. Prorated hourly.     | `NumberofSpeakerProfiles`             | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Speaker Recognition Transactions** Number of speaker recognition transactions          | `SpeakerRecognitionTransactions`      | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Speech Model Hosting Hours** Number of speech model hosting hours                      | `SpeechModelHostingHours`             | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Synthesized Characters** Number of Characters.                                         | `SynthesizedCharacters`               | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Video Seconds Synthesized** Number of seconds synthesized                              | `VideoSecondsSynthesized`             | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Voice Live Audio Input Tokens** Number of audio input tokens, excluding cached tokens. | `VoiceLiveAudioInputTokens`           | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Voice Live Audio Output Tokens** Number of audio output tokens.                        | `VoiceLiveAudioOutputTokens`          | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Voice Live Cached Audio Input Tokens** Number of cached audio input tokens.            | `VoiceLiveCachedAudioInputTokens`     | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Voice Live Cached Text Input Tokens** Number of cached text input tokens.              | `VoiceLiveCachedTextInputTokens`      | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Voice Live Text Input Tokens** Number of text input tokens, excluding cached tokens.   | `VoiceLiveTextInputTokens`            | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Voice Live Text Output Tokens** Number of text output tokens.                          | `VoiceLiveTextOutputTokens`           | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Voice Model Hosting Hours** Number of Hours.                                           | `VoiceModelHostingHours`              | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Voice Model Training Minutes** Number of Minutes.                                      | `VoiceModelTrainingMinutes`           | Count | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |

### Category: Translator Services - Usage

| Metric                                                                                                                    | Name in REST API                        | Unit    | Aggregation | Dimensions                                         | Time Grains | DS Export |
| ------------------------------------------------------------------------------------------------------------------------- | --------------------------------------- | ------- | ----------- | -------------------------------------------------- | ----------- | --------- |
| **Characters Trained (Deprecated)** Total number of characters trained.                                                   | `CharactersTrained`                     | Count   | Total (Sum) | `ApiName`, `OperationName`, `Region`               | PT1M        | Yes       |
| **Characters Translated (Deprecated)** Total number of characters in incoming text request.                               | `CharactersTranslated`                  | Count   | Total (Sum) | `ApiName`, `OperationName`, `Region`               | PT1M        | Yes       |
| **Document Characters Translated** Number of characters in document translation request.                                  | `DocumentCharactersTranslated`          | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Document Custom Characters Translated** Number of characters in custom document translation request.                    | `DocumentCustomCharactersTranslated`    | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Document Sync Characters Translated** Number of characters in document translation (synchronous) request.               | `OneDocumentCharactersTranslated`       | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Document Sync Custom Characters Translated** Number of characters in custom document translation (synchronous) request. | `OneDocumentCustomCharactersTranslated` | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Text Characters Translated** Number of characters in incoming text translation request.                                 | `TextCharactersTranslated`              | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Text Custom Characters Translated** Number of characters in incoming custom text translation request.                   | `TextCustomCharactersTranslated`        | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Text Trained Characters** Number of characters trained using text translation.                                          | `TextTrainedCharacters`                 | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |
| **Translator Pro App Seconds** Number of seconds of Translator Pro App usage.                                             | `TranslatorProAppSeconds`               | Seconds | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region` | PT1M        | Yes       |

### Category: Usage

| Metric                                                                                                 | Name in REST API               | Unit    | Aggregation | Dimensions                                                                     | Time Grains | DS Export |
| ------------------------------------------------------------------------------------------------------ | ------------------------------ | ------- | ----------- | ------------------------------------------------------------------------------ | ----------- | --------- |
| **Inference Count** Inference Count of Carnegie Frontdoor Service                                      | `CarnegieInferenceCount`       | Count   | Total (Sum) | `Region`, `Modality`, `Category`, `Language`, `SeverityLevel`, `UseCustomList` | PT1M        | Yes       |
| **Computer Vision Transactions** Number of Computer Vision Transactions                                | `ComputerVisionTransactions`   | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Custom Vision Training Time** Custom Vision training time                                            | `CustomVisionTrainingTime`     | Seconds | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Custom Vision Transactions** Number of Custom Vision prediction transactions                         | `CustomVisionTransactions`     | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Face Images Trained** Number of images trained. 1,000 images trained per transaction.                | `FaceImagesTrained`            | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Faces Stored** Number of faces stored, prorated daily. The number of faces stored is reported daily. | `FacesStored`                  | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Face Transactions** Number of API calls made to Face service                                         | `FaceTransactions`             | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Images Stored** Number of Custom Vision images stored.                                               | `ImagesStored`                 | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Learned Events** Number of Learned Events.                                                           | `LearnedEvents`                | Count   | Total (Sum) | `IsMatchBaseline`, `Mode`, `RunId`                                             | PT1M        | Yes       |
| **LUIS Speech Requests** Number of LUIS speech to intent understanding requests                        | `LUISSpeechRequests`           | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **LUIS Text Requests** Number of LUIS text requests                                                    | `LUISTextRequests`             | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Matched Rewards** Number of Matched Rewards.                                                         | `MatchedRewards`               | Count   | Total (Sum) | `Mode`, `RunId`                                                                | PT1M        | Yes       |
| **Non Activated Events** Number of skipped events.                                                     | `NonActivatedEvents`           | Count   | Total (Sum) | `Mode`, `RunId`                                                                | PT1M        | Yes       |
| **Observed Rewards** Number of Observed Rewards.                                                       | `ObservedRewards`              | Count   | Total (Sum) | `Mode`, `RunId`                                                                | PT1M        | Yes       |
| **Processed Characters** Number of Characters processed by Immersive Reader.                           | `ProcessedCharacters`          | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Processed Health Text Records** Number of health text records processed                              | `ProcessedHealthTextRecords`   | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Processed Images** Number of images processed                                                        | `ProcessedImages`              | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Processed Pages** Number of pages processed                                                          | `ProcessedPages`               | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Processed Text Records** Count of Text Records.                                                      | `ProcessedTextRecords`         | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **QA Text Records** Number of text records processed                                                   | `QuestionAnsweringTextRecords` | Count   | Total (Sum) | `ApiName`, `FeatureName`, `UsageChannel`, `Region`                             | PT1M        | Yes       |
| **Speech Session Duration (Deprecated)** Total duration of speech session in seconds.                  | `SpeechSessionDuration`        | Seconds | Total (Sum) | `ApiName`, `OperationName`, `Region`                                           | PT1M        | Yes       |
| **Total Events** Number of events.                                                                     | `TotalEvents`                  | Count   | Total (Sum) | `Mode`, `RunId`                                                                | PT1M        | Yes       |
| **Total Transactions (Deprecated)** Total number of transactions.                                      | `TotalTransactions`            | Count   | Total (Sum) | \<none>                                                                        | PT1M        | Yes       |

## Metric dimensions

For information about what metric dimensions are, see [Multi-dimensional metrics](https://learn.microsoft.com/en-us/azure/azure-monitor/platform/data-platform-metrics#multi-dimensional-metrics).

This service has the following dimensions associated with its metrics.

* ApiName
* FeatureName
* ModelDeploymentName
* ModelName
* ModelVersion
* OperationName
* Region
* StatusCode
* StreamType
* UsageChannel

## Resource logs

This section lists the types of resource logs you can collect for this service. The section pulls from the list of [all resource logs category types supported in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/platform/resource-logs-schema).

### Supported resource logs for Microsoft.CognitiveServices/accounts

| Category                  | Category display name      | Log table                                                                                                                                      | [Supports basic log plan](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/basic-logs-configure?tabs=portal-1#compare-the-basic-and-analytics-log-data-plans) | [Supports ingestion-time transformation](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/data-collection-transformations) | Example queries | Costs to export |
| ------------------------- | -------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------ | --------------- | --------------- |
| `Audit`                   | Audit Logs                 | [AzureDiagnostics](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/azurediagnostics)Logs from multiple Azure resources. | No                                                                                                                                                                      | No                                                                                                                                         |                 | No              |
| `AzureOpenAIRequestUsage` | Azure OpenAI Request Usage | [AzureDiagnostics](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/azurediagnostics)Logs from multiple Azure resources. | No                                                                                                                                                                      | No                                                                                                                                         |                 | Yes             |
| `RequestResponse`         | Request and Response Logs  | [AzureDiagnostics](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/azurediagnostics)Logs from multiple Azure resources. | No                                                                                                                                                                      | No                                                                                                                                         |                 | No              |
| `Trace`                   | Trace Logs                 | [AzureDiagnostics](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/azurediagnostics)Logs from multiple Azure resources. | No                                                                                                                                                                      | No                                                                                                                                         |                 | No              |

## Azure Monitor Logs tables

This section lists the Azure Monitor Logs tables relevant to this service, which are available for query by Log Analytics using Kusto queries. The tables contain resource log data and possibly more depending on what is collected and routed to them.

### Azure OpenAI microsoft.cognitiveservices/accounts

* [AzureActivity](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/azureactivity#columns)
* [AzureMetrics](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/azuremetrics#columns)
* [AzureDiagnostics](https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/azurediagnostics#columns)

## Activity log

The linked table lists the operations that can be recorded in the activity log for this service. These operations are a subset of [all the possible resource provider operations in the activity log](https://learn.microsoft.com/en-us/azure/role-based-access-control/resource-provider-operations).

For more information on the schema of activity log entries, see [Activity Log schema](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log-schema).

* [AI + machine learning resource provider operations](https://learn.microsoft.com/en-us/azure/role-based-access-control/resource-provider-operations#microsoftsearch)

## Related content

* See [Monitor Azure OpenAI](how-to/monitor-openai) for a description of monitoring Azure OpenAI.
* See [Monitor Azure resources with Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/monitor-azure-resource) for details on monitoring Azure resources.

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this tutorial, you learn how to deploy and use a DeepSeek reasoning model in Microsoft Foundry. This tutorial uses [DeepSeek-R1](https://ai.azure.com/explore/models/deepseek-r1/version/1/registry/azureml-deepseek?cid=learnDocs) for illustration. However, the content also applies to the newer [DeepSeek-R1-0528](https://ai.azure.com/explore/models/deepseek-r1-0528/version/1/registry/azureml-deepseek?cid=learnDocs) reasoning model.

**What you'll accomplish:**

In this tutorial, you'll deploy the DeepSeek-R1 reasoning model, send inference requests programmatically using code, and parse the reasoning output to understand how the model arrives at its answers.

The steps you perform in this tutorial are:

* Create and configure the Azure resources to use DeepSeek-R1 in Foundry Models.
* Configure the model deployment.
* Use DeepSeek-R1 with the next generation v1 Azure OpenAI APIs to consume the model in code.

## Prerequisites

To complete this article, you need:

* An Azure subscription with a valid payment method. If you don't have an Azure subscription, create a [paid Azure account](https://azure.microsoft.com/pricing/purchase-options/pay-as-you-go) to begin. If you're using [GitHub Models](https://docs.github.com/en/github-models/), you can [upgrade from GitHub Models to Microsoft Foundry Models](../how-to/quickstart-github-models) and create an Azure subscription in the process.

* Access to Microsoft Foundry with appropriate permissions to create and manage resources. Typically requires Contributor or Owner role on the resource group for creating resources and deploying models.

* Install the Azure OpenAI SDK for your programming language:

  * **Python**: `pip install openai azure-identity`
  * **.NET**: `dotnet add package Azure.Identity` and install the OpenAI package
  * **JavaScript**: `npm install openai @azure/identity`
  * **Java**: Add the Azure Identity package (see code examples for details)

DeepSeek-R1 is a reasoning model that generates explanations alongside answers—see [About reasoning models](#about-reasoning-models) for details.

## Create the resources

To create a Foundry project that supports deployment for DeepSeek-R1, follow these steps. You can also create the resources using [Azure CLI](../how-to/quickstart-create-resources?pivots=programming-language-cli) or [infrastructure as code, with Bicep](../how-to/quickstart-create-resources?pivots=programming-language-bicep).

1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/new-foundry.png)
2. The project you're working on appears in the upper-left corner.
3. To create a new project, select the project name, then **Create new project**.
4. Give your project a name and select **Create project**.

## Deploy the model

1. Add a model to your project. Select **Build** in the middle of the page, then **Model**.
2. Select **Deploy base model** to open the model catalog.
3. Find and select the **DeepSeek-R1** model tile to open its model card and select **Deploy**. You can select **Quick deploy** to use the defaults, or select **Customize deployment** to see and change the deployment settings.

When the deployment finishes, you land on its playground, where you can start to interact with the deployment.

If you prefer to explore the model interactively first, skip to [Use the model in the playground](#use-the-model-in-the-playground).

## Use the model in code

Use the Foundry Models endpoint and credentials to connect to the model.

1. Select the **Details** pane from the upper pane of the Playgrounds to see the deployment's details. Here, you can find the deployment's URI and API key.
2. Get your resource name from the deployment's URI to use for inferencing the model via code.

Use the next generation v1 Azure OpenAI APIs to consume the model in your code. These code examples use a secure, keyless authentication approach, Microsoft Entra ID, via the [Azure Identity library](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/identity-readme?view=azure-dotnet\&preserve-view=true).

The following code examples demonstrate how to:

1. Authenticate with Microsoft Entra ID using `DefaultAzureCredential`, which automatically attempts multiple authentication methods in sequence:

   1. **Environment variables** - Checks for service principal credentials in environment variables
   2. **Managed identity** - Uses managed identity if running in Azure (App Service, Functions, VM, etc.)
   3. **Azure CLI** - Falls back to Azure CLI credentials if you're authenticated locally
   4. **Other methods** - Continues through additional authentication methods as needed

   <Callout type="tip">
     For local development, ensure you're authenticated with Azure CLI by running `az login`. For production deployments in Azure, configure managed identity for your application.
   </Callout>

2. Create a chat completion client connected to your model deployment

3. Send a basic prompt to the DeepSeek-R1 model

4. Receive and display the response

**Expected output:** A JSON response containing the model's answer, reasoning process (within `<think>` tags), token usage statistics (prompt tokens, completion tokens, total tokens), and model information.

<Tabs>
  <Tab title="Python">
    Install the package `openai` using your package manager, like pip:

    ```bash
    pip install --upgrade openai
    ```

    The following example shows how to create a client to consume chat completions and then generate and print out the response:

    ```python

    from openai import OpenAI
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider

    token_provider = get_bearer_token_provider(
        DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
    )

    client = OpenAI(
      base_url = "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/",
      api_key=token_provider,
    )
    response = client.chat.completions.create(
      model="DeepSeek-R1", # Replace with your model deployment name.
      messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "How many languages are in the world?"}
      ]
    )

    #print(response.choices[0].message)
    print(response.model_dump_json(indent=2))
    ```
  </Tab>

  <Tab title="JavaScript">
    First install the Azure Identity client library before you can use DefaultAzureCredential:

    Install the package `@azure/identity` using npm:

    ```bash
    npm install @azure/identity
    ```

    To authenticate the `OpenAI` client, use the `getBearerTokenProvider` function from the `@azure/identity` package. This function creates a token provider that `OpenAI` uses internally to obtain tokens for each request.

    The following code creates the token provider, creates a client to consume chat completions, and generates the response:

    ```javascript
    import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
    import { OpenAI } from "openai";

    const tokenProvider = getBearerTokenProvider(
        new DefaultAzureCredential(),
        'https://cognitiveservices.azure.com/.default');

    const client = new OpenAI({
        baseURL: "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/",
        apiKey: tokenProvider
    });

    const messages = [
        { role: 'system', content: 'You are a helpful assistant.' },
        { role: 'user', content: 'How many languages are in the world?' }
    ];

    // Make the API request with top-level await
    const result = await client.chat.completions.create({
        messages,
        model: 'DeepSeek-R1', // Your model deployment name
        max_tokens: 100
    });

    // Print the full response
    console.log('Full response:', result);

    // Print just the message content from the response
    console.log('Response content:', result.choices[0].message.content);
    ```
  </Tab>

  <Tab title="C#">
    First install the [Azure Identity library](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/identity-readme?view=azure-dotnet\&preserve-view=true) before you can use DefaultAzureCredential:

    ```dotnetcli
    dotnet add package Azure.Identity
    ```

    Use the desired credential type from the library. For example, [`DefaultAzureCredential`](https://learn.microsoft.com/en-us/dotnet/api/azure.identity.defaultazurecredential?view=azure-dotnet\&preserve-view=true). Then create the token provider, create a client to consume chat completions, and generate the response.

    ```csharp
    using Azure.Identity;
    using OpenAI;
    using OpenAI.Chat;
    using System.ClientModel.Primitives;

    #pragma warning disable OPENAI001

    BearerTokenPolicy tokenPolicy = new(
        new DefaultAzureCredential(),
        "https://cognitiveservices.azure.com/.default");

    ChatClient client = new(
        model: "DeepSeek-R1", // Replace with your model deployment name.
        authenticationPolicy: tokenPolicy,
        options: new OpenAIClientOptions() {
            Endpoint = new Uri("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1")
       }
    );

    ChatCompletion completion = client.CompleteChat("How many languages are in the world?");

    Console.WriteLine($"[ASSISTANT]: {completion.Content[0].Text}");
    ```
  </Tab>

  <Tab title="Java">
    Authentication with Microsoft Entra ID requires some initial setup:

    Add the Azure Identity package:

    ```xml
    <dependency>
        <groupId>com.azure</groupId>
        <artifactId>azure-identity</artifactId>
        <version>1.18.0</version>
    </dependency>
    ```

    After setup, you can choose which type of credential from `azure.identity` to use. As an example, `DefaultAzureCredential` can be used to authenticate the client.

    Authentication is easiest using `DefaultAzureCredential`. It finds the best credential to use in its running environment.

    ```java
    Credential tokenCredential = BearerTokenCredential.create(
            AuthenticationUtil.getBearerTokenSupplier(
                    new DefaultAzureCredentialBuilder().build(),
                    "https://cognitiveservices.azure.com/.default"));
    OpenAIClient client = OpenAIOkHttpClient.builder()
            .baseUrl("https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/")
            .credential(tokenCredential)
            .build();
    ```

    For more information about Azure OpenAI keyless authentication, see [Use Azure OpenAI without keys](https://learn.microsoft.com/en-us/azure/developer/ai/keyless-connections?tabs=java%2Cazure-cli).

    **Chat completion**:

    ```java
    package com.example;

    import com.openai.client.OpenAIClient;
    import com.openai.client.okhttp.OpenAIOkHttpClient;
    import com.openai.models.ChatModel;
    import com.openai.models.chat.completions.ChatCompletion;
    import com.openai.models.chat.completions.ChatCompletionCreateParams;

    public class OpenAITest {
        public static void main(String[] args) {
            String resourceName = "https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1";
            String modelDeploymentName = "DeepSeek-R1"; //replace with you model deployment name

            try {
                OpenAIClient client = OpenAIOkHttpClient.builder()
                        .baseUrl(resourceName)
                        // Set the Azure Entra ID
                        .credential(BearerTokenCredential.create(AuthenticationUtil.getBearerTokenSupplier(
                            new DefaultAzureCredentialBuilder().build(), "https://cognitiveservices.azure.com/.default")))
                        .build();

               ChatCompletionCreateParams params = ChatCompletionCreateParams.builder()
                  .addUserMessage("How many languages are in the world?")
                  .model(modelDeploymentName)
                  .build();
               ChatCompletion chatCompletion = client.chat().completions().create(params);
            }
        }
    }
    ```
  </Tab>

  <Tab title="REST">
    ```bash
    curl -X POST https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_OPENAI_AUTH_TOKEN" \
      -d '{
          "model": "DeepSeek-R1",
          "messages": [
          {
            "role": "system",
            "content": "You are a helpful assistant."
          },
          {
            "role": "user",
            "content": "How many languages are in the world?"
          }
        ]
      }'
    ```

    To retrieve a response:

    ```bash
    curl -X GET https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/chat/completions/{response_id} \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_OPENAI_AUTH_TOKEN"
    ```
  </Tab>
</Tabs>

**API Reference:**

* [OpenAI Python client](https://github.com/openai/openai-python)
* [OpenAI JavaScript client](https://github.com/openai/openai-node)
* [OpenAI .NET client](https://github.com/openai/openai-dotnet)
* [DefaultAzureCredential class](https://learn.microsoft.com/en-us/dotnet/api/azure.identity.defaultazurecredential)
* [Chat completions API reference](../../openai/latest#create-chat-completion)
* [Azure Identity library overview](https://learn.microsoft.com/en-us/dotnet/api/overview/azure/identity-readme)

Reasoning might generate longer responses and consume a larger number of tokens. See the [rate limits](../quotas-limits) that apply to DeepSeek-R1 models. Consider having a retry strategy to handle rate limits. You can also [request increases to the default limits](../quotas-limits#request-increases-to-the-default-limits).

## About reasoning models

Reasoning models can reach higher levels of performance in domains like math, coding, science, strategy, and logistics. The way these models produce outputs is by explicitly using chain of thought to explore all possible paths before generating an answer. They verify their answers as they produce them, which helps to arrive at more accurate conclusions. As a result, reasoning models might require less context prompts in order to produce effective results.

Reasoning models produce two types of content as outputs:

* Reasoning completions
* Output completions

Both of these completions count towards content generated from the model. Therefore, they contribute to the token limits and costs associated with the model. Some models, like `DeepSeek-R1`, might respond with the reasoning content. Others, like `o1`, output only the completions.

### Reasoning content

Some reasoning models, like DeepSeek-R1, generate completions and include the reasoning behind them. The reasoning associated with the completion is included in the response's content within the tags `<think>` and `</think>`. The model can select the scenarios for which to generate reasoning content. The following example shows how to generate the reasoning content, using Python:

```python
import re

match = re.match(r"<think>(.*?)</think>(.*)", response.choices[0].message.content, re.DOTALL)

print("Response:", )
if match:
    print("\tThinking:", match.group(1))
    print("\tAnswer:", match.group(2))
else:
    print("\tAnswer:", response.choices[0].message.content)
print("Model:", response.model)
print("Usage:")
print("\tPrompt tokens:", response.usage.prompt_tokens)
print("\tTotal tokens:", response.usage.total_tokens)
print("\tCompletion tokens:", response.usage.completion_tokens)
```

```console
Thinking: Okay, the user is asking how many languages exist in the world. I need to provide a clear and accurate answer. Let's start by recalling the general consensus from linguistic sources. I remember that the number often cited is around 7,000, but maybe I should check some reputable organizations.\n\nEthnologue is a well-known resource for language data, and I think they list about 7,000 languages. But wait, do they update their numbers? It might be around 7,100 or so. Also, the exact count can vary because some sources might categorize dialects differently or have more recent data. \n\nAnother thing to consider is language endangerment. Many languages are endangered, with some having only a few speakers left. Organizations like UNESCO track endangered languages, so mentioning that adds context. Also, the distribution isn't even. Some countries or regions have hundreds of languages, like Papua New Guinea with over 800, while others have just a few. \n\nA user might also wonder why the exact number is hard to pin down. It's because the distinction between a language and a dialect can be political or cultural. For example, Mandarin and Cantonese are considered dialects of Chinese by some, but they're mutually unintelligible, so others classify them as separate languages. Also, some regions are under-researched, making it hard to document all languages. \n\nI should also touch on language families. The 7,000 languages are grouped into families like Indo-European, Sino-Tibetan, Niger-Congo, etc. Maybe mention a few of the largest families. But wait, the question is just about the count, not the families. Still, it's good to provide a bit more context. \n\nI need to make sure the information is up-to-date. Let me think – recent estimates still hover around 7,000. However, languages are dying out rapidly, so the number decreases over time. Including that note about endangerment and language extinction rates could be helpful. For instance, it's often stated that a language dies every few weeks. \n\nAnother point is sign languages. Does the count include them? Ethnologue includes some, but not all sources might. If the user is including sign languages, that adds more to the count, but I think the 7,000 figure typically refers to spoken languages. For thoroughness, maybe mention that there are also over 300 sign languages. \n\nSummarizing, the answer should state around 7,000, mention Ethnologue's figure, explain why the exact number varies, touch on endangerment, and possibly note sign languages as a separate category. Also, a brief mention of Papua New Guinea as the most linguistically diverse country/region. \n\nWait, let me verify Ethnologue's current number. As of their latest edition (25th, 2022), they list 7,168 living languages. But I should check if that's the case. Some sources might round to 7,000. Also, SIL International publishes Ethnologue, so citing them as reference makes sense. \n\nOther sources, like Glottolog, might have a different count because they use different criteria. Glottolog might list around 7,000 as well, but exact numbers vary. It's important to highlight that the count isn't exact because of differing definitions and ongoing research. \n\nIn conclusion, the approximate number is 7,000, with Ethnologue being a key source, considerations of endangerment, and the challenges in counting due to dialect vs. language distinctions. I should make sure the answer is clear, acknowledges the variability, and provides key points succinctly.

Answer: The exact number of languages in the world is challenging to determine due to differences in definitions (e.g., distinguishing languages from dialects) and ongoing documentation efforts. However, widely cited estimates suggest there are approximately **7,000 languages** globally.
Model: DeepSeek-R1
Usage:
  Prompt tokens: 11
  Total tokens: 897
  Completion tokens: 886
```

**API Reference:**

* [Python re module documentation](https://docs.python.org/3/library/re.html)
* [ChatCompletion object reference](https://github.com/openai/openai-python/blob/main/src/openai/types/chat/chat_completion.py)

### Prompt reasoning models

When building prompts for reasoning models, take the following into consideration:

* Use simple instructions and avoid using chain-of-thought techniques.
* Built-in reasoning capabilities make simple zero-shot prompts as effective as more complex methods.
* When providing additional context or documents, like in RAG scenarios, including only the most relevant information might help prevent the model from over-complicating its response.
* Reasoning models may support the use of system messages. However, they might not follow them as strictly as other non-reasoning models.
* When creating multi-turn applications, consider appending only the final answer from the model, without it's reasoning content, as explained in the [Reasoning content](#reasoning-content) section.

Notice that reasoning models can take longer times to generate responses. They use long reasoning chains of thought that enable deeper and more structured problem-solving. They also perform self-verification to cross-check their answers and correct their mistakes, thereby showcasing emergent self-reflective behaviors.

### Parameters

Reasoning models support a subset of the standard chat completion parameters to maintain the integrity of their reasoning process.

**Supported parameters:**

* `max_tokens` - Maximum number of tokens to generate in the response
* `stop` - Sequences where the API stops generating tokens
* `stream` - Enable streaming responses
* `n` - Number of completions to generate
* `frequency_penalty` - Reduces repetition of token sequences

**Unsupported parameters** (reasoning models don't support these):

* `temperature` - Fixed to optimize reasoning quality
* `top_p` - Not configurable for reasoning models
* `presence_penalty` - Not available
* `repetition_penalty` - Use `frequency_penalty` instead

**Example using `max_tokens`:**

```python
response = client.chat.completions.create(
    model="DeepSeek-R1",
    messages=[
        {"role": "user", "content": "Explain quantum computing"}
    ],
    max_tokens=1000  # Limit response length
)
```

For the complete list of supported parameters, see the [Chat completions API reference](../../openai/latest#create-chat-completion).

## Use the model in the playground

Use the model in the playground to get an idea of the model's capabilities.

As stated previously, immediately a model deployment is complete, you land on the model's playground, where you can start to interact with the deployment. For example, you can enter your prompts, such as "How many languages are in the world?" in the playground.

## What you learned

In this tutorial, you accomplished the following:

* Created Foundry resources for hosting AI models
* Deployed the DeepSeek-R1 reasoning model
* Made authenticated API calls using Microsoft Entra ID
* Sent inference requests and received reasoning outputs
* Parsed reasoning content from model responses to understand the model's thought process

## Related content

* [How to generate chat completions with Foundry Models](../../openai/api-version-lifecycle)
* [Use chat reasoning models](../how-to/use-chat-reasoning)
* [Azure OpenAI supported programming languages](../../openai/supported-languages)

These techniques aren't recommended for reasoning models like gpt-5 and o-series models.

Prompt construction can be difficult. In practice, the prompt acts assist the model complete the desired task, but it's more of an art than a science, often requiring experience and intuition to craft a successful prompt. The goal of this article is to help get you started with this learning process. It attempts to capture general concepts and patterns that apply to all GPT models. However it's important to understand that each model behaves differently, so the learnings might not apply equally to all models.

## Basics

This section covers the basic concepts and elements of GPT prompts.

Text prompts are how users interact with GPT models. As with all generative language models, GPT models attempt to produce the next series of words that are most likely to follow from the previous text. It's as if we're saying *What is the first thing that comes to your mind when I say `<prompt>`?* The examples below demonstrate this behavior. Given the first words of famous content, the model is able to accurately continue the text.

| Prompt                             | Completion                                                                                                                                                                                                             |
| ---------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Four score and seven years ago our | fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. \[…]                                                                     |
| "Call me Ishmael."                 | "Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world." \[…] |

As you develop more complex prompts, it's helpful to keep this fundamental behavior in mind. Regardless of the prompt that is provided, the model is simply responding with what it determines is most likely (given its training data and training targets). If you ask a question in your prompt, the model isn't following a separate “Q\&A” code path, but rather it appears to answer the question because an answer is the most likely sort of response for the given question as input.

### Prompt components

When using the Completion API while there's no differentiation between different parts of the prompt, it can still be useful for learning and discussion to identify underlying prompt components. With the [Chat Completion API](../how-to/chatgpt) there are distinct sections of the prompt that are sent to the API in the form of an array of dictionaries with associated roles: system, user, and assistant. This guidance focuses more generally on how to think about prompt construction rather than providing prescriptive guidance that is specific to one API over another.

It's also important to understand that while there could be other valid ways to dissect prompts, the goal of this breakdown is to provide a relatively simple way to think about prompt construction. With the Completion API, all the components are optional, but at least one must be present and most prompts include more than one component. There can be some grey area between components as well. The order presented below roughly corresponds to how commonly each component is used, from most to least.

#### Instructions

Instructions are likely the most commonly used prompt component. Instructions are simply that: instructions to the model on what to do. While simple in concept, they can be complex in practice. The following table illustrates this by showing both simple and more complex instructions, in two different forms.

| Instruction Type     | Prompt                                                                                                                                                                                                                                                         | Completion                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| -------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Simple instructions  | Write an introduction for a weekly newsletter.                                                                                                                                                                                                                 | Welcome to our weekly newsletter. This week we have a special focus on productivity. We'll be sharing tips and tricks on how to be productive in all areas of your life. We hope you enjoy and please share with your friends!                                                                                                                                                                                                                            |
| Complex instructions | Write an introduction for a weekly newsletter to my company named Contoso. It should mention the great all hands meeting we had last week and thank the team for all their hard work over the tough past few months.                                           | Welcome to Contoso's weekly newsletter! We had a great all hands meeting last week and I want to thank the team for all their hard work over the tough past few months. We're looking forward to a bright future and I'm confident that with our team's dedication and commitment, we'll achieve great things. Thank you for being a part of our team and I look forward to working with you all in the coming weeks and months. Sincerely, \[Your name]. |
| Complex Instructions | Write an introduction for a weekly newsletter, including the following: - My company is named contoso - An exuberant greeting - Thanking the team for their hard work over a couple tough months - Positive outlook for the coming quarter - Signed by the SLT | Hello Contoso team! I wanted to take a moment to thank you for all your hard work over the past few months. It's been a tough couple of months, but you've all pulled together and done an amazing job. I'm really proud of you all. Looking ahead to the next quarter, I'm feeling really positive. I think we're going to achieve some great things. Thanks again, The Senior Leadership Team.                                                          |

### Primary Content

Primary content refers to some sort of text that is being processed or transformed by the model. Primary content is typically used in conjunction with instructions. A simple example would be language translation. In the example below, the English text would be considered the primary content while "Translate to French:" is an instruction.

| Prompt                                                                | Completion                                                    |
| --------------------------------------------------------------------- | ------------------------------------------------------------- |
| Can you please tell me how to get to the museum? Translate to French: | `Pouvez-vous s'il vous plaît me dire comment aller au musée?` |

Primary content can also be much longer. In the following example, the primary content is the introduction section from the [Wikipedia entry on time complexity](https://en.wikipedia.org/wiki/Time_complexity), which is almost 300 words long. This content is abbreviated in our table for ease of display.

| Prompt                                                                                                                                                                                                                                            | Completion                                                                                                                                                                                                                                                                                                                                                                     |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by \[...] Summarize the previous text in a few sentences: | Time complexity is a measure of how long an algorithm takes to run. It's typically expressed as a function of the size of the input. The most common measure is worst-case time complexity, which is the maximum amount of time required for inputs of a given size.                                                                                                           |
| In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by \[...] `TL;DR` (Too long didn't read)                  | The time complexity of an algorithm is the amount of time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. The time complexity is generally expressed as a function of the size of the input. |

GPT models can also handle primary content that is structured. In the example below, a TSV about beers in Belgium (abbreviated for display in this article) is passed to the model as part of the prompt. It's capable of properly interpreting the content and answering questions about the data.

| Prompt                                                                                                                                                                                                                                                                                            | Completion                                                 |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- |
| `Belgian Beer Brands` `Beer name Beer style color ABV Brewery` `"Chimay Gold: ""Doree""" trappist pale ale gold 4.80% Chimay Brewery (official trappist)` `"Chimay Blue: ""Grande Reserve""" trappist dark ale dark 9.00%` \[…] Referencing the table above, how many beers are less than 6% ABV? | There are 10 beers in the table that are less than 6% ABV. |

### Examples

Successful prompts often rely on the practice of “one-shot” or “few-shot” learning. This refers to the inclusion of one or more examples of the desired behavior of the model, typically by including input and output pairs. This isn't learning in the sense that the model is permanently changed, but rather that the examples better condition the model to respond as desired for only the current inference. The use of prompts with no examples is sometimes referred to as “zero-shot” learning. Note that with the Chat Completion API few-shot learning examples are typically added to the messages array in the form of example user/assistant interactions after the initial system message.

| Learning Type | Prompt                                                                                                                                                                                                                                                                          | Completion                                                                                                                                                                 |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Zero-shot     | Headline: Coach confident injury won't derail Warriors Topic:                                                                                                                                                                                                                   | The coach is confident that the injury won't derail the Warriors' season. The team is still focused on their goals and that they'll continue to work hard to achieve them. |
| Few-shot      | Headline: Twins' Correa to use opt-out, test free agency Topic: Baseball Headline: Qatar World Cup to have zones for sobering up Topic: Soccer Headline: Yates: Fantasy football intel for Week 6 Topic: Football Headline: Coach confident injury won't derail Warriors Topic: | Basketball                                                                                                                                                                 |

The example above illustrates the utility of few-shot learning. Without the examples, the model seems to be guessing at the desired behavior, while the examples cleanly show the model how to operate. This also demonstrates the power of the model, it can infer the category of label that is wanted, even without a "basketball" label in the examples.

### Cue

Cues act as the "jumpstart" for the output of the model, helping to direct the model to the desired output. It's often a prefix that the model can build onto. In the example below, we show the cue directing the story to include key elements. Cues are often used along with instructions, but not always. In the examples below, the model is being used to summarize a long announcement email. A cue is used in one case to help focus the output of the model and to suggest a specific output format (bullet points) in the other.

In the context of the Chat Completion API instructions take the form of the system message, and few-shot examples are indicated by providing a series of example user/assistant interactions to help prime the model.

| Number of Cues | Prompt                                                                                                                                                                    | Completion                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 0              | Performance reviews are a core component of our Performance & Development approach at Contoso. \[…] Summarize the above email message:                                    | The purpose of this email is to inform employees of recent changes to the performance review experience, which is part of Contoso's Performance & Development approach. The changes include an improved structure, upgraded in-tool user experience, and focused prompts. Additionally, there's now an optional Conversation Starters section for employees to flag topics they'd like to discuss with their managers.                                                                                                                                                                                                                              |
| 1              | Performance reviews are a core component of our Performance & Development approach at Contoso. \[…] Summarize the above email message: The key takeaway of the message is | that Contoso is improving the performance review experience by making it more focused on core priorities, expectations, and impact. Additionally, they're upgrading the in-tool user experience with tips, resources, and guidance. Finally, they're adding guidance on how to activate company-wide core priorities.                                                                                                                                                                                                                                                                                                                               |
| 2              | Performance reviews are a core component of our Performance & Development approach at Contoso. \[…] Summarize the above email message: Key Points: •                      | Contoso is introducing changes to the performance review experience, based on feedback from employees. • The changes include a new focus on core priorities, expectations, and impact; an upgraded user experience; and new guidance on company-wide core priorities. • Performance reviews are a time to reflect on past performance and learning as well as to plan for future priorities and growth opportunities. • Employees and managers should write comments in advance of the performance review meeting. • There should be at least two performance reviews per year. • The time investment for each review should remain about the same. |

### Supporting content

Supporting content is information that the model can utilize to influence the output in some way. It differs from primary content in that it isn't the main target of task, but it's typically used along with primary content. Common examples include contextual information such as the current date, the name of the user, user preferences, and so on. The example below uses supporting content to help organize a set of planned workshops for the user. Without the supporting (important topics) the model simply lists off the workshops (truncated for display), when informed of my important topics, the model is able to accurately group the meetings.

| Supporting Content | Prompt                                                                                                                                                                                                                                                                                                                                                                                     | Completion                                                                                                                                                                                      |
| ------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Without            | Workshop list: Title Description Planning Workshop Kickoff Please join us to kick off our workshop! Hear from our leaders about cross group priorities and learnings for the year ahead. The invitation to this event has already been sent to everyone in \[…] Summarize the workshops above, grouped by topic:                                                                           | Workshop list: Planning Workshop Kickoff -Planning Workshop Wrap-up - Core MeetUp - Customer Driven Perspectives -Impact and Key Focus Areas -Best practices -Accelerating app adoption \[…]    |
| With               | Workshop list: Title Description Planning Workshop Kickoff Please join us to kick off our workshop! Hear from our leaders about cross group priorities and learnings for the year ahead. The invitation to this event has already been sent to everyone in \[…] My Important Topics: prompt engineering, search, GPT models Summarize the workshops above, grouped by my important topics: | 1. Prompt Engineering: - New prompt engineering tips - The art of prompt engineering 2. Search: -Intro to vector search with embedding 3. GPT Models: - Intro to GPT-4 - GPT-35-Turbo in-depth. |

## Scenario-specific guidance

The techniques in this section will teach you strategies for increasing the accuracy and grounding of responses you generate with a Large Language Model (LLM). It is, however, important to remember that even when using prompt engineering effectively you still need to validate the responses the models generate. Just because a carefully crafted prompt worked well for a particular scenario doesn't necessarily mean it will generalize more broadly to certain use cases. Understanding the [limitations of LLMs](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/transparency-note#limitations), is just as important as understanding how to leverage their strengths.

This guide doesn't go in-depth into the mechanics behind the message structure for Chat Completions. If you aren't familiar with interacting with Chat Completions models programmatically, we recommend reading our [how-to guide on the Chat Completion API first](../how-to/chatgpt).

<Callout type="note">
  All of the examples in this section of the guide were tested against a base GPT-4 model in English. Some techniques may produce different results with newer models like gpt-4o, and gpt 4.5. If you are reading a localized version of this article in another language, these responses represent a localized translation of the English results. To learn more about potential limitations depending on what language you are using to prompt a model, please consult our [Responsible AI transparency note](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/transparency-note#limitations).
</Callout>

## Few-shot learning

A common way to adapt language models to new tasks is to use few-shot learning. In few-shot learning, a set of training examples is provided as part of the prompt to give additional context to the model.

When using the Chat Completions API, a series of messages between the User and Assistant (written in the [new prompt format](../how-to/chatgpt?pivots=programming-language-chat-completions#few-shot-learning-with-chat-completion)), can serve as examples for few-shot learning. These examples can be used to prime the model to respond in a certain way, emulate particular behaviors, and seed answers to common questions.

| System message                                                                                                                                                                                                                                                                                      | Few-shot examples                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Assistant is an intelligent chatbot designed to help users answer their tax related questions. Instructions - Only answer questions related to taxes. - If you're unsure of an answer, you can say "I don't know" or "I'm not sure" and recommend users go to the IRS website for more information. | **User** - "When do I need to file my taxes by?" **Assistant** - "In 2023, you'll need to file your taxes by April 18th. The date falls after the usual April 15th deadline because April 15th falls on a Saturday in 2023. For more details, see [https://www.irs.gov/filing/individuals/when-to-file](https://www.irs.gov/filing/individuals/when-to-file) " **User** - "How can I check the status of my tax refund?" **Assistant** - "You can check the status of your tax refund by visiting [https://www.irs.gov/refunds](https://www.irs.gov/refunds) " |

The table above describes the basic prompt structure, but for a detailed explanation of the exact prompt format you'll need to refer to the [Chat Completion guide](../how-to/chatgpt).

## Non chat scenarios

While the Chat Completion API is optimized to work with multi-turn conversations, it also can be used for non chat scenarios. For example, for a sentiment analysis scenario, you might use the following prompt:

| System message                                                                                                                                                                                                                                                | User                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | Assistant                                                                                                                                                                                                                                                                                                                                                                                                     |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You're an assistant designed to analyze sentiment from speech data. Users will paste in a string of text and you'll respond with an assessment of the response of the speaker. Rate on a scale of 1-10 (10 being highest). Explain why this rating was given. | hi thank you for calling contoso pharmacy who am i speaking with today i understand your frustration mary um can you tell me what exactly you're trying to accomplish OK uh certainly happy to check that for you one moment please i see here that you were on a generic form of lipitor is that right OK uh so i see that your doctor stopped prescribing these drugs in twenty twenty one actually OK uh yeah i'd be happy to check that for you uh because sometimes there's a gap in the system and it just doesn't get reported so let me take a look here just a moment so i'm seeing here that your doctor had these drugs prescribed to you from twenty twelve through twenty twenty one OK um yeah according to the latest records uh provided by doctor's office you're now on a different drug metformin would you like us to go ahead and fill that prescription for you for pick up a bit of form in five hundred milligrams uh you're very welcome mary please let us know if there's anything else we can do for you today. | Sentiment rating: 8 The speaker in this text seems to be polite, helpful, and patient while addressing the concerns of the customer. They take the time to check the information, provide clear explanations, and offer assistance. However, the rating isn't a 10 because there's still a hint of uncertainty in some parts of the conversation, which might not convey complete confidence to the customer. |

## Start with clear instructions

The sequence information appears in the prompt matters. This is because GPT style models are built in a certain way that defines how they process the input. Our research suggests that telling the model the task you want it to do at the beginning of the prompt, before sharing additional contextual information or examples, can help produce higher-quality outputs.

<Callout type="note">
  Although following this technique is still generally recommended, in contrast to previous model versions (GPT-3 and prior), our testing showed that the model response with ChatGPT and GPT-4 models was the same regardless of whether the technique is utilized. In the case of the below example, we see that adding the statement “Several sources … eruption” at the beginning or the end of the prompt did not cause any change in the final model response.
</Callout>

| System message                                             | User                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Assistant                                                                                                                                                                                                                                              |
| ---------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| You're an AI assistant that helps people find information. | Your task is to verify if the statement "Several sources mention a chance of another large eruption" is supported by a specific quote from the following set of snippets. --- SNIPPETS \[1] 14 percent chance of megaquake hitting Seattle, experts say SEATTLE - There's a 14 percent chance of a magnitude 9 Cascadia earthquake hitting Seattle in the next 50 years, the U.S. Geological Survey estimates. "Unfortunately, we're unable to... \[2] Earthquake experts lay out latest outlook for Seattle's 'Really Big One’ “We say that there's approximately a 14% chance of another approximately magnitude-9 earthquake occurring in the next 50 years,” said a geophysicist at the University of Washington... --- Is the statement "Several sources mention a chance of another large eruption" directly implied or stated by the snippets? | No, the statement isn't directly implied or stated by the snippets. The snippets mention a chance of a "megaquake" and a "magnitude 9 Cascadia earthquake" hitting Seattle in the next 50 years, but don't mention a chance of another large eruption. |

## Repeat instructions at the end

Models can be susceptible to **recency bias**, which in this context means that information at the end of the prompt might have more significant influence over the output than information at the beginning of the prompt. Therefore, it's worth experimenting with repeating the instructions at the end of the prompt and evaluating the impact on the generated response.

## Prime the output

This refers to including a few words or phrases at the end of the prompt to obtain a model response that follows the desired form. For example, using a cue such as `“Here’s a bulleted list of key points:\n- ”` can help make sure the output is formatted as a list of bullet points.

| System message                                                                                 | User                                                                                                                                                                                             | Assistant                                                             |
| ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------- |
| You're an AI assistant that helps people find information. Answer in as few words as possible. | John Smith is married to Lucy Smith. They have five kids, and he works as a software engineer at Microsoft. What search queries should I do to fact-check this? ## One possible search query is: | "John Smith married Lucy Smith five kids software engineer Microsoft" |

In the above prompt, the text *One possible search query is:* primes the model to produce a single output. Without this cue the model produces several search queries as output.

## Add clear syntax

Using clear syntax for your prompt—including punctuation, headings, and section markers—helps communicate intent and often makes outputs easier to parse.

In the example below, separators (`---` in this case) have been added between different sources of information or steps. This allows the use of `---` as a stopping condition for generation. In addition, section headings or special variables are presented in uppercase to differentiate them.

| System message                                             | User                                                                                                                                                                                                                                                                                                              | Assistant                                                                                                                                                                                                                              |
| ---------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You're an AI assistant that helps people find information. | You'll read a paragraph, and then issue queries to a search engine in order to fact-check it. Also explain the queries. --- PARAGRAPH John Smith is married to Lucy Smith. They have five kids, and he works as a software engineer at Microsoft. What search queries should I do to fact-check this? --- QUERIES | 1. "John Smith Microsoft" - To check if John Smith is indeed employed at Microsoft. 2. "John Smith Lucy Smith" - To check if John Smith is married to Lucy Smith. 3. "John Smith children" - To check if John Smith has five children. |

If you're not sure what syntax to use, consider using Markdown or XML. The models have been trained on a large quantity web content in XML and Markdown, which might provide better results.

## Break the task down

Large language models (LLMs) often perform better if the task is broken down into smaller steps. For example, in the search query prompt referenced earlier, the prompt can be restructured so that the model is first instructed to extract relevant facts, and then instructed to generate search queries that can be used to verify those facts.

| System message                                             | User                                                                                                                                                                                                                                                                                                                                                                                                                           | Assistant                                                                                                                                                                                                                                       |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You're an AI assistant that helps people find information. | You'll read a paragraph, and then issue queries to a search engine in order to fact-check it. --- PARAGRAPH John Smith is married to Lucy Smith. They have five kids, and he works as a software engineer at Microsoft. What search queries should I do to fact-check this? --- Now you'll extract factual claims, and issue queries to fact-check them. When issuing a query, use the function SEARCH("query") FACTUAL CLAIMS | 1. John Smith is married to Lucy Smith 2. They have five kids 3. He works as a software engineer at Microsoft QUERIES SEARCH("John Smith married Lucy Smith") SEARCH("John Smith family size") SEARCH("John Smith Microsoft software engineer") |

Notice the use of clear syntax to differentiate the sections and prime the output. In this simple example, breaking the task down from one to two steps isn't very dramatic, but when trying to do this for a larger piece of text with many factual claims, breaking the task down can make a significant difference.

## Use of affordances

Sometimes we can get the model to use an [affordance](https://www.interaction-design.org/literature/topics/affordances) instead of relying on its own parameters for information and answers. Search, for example, can be an affordance to help mitigate against fabricated answers, and to get up-to-date information.

One simple way to use an affordance is to stop generation once the affordance calls are generated by the model, then paste the outcomes back into the prompt. Below is an example of what the follow-up call could be after doing the **SEARCH** calls above. Notice how we pasted the search results into the prompt and replaced the previous **SEARCH** calls.

| System message                                             | User                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Assistant                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ---------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You're an AI assistant that helps people find information. | You'll read a paragraph, extract factual claims, and then use search engine results to fact-check them --- PARAGRAPH John Smith is married to Lucy Smith. They have five kids, and he works as a software engineer at Microsoft. What search queries should I do to fact-check this? --- FACTUAL CLAIMS - John Smith is married to Lucy Smith - John and Lucy have five kids - John works as a software engineer at Microsoft --- Here are various search queries issued to research the claims above: SEARCH QUERIES - John Smith married to Lucy Smith - John Smith number of children - John Smith software engineer Microsoft --- Here are snippets of the search results: SNIPPETS: \[1] … John Smith’s wedding was on September 25, 2012 … \[2] … John Smith was accompanied by his wife Lucy to a party \[3]John was accompanied to the soccer game by his two daughters and three sons \[4] … After spending 10 years at Microsoft, Smith founded his own startup, Tailspin Toys \[5] John M is the town smith, and he married Fiona. They have a daughter named Lucy --- Given the snippets, fact check each of the factual claims above: | - John Smith is married to Lucy Smith: Confirmed. Snippet \[1] and \[2] both mention John Smith's wife as Lucy. - John and Lucy have five kids: Confirmed. Snippet \[3] mentions that John was accompanied by his two daughters and three sons. - John works as a software engineer at Microsoft: Partially confirmed. Snippet \[4] mentions that John spent 10 years at Microsoft, but it doesn't explicitly state that he currently works there. Further research might be needed to confirm his current employment status. |

## Chain of thought prompting

<Callout type="important">
  This technique is only applicable non-reasoning models. Attempting to extract model reasoning through methods other than the reasoning summary parameter are not supported, may violate the Acceptable Use Policy, and may result in throttling or suspension when detected.
</Callout>

This is a variation on the **break the task down** technique. Instead of splitting a task into smaller steps, in this approach, the model response is instructed to proceed step-by-step and present all the steps involved. Doing so reduces the possibility of inaccuracy of outcomes and makes assessing the model response easier.

| System message                                             | User                                                                                                                                                                                                                                                              | Assistant                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| ---------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| You're an AI assistant that helps people find information. | Who was the most decorated (maximum medals) individual athlete in the Olympic games that were held at Sydney? Take a step-by-step approach in your response, cite sources and give reasoning before sharing final answer in the below format: ANSWER is: `<name>` | Step 1: Determine the year of the Sydney Olympic games. The Sydney Olympic games were held in the year 2000. Step 2: Find a list of individual athletes who won multiple medals in the 2000 Sydney Olympics. According to the official website of the Olympic Games, there were several individual athletes who won multiple medals in the 2000 Sydney Olympics. Some of the notable names include: • Ian Thorpe (Australia) - 3 gold, 2 silver, 0 bronze • Pieter van den Hoogenband (Netherlands) - 2 gold, 0 silver, 2 bronze • Lenny Krayzelburg (USA) - 3 gold, 0 silver, 0 bronze • Inge de Bruijn (Netherlands) - 3 gold, 1 silver, 0 bronze Step 3: Determine the athlete with the most medals. After reviewing the list of individual athletes who won multiple medals in the 2000 Sydney Olympics, we can see that the athlete with the most medals is Ian Thorpe from Australia, who won a total of 5 medals (3 gold and 2 silver). Therefore, the ANSWER is: Ian Thorpe. |

## Specifying the output structure

Using your prompt to specify the structure of the output can have a significant effect on the nature and quality of the results. Sometimes system message inputs as “only write true facts” or “don't fabricate information” might not be adequate mitigations. Instead, asking for the model response to also include citations can help result in a reduction of the prevalence of incorrect responses.

If you instruct the model to cite the source material when it makes statements, those statements are much more likely to be grounded. Asking for citations makes it so that the model must make two errors every time it generates a response: the first error is the fabricated response, and the second is the bad citation. Note that the closer the citation is to the text it supports, the shorter the distance ahead the model needs to anticipate the citation, which suggests that inline citations are better mitigations for false content generation than citations at the end of the content.

Similarly, if you ask the model to extract factual statements from a paragraph, it might extract compound statements such as ‘X is doing Y AND Z’ (which can be harder to verify). This can be avoided by specifying an output structure such as (entity1, relationship, entity2).

The examples below show the use of citations and directing the model response to fit a defined structure.

| System message                                            | User                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Assistant                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You're an AI assistant that helps people find information | List down the different behaviors exhibited by the Customer Support agent in the following conversation. --- hi thank you for calling contoso pharmacy who am i speaking with today i understand your frustration mary um can you tell me what exactly you're trying to accomplish OK uh certainly happy to check that for you one moment please i see here that you were on a generic form of lipitor is that right OK uh so i see that your doctor stopped prescribing these drugs in twenty twenty one actually OK uh yeah i'd be happy to check that for you uh because sometimes there's a gap in the system and it just doesn't get reported so let me take a look here just a moment so i'm seeing here that your doctor had these drugs prescribed to you from twenty twelve through twenty twenty one OK um yeah according to the latest records uh provided by doctor's office you're now on a different drug metformin would you like us to go ahead and fill that prescription for you for pick up a bit of form in five hundred milligrams uh you're very welcome mary please let us know if there's anything else we can do for you today. --- How was the response of the Customer Support agent? Classify and list down their various behaviors and cite the reason behind the classification, using the format BEHAVIOR("reason") | 1. Polite Greeting BEHAVIOR("Agent starts the conversation with a polite greeting and asks for the caller's name") 2. Empathetic BEHAVIOR("Agent acknowledges Mary's frustration and tries to understand her concern") 3. Attentive BEHAVIOR("Agent listens to the customer's problem and asks for more details") 4. Proactive BEHAVIOR("Agent offers to check the issue for the customer") 5. Informative BEHAVIOR("Agent provides the relevant information about the customer's prescription history") 6. Thorough BEHAVIOR("Agent checks for any possible gaps in the system and provides a complete response") 7. Confirmatory BEHAVIOR("Agent confirms the current medication with the customer") 8. Solution-oriented BEHAVIOR("Agent offers to fill the new prescription for the customer") 9. Courteous BEHAVIOR("Agent thanks the customer and offers further assistance if needed") |

| System message                                             | User                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | Assistant                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You're an AI assistant that helps people find information. | Cluster the following news headlines into topic categories based on patterns seen within the text. Also mention reasoning behind how these categories were defined. Output format: \{ "TOPIC\_NAME": "", "HEADLINES": \[], "REASONING": "" } Input news headlines: 1. "From books to presentations in 10s with AR + ML" 2. "Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition" 3. "First Order Motion Model applied to animate paintings" 4. "Robinhood and other brokers literally blocking purchase of $GME, $NOK, $BB, $AMC; allow sells" 5. "United Airlines stock down over 5% premarket trading" Output:  |  \{ "ARTIFICIAL\_INTELLIGENCE": \{ "HEADLINES": \[ "From books to presentations in 10s with AR + ML", "Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition", "First Order Motion Model applied to animate paintings" ], "REASONING": "These headlines are related to artificial intelligence, machine learning, and their applications in various fields." }, "FINANCIAL\_MARKETS": \{ "HEADLINES": \[ "Robinhood and other brokers literally blocking purchase of $GME, $NOK, $BB, $AMC; allow sells", "United Airlines stock down over 5% premarket trading", "Bitcoin was nearly $20,000 a year ago today" ], "REASONING": "These headlines are related to financial markets, stocks, cryptocurrencies, and trading platforms." } }  |

## Temperature and Top\_p parameters

Changing the temperature parameter changes the output of the model. The temperature parameter can be set between 0 and 2. A higher value, for example 0.7 makes the output more random and produce more divergent responses, while a lower value, like. 0.2, makes the output more focused and concrete. A fictional story could be generated using a higher temperature. Whereas to generate a legal document it's recommended to use a much lower temperature. Top\_probability is another parameter and is similar to Temperature in that it also controls the randomness of the model response, but it does so in a different way. The general recommendation is to alter one of these two parameters at a time, not both.

## Provide grounding context

One of the most effective ways to provide reliable answers is to give the model data to draw its responses from (grounding data). If your use case relies on up-to-date, reliable information and isn't purely a creative scenario, we strongly recommend providing grounding data. In general, the closer you can get your source material to the final form of the answer you want, the less work the model needs to do, which means there's less opportunity for error. Here's an example where the system is provided a [recent blog describing the launch of GPT-4 in Azure OpenAI in Microsoft Foundry Models](https://azure.microsoft.com/blog/introducing-gpt4-in-azure-openai-service/), and asked to name some of its early customers.

| System message                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | User                                                                  | Assistant                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| You're an AI assistant that helps people find information. You'll provide answers exclusively from below text and respond in 100 words or less: "At Microsoft, we're constantly discovering new ways to unleash creativity, unlock productivity, and up level skills so that more people can benefit from using AI. This is allowing our customers to … … thereby reducing the likelihood that the model generates a harmful response.”1 Answer in 100 words or less. Use bullet lists wherever possible. | Who are some of the early customers of the Azure OpenAI GPT-4 models? | Here are some of the early customers of the Azure OpenAI GPT-4 models: - Epic Healthcare: They plan to use GPT-4 to help physicians and nurses spend less time on data entry and investigation. - Coursera: They're using Azure OpenAI with GPT-4 to create a new AI-powered learning experience on their platform. - Coca-Cola: They're using Azure OpenAI to build a knowledge hub and plan to leverage GPT-4's multimodal capabilities for marketing, advertising, public relations, and customer relations. - Other companies of all sizes are using Azure AI to improve customer experiences, summarize long-form content, help write software, and reduce risk by predicting the right tax data. |

1 Text from [Azure OpenAI GPT-4 launch blog](https://azure.microsoft.com/blog/introducing-gpt4-in-azure-openai-service/).

## Best practices

* **Be Specific**. Leave as little to interpretation as possible. Restrict the operational space.
* **Be Descriptive**. Use analogies.
* **Double Down**. Sometimes you might need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc.
* **Order Matters**. The order in which you present information to the model might impact the output. Whether you put instructions before your content (“summarize the following…”) or after (“summarize the above…”) can make a difference in output. Even the order of few-shot examples can matter. This is referred to as recency bias.
* **Give the model an “out”**. It can sometimes be helpful to give the model an alternative path if it's unable to complete the assigned task. For example, when asking a question over a piece of text you might include something like "respond with "not found" if the answer isn't present." This can help the model avoid generating false responses.

## Space efficiency

While the input size increases with each new generation of GPT models, there will continue to be scenarios that provide more data than the model can handle. GPT models break words into "tokens." While common multi-syllable words are often a single token, less common words are broken in syllables. Tokens can sometimes be counter-intuitive, as shown by the example below which demonstrates token boundaries for different date formats. In this case, spelling out the entire month is more space efficient than a fully numeric date.

![Screenshot of a string of text with highlighted colors delineating token boundaries.](https://learn.microsoft.com/azure/ai-foundry/media/prompt-engineering/space-efficiency.png)

Given this limited space, it's important to use it as efficiently as possible.

* Tables – As shown in the examples in the previous section, GPT models can understand tabular formatted data quite easily. This can be a space efficient way to include data, rather than preceding every field with name (such as with JSON).
* White Space – Consecutive whitespaces are treated as separate tokens, which can be an easy way to waste space. Spaces preceding a word, on the other hand, are typically treated as part of the same token as the word. Carefully watch your usage of whitespace and don’t use punctuation when a space alone will do.

This article provides you with background around how latency and throughput works with Azure OpenAI and how to optimize your environment to improve performance.

## Understanding throughput vs latency

There are two key concepts to think about when sizing an application: (1) System level throughput measured in tokens per minute (TPM) and (2) Per-call response times (also known as latency).

### System level throughput

This looks at the overall capacity of your deployment – how many requests per minute and total tokens that can be processed.

For a standard deployment, the quota assigned to your deployment partially determines the amount of throughput you can achieve. However, quota only determines the admission logic for calls to the deployment and isn't directly enforcing throughput. Due to per-call latency variations, you might not be able to achieve throughput as high as your quota.

In a provisioned deployment, a set amount of model processing capacity is allocated to your endpoint. The amount of throughput that you can achieve on the endpoint is a factor of the workload shape including input token amount, output amount, call rate and cache match rate. The number of concurrent calls and total tokens processed can vary based on these values.

For all deployment types, understanding system level throughput is a key component of optimizing performance. It is important to consider system level throughput for a given model, version, and workload combination as the throughput will vary across these factors.

#### Estimating system level throughput

##### Estimating TPM with Azure Monitor metrics

One approach to estimating system level throughput for a given workload is using historical token usage data. For Azure OpenAI workloads, all historical usage data can be accessed and visualized with the native monitoring capabilities offered within Azure OpenAI. Two metrics are needed to estimate system level throughput for Azure OpenAI workloads: (1) **Processed Prompt Tokens** and (2) **Generated Completion Tokens**.

When combined, the **Processed Prompt Tokens** (input TPM) and **Generated Completion Tokens** (output TPM) metrics provide an estimated view of system level throughput based on actual workload traffic. This approach does not account for benefits from prompt caching, so it will be a conservative system throughput estimate. These metrics can be analyzed using minimum, average, and maximum aggregation over 1-minute windows across a multi-week time horizon. It is recommended to analyze this data over a multi-week time horizon to ensure there are enough data points to assess. The following screenshot shows an example of the **Processed Prompt Tokens** metric visualized in Azure Monitor, which is available directly through the Azure portal.

![Screenshot of Azure Monitor graph showcasing the Processed Prompt Tokens metric split by model and version.](https://learn.microsoft.com/azure/ai-foundry/media/latency/processed-prompt-token-graph.png)

##### Estimating TPM from request data

A second approach to estimated system level throughput involves collecting token usage information from API request data. This method provides a more granular approach to understanding workload shape per request. Combining per request token usage information with request volume, measured in requests per minute (RPM), provides an estimate for system level throughput. It is important to note that any assumptions made for consistency of token usage information across requests and request volume will impact the system throughput estimate. The token usage output data can be found in the API response details for a given Azure OpenAI in Microsoft Foundry Models chat completions request.

```json
{
  "body": {
    "id": "chatcmpl-7R1nGnsXO8n4oi9UPz2f3UHdgAYMn",
    "created": 1686676106,
    "choices": [...],
    "usage": {
      "completion_tokens": 557,
      "prompt_tokens": 33,
      "total_tokens": 590
    }
  }
}
```

Assuming all requests for a given workload are uniform, the prompt tokens and completion tokens from the API response data can each be multiplied by the estimated RPM to identify the input and output TPM for the given workload.

##### How to use system level throughput estimates

Once system level throughput has been estimated for a given workload, these estimates can be used to size Standard and Provisioned deployments. For Standard deployments, the input and output TPM values can be combined to estimate the total TPM to be assigned to a given deployment. For Provisioned deployments, the request token usage data or input and output TPM values can be used to estimate the number of PTUs required to support a given workload with the deployment capacity calculator experience.

Here are a few examples for the GPT-4o mini model:

| Prompt Size (tokens) | Generation size (tokens) | Requests per minute | Input TPM | Output TPM | Total TPM | PTUs required |
| -------------------- | ------------------------ | ------------------- | --------- | ---------- | --------- | ------------- |
| 800                  | 150                      | 30                  | 24,000    | 4,500      | 28,500    | 15            |
| 5,000                | 50                       | 1,000               | 5,000,000 | 50,000     | 5,050,000 | 140           |
| 1,000                | 300                      | 500                 | 500,000   | 150,000    | 650,000   | 30            |

The number of PTUs scales roughly linearly with call rate when the workload distribution remains constant.

### Latency: The per-call response times

The high level definition of latency in this context is the amount of time it takes to get a response back from the model. For completion and chat completion requests, latency is largely dependent on model type, the number of tokens in the prompt and the number of tokens generated. In general, each prompt token adds little time compared to each incremental token generated.

Estimating your expected per-call latency can be challenging with these models. Latency of a completion request can vary based on four primary factors: (1) the model, (2) the number of tokens in the prompt, (3) the number of tokens generated, and (4) the overall load on the deployment & system. One and three are often the main contributors to the total time. The next section goes into more details on the anatomy of a large language model inference call.

## Improve performance

There are several factors that you can control to improve per-call latency of your application.

### Model selection

Latency varies based on what model you're using. For an identical request, expect that different models have different latencies for the chat completions call. If your use case requires the lowest latency models with the fastest response times, we recommend the latest [GPT-4o mini model](../../foundry-models/concepts/models-sold-directly-by-azure).

### Generation size and Max tokens

When you send a completion request to the Azure OpenAI endpoint, your input text is converted to tokens that are then sent to your deployed model. The model receives the input tokens and then begins generating a response. It's an iterative sequential process, one token at a time. Another way to think of it is like a for loop with `n tokens = n iterations`. For most models, generating the response is the slowest step in the process.

At the time of the request, the requested generation size (`max_tokens` parameter) is used as an initial estimate of the generation size. The compute-time for generating the full size is reserved by the model as the request is processed. Once the generation is completed, the remaining quota is released. Ways to reduce the number of tokens:

* Set the `max_tokens` parameter on each call as small as possible.
* Include stop sequences to prevent generating extra content.
* Generate fewer responses: The best\_of & n parameters can greatly increase latency because they generate multiple outputs. For the fastest response, either don't specify these values or set them to 1.

In summary, reducing the number of tokens generated per request reduces the latency of each request.

<Callout type="note">
  `max_tokens` only changes the length of a response and in some cases might truncate it. The parameter doesn't change the quality of the response.
</Callout>

### Streaming

Setting `stream: true` in a request makes the service return tokens as soon as they're available, instead of waiting for the full sequence of tokens to be generated. It doesn't change the time to get all the tokens, but it reduces the time for first response. This approach provides a better user experience since end-users can read the response as it is generated.

Streaming is also valuable with large calls that take a long time to process. Many clients and intermediary layers have timeouts on individual calls. Long generation calls might be canceled due to client-side time outs. By streaming the data back, you can ensure incremental data is received.

**Examples of when to use streaming**:

Chat bots and conversational interfaces.

Streaming impacts perceived latency. With streaming enabled you receive tokens back in chunks as soon as they're available. For end-users, this approach often feels like the model is responding faster even though the overall time to complete the request remains the same.

**Examples of when streaming is less important**:

Sentiment analysis, language translation, content generation.

There are many use cases where you're performing some bulk task where you only care about the finished result, not the real-time response. If streaming is disabled, you won't receive any tokens until the model has finished the entire response.

### Content filtering

Azure OpenAI includes a [content filtering system](content-filters) that works alongside the core models. This system works by running both the prompt and completion through an ensemble of classification models aimed at detecting and preventing the output of harmful content.

The content filtering system detects and takes action on specific categories of potentially harmful content in both input prompts and output completions.

The addition of content filtering comes with an increase in safety, but also latency. There are many applications where this tradeoff in performance is necessary, however there are certain lower risk use cases where disabling the content filters to improve performance might be worth exploring.

Learn more about requesting modifications to the default, [content filtering policies](content-filters).

### Separation of workloads

Mixing different workloads on the same endpoint can negatively affect latency. This is because (1) they're batched together during inference and short calls can be waiting for longer completions and (2) mixing the calls can reduce your cache hit rate as they're both competing for the same space. When possible, it's recommended to have separate deployments for each workload.

### Prompt Size

While prompt size has smaller influence on latency than the generation size it affects the overall time, especially when the size grows large.

### Batching

If you're sending multiple requests to the same endpoint, you can batch the requests into a single call. This reduces the number of requests you need to make and depending on the scenario it might improve overall response time. We recommend testing this method to see if it helps.

## How to measure your throughput

We recommend measuring your overall throughput on a deployment with two measures:

* Calls per minute: The number of API inference calls you're making per minute. This can be measured in Azure-monitor using the Azure OpenAI Requests metric and splitting by the ModelDeploymentName
* Total Tokens per minute: The total number of tokens being processed per minute by your deployment. This includes prompt & generated tokens. This is often further split into measuring both for a deeper understanding of deployment performance. This can be measured in Azure-Monitor using the Processed Inference tokens metric.

You can learn more about [Monitoring Azure OpenAI](monitor-openai).

## How to measure per-call latency

The time it takes for each call depends on how long it takes to read the model, generate the output, and apply content filters. The way you measure the time will vary if you're using streaming or not. We suggest a different set of measures for each case.

You can learn more about [Monitoring Azure OpenAI](monitor-openai).

### Non-Streaming:

* End-to-end Request Time: The total time taken to generate the entire response for non-streaming requests, as measured by the API gateway. This number increases as prompt and generation size increases.

### Streaming:

* Time to Response: Recommended latency (responsiveness) measure for streaming requests. Applies to PTU and PTU-managed deployments. Calculated as time taken for the first response to appear after a user sends a prompt, as measured by the API gateway. This number increases as the prompt size increases and/or hit size reduces.
* Average Token Generation Rate Time from the first token to the last token, divided by the number of generated tokens, as measured by the API gateway. This measures the speed of response generation and increases as the system load increases. Recommended latency measure for streaming requests.

## Summary

* **Model latency**: If model latency is important to you, we recommend trying out the [GPT-4o mini model](../../foundry-models/concepts/models-sold-directly-by-azure).

* **Lower max tokens**: OpenAI has found that even in cases where the total number of tokens generated is similar the request with the higher value set for the max token parameter will have more latency.

* **Lower total tokens generated**: The fewer tokens generated the faster the overall response will be. Remember this is like having a for loop with `n tokens = n iterations`. Lower the number of tokens generated and overall response time will improve accordingly.

* **Streaming**: Enabling streaming can be useful in managing user expectations in certain situations by allowing the user to see the model response as it is being generated rather than having to wait until the last token is ready.

* **Content Filtering** improves safety, but it also impacts latency. Evaluate if any of your workloads would benefit from [modified content filtering policies](content-filters).

This guide offers some potential strategies for planning how to set up and manage red teaming for responsible AI (RAI) risks throughout the large language model (LLM) product life cycle.

## What is red teaming?

The term *red teaming* has historically described systematic adversarial attacks for testing security vulnerabilities. With the rise of LLMs, the term has extended beyond traditional cybersecurity and evolved in common usage to describe many kinds of probing, testing, and attacking of AI systems. With LLMs, both benign and adversarial usage can produce potentially harmful outputs, which can take many forms, including harmful content such as hate speech, incitement or glorification of violence, or sexual content.

## Why is RAI red teaming an important practice?

Red teaming is a best practice in the responsible development of systems and features using LLMs. While not a replacement for systematic measurement and mitigation work, red teamers help to uncover and identify harms and, in turn, enable measurement strategies to validate the effectiveness of mitigations.

While Microsoft has conducted red teaming exercises and implemented safety systems (including [content filters](../how-to/content-filters) and other [mitigation strategies](prompt-engineering)) for its Azure OpenAI in Microsoft Foundry Models (see this [Overview of responsible AI practices](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/overview)), the context of each LLM application will be unique and you also should conduct red teaming to:

* Test the LLM base model and determine whether there are gaps in the existing safety systems, given the context of your application.

* Identify and mitigate shortcomings in the existing default filters or mitigation strategies.

* Provide feedback on failures in order to make improvements.

* Note that red teaming is not a replacement for systematic measurement. A best practice is to complete an initial round of manual red teaming before conducting systematic measurements and implementing mitigations. As highlighted above, the goal of RAI red teaming is to identify harms, understand the risk surface, and develop the list of harms that can inform what needs to be measured and mitigated.

Here is how you can get started and plan your process of red teaming LLMs. Advance planning is critical to a productive red teaming exercise.

## Before testing

### Plan: Who will do the testing

**Assemble a diverse group of red teamers**

Determine the ideal composition of red teamers in terms of people’s experience, demographics, and expertise across disciplines (for example, experts in AI, social sciences, security) for your product’s domain. For example, if you’re designing a chatbot to help health care providers, medical experts can help identify risks in that domain.

**Recruit red teamers with both benign and adversarial mindsets**

Having red teamers with an adversarial mindset and security-testing experience is essential for understanding security risks, but red teamers who are ordinary users of your application system and haven’t been involved in its development can bring valuable perspectives on harms that regular users might encounter.

**Assign red teamers to harms and/or product features**

* Assign RAI red teamers with specific expertise to probe for specific types of harms (for example, security subject matter experts can probe for jailbreaks, meta prompt extraction, and content related to cyberattacks).

* For multiple rounds of testing, decide whether to switch red teamer assignments in each round to get diverse perspectives on each harm and maintain creativity. If switching assignments, allow time for red teamers to get up to speed on the instructions for their newly assigned harm.

* In later stages, when the application and its UI are developed, you might want to assign red teamers to specific parts of the application (i.e., features) to ensure coverage of the entire application.

* Consider how much time and effort each red teamer should dedicate (for example, those testing for benign scenarios might need less time than those testing for adversarial scenarios).

It can be helpful to provide red teamers with:

* Clear instructions that could include:
  * An introduction describing the purpose and goal of the given round of red teaming; the product and features that will be tested and how to access them; what kinds of issues to test for; red teamers’ focus areas, if the testing is more targeted; how much time and effort each red teamer should spend on testing; how to record results; and who to contact with questions.
* A file or location for recording their examples and findings, including information such as:
  * The date an example was surfaced; a unique identifier for the input/output pair if available, for reproducibility purposes; the input prompt; a description or screenshot of the output.

### Plan: What to test

Because an application is developed using a base model, you might need to test at several different layers:

* The LLM base model with its safety system in place to identify any gaps that may need to be addressed in the context of your application system. (Testing is usually done through an API endpoint.)

* Your application. (Testing is best done through a UI.)

* Both the LLM base model and your application, before and after mitigations are in place.

The following recommendations help you choose what to test at various points during red teaming:

* You can begin by testing the base model to understand the risk surface, identify harms, and guide the development of RAI mitigations for your product.

* Test versions of your product iteratively with and without RAI mitigations in place to assess the effectiveness of RAI mitigations. (Note, manual red teaming might not be sufficient assessment—use systematic measurements as well, but only after completing an initial round of manual red teaming.)

* Conduct testing of application(s) on the production UI as much as possible because this most closely resembles real-world usage.

When reporting results, make clear which endpoints were used for testing. When testing was done in an endpoint other than product, consider testing again on the production endpoint or UI in future rounds.

### Plan: How to test

**Conduct open-ended testing to uncover a wide range of harms.**

The benefit of RAI red teamers exploring and documenting any problematic content (rather than asking them to find examples of specific harms) enables them to creatively explore a wide range of issues, uncovering blind spots in your understanding of the risk surface.

**Create a list of harms from the open-ended testing.**

* Consider creating a list of harms, with definitions and examples of the harms.
* Provide this list as a guideline to red teamers in later rounds of testing.

**Conduct guided red teaming and iterate: Continue probing for harms in the list; identify new harms that surface.**

Use a list of harms if available and continue testing for known harms and the effectiveness of their mitigations. In the process, you will likely identify new harms. Integrate these into the list and be open to shifting measurement and mitigation priorities to address the newly identified harms.

Plan which harms to prioritize for iterative testing. Several factors can inform your prioritization, including, but not limited to, the severity of the harms and the context in which they are more likely to surface.

### Plan: How to record data

**Decide what data you need to collect and what data is optional.**

* Decide what data the red teamers will need to record (for example, the input they used; the output of the system; a unique ID, if available, to reproduce the example in the future; and other notes.)

* Be strategic with what data you are collecting to avoid overwhelming red teamers, while not missing out on critical information.

**Create a structure for data collection**

A shared Excel spreadsheet is often the simplest method for collecting red teaming data. A benefit of this shared file is that red teamers can review each other’s examples to gain creative ideas for their own testing and avoid duplication of data.

## During testing

**Plan to be on active standby while red teaming is ongoing**

* Be prepared to assist red teamers with instructions and access issues.
* Monitor progress on the spreadsheet and send timely reminders to red teamers.

## After each round of testing

**Report data**

Share a short report on a regular interval with key stakeholders that:

1. Lists the top identified issues.

2. Provides a link to the raw data.

3. Previews the testing plan for the upcoming rounds.

4. Acknowledges red teamers.

5. Provides any other relevant information.

**Differentiate between identification and measurement**

In the report, be sure to clarify that the role of RAI red teaming is to expose and raise understanding of risk surface and is not a replacement for systematic measurement and rigorous mitigation work. It is important that people do not interpret specific examples as a metric for the pervasiveness of that harm.

Additionally, if the report contains problematic content and examples, consider including a content warning.

The guidance in this document is not intended to be, and should not be construed as providing, legal advice. The jurisdiction in which you're operating may have various regulatory or legal requirements that apply to your AI system. Be aware that not all of these recommendations are appropriate for every scenario and, conversely, these recommendations may be insufficient for some scenarios.

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

This guide outlines key decisions for rolling out Microsoft Foundry, including environment setup, data isolation, integration with other Azure services, capacity management, and monitoring. Use this guide as a starting point and adapt it to your needs. For implementation details, see the linked articles for further guidance.

## Example organization

Contoso is a global enterprise exploring GenAI adoption across five business groups, each with distinct needs and technical maturity.

To accelerate adoption while maintaining oversight, Contoso Enterprise IT aims to enable a model with common shared resources including networking and centralized data management, while enabling self-serve access to Foundry for each team within a governed, secure environment to manage their use cases.

## Rollout considerations

The Foundry resource defines the scope for configuring, securing, and monitoring your team's environment. It's available in the Foundry portal and through Azure APIs. Projects are like folders to organize your work within this resource context. Projects also control access and permissions to Foundry developer APIs and tools.

![Screenshot of a diagram showing Foundry resource.](https://learn.microsoft.com/azure/ai-foundry/media/planning/foundry-resource.png)

To ensure consistency, scalability, and governance across teams, consider the following environment setup practices when rolling out Foundry:

* **Establish distinct environments for development, testing, and production.** Use separate resource groups or subscriptions, and Foundry resources to isolate workflows, manage access, and support experimentation with controlled releases.

* **Create a separate Foundry resource for each business group.** Align deployments with logical boundaries such as data domains or business functions to ensure autonomy, governance, and cost tracking.

* **Associate projects with use cases.** Foundry projects are designed to represent specific use cases. They're containers to organize components such as agents or files for an application. While they inherit security settings from their parent resource, they can also implement their own access controls, data integration, and other governance controls.

## Securing the Foundry environment

Foundry is built on the Azure platform, so you can customize security controls to meet your organization's needs. Key configuration areas include:

* **Identity**: Use Microsoft Entra ID to manage user and service access. Foundry supports managed identities to allow secure, passwordless authentication to other Azure services. You can assign managed identities at the **Foundry resource level** and optionally at the **project level** for fine-grained control. [Learn more about managed identities.](https://learn.microsoft.com/en-us/security/benchmark/azure/baselines/azure-ai-foundry-security-baseline)

* **Networking**: Deploy Foundry into a Virtual Network to isolate traffic and control access by using Network Security Groups (NSGs). [Learn more about networking security.](https://learn.microsoft.com/en-us/security/benchmark/azure/baselines/azure-ai-foundry-security-baseline)

* **Customer-Managed Keys (CMK)**: Azure supports CMK for encrypting data at rest. Foundry supports CMK optionally for customers with strict compliance needs. [Learn more about CMK](https://learn.microsoft.com/en-us/security/benchmark/azure/baselines/azure-ai-foundry-security-baseline).

* **Authentication and Authorization**: Foundry supports both **API key-based access** for simple integration and **Azure RBAC** for fine-grained control. Azure enforces a clear separation between the **control plane** (resource management) and the **data plane** (model and data access). Start with built-in roles, and define custom roles as needed. [Learn more about authentication.](https://learn.microsoft.com/en-us/security/benchmark/azure/baselines/azure-ai-foundry-security-baseline)

* **Templates**: Use ARM templates or Bicep to automate secure deployments. Explore the [sample templates](https://learn.microsoft.com/en-us/security/benchmark/azure/baselines/azure-ai-foundry-security-baseline).

* **Storage resource**: You might choose to use built-in storage capabilities in Foundry or use your own storage resources. For the Agent Service, threads and messages can optionally be stored in [resources managed by you](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/use-your-own-resources).

## Example: Contoso's security approach

Contoso secures its Foundry deployments by using private networking with Enterprise IT managing a central hub network. Each business group connects via a spoke virtual network. They use built-in Role Based Access Control (RBAC) to separate access:

* **Admins** manage deployments, connections, and shared resources
* **Project Managers** oversee specific projects
* **Users** interact with GenAI tools

For most use cases, Contoso relies on Microsoft-managed encryption by default and does **not use Customer-Managed Keys**.

## Plan user access

Effective access management is foundational to a secure and scalable Foundry setup.

* **Define required access roles and responsibilities**

  * Identify which user groups require access to various aspects of the Foundry environment.

  * Assign built-in or custom Azure RBAC roles based on responsibilities such as:

    * Account owner: Manage top-level configurations such as security and shared resource connections.
    * Project Managers: Create and manage Foundry projects and their contributors.
    * Project Users: contribute to existing projects.

* **Determine access scope**

  * Choose the appropriate scope for access assignments:

    * Subscription level: broadest access, typically suitable for central IT or platform teams or smaller organizations.
    * Resource group level: Useful for grouping related resources with shared access policies. For example, an Azure Function that follows the same application lifecycle as your Foundry environment.
    * Resource or project level: Ideal for fine-grained control, especially when dealing with sensitive data or enabling self-service.

* **Align identity strategy**

  * For data sources and tools integrated with Foundry, determine whether users should authenticate by using:

    * Managed identities or API key: suitable for automated services and shared access across users.
    * User identities: Preferred when user-level accountability or auditability is required.

  * Use Microsoft Entra ID groups to simplify access management and ensure consistency across environments.

## Establish connectivity with other Azure services

Foundry supports **connections**, which are reusable configurations that enable access to application components on Azure and non-Azure services. These connections also act as **identity brokers**, allowing Foundry to authenticate to external systems by using managed identities or service principals on behalf of project users.

Create connections at the **Foundry resource level** for shared services like Azure Storage or Key Vault. Scope connections to a **specific project** for sensitive or project-specific integrations. This flexibility allows teams to balance reuse and isolation based on their needs. [Learn more about connections in Foundry](../how-to/connections-add).

Configure connection authentication to use either shared access tokens, such as Microsoft Entra ID managed identities or API keys, for simplified management and onboarding, or user tokens via Entra ID passthrough, which offer greater control when accessing sensitive data sources.

![Screenshot of a diagram showing Foundry project connectivity and integration with other Azure services.](https://learn.microsoft.com/azure/ai-foundry/media/planning/connectivity.png)

### Example: Contoso's connectivity strategy

* Contoso creates a Foundry resource for every business group, ensuring projects with similar data needs share the same connected resources.
* By default, connected resources use shared authentication tokens and are shared across all projects.
* Projects that use sensitive data workloads connect to data sources with project-scoped connections and Microsoft Entra ID passthrough authentication.

## Governance

Effective governance in Foundry ensures secure, compliant, and cost-efficient operations across business groups.

* **Model Access Control with Azure Policy** Azure Policy enforces rules across Azure resources. In Foundry, use policies to restrict which models or model families specific business groups can access. *Example*: Contoso’s **Finance & Risk** group is restricted from using preview or noncompliant models by applying a policy at their business group’s subscription level.
* **Cost Management by Business Group** By deploying Foundry per business group, Contoso can track and manage costs independently. Use Microsoft Cost Management to view detailed usage and spending per Foundry deployment or project.
* **Usage Tracking with Azure Monitor** Azure Monitor provides metrics and dashboards to track usage patterns, performance, and health of Foundry resources.
* **Verbose Logging with Azure Log Analytics** Azure Log Analytics enables deep inspection of logs for operational insights. For example, log request usage, token usage, and latency to support auditing and optimization.

## Configure and optimize model deployments

When deploying models in Foundry, teams can choose between standard and provisioned [deployment types](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/deployment-types). Standard deployments are ideal for development and experimentation, offering flexibility and ease of setup. Provisioned deployments are recommended for production scenarios where predictable performance, cost control, and model version pinning are required.

To support cross-region scenarios and let you access existing model deployments, Foundry allows [connections](../how-to/connections-add) to model deployments hosted in other Foundry or Azure OpenAI instances. By using connections, teams can centralize deployments for experimentation while still enabling access from distributed projects. For production workloads, consider having use cases manage their own deployments to ensure tighter control over model lifecycle, versioning, and rollback strategies.

To prevent overuse and ensure fair resource allocation, you can apply [Tokens Per Minute (TPM) limits at the deployment level](../openai/concepts/provisioned-throughput?tabs=global-ptum). TPM limits help control consumption, protect against accidental spikes, and align usage with project budgets or quotas. Consider setting conservative limits for shared deployments and higher thresholds for critical production services.

## Learn more

* Secure the Foundry environment

  * Authentication and RBAC: [Role-based access control in Foundry](rbac-foundry)
  * Networking: [Use a virtual network with Foundry](../how-to/configure-private-link)
  * Identity and managed identity: [Configure managed identity in Foundry](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity)
  * Customer-managed keys (CMK): [Customer-managed keys in Foundry](encryption-keys-portal)
  * Example infrastructure: [templates repository with sample infrastructure templates](https://github.com/microsoft-foundry/foundry-samples/tree/main/samples)
  * [Recover or purge deleted Foundry resources](https://learn.microsoft.com/en-us/azure/ai-services/recover-purge-resources?toc=/azure/ai-foundry/toc.json\&bc=/azure/ai-foundry/breadcrumb/toc.json)

* Establish connectivity with other Azure services

  * Overview of connections: [Add a new connection in Foundry](../how-to/connections-add)

* Governance

  * Model access control with Azure Policy: [Control model deployment with built-in policies](../how-to/built-in-policy-model-deployment)
  * Cost management: [Plan and manage costs for Foundry](../how-to/costs-plan-manage)
  * Azure Monitor for usage tracking: [Monitor your Generative AI applications](../how-to/monitor-applications)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this article, you deploy the infrastructure needed to create agents with Foundry Agent Service. After completing this setup, you can create and configure agents using either the SDK of your choice or the Foundry portal.

Creating your first agent is a two-step process:

1. Set up your agent environment (this article).
2. Create and configure your agent.

### Required permissions

| Action                                                                                                           | Required Role                           |
| ---------------------------------------------------------------------------------------------------------------- | --------------------------------------- |
| Create an account and project                                                                                    | Azure AI Account Owner                  |
| [standard setup](#choose-your-setup) Only: Assign RBAC for required resources (Cosmos DB, Search, Storage, etc.) | Role Based Access Control Administrator |
| Create and edit agents                                                                                           | Azure AI User                           |

## Set up your agent environment

To get started, you need a Microsoft Foundry resource and a Foundry project.\
Agents are created within a specific project, and each project acts as an isolated workspace. This means:

* All agents in the same project share access to the same file storage, thread storage (conversation history), and search indexes.
* Data is isolated between projects. Agents in one project cannot access resources from another. Projects are currently the unit of sharing and isolation in Foundry. See the [what is AI foundry](../what-is-foundry) article for more information on Foundry projects.

### Prerequisites

* An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).

* Ensure that the individual creating the account and project has the **Azure AI Account Owner** role at the subscription scope

* If configuring a [standard setup](#choose-your-setup), the same individual must also have permissions to assign roles to required resources (Cosmos DB, Azure AI Search, Azure Blob Storage). For more information on RBAC roles, specific to Agent Service, see [Agent Service RBAC roles](../concepts/rbac-foundry).

  * The built-in role needed is **Role Based Access Administrator**.
  * Alternatively, having the **Owner** role at the subscription level also satisfies this requirement.
  * The key permission needed is: `Microsoft.Authorization/roleAssignments/write`

### Choose your setup

Agent Service offers three environment configuration modes to suit different needs:

* **Basic Setup**:

  This setup is compatible with OpenAI Assistants and manages agent states using the platform's built-in storage. It includes the same tools and capabilities as the Assistants API, with added support for non-OpenAI models and tools such as Azure AI Search, and Bing.

* **Standard Setup**:

  Includes everything in the basic setup and fine-grained control over your data by allowing you to use your own Azure resources. All customer data—including files, threads, and vector stores—are stored in your own Azure resources, giving you full ownership and control.

* **Standard Setup with Bring Your Own (BYO) Virtual Network**:

  Includes everything in the Standard Setup, with the added ability to operate entirely within your own virtual network. This setup supports Bring Your Own Virtual Network (BYO virtual network), allowing for strict control over data movement and helping prevent data exfiltration by keeping traffic confined to your network environment.

### Compare setup options

<Callout type="note">
  Private Network Isolation in the table below refers to Secured Agent outbound communication. Basic setup doesn't apply, and you can use Private Network Isolation for your Agents with Standard Setup only.

  Inbound secured communication can be applied to all of setups below, by adding a private endpoint and disabling the inbound public access for your Foundry Account.
</Callout>

| Use Cases                                                                          | Basic Setup | Standard Setup with Public Networking | Standard Setup with Private Networking |
| ---------------------------------------------------------------------------------- | ----------- | ------------------------------------- | -------------------------------------- |
| Get started quickly without managing resources                                     | ✅           |                                       |                                        |
| All conversation history, file, and vector stores are stored in your own resources |             | ✅                                     | ✅                                      |
| Support for Customer Managed Keys (CMK)                                            |             | ✅                                     | ✅                                      |
| Private Network Isolation (Bring your own virtual network)                         |             |                                       | ✅                                      |

### Deployment options

To customize these templates, see [use your own resources](how-to/use-your-own-resources).

If you want support for Private Network Isolation, see [network-secured setup](how-to/virtual-networks) for more information on how to bring your own virtual network.

| Description and Autodeploy                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | Diagram (click to zoom in)                                                                                                                                |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Deploy a basic agent setup that uses **Managed Identity** for authentication. An account and project are created. A GPT-4.1 model is deployed. A Microsoft-managed Key Vault is used by default. ![Deploy To Azure](https://aka.ms/deploytoazurebutton)                                                                                                                                                                                                                                                                       | ![An architecture diagram for basic agent setup.](https://learn.microsoft.com/azure/ai-foundry/agents/media/quickstart/basic-setup-resources-foundry.png) |
| Deploy a standard agent setup that uses **Managed Identity** for authentication. An account and project are created. A GPT-4.1 model is deployed. Azure resources for storing customer data—**Azure Storage**, **Azure Cosmos DB**, and **Azure AI Search**—are automatically created if existing resources aren't provided. These resources are connected to your project to store files, threads, and vector data. A Microsoft-managed Key Vault is used by default. ![Deploy To Azure](https://aka.ms/deploytoazurebutton) | ![An architecture diagram for standard agent setup.](https://learn.microsoft.com/azure/ai-foundry/agents/media/quickstart/standard-agent-setup.png)       |

### \[Optional] Model selection in autodeploy template

<Callout type="important">
  **Don't change the modelFormat parameter.**

  The templates only support deployment of Azure OpenAI models. See which Azure OpenAI models are supported in the [model support](concepts/model-region-support) article.
</Callout>

You can customize the model used by your agent by editing the model parameters in the autodeploy template. To deploy a different model, you need to update at least the `modelName` and `modelVersion` parameters.

By default, the deployment template is configured with the following values:

| Model Parameter | Default Value             |
| --------------- | ------------------------- |
| modelName       | gpt-4.1                   |
| modelFormat     | OpenAI (for Azure OpenAI) |
| modelVersion    | 2025-04-14                |
| modelSkuName    | GlobalStandard            |
| modelLocation   | eastus                    |

### Verify your deployment

After deployment completes (typically 5-10 minutes), verify that your resources were created successfully:

1. Go to the [Azure portal](https://portal.azure.com).

2. Search for your resource group name.

3. Confirm that the following resources exist:

   * **Basic setup**: Foundry account, project, and model deployment.
   * **Standard setup**: All basic resources plus Azure Storage account, Azure Cosmos DB account, and Azure AI Search service.

<Callout type="tip">
  If the deployment fails, check the **Deployments** section in your resource group for error details. Common issues include insufficient quota for the model or missing permissions.
</Callout>

### Troubleshooting

| Issue                                      | Cause                                                 | Solution                                                                                         |
| ------------------------------------------ | ----------------------------------------------------- | ------------------------------------------------------------------------------------------------ |
| Deployment fails with quota error          | Insufficient quota for GPT-4.1 in the selected region | Request a quota increase or select a different region                                            |
| Permission denied during deployment        | Missing **Role Based Access Administrator** role      | Ask your subscription owner to grant you the required role                                       |
| Resources created but agent creation fails | Project not properly connected to resources           | Verify the connection in the Foundry portal under **Project settings** > **Connected resources** |
| Model not available                        | Model not deployed in your region                     | Check [model region support](concepts/model-region-support) and select an available region       |

### What's next?

* [Create your first agent](../quickstarts/get-started-code)

- Explore more:

  * [Use your existing resources](how-to/use-your-own-resources)
  * [Network secured agent setup](how-to/virtual-networks)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

In this article, you learn how to add a new connection in [Microsoft Foundry portal](https://ai.azure.com/?cid=learnDocs).

Connections are a way to authenticate and consume both Microsoft and other resources within your Foundry projects. They're required for scenarios such as building Standard Agents or building with Agent knowledge tools. Certain connections can be created in the Foundry UI while others require deployment through code in Bicep template. See our [foundry-samples on GitHub](https://github.com/azure-ai-foundry/foundry-samples/tree/main/infrastructure/infrastructure-setup-bicep/01-connections). Read the table descriptions below to learn more.

## Prerequisites

* If you don't have one, [create a project](create-projects).

## Connection types

| Service connection type           | Preview | Description                                                                                                                                                                                                                                                                                                                                                                                                                               |
| --------------------------------- | :-----: | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Azure AI Search                   |         | Azure AI Search is an Azure resource that supports information retrieval over your vector and textual data stored in search indexes. Required for Standard Agent deployment.                                                                                                                                                                                                                                                              |
| Azure Storage                     |         | Azure Storage is a cloud storage solution for storing unstructured data like documents, images, videos, and application installers. Required for Standard Agent deployment.                                                                                                                                                                                                                                                               |
| Azure Cosmos DB                   |    ✅    | Azure Cosmos DB is a globally distributed, multi-model database service that offers low latency, high availability, and scalability across multiple geographical regions. Required for Standard Agent deployment. Connection creation only supported through code.                                                                                                                                                                        |
| Azure OpenAI                      |         | Azure OpenAI is a service that provides access to OpenAI's models including the GPT-5, GPT-4o, DALLE-3, and Embeddings model series with the security and enterprise capabilities of Azure.                                                                                                                                                                                                                                               |
| Application Insights              |         | Azure Application Insights is a service that enables developers to automatically detect performance anomalies, diagnose issues, and gain deep insights into application usage and behavior.                                                                                                                                                                                                                                               |
| Azure Key Vault                   |         | Azure service for securely storing and accessing secrets. (See limitations below)                                                                                                                                                                                                                                                                                                                                                         |
| Foundry                           |         | Connect to other Foundry resources.                                                                                                                                                                                                                                                                                                                                                                                                       |
| OpenAI                            |         | Connect to your OpenAI models.                                                                                                                                                                                                                                                                                                                                                                                                            |
| Serp                              |         | Serp connects to Search Engine Results Pages (SERP) for real-time data access. Supports scenarios that need the latest search results.                                                                                                                                                                                                                                                                                                    |
| API key                           |         | API Key connections handle authentication to your specified target on an individual basis.                                                                                                                                                                                                                                                                                                                                                |
| Custom key                        |         | Custom connections allow you to securely store and access keys while storing related properties, such as targets and versions. Custom connections are useful when you have many targets or cases where you wouldn't need a credential to access. LangChain scenarios are a good example where you would use custom service connections. Custom connections don't manage authentication, so you have to manage authentication on your own. |
| Grounding with Bing Search        |         | Connects to Bing Search to provide real-time web grounding for queries. Enables AI agents to reference current web data in responses.                                                                                                                                                                                                                                                                                                     |
| Serverless Model                  |    ✅    | Serverless Model connections allow you to serverless API deployment. Connection creation only supported through code.                                                                                                                                                                                                                                                                                                                     |
| Azure Databricks                  |    ✅    | Azure Databricks connector allows you to connect your Foundry Agents to Azure Databricks to access workflows and Genie Spaces during runtime. Connection creation only supported through code.                                                                                                                                                                                                                                            |
| Sharepoint                        |    ✅    | Sharepoint is a Microsoft platform for document storage and collaboration. It allows agents to access and manage organizational documents. Connection creation only supported through code.                                                                                                                                                                                                                                               |
| Microsoft Fabric                  |    ✅    | AI skills allow you to create your own conversational Q\&A systems on Fabric using generative AI. Connection creation only supported through code.                                                                                                                                                                                                                                                                                        |
| Grounding with Bing Custom Search |    ✅    | Integrates with a custom Bing search instance for tailored web grounding. Connection creation only supported through code.                                                                                                                                                                                                                                                                                                                |
| Azure APIM                        |    ✅    | APIM allows for governance of AI Models called in the Foundry Agent service. Connection creation only supported through code.                                                                                                                                                                                                                                                                                                             |
| Model Gateway                     |    ✅    | Model Gateway allows for governance of AI Models called in the Foundry Agent service. Connection creation only supported through code.                                                                                                                                                                                                                                                                                                    |

### Azure Key Vault limitations

Foundry stores connections details in a managed Azure Key Vault if no Key Vault connection is created. Users that prefer to manage their secrets themselves can bring their own Azure Key Vault via a connection. All Foundry projects use a managed Azure Key Vault (not shown in your subscription). If you bring your own Azure Key Vault, note:

* Only one Azure Key Vault connection per Foundry resource at a time.
* You can delete an Azure Key Vault connection only if there are no other existing connections on the Foundry resource or project level.
* Secret migration isn't supported; recreate connections after attaching the Key Vault.
* Deleting the underlying Azure Key Vault breaks the Foundry resource (connections depend on stored secrets).
* Deleting secrets in your BYO Key Vault may break connections to other services.

### Azure Databricks connection (preview) limitations

It supports three connection types - **Jobs**, **Genie**, and **Other**. You can pick the Job or Genie space you want associated with this connection while setting up the connection in the Foundry UI. You can also use the Other connection type and allow your agent to access workspace operations in Azure Databricks. Authentication is handled through Microsoft Entra ID for users or service principals. For examples of using this connector, see [Jobs](https://github.com/Azure-Samples/AI-Foundry-Connections/blob/main/src/samples/python/sample_agent_adb_job.py) and [Genie](https://github.com/Azure-Samples/AI-Foundry-Connections/blob/main/src/samples/python/sample_agent_adb_genie.py). Note: Usage of this connection is available only via the Foundry SDK in code and is integrated into agents as a FunctionTool (please see the samples above for details). Usage of this connection in Foundry Playground is currently not supported.

## Create a new connection

Use the portal or a Bicep template to add a connection.

<Tabs>
  <Tab title="Foundry portal">
    Follow these steps to create a new connection that's available for the current project.

    1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.

       ![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/new-foundry.png)

    2. Select **Operate** in the upper-right navigation.

    3. Select **Admin** in the left pane.

    4. Select your project name in the **Manage all projects** list.

    5. Select **Add connection** in the upper-right corner.

    6. Select the service you want to connect to from the list of available external resources. For example, select **Azure AI Search**.

    7. Browse for and select your Azure AI Search service from the list of available services and then select the type of **Authentication** to use for the resource. Select **Add connection**.

       <Callout type="tip">
         Different connection types support different authentication methods. Using Microsoft Entra ID might require specific Azure role-based access permissions for your developers. For more information, visit [Role-based access control](../concepts/rbac-foundry).
       </Callout>
  </Tab>

  <Tab title="Bicep">
    See [Connection templates](https://github.com/azure-ai-foundry/foundry-samples/tree/main/infrastructure/infrastructure-setup-bicep/01-connections) for examples of common connection templates.
  </Tab>
</Tabs>

## Network isolation

For end-to-end [network isolation](configure-private-link) with Foundry, you need private endpoints to connect to your connected resource. For example, if your Azure Storage account is set to public network access as **Disabled**, then a private endpoint should be deployed in your virtual network to access in Foundry.

For more on how to set private endpoints to your connected resources, see the following documentation:

| Private resource     | Documentation                                                                                                                                           |
| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Azure Storage        | [Use private endpoints](https://learn.microsoft.com/en-us/azure/storage/common/storage-private-endpoints)                                               |
| Azure Cosmos DB      | [Configure Azure Private Link for Azure Cosmos DB](https://learn.microsoft.com/en-us/azure/cosmos-db/how-to-configure-private-endpoints?tabs=arm-bicep) |
| Azure AI Search      | [Create a private endpoint for a secure connection](https://learn.microsoft.com/en-us/azure/search/service-create-private-endpoint)                     |
| Azure OpenAI         | [Securing Azure OpenAI inside a virtual network with private endpoints](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/network)       |
| Application Insights | [Use Azure Private Link to connect networks to Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/private-link-security)         |

<Callout type="note">
  Cross-subscription connections used for model deployment are not supported (Foundry, Azure OpenAI). You can't connect to resources from different subscriptions for model deployments.
</Callout>

## Related content

* [How to create vector indexes](index-add)

Authentication and authorization in Microsoft Foundry define how principals prove identity and gain permission to perform control plane and data plane operations. Foundry supports API key and Microsoft Entra ID token-based authentication. Microsoft Entra ID enables conditional access, managed identities, granular role-based access control (RBAC) actions, and least privilege scenarios. API keys remain available for rapid prototyping and legacy integration but lack per-user traceability. This article explains the control plane and data plane model, compares API key and Microsoft Entra ID authentication, maps identities to roles, and describes common least privilege scenarios.

<Callout type="important">
  Use Microsoft Entra ID for production workloads to enable conditional access, managed identities, and least privilege RBAC. API keys are convenient for quick evaluation but provide coarse-grained access.
</Callout>

## Prerequisites

* An Azure subscription. If you don't have one, [create a free account](https://azure.microsoft.com/free/).
* A Microsoft Foundry resource with a [custom subdomain](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-custom-subdomains) configured.
* Understanding of [Azure RBAC concepts](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview).
* To assign roles, you need the **Owner** role or **User Access Administrator** role at the appropriate scope.
* (Optional) The [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) or [Azure SDK for Python](https://pypi.org/project/azure-identity/) installed for programmatic authentication.

## Control plane and data plane

Azure operations divide into two categories: control plane and data plane. Azure separates resource management (control plane) from operational runtime (data plane). Therefore, you use the control plane to manage resources in your subscription and use the data plane to use capabilities exposed by your instance of a resource type. To learn more about control plane and data plane, see [Azure control plane and data plane](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/control-plane-and-data-plane). In Foundry, there's a clear distinction between control plane operations versus data plane operations. The following table explains the difference between the two, the scope in Foundry, typical operations of a user, example tools and features, and the authorization surface to use each.

| Plane         | Scope in Foundry                                                                                 | Typical operations                                                                                                    | Example tools                                            | Authorization surface  |
| ------------- | ------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------- | ---------------------- |
| Control plane | Setting up and configuring resource, projects, networking, encryption, and connections           | Create or delete resources, assign roles, rotate keys, set up Private Link                                            | Azure portal, Azure CLI, ARM templates, Bicep, Terraform | Azure RBAC actions     |
| Data plane    | Running and using model inference, agent interactions, evaluation jobs, and content safety calls | Chat completions, embedding generation, start fine-tune jobs, send agent messages, analyzer and classifier operations | SDKs, REST APIs, Foundry portal playground               | Azure RBAC dataActions |

For all Bicep, Terraform, and SDK samples, see the [foundry-samples repository on GitHub](https://github.com/azure-ai-foundry/foundry-samples) for Foundry.

### Control plane and data plane diagram

Within Foundry, there's a clear separation between control plane and data plane actions. Control plane actions within Foundry include:

* Foundry resource creation
* Foundry project creation
* Account Capability Host creation
* Project Capability Host creation
* Model deployment
* Account and project connection creation

Data plane actions within Foundry include:

* Building agents
* Running an evaluation
* Tracing and monitoring
* Fine-tuning

The following diagram shows the view of control plane versus data plane separation in Foundry alongside role-based access control (RBAC) assignments and what access a user might have in either the control plane or data plane or both. As seen in the diagram, RBAC "actions" are associated with control plane while RBAC "dataActions" are associated with data plane.

![Diagram illustrating separation of control plane and data plane operations with associated RBAC surfaces.](https://learn.microsoft.com/azure/ai-foundry/media/authentication-authorization/foundry-rbac-control-data-separation.png)

## Authentication methods

Foundry supports Microsoft Entra ID (token-based, keyless) and API keys.

### Microsoft Entra ID

Microsoft Entra ID uses OAuth 2.0 bearer tokens scoped to `https://cognitiveservices.azure.com/.default`.

Use Microsoft Entra ID for:

* Production workloads.
* Conditional access, multifactor authentication (MFA), and just-in-time access.
* Least privilege RBAC and managed identity integration.

Advantages: Fine-grained role assignments, per-principal auditing, controllable token lifetimes, automatic secret hygiene, and managed identities for services.

Limitations: Higher initial setup complexity. Requires understanding of role-based access control (RBAC). For more on RBAC in Foundry, see [Role-based access control for Microsoft Foundry](rbac-foundry).

### API keys

API keys are static secrets scoped to a Foundry resource.

Use API keys for:

* Rapid prototyping.
* Isolated test environments where single-secret rotation is acceptable.

Advantages: Simple, language agnostic, and doesn't require token acquisition.

Limitations: Can't express user identity, is difficult to scope granularly, and is harder to audit. Generally not accepted by enterprise production workloads and not recommended by Microsoft.

For more information on enabling keyless authentication, see [Configure key-less authentication with Microsoft Entra ID](../foundry-models/how-to/configure-entra-id).

### Authenticate with Microsoft Entra ID (Python)

The following example shows how to authenticate with Microsoft Entra ID by using the `azure-identity` library and make a request to a Foundry endpoint:

```python
from azure.identity import DefaultAzureCredential
import requests

# Create a credential object using DefaultAzureCredential
# This automatically uses environment variables, managed identity, or Azure CLI credentials
credential = DefaultAzureCredential()

# Get an access token for the Cognitive Services scope
token = credential.get_token("https://cognitiveservices.azure.com/.default")

# Use the token in your API request
headers = {
    "Authorization": f"Bearer {token.token}",
    "Content-Type": "application/json"
}

# Replace with your Foundry endpoint
endpoint = "https://<your-resource-name>.cognitiveservices.azure.com"

# Example: List deployments (adjust the path for your specific API)
response = requests.get(f"{endpoint}/openai/deployments?api-version=2024-10-21", headers=headers)
print(response.json())
```

**Expected output**: A JSON response listing your model deployments, or an authentication error if credentials are missing or the role assignment isn't configured.

**Reference**: [DefaultAzureCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential) | [azure-identity library](https://learn.microsoft.com/en-us/python/api/overview/azure/identity-readme)

### Authenticate with an API key (Python)

The following example shows how to authenticate by using an API key. Use this approach for quick prototyping only; Microsoft Entra ID is recommended for production.

```python
import requests

# Replace with your actual API key and endpoint
api_key = "<your-api-key>"
endpoint = "https://<your-resource-name>.cognitiveservices.azure.com"

headers = {
    "api-key": api_key,
    "Content-Type": "application/json"
}

# Example: List deployments
response = requests.get(f"{endpoint}/openai/deployments?api-version=2024-10-21", headers=headers)
print(response.json())
```

<Callout type="warning">
  API keys provide full access to the resource and can't be scoped to specific users or actions. Rotate keys regularly and avoid committing them to source control.
</Callout>

**Expected output**: A JSON response listing your model deployments, or a 401 error if the API key is invalid.

**Reference**: [Rotate API access keys](https://learn.microsoft.com/en-us/azure/ai-services/rotate-keys?context=/azure/ai-foundry/context/context)

## Feature support matrix

Reference the following matrix to understand what capabilities in Foundry support API key versus Microsoft Entra ID.

| Capability or feature                          | API key      | Microsoft Entra ID                          | Notes                                          |
| ---------------------------------------------- | ------------ | ------------------------------------------- | ---------------------------------------------- |
| Basic model inference (chat, embeddings)       | Yes          | Yes                                         | Fully supported.                               |
| Fine-tuning operations                         | Yes          | Yes                                         | Entra ID adds per-principal audit.             |
| Agents service                                 | No           | Yes                                         | Use Entra ID for managed identity tool access. |
| Evaluations                                    | No           | Yes                                         | Use Entra ID.                                  |
| Content safety analyze calls                   | Yes          | Yes                                         | Use RBAC to limit high-risk operations.        |
| Batch analysis jobs (Content Understanding)    | Yes          | Yes                                         | Entra ID recommended for scale.                |
| Portal playground usage                        | Yes          | Yes                                         | Playground uses project connection mode.       |
| Network isolation with Private Link            | Yes          | Yes                                         | Entra ID adds conditional access.              |
| Least privilege with built-in and custom roles | No           | Yes                                         | Keys are all-or-nothing per resource.          |
| Managed identity (system or user-assigned)     | No           | Yes                                         | Enables secret-less auth.                      |
| Per-request user attribution                   | No           | Yes                                         | Token contains tenant and object IDs.          |
| Revocation (immediate)                         | Rotate key   | Remove role or disable principal            | Short token lifetime applies.                  |
| Support in automation pipelines                | Yes (secret) | Yes (service principal or managed identity) | Entra ID reduces secret rotation.              |
| Assistants API                                 | Yes          | Yes                                         | Recommended to use Entra ID.                   |
| Batch inferencing                              | Yes          | Yes                                         |                                                |

## Identity types

Azure resources and applications authenticate by using different identity types, each designed for specific scenarios. User principals represent human users, service principals represent applications or automated processes, and managed identities provide a secure, credential-free way for Azure resources to access other services. Understanding these distinctions helps you choose the right identity for interactive sign-ins, app-to-app communication, or workload automation.

Azure supports the following identity types.

| Identity type                                                                                                                        | Description                                                          |
| ------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------- |
| User principal                                                                                                                       | Individual user in Microsoft Entra ID                                |
| [Service principal](https://learn.microsoft.com/en-us/entra/identity-platform/app-objects-and-service-principals) (app registration) | Application identity that uses a client secret or certificate        |
| [Managed identity](https://learn.microsoft.com/en-us/entra/identity/managed-identities-azure-resources/overview) (system-assigned)   | Azure resource-bound identity automatically managed by the platform. |
| Managed identity (user-assigned)                                                                                                     | Standalone identity that attaches to multiple resources.             |

## Built-in roles overview

In Foundry, use the built-in roles to separate the allowed actions for a user. Most enterprises want a separation of control and data plane actions for their built-in roles. Others expect a combined data and control plane role to minimize the number of role assignments required. The following table lists scenarios and the corresponding built-in Foundry roles that best fit each scenario.

| Scenario                                          | Typical built-in roles   | Notes                                                                                                                                                       |
| ------------------------------------------------- | ------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Build agents with pre-deployed models             | Azure AI User            | Data plane usage only; no management writes.                                                                                                                |
| Manage deployments or fine-tune models            | Azure AI Project Manager | Includes model deployment creation and update.                                                                                                              |
| Rotate keys or manage resource                    | Azure AI Account Owner   | High privilege; consider custom role for least privilege.                                                                                                   |
| Manage resource, manage deployments, build agents | Azure AI Owner           | Highly privileged self-serve role for users who need both control plane and data plane access. Combine with Azure Monitor Reader if observability required. |
| Observability, tracing, monitoring                | Azure AI User (minimum)  | Add Azure Monitor Reader on Application Insights.                                                                                                           |

To understand the breakdown of built-in roles and the control and data plane actions, review the following diagram.

![Diagram mapping built-in roles to control plane actions and data plane actions in Foundry.](https://learn.microsoft.com/azure/ai-foundry/media/authentication-authorization/role-mapping-diagram.png)

<Callout type="tip">
  Create a custom role if a built-in role grants excess permissions for your use case.
</Callout>

## Set up Microsoft Entra ID

For high-level guidance on setting up Entra ID authentication in Foundry, see [Configure key-less authentication](../foundry-models/how-to/configure-entra-id).

1. Ensure your Microsoft Foundry resource has a custom subdomain configured. See [Custom subdomains](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-custom-subdomains). A custom subdomain is required for token-based authentication.

2. Assign the needed built-in or custom role to each principal. You need the **Owner** or **User Access Administrator** role at the target scope to assign roles. Common role assignments:

   * **Azure AI User**: For developers who need to build and test with pre-deployed models.
   * **Azure AI Project Manager**: For team leads who need to create projects and manage deployments.
   * **Azure AI Account Owner**: For administrators who need full resource management without data plane access.
   * **Azure AI Owner**: For users who need both full resource management and data plane access.

3. (Optional) For a service principal, create an app registration, add a client secret or certificate, and note the tenant ID, client ID, and secret or certificate.

4. (Optional) For a managed identity, enable the system-assigned identity on the calling service or attach a user-assigned identity, then assign a role to it on the Foundry resource.

5. Remove key-based authentication after all callers use token authentication. Optionally disable local authentication in deployment templates.

**Reference**: [Assign Azure roles](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal) | [Role-based access control for Foundry](rbac-foundry)

## Related content

* [Role-based access control for Foundry](rbac-foundry)
* [Configure key-less authentication with Microsoft Entra ID](../foundry-models/how-to/configure-entra-id)
* [Rotate API access keys](https://learn.microsoft.com/en-us/azure/ai-services/rotate-keys?context=/azure/ai-foundry/context/context)
* [Azure built-in roles (AI + machine learning)](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#ai-+-machine-learning)
* [Authentication vs. authorization (Microsoft Entra ID)](https://learn.microsoft.com/en-us/entra/identity-platform/authentication-vs-authorization)
* [Identity fundamentals](https://learn.microsoft.com/en-us/entra/fundamentals/identity-fundamental-concepts)

Welcome! This article highlights key changes and updates in Microsoft Foundry documentation for December 2025.

This month marks a significant update to our documentation structure. With the introduction of the new Microsoft Foundry portal, we now maintain two corresponding versions of the documentation to support each portal experience. This dual-version approach ensures that users can access accurate, version-specific guidance tailored to their portal environment.

## New articles

Available in Foundry (new) only:

* [Developer journey: Idea to prototype](tutorials/developer-journey-idea-to-prototype)
* [Publish agents in Microsoft Foundry](agents/how-to/publish-agent)
* [Agent memory concepts](agents/concepts/what-is-memory)
* [Build your own MCP server](mcp/build-your-own-mcp-server)
* [Manage agent identities with Microsoft Entra ID](agents/concepts/agent-identity)
* [Optimization model upgrade](observability/how-to/optimization-model-upgrade)
* [Cluster analysis](observability/how-to/cluster-analysis)
* [Optimization dashboard](observability/how-to/optimization-dashboard)
* [Human evaluation](observability/how-to/human-evaluation)
* [Azure Language tools and agents](../ai-services/language-service/concepts/foundry-tools-agents)
* [Azure Language CLU Multi-turn conversations](../ai-services/language-service/conversational-language-understanding/concepts/multi-turn-conversations)

Available in both Foundry (new) and Foundry (classic):

* [Install CLI SDK](how-to/develop/install-cli-sdk)
* [SDK overview](how-to/develop/sdk-overview)
* [High availability and resiliency](how-to/high-availability-resiliency)
* [Agent service disaster recovery](how-to/agent-service-disaster-recovery)
* [Agent service operator disaster recovery](how-to/agent-service-operator-disaster-recovery)
* [Agent service platform disaster recovery](how-to/agent-service-platform-disaster-recovery)
* [Integrate with other apps](how-to/integrate-with-other-apps)
* [Create a custom photo avatar](../ai-services/speech-service/text-to-speech-avatar/custom-photo-avatar-create)
* [Customize voice live](../ai-services/speech-service/voice-live-how-to-customize)
* [Bring your own model](../ai-services/speech-service/how-to-bring-your-own-model)
* [Use the LLM-speech API](../ai-services/speech-service/llm-speech)
* [Priority processing for Foundry Models](openai/concepts/priority-processing)
* [Classification in Content Understanding Studio](../ai-services/content-understanding/how-to/classification-content-understanding-studio)
* [Foundry playgrounds](concepts/concept-playgrounds)
* [Use Claude in Foundry Models](foundry-models/how-to/use-foundry-models-claude)
* [Monitor and manage agents with Foundry control plane](control-plane/overview)

### Updated articles

All articles were updated in some way this month:

* Articles that apply to the new version were updated to add version-specific information.
* Articles that apply to both the new Microsoft Foundry and classic versions include banners that you can use to switch between the two versions to see the relevant content for each.
* Articles that apply only to the classic version include a banner indicating this limitation.

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

The [Microsoft Foundry Status Dashboard](https://status.ai.azure.com/) provides visibility into the health and availability of key Foundry services. It helps customers monitor service status, stay informed about ongoing incidents, and plan around scheduled maintenance windows.

This dashboard is currently in **preview**, and it might not reflect all components or issues.

## Prerequisites

* A web browser.

## Check service status

1. Open the [Microsoft Foundry Status Dashboard](https://status.ai.azure.com/).
2. Review the overall status at the top of the page.
3. Select a component to view details and recent status changes.
4. Select **Incidents** to review incident history.
5. Select **Subscribe to updates** to get notified about updates.

## Key features

* **Live status indicators** for core Foundry services.
* **Incident reports** with timelines, resolutions, and root cause summaries.
* **Historical uptime** to help assess service reliability over time.

## Frequently asked questions

**Q: Is this data real-time?**\
The dashboard updates as incident and maintenance status changes are posted.

**Q: What does it mean that this dashboard is in “Preview”?**\
During preview, service coverage is expanding and the experience is still being refined. Some services might not appear, and update timing might vary.

**Q: Can I subscribe to updates?**\
Yes. Select **Subscribe to updates** in the dashboard.

**Q: Does the dashboard cover all regions and environments?**\
The dashboard is in preview, and coverage might vary by component.

**Q: How should I report discrepancies or missing status updates?**\
If you notice a gap between your experience and what you see on the dashboard, contact Microsoft support or create an Azure support request.

## Feedback and support

If you have questions, suggestions, or run into problems, contact Microsoft support.

## Related content

* [Foundry documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/)
* [Feature availability across cloud regions](reference/region-support)
* [Azure status history](https://status.azure.com/status)
* [Azure Service Health](https://azure.microsoft.com/status/)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Microsoft Foundry is updated regularly and the product team is continually improving and enhancing its features and capabilities. This article lists known issues related to Foundry and provides steps to resolve them. Before submitting a support request, review the following list to see if your problem is already being addressed and to find a possible solution.

* For more information about service-level outages, check the [Azure status page](https://status.azure.com/status).
* To set up outage notifications and alerts, visit the [Azure Service Health Portal](https://learn.microsoft.com/en-us/azure/service-health/service-health-portal-update).

## General Foundry known issues

| Issue ID | Category       | Title                                  | Description                                                                                                                                        | Workaround                                                                                                                                                                                                                                             | Issues publish date |
| -------- | -------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------- |
| 0001     | Foundry Portal | Network isolation in new Foundry       | The new Foundry portal experience doesn't support end-to-end network isolation.                                                                    | When you configure network isolation (disable public network access, enable private endpoints, and use virtual network-injected Agents), you must use the classic Foundry portal experience, the SDK, or CLI to securely access your Foundry projects. | December 5, 2025    |
| 0002     | Foundry Portal | Multiple projects per Foundry resource | The new Foundry portal experience doesn't support multiple projects per Foundry resource. Each Foundry resource supports only one default project. | None                                                                                                                                                                                                                                                   | December 5, 2025    |

### AI Speech

The following tables describe the current known issues for the Speech services, including Speech to Text (STT), Text to Speech (TTS), and Speech SDK/Runtime.

#### Speech to Text (STT) active known issues

This table lists the current known issues for the Speech to text feature:

| Issue ID | Category   | Title                                                                             | Description                                                                                                                                                                                                                                                                                                                                                                                | Workaround                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Issues publish date |
| -------- | ---------- | --------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| 1001     | Content    | STT transcriptions with pound units                                               | In certain instances, the use of pounds units can pose difficulties for transcription. When phrases are spoken in a UK dialect, they're often inaccurately converted during real-time transcription, leading to the term "pounds" being automatically translated to "lbs" irrespective of the language setting.                                                                            | Users can use Custom Display Post Processing (DPP) to train a custom speech model to correct default DPP results (for example, Pounds \{tab} Pounds). Refer to Custom Rewrite Rules.                                                                                                                                                                                                                                                                                       | June 9, 2025        |
| 1002     | Content    | STT transcriptions with cardinal directions                                       | The speech recognition model 20241218 might inaccurately interpret audio inputs that include cardinal directions, resulting in unexpected transcription outcomes. For instance, an audio file containing "SW123456" might be transcribed as "Southwest 123456", and similar errors can occur with other cardinal directions.                                                               | Potential workaround is to use Custom Display formatting where "Southwest" is mapped to "SW" in a rewrite rule: Custom Rewrite Rules.                                                                                                                                                                                                                                                                                                                                      | June 9, 2025        |
| 1003     | Model      | STT transcriptions might include unexpected internal system tags.                 | Unexpected tags like "nsnoise" appear in transcription results. Customers initially reported this issue for the Arabic model (ar-SA), but it's also observed in English models (en-US and en-GB). These tags cause intermittent problems in the transcription outputs. To address this problem, a filter will be added to remove "nsnoise" from the training data in future model updates. | N/A                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | June 9, 2025        |
| 1004     | Model      | STT Transcriptions with inaccurate spellings of language specific names and words | Inaccurate transcription of language specific names due to lack of entity coverage in base model for tier 2 locales (scenario specific to when base models didn't see a specific word before).                                                                                                                                                                                             | Customers can train Custom Speech models to include unknown names and words as training data. As a second step, unknown words can be added as Phrase List at runtime. Biasing phrase list to a word known in the training corpus can greatly improve recognition accuracy.                                                                                                                                                                                                 | June 9, 2025        |
| 1005     | File Types | Words out of context added in STT real time output occasionally                   | Audio files that consist solely of background noise can result in inaccurate transcriptions. Ideally, only spoken sentences should be transcribed, but this isn't occurring with the nl-NL model.                                                                                                                                                                                          | Audio files that consist of background noise, captured echo reflections from surfaces in an environment, or audio playback from a device while device microphone is active can result in inaccurate transcriptions. Customers can use the Microsoft Audio Stack built into the Speech SDK for noise suppression of observed background noise and echo cancellation. This should help optimize the audio being fed to the STT service: Use the Microsoft Audio Stack (MAS). | June 9, 2025        |
| 1006     | Filetypes  | MP4 decoding failure due to 'moov atom' position                                  | The decoding of MP4 container files might fail because the "moov atom" is located at the end of the file instead of the beginning. This structure makes the file unstreamable for the current service and the underlying Microsoft MTS service, especially for files larger than 10 MB. Supporting such formats would require fundamental changes.                                         | Preprocess the file using audio codec utilities to move the "moov atom" to the beginning or convert to MP3.                                                                                                                                                                                                                                                                                                                                                                | August 8, 2025      |

#### Text to Speech (TTS) active known issues

This table lists the current known issues for the Text-to-Speech feature.

| Issue ID | Category   | Title                                      | Description                                                                                                                                                                                                                                                                                         | Workaround                                                                                                                                                                             | Issues publish date |
| -------- | ---------- | ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| 2001     | Service    | Model copying via Rest API                 | The TTS service doesn't allow model copying via the REST API for disaster recovery purposes.                                                                                                                                                                                                        | N/A                                                                                                                                                                                    | June 9, 2025        |
| 2002     | TTS Avatar | Missing parameters                         | TTS Avatar parameters "avatarPosition" and "avatarSize" not supported in Batch synthesis.                                                                                                                                                                                                           | N/A                                                                                                                                                                                    | June 9, 2025        |
| 2003     | TTS Avatar | Missing Blob file names                    | The 'outputs': 'result' URL of Batch avatar synthesis job doesn't have the blob file name.                                                                                                                                                                                                          | Customers should use 'subtitleType = soft\_embedded' as a temporary workaround.                                                                                                        | June 9, 2025        |
| 2004     | TTS Avatar | Batch synthesis unsupported for TTS        | Batch synthesis for avatar doesn't support bring-your-own-storage (BYOS) and it requires the storage account to allow external traffic.                                                                                                                                                             | N/A                                                                                                                                                                                    | June 9, 2025        |
| 2005     | Service    | DNS cache refresh before end of July 2025  | Due to compliance reasons, the legacy Speech TTS clusters in Asia are removed on July 31, 2025. All traffic is migrated from the old IPs to the new ones.                                                                                                                                           | Some customers still access the old clusters even after DNS redirection is completed. This behavior indicates that some customers might have persistent local or secondary DNS caches. | July 24, 2025       |
| 2006     | TTS        | Word boundary duplication in output        | Azure TTS sometimes returns duplicated word boundary entries in the synthesis output, particularly when using certain SSML configurations. This duplication can lead to inaccurate timing data and misalignment in downstream applications.                                                         | Post-process the output to filter out duplicate word boundaries based on timestamp and word content.                                                                                   | August 8, 2025      |
| 2007     | TTS        | Partially generated words in Arabic voices | Arabic voice outputs only contain partially generated words in cases of unclear or incomplete pronunciation, especially for words ending with ة or ت. This problem is reproducible across multiple voices. This problem is acknowledged as a known problem without an immediate solution available. | To mitigate the problem, consider rephrasing the voice output, if the problem occurs.                                                                                                  | September 16, 2025  |

#### Speech SDK/Runtime active known issues

This table lists the current known issues for the Speech SDK/Runtime feature.

| Issue ID | Category       | Title                                                            | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | Workaround                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | Issues publish date |
| -------- | -------------- | ---------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| 3001     | SDK/SR runtime | Handling of the InitialSilence Timeout parameter                 | The problem is related to the handling of the InitialSilenceTimeout parameter. When set to 0, it unexpectedly causes customers to encounter 400 errors. Additionally, the endSilenceTimeout parameter might lead to incorrect transcriptions. When the endSilenceTimeout is set to a value other than "0," the system disregards user input after the specified duration, even if the user continues speaking. Customers want all the parts of the conversation to be transcribed, including segments after pauses, to ensure no user input is lost. | The 400 error occurs because the "InitialSilenceTimeout" parameter isn't currently exposed directly in Real-time Speech Recognition endpoint resulting in a failed URL consistency check. To bypass this error, customers can perform the following steps: SpeechConfig = fromSubscription (String subscriptionKey, String region); where region is the Azure Region where the Speech resource is located. Set the parameter "InitialSilenceTimeoutMs" to 0, which in effect disables timeout due to initial silence in the recognition audio stream. Note: For single shot recognition, the session terminates after 30 seconds of initial silence. For continuous recognition, the service reports empty phrase after 30 seconds and continues to recognition process. This problem is due to a second parameter `Speech_Segmentation MaximumTimeMs`, which determines the maximum length of a phrase and has default value of 30,000 ms. | June 9, 2025        |
| 3002     | SDK/SR Runtime | Handling of SegmentationTimeout parameter                        | Customers experience random words being generated as part of Speech recognition results (incorrect information) when the SegmentationSilenceTimeout parameter is set to > 1,000 ms.                                                                                                                                                                                                                                                                                                                                                                  | Customers should maintain the default "SegmentationTimeout" value of 650 ms.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | June 9, 2025        |
| 3003     | SDK/SR Runtime | Handling of speaker duration during Real-time diarization in STT | Python SDK not showing duration of speakers when using Real-time diarization with STT.                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Check offset and duration on the result following steps on the following documentation: Conversation Transcription Result Class.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | June 9, 2025        |
| 3004     | SDK/TTS Avatar | Frequent disconnections with JavaScript SDK                      | TTS Avatar isn't loading/frequent disconnections and reconnection of a custom avatar using the JavaScript SDK.                                                                                                                                                                                                                                                                                                                                                                                                                                       | Customers should open the UDP 3478 port.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | June 9, 2025        |

### AI Translator

The following tables describe the current known issues for Azure Translator in Foundry Tools.

#### Text Translation active known issues

| Issue ID | Category | Title                                          | Description                                                                                                                                                                                                                                                                                                | Workaround                                                                                                                 | Issues publish date |
| -------- | -------- | ---------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| 1004     | Model    | Preserving context and pronouns                | Some translation models don't handle pronouns well, especially third-person pronouns. This problem occurs because sentence-level training and inference don't preserve context. The product team is actively working to shift all models to document-level training and inference to preserve the context. | Currently, there's no direct workaround. Manually review and adjust pronoun usage as needed.                               | February 5, 2025    |
| 1006     | Content  | Translating sentences with mixed language text | The Text translation API doesn't support translating sentences that contain mixed language input. As a result, translations can be incorrect or incomplete when a single sentence includes multiple languages.                                                                                             | Specify the intended source language, remove the mixed-language sentence, or split the text into single-language segments. | February 5, 2025    |

#### Document Translation active known issues

| Issue ID | Category   | Title                                                         | Description                                                                                                                                                                                                                                                                                                                                                                                                 | Workaround                                                                                                                                                                                                                       | Issues publish date |
| -------- | ---------- | ------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------- |
| 3001     | Formatting | Formatting of mathematical expressions                        | In some cases, translated documents don't fully retain the formatting of mathematical expressions. Superscript and subscript numbers can be reformatted incorrectly, leading to discrepancies between expected and actual output.                                                                                                                                                                           | Currently, there's no direct workaround. Manually adjust the formatting of mathematical expressions as needed.                                                                                                                   | February 5, 2025    |
| 3007     | Content    | Translating documents with mixed source languages             | In some cases, document translation doesn't translate source documents with multiple languages leading to incorrect or incomplete results. For example, a sentence that contains more than one language.                                                                                                                                                                                                    | To ensure that the desired language is translated to the target language, specify the intended source language. Alternatively, remove the mixed-language sentence, or split the text into segments containing only one language. | February 5, 2025    |
| 3008     | File types | Translating complex documents                                 | Translating documents with thousands of intricate pages can be challenging. These documents often include images, embedded text within images, and manually typed text. As a result, the batch translation request can encounter failures during the extraction, translation, and reassembly processes.                                                                                                     | Split the large document into smaller sections (for example, divide a 1,000-page file into approximately 10 files of 100 pages each) and submit them individually for the best results.                                          | February 5, 2025    |
| 3009     | Formatting | Translating documents containing borderless charts and tables | Complex tables and charts can present significant challenges during translation, especially when they're large and intricate. Charts and tables with mixed horizontal and vertical text, varying cell sizes, or grid structures that are borderless, are difficult to format accurately. These types of tables might require added processing to ensure precision without compromising overall performance. | To improve the quality of translation output, consider recreating documents using bordered tables and charts rather than borderless ones.                                                                                        | April 1, 2025       |
| 3010     | Content    | Translating documents containing visible watermarks or seals  | Documents with visible watermarks or seals can significantly hinder the translation process, as the watermarks might overlap with the text, making it difficult for the models to accurately recognize and process the content. This problem can result in the document remaining untranslated or only partially translated.                                                                                | For optimal translation results, use clean, watermark-free documents. Files without visible watermarks or seals translate accurately and as expected.                                                                            | May 21, 2025        |

### AI Vision

No currently active known issues.

## AI Search

No currently active known issues.

## Azure OpenAI (Foundry Models)

No currently active known issues.

## Observability

No currently active known issues.

## Azure Machine Learning

For known problems related to Azure Machine Learning, see [Azure Machine Learning known problems](../../machine-learning/known-issues/azure-machine-learning-known-issues).

## Related content

* [Azure Service Health Portal](https://learn.microsoft.com/en-us/azure/service-health/service-health-portal-update)
* [Azure Status overview](https://learn.microsoft.com/en-us/azure/service-health/azure-status-overview)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

[Microsoft Foundry](https://ai.azure.com/?cid=learnDocs) brings together various Azure AI capabilities that were previously only available as standalone Azure services. While Microsoft strives to make all features available in all regions where Microsoft Foundry is supported at the same time, feature availability might vary by region. In this article, you learn what Foundry features are available across cloud regions.

## Foundry projects

Foundry is currently available in the following Azure regions.

* Australia East
* Brazil South
* Canada Central
* Canada East
* Central India
* East Asia
* East US
* East US 2
* France Central
* Germany West Central
* Italy North

- Japan East
- Korea Central
- North Central US
- North Europe
- Norway East
- Qatar Central
- South Africa North
- South Central US
- South India
- Southeast Asia

* Spain Central
* Sweden Central
* Switzerland North
* UAE North
* UK South
* West Europe
* West US
* West US 3

## Foundry features

For maximum feature availability, consider East US 2 or Sweden Central as your primary regions, as they offer the most comprehensive coverage across features.

Use the following table to investigate regional availability for specific features you plan to use.

| Service                 | Description                                                                                                                                       | Link                                                                                                                                               |
| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| Azure OpenAI            | Some models might not be available within the Foundry model catalog.                                                                              | [Azure OpenAI quotas and limits](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/quotas-limits#regional-quota-capacity-limits).          |
| Speech capabilities     | Azure Speech in Foundry Tools capabilities, including custom neural voice, vary in regional availability due to underlying hardware availability. | [Speech service supported regions](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/regions)                                     |
| Azure AI Content Safety | To use the Content Safety APIs, create your Azure AI Content Safety resource in a supported region.                                               | [What is Azure AI Content Safety?](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview#region-availability).               |
| Foundry Agent Service   | Agent Service supports the same models as the chat completions API in Azure OpenAI.                                                               | [Agent Service region availability](https://learn.microsoft.com/en-us/azure/ai-services/agents/concepts/model-region-support#azure-openai-models). |

## Use AI to find the best region

Use AI to help you find the right region for your needs. Open [Ask AI](../concepts/ask-ai) and customize this prompt for your specific case:

```copilot-prompt
   Based on the features I need for my Foundry project, which regions would you recommend to create the project?
   I need: [list your required features here, such as: gpt-4o models, speech capabilities, custom avatar training, etc.]
```

*Copilot is powered by AI, so surprises and mistakes are possible. For more information, see [Copilot general use FAQs](https://aka.ms/copilot-general-use-faqs).*

## Next step

* [Azure global infrastructure products by region](https://azure.microsoft.com/global-infrastructure/services/)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Microsoft Foundry Models are continually refreshed with newer and more capable models. As part of this process, model providers might deprecate and retire their older models, and you might need to update your applications to use a newer model. This document communicates information about the model lifecycle and deprecation timelines and explains how you're informed of model lifecycle stages.

This article covers general deprecation and retirement information for Foundry Models. For details specific to Azure OpenAI in Foundry Models, see [Azure OpenAI in Foundry Models model deprecations and retirements](../openai/concepts/model-retirements).

## Model lifecycle stages

Models in the model catalog belong to one of these stages:

* Preview
* Generally available
* Legacy
* Deprecated
* Retired

### Preview

Models labeled *Preview* are experimental in nature. A model's weights, runtime, and API schema can change while the model is in preview. Models in preview aren't guaranteed to become generally available. Models in preview have a *Preview* label next to their name in the model catalog.

### Generally available (GA)

This stage is the default model stage. Models that don't include a lifecycle label next to their name are GA and suitable for use in production environments. In this stage, model weights and APIs are fixed. However, model containers or runtimes with vulnerabilities might get patched, but patches don't affect model outputs.

### Legacy

Models labeled *Legacy* are intended for deprecation. You should plan to move to a different model, such as a new, improved model that might be available in the same model family. While a model is in the legacy stage, existing deployments of the model continue to work, and you can create new deployments of the model until the deprecation date.

### Deprecated

Models labeled *Deprecated* are no longer available for new deployments. You can't create any new deployments for the model; however, existing deployments continue to work until the retirement date.

### Retired

Models labeled *Retired* are no longer available for use. You can't create new deployments, and attempts to use existing deployments return `404` errors.

## Notifications for Foundry Models

Customers that have Foundry Model deployments receive notifications for upcoming model retirements according to the following schedule:

* Models are labeled as *Legacy* and remain in the legacy state for at least 30 days before being moved to the deprecated state. During this notification period, you can create new deployments as you prepare for deprecation and retirement.

* Models are labeled *Deprecated* and remain in the deprecated state for at least 90 days before being moved to the retired state. During this notification period, you can migrate any existing deployments to newer or replacement models.

For each subscription that has a model deployed as a serverless API deployment or deployed to a Foundry resource, members of the *owner*, *contributor*, *reader*, *monitoring contributor*, and *monitoring reader* roles receive a notification when a model deprecation is announced. The notification contains the dates when the model enters legacy, deprecated, and retired states. The notification might provide information about possible replacement model options, if applicable.

## Notifications for Azure OpenAI in Foundry Models

For Azure OpenAI models, customers with active Azure OpenAI deployments receive notice for models with upcoming retirement as follows:

* At model launch, we programmatically designate a "not sooner than" retirement date (typically one year out).
* At least 60 days notice before model retirement for Generally Available (GA) models.
* At least 30 days notice before preview model version upgrades.

Members of the *owner*, *contributor*, *reader*, *monitoring contributor*, and *monitoring reader* roles receive notification for each subscription with a deployment of a model that has an upcoming retirement.

Retirements are done on a rolling basis, region by region. Notifications are sent from an unmonitored mailbox, `azure-noreply@microsoft.com`.

To learn more about the Azure OpenAI models lifecycle, including information for current, deprecated, and retired models, see [Azure OpenAI in Foundry Models model deprecations and retirements](../openai/concepts/model-retirements).

## Upcoming retirements for Foundry Models

The following tables list the timelines for models that are on track for retirement. The lifecycle stages go into effect at 00:00:00 UTC on the specified dates.

#### Cohere

| Model                                                                                                                                               | Legacy date       | Deprecation date  | Retirement date | Suggested replacement model                                                                                                                                                                                                                                                      |
| --------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------- | ----------------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Cohere-rerank-v3.5](https://ai.azure.com/explore/models/Cohere-rerank-v3.5/version/1/registry/azureml-cohere/?cid=learnDocs)                       | January 14, 2026  | February 14, 2026 | May 14, 2026    | [Cohere-rerank-v4.0-pro](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-pro/version/1/registry/azureml-cohere/?cid=learnDocs), [Cohere-rerank-v4.0-fast](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-fast/version/2/registry/azureml-cohere/?cid=learnDocs) |
| [Cohere-command-r-08-2024](https://ai.azure.com/explore/models/Cohere-command-r-08-2024/version/1/registry/azureml-cohere/?cid=learnDocs)           | February 12, 2026 | March 12, 2026    | May 12, 2026    | [Cohere-command-a](https://ai.azure.com/explore/models/Cohere-command-a/version/1/registry/azureml-cohere/?cid=learnDocs)                                                                                                                                                        |
| [Cohere-command-r-plus-08-2024](https://ai.azure.com/explore/models/Cohere-command-r-plus-08-2024/version/1/registry/azureml-cohere/?cid=learnDocs) | February 12, 2026 | March 12, 2026    | May 12, 2026    | [Cohere-command-a](https://ai.azure.com/explore/models/Cohere-command-a/version/1/registry/azureml-cohere/?cid=learnDocs)                                                                                                                                                        |

#### Microsoft

| Model                                                                                                | Legacy date      | Deprecation date | Retirement date   | Suggested replacement model                       |
| ---------------------------------------------------------------------------------------------------- | ---------------- | ---------------- | ----------------- | ------------------------------------------------- |
| [MAI-DS-R1](https://ai.azure.com/explore/models/MAI-DS-R1/version/1/registry/azureml/?cid=learnDocs) | January 16, 2026 | January 27, 2026 | February 27, 2026 | Any DeepSeek model available in the Model catalog |

## Retired Foundry Models

The following models were retired at 00:00:00 UTC on the specified dates and aren't available for new deployments or inference.

#### AI21 Labs

| Model                | Retirement date | Suggested replacement model |
| -------------------- | --------------- | --------------------------- |
| Jamba Instruct       | March 1, 2025   | N/A                         |
| AI21-Jamba-1.5-Large | August 1, 2025  | N/A                         |
| AI21-Jamba-1.5-Mini  | August 1, 2025  | N/A                         |

#### Bria

| Model         | Retirement date  | Suggested replacement model |
| ------------- | ---------------- | --------------------------- |
| Bria-2.3-Fast | October 31, 2025 | N/A                         |

#### Cohere

| Model                         | Retirement date | Suggested replacement model                                                                                                                                                                                                                                                      |
| ----------------------------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Command R                     | June 30, 2025   | [Cohere Command R 08-2024](https://aka.ms/azureai/landing/Cohere-command-r-08-2024?cid=learnDocs)                                                                                                                                                                                |
| Command R+                    | June 30, 2025   | [Cohere Command R+ 08-2024](https://aka.ms/azureai/landing/Cohere-command-r-plus-08-2024?cid=learnDocs)                                                                                                                                                                          |
| Cohere-rerank-v3-english      | June 30, 2025   | [Cohere-rerank-v4.0-pro](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-pro/version/1/registry/azureml-cohere/?cid=learnDocs), [Cohere-rerank-v4.0-fast](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-fast/version/2/registry/azureml-cohere/?cid=learnDocs) |
| Cohere-rerank-v3-multilingual | June 30, 2025   | [Cohere-rerank-v4.0-pro](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-pro/version/1/registry/azureml-cohere/?cid=learnDocs), [Cohere-rerank-v4.0-fast](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-fast/version/2/registry/azureml-cohere/?cid=learnDocs) |

#### Core42

| Model         | Retirement date  | Suggested replacement model |
| ------------- | ---------------- | --------------------------- |
| jais-30b-chat | January 30, 2026 | N/A                         |

#### DeepSeek

| Model       | Retirement date | Suggested replacement model                                                       |
| ----------- | --------------- | --------------------------------------------------------------------------------- |
| DeepSeek-V3 | August 31, 2025 | [DeepSeek-V3-0324](https://aka.ms/azureai/landing/DeepSeek-V3-0324?cid=learnDocs) |

#### Gretel

| Model                    | Retirement date    | Suggested replacement model |
| ------------------------ | ------------------ | --------------------------- |
| Gretel-Navigator-Tabular | September 16, 2025 | N/A                         |

#### Meta

| Model                       | Retirement date | Suggested replacement model                                                                                                                 |
| --------------------------- | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| Llama-2-13b                 | June 30, 2025   | [Meta-Llama-3.1-8B-Instruct](https://ai.azure.com/explore/models/Meta-Llama-3.1-8B-Instruct/version/4/registry/azureml-meta/?cid=learnDocs) |
| Llama-2-13b-chat            | June 30, 2025   | [Meta-Llama-3.1-8B-Instruct](https://ai.azure.com/explore/models/Meta-Llama-3.1-8B-Instruct/version/4/registry/azureml-meta/?cid=learnDocs) |
| Llama-2-70b                 | June 30, 2025   | [Llama-3.3-70B-Instruct](https://ai.azure.com/explore/models/Llama-3.3-70B-Instruct/version/4/registry/azureml-meta/?cid=learnDocs)         |
| Llama-2-70b-chat            | June 30, 2025   | [Llama-3.3-70B-Instruct](https://ai.azure.com/explore/models/Llama-3.3-70B-Instruct/version/4/registry/azureml-meta/?cid=learnDocs)         |
| Llama-2-7b                  | June 30, 2025   | [Meta-Llama-3.1-8B-Instruct](https://ai.azure.com/explore/models/Meta-Llama-3.1-8B-Instruct/version/4/registry/azureml-meta/?cid=learnDocs) |
| Llama-2-7b-chat             | June 30, 2025   | [Meta-Llama-3.1-8B-Instruct](https://ai.azure.com/explore/models/Meta-Llama-3.1-8B-Instruct/version/4/registry/azureml-meta/?cid=learnDocs) |
| Meta-Llama-3-70B-Instruct   | June 30, 2025   | [Llama-3.3-70B-Instruct](https://ai.azure.com/explore/models/Llama-3.3-70B-Instruct/version/4/registry/azureml-meta/?cid=learnDocs)         |
| Meta-Llama-3-8B-Instruct    | June 30, 2025   | [Meta-Llama-3.1-8B-Instruct](https://ai.azure.com/explore/models/Meta-Llama-3.1-8B-Instruct/version/4/registry/azureml-meta/?cid=learnDocs) |
| Meta-Llama-3.1-70B-Instruct | June 30, 2025   | [Llama-3.3-70B-Instruct](https://ai.azure.com/explore/models/Llama-3.3-70B-Instruct/version/4/registry/azureml-meta/?cid=learnDocs)         |

#### Microsoft

| Model                      | Retirement date | Suggested replacement model                                                                                              |
| -------------------------- | --------------- | ------------------------------------------------------------------------------------------------------------------------ |
| Phi-3-medium-4k-instruct   | August 30, 2025 | [Phi-4](https://ai.azure.com/explore/models/Phi-4/version/8/registry/azureml/?cid=learnDocs)                             |
| Phi-3-medium-128k-instruct | August 30, 2025 | [Phi-4](https://ai.azure.com/explore/models/Phi-4/version/8/registry/azureml/?cid=learnDocs)                             |
| Phi-3-mini-4k-instruct     | August 30, 2025 | [Phi-4-mini-instruct](https://ai.azure.com/explore/models/Phi-4-mini-instruct/version/1/registry/azureml/?cid=learnDocs) |
| Phi-3-mini-128k-instruct   | August 30, 2025 | [Phi-4-mini-instruct](https://ai.azure.com/explore/models/Phi-4-mini-instruct/version/1/registry/azureml/?cid=learnDocs) |
| Phi-3-small-8k-instruct    | August 30, 2025 | [Phi-4-mini-instruct](https://ai.azure.com/explore/models/Phi-4-mini-instruct/version/1/registry/azureml/?cid=learnDocs) |
| Phi-3-small-128k-instruct  | August 30, 2025 | [Phi-4-mini-instruct](https://ai.azure.com/explore/models/Phi-4-mini-instruct/version/1/registry/azureml/?cid=learnDocs) |
| Phi-3.5-mini-instruct      | August 30, 2025 | [Phi-4-mini-instruct](https://ai.azure.com/explore/models/Phi-4-mini-instruct/version/1/registry/azureml/?cid=learnDocs) |
| Phi-3.5-MoE-instruct       | August 30, 2025 | [Phi-4-mini-instruct](https://ai.azure.com/explore/models/Phi-4-mini-instruct/version/1/registry/azureml/?cid=learnDocs) |
| Phi-3.5-vision-instruct    | August 30, 2025 | [Phi-4-mini-instruct](https://ai.azure.com/explore/models/Phi-4-mini-instruct/version/1/registry/azureml/?cid=learnDocs) |

#### Mistral AI

| Model              | Retirement date  | Suggested replacement model                                                                                                                |
| ------------------ | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------ |
| Mistral-Nemo       | January 30, 2026 | [Mistral-small-2503](https://aka.ms/aistudio/landing/mistral-small-2503)                                                                   |
| Mistral-large-2411 | January 30, 2026 | [Mistral-medium-2505](https://ai.azure.com/explore/models/mistral-medium-2505/version/1/registry/azureml-mistral/?cid=learnDocs)           |
| Mistral-ocr-2503   | January 30, 2026 | [Mistral-document-ai-2505](https://ai.azure.com/explore/models/mistral-document-ai-2505/version/1/registry/azureml-mistral/?cid=learnDocs) |
| Mistral-small      | July 31, 2025    | [Mistral-small-2503](https://aka.ms/aistudio/landing/mistral-small-2503)                                                                   |
| Mistral-large-2407 | May 13, 2025     | [Mistral-medium-2505](https://ai.azure.com/explore/models/mistral-medium-2505/version/1/registry/azureml-mistral/?cid=learnDocs)           |
| Mistral-large      | April 15, 2025   | [Mistral-medium-2505](https://ai.azure.com/explore/models/mistral-medium-2505/version/1/registry/azureml-mistral/?cid=learnDocs)           |

## Related content

* [Azure OpenAI in Foundry Models model deprecations and retirements](../openai/concepts/model-retirements)
* [Data, privacy, and security for use of models through the model catalog in Foundry portal](../how-to/concept-data-privacy)

