<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

**Microsoft Foundry** is a unified Azure platform-as-a-service offering for enterprise AI operations, model builders, and application development. This foundation combines production-grade infrastructure with friendly interfaces, enabling developers to focus on building applications rather than managing infrastructure.

Microsoft Foundry unifies agents, models, and tools under a single management grouping with built-in enterprise-readiness capabilities including tracing, monitoring, evaluations, and customizable enterprise setup configurations. The platform provides streamlined management through unified Role-based access control (RBAC), networking, and policies under one Azure resource provider namespace.

<Callout type="tip">
  Azure AI Foundry is now Microsoft Foundry. Screenshots appearing throughout this documentation are in the process of being updated.
</Callout>

## Microsoft Foundry portals

There are two different portals for you to use to interact with Microsoft Foundry. A toggle in the portal banner allows you to switch between the two versions.

| Portal                      | Banner display                                                                             | When to use                                                                                                                                                                                                                                   |
| --------------------------- | ------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Microsoft Foundry (classic) | ![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/classic-foundry.png) | Choose this portal when working with multiple resource types: Azure OpenAI, Foundry resources, hub-based projects, or Foundry projects.                                                                                                       |
| Microsoft Foundry (new)     | ![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/new-foundry.png)     | Choose this portal for a seamless experience that combines simplicity with powerful and secure tools to build, manage and grow multi-agent applications. Only Foundry projects are visible here - use (classic) for all other resource types. |

<Callout type="tip">
  All links to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs) open whichever version you last used.
</Callout>

## Microsoft Foundry (new)

**Microsoft Foundry (new)** delivers a modernized experience with powerful enhancements designed for flexibility and scale:

* **[Multi-Agent Orchestration and Workflows](agents/concepts/workflow?view=foundry\&preserve-view=true)** – Build advanced automation using SDKs for C# and Python that enable collaborative agent behavior and complex workflow execution.
* **[Expanded Integration Options](agents/how-to/publish-copilot?view=foundry\&preserve-view=true)** – Publish agents to Microsoft 365, Teams, and BizChat, plus leverage containerized deployments for greater portability.
* **[Expanded Tool Access](agents/concepts/tool-catalog?view=foundry\&preserve-view=true)** – Access the Foundry tool catalog (preview) with a public tool catalog and your own private catalogs, connecting over 1,400 tools in Microsoft Foundry.
* **[Enhanced Memory Capabilities](agents/concepts/what-is-memory?view=foundry\&preserve-view=true)** – Use memory to help your agent retain and recall contextual information across interactions. Memory maintains continuity, adapts to user needs, and delivers tailored experiences without requiring repeated input.
* **[Knowledge Integration](agents/concepts/what-is-foundry-iq?view=foundry\&preserve-view=true)** – Connect your agent to a Foundry IQ knowledge base to ground responses in enterprise or web content. This integration provides reliable, citation-backed answers for multi-turn conversations.
* **[Real-Time Observability](observability/how-to/how-to-monitor-agents-dashboard#set-up-continuous-evaluation-python-sdk)** – Monitor performance and governance using built-in metrics and model tracking tools.
* **Enhanced Enterprise Support** – Use open protocols in Foundry Agent Service with full authentication support in MCP and A2A tool, AI gateway integration, and Azure Policy integration.
* **Centralized AI asset management** - Observe, optimize, and manage 100% of your AI assets (agents, models, tools) in one place, the **Operate** section. Register agents from other clouds, get alerts when an agent or model requires your attention, and effectively manage your AI fleet health as that fleet scales.
* **Optimized Developer Experience** – Experience faster load times and dynamic prefetching for smooth development and deployment.
* **Streamlined Navigation** – Navigate efficiently with a redesigned interface that places key controls where you need them, improving workflow efficiency.

## Choosing a project

In the Foundry (new) portal, the project you're working with appears in the upper-left corner of most pages.

* If you see a long list of projects instead, select a project to begin. This brings you to the **Home** page with the project name in the upper-left corner.
* To switch to another recently used project, select the project name in the upper-left corner, then select the other project.
* To see all of your Foundry projects, select the project name in the upper-left corner, the select **View all projects**. Select the next project you want to work on.

## Find other resources

The Foundry (new) portal displays only the **default** project for each Foundry resource, not other resources or hub-based projects you might have created in Foundry (classic). If you created multiple projects under the same Foundry resource, you can identify which project is the default by checking the Microsoft Foundry (classic) portal. The default project is marked with (default) next to its name.

To find these other resources, select the project name in the upper-left corner, then select **View all resources**. A new browser tab opens the Foundry (classic) portal. [Switch to Microsoft Foundry (classic) documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/what-is-foundry?view=foundry-classic\&preserve-view=true) to work with these other resources in the Foundry (classic) portal.

## Microsoft Foundry API and SDKs

The [Microsoft Foundry API](https://learn.microsoft.com/en-us/rest/api/aifoundry/) is designed specifically for building agentic applications and provides a consistent contract for working across different model providers. The API is complemented by SDKs to make it easy to integrate AI capabilities into your applications. [SDK Client libraries](how-to/develop/sdk-overview) are available for:

* Python
* C#
* JavaScript/TypeScript (preview)
* Java (preview)

The [Microsoft Foundry for VS Code Extension](how-to/develop/get-started-projects-vs-code) helps you explore models and develop agents directly in your development environment.

## Pricing and billing

Microsoft Foundry is monetized through individual products customer access and consume in the platform, including API and models, complete AI toolchain, and responsible AI and enterprise grade production at scale products. Each product has its own billing model and price.

The platform is free to use and explore. Pricing occurs at deployment level.

Using Foundry also incurs cost associated with the underlying services. To learn more, read [Plan and manage costs for Foundry Tools](concepts/manage-costs).

## Region availability

Foundry is available in most regions where Foundry Tools are available. For more information, see [region support for Microsoft Foundry](reference/region-support).

## How to get access

You need an [Azure account](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn). Then sign in to [Microsoft Foundry](https://ai.azure.com?cid=learnDocs) and toggle the **Try the new Foundry** on.

## Related content

* [Quickstart: Get started with Microsoft Foundry](tutorials/quickstart-create-foundry-resources)

- [Create a project](how-to/create-projects)
- [Get started with an AI template](how-to/develop/ai-template-get-started)
- [What's new in Microsoft Foundry documentation?](whats-new-foundry)


In this quickstart, you create a [Microsoft Foundry](https://ai.azure.com) project and deploy a model. If you're managing a team, you also grant access to team members. After you complete these steps, you or your team can start building AI applications using the deployed model.

<Callout type="tip">
  This quickstart shows you how to create resources to build an agent with a basic setup. For more advanced scenarios that use your own resources, see [Set up your environment for agent development](../agents/environment-setup).
</Callout>

## Prerequisites

* An Azure account with an active subscription. If you don't have one, create a [free Azure account, which includes a free trial subscription](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).

* If you're creating the project for yourself:
  * Access to a role that allows you to create a Foundry resource, such as **Azure Account AI Owner** or **Azure AI Owner** on the subscription or resource group. For more information about permissions, see [Role-based access control for Microsoft Foundry](../concepts/rbac-foundry#permissions-for-each-built-in-role).

* If you're creating the project for a team:

  * Access to a role that allows you to complete role assignments, such as **Owner**. For more information about permissions, see [Role-based access control for Microsoft Foundry](../concepts/rbac-foundry#permissions-for-each-built-in-role).
  * A list of user email addresses or Microsoft Entra security group IDs for team members who need access.

Select your preferred method by using the following tabs:

<Tabs>
  <Tab title="Azure CLI">
    * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli).

    * Sign in to Azure:

      ```azurecli
      az login
      ```
  </Tab>

  <Tab title="Foundry portal">
    * Access to the [Microsoft Foundry portal](https://ai.azure.com).
  </Tab>
</Tabs>

## Create a project

Create a Foundry project to organize your work. The project contains models, agents, and other resources your team uses.

<Tabs>
  <Tab title="Azure CLI">
    1. Create a resource group or use an existing one. For example, create `my-foundry-rg` in `eastus`:

       ```azurecli
       az group create --name my-foundry-rg --location eastus
       ```

    2. Create the Foundry resource. For example, create `my-foundry-resource` in the `my-foundry-rg` resource group:

       ```azurecli
       az cognitiveservices account create \
           --name my-foundry-resource \
           --resource-group my-foundry-rg \
           --kind AIServices \
           --sku s0 \
           --location eastus \
          --allow-project-management
       ```

       The `--allow-project-management` flag enables project creation within this resource.

    3. Create a custom subdomain for the resource. The custom domain name must be globally unique. If `my-foundry-resource` is taken, try a more unique name.

       ```azurecli
       az cognitiveservices account update \
           --name my-foundry-resource \
           --resource-group my-foundry-rg \
           --custom-domain my-foundry-resource
       ```

    4. Create the project. For example, create `my-foundry-project` in the `my-foundry-resource`:

       ```azurecli
       az cognitiveservices account project create \
           --name my-foundry-resource \
           --resource-group my-foundry-rg \
           --project-name my-foundry-project \
           --location eastus
       ```

    5. Verify the project was created:

       ```azurecli
       az cognitiveservices account project show \
           --name my-foundry-resource \
           --resource-group my-foundry-rg \
           --project-name my-foundry-project
       ```

       The output displays the project properties, including its resource ID.

    Reference: [az cognitiveservices account](https://learn.microsoft.com/en-us/cli/azure/cognitiveservices/account)
  </Tab>

  <Tab title="Foundry portal">
    1. Go to [Microsoft Foundry](https://ai.azure.com).

    2. Sign in with your Azure account.

    3. Select the project name in the upper-left corner, and then select **Create new project**.

    4. Enter a project name, such as `my-foundry-project`.

    5. Select **Advanced options** to configure the resource group and location:

       * **Resource group**: Create a new resource group or select an existing one. If you create a new resource group, you can more easily manage the project and all its resources together.
       * **Location**: Select the region closest to your team.

    6. Select **Create project**.
  </Tab>
</Tabs>

## Deploy a model

Deploy a model that you can use. This example uses **gpt-4.1-mini**, but you can choose any available model.

<Tabs>
  <Tab title="Azure CLI">
    ```azurecli
    az cognitiveservices account deployment create \
        --name my-foundry-resource \
        --resource-group my-foundry-rg \
        --deployment-name gpt-4.1-mini \
        --model-name gpt-4.1-mini \
        --model-version "2025-04-14" \
        --model-format OpenAI \
        --sku-capacity 10 \
        --sku-name Standard
    ```

    Verify the deployment succeeded:

    ```azurecli
    az cognitiveservices account deployment show \
        --name my-foundry-resource \
        --resource-group my-foundry-rg \
        --deployment-name gpt-4.1-mini
    ```

    When the deployment is ready, the output shows `"provisioningState": "Succeeded"`.

    Reference: [az cognitiveservices account deployment](https://learn.microsoft.com/en-us/cli/azure/cognitiveservices/account/deployment)
  </Tab>

  <Tab title="Foundry portal">
    1. In your project, select **Discover** in the upper-right navigation.
    2. Select **Models**.
    3. Search for **gpt-4.1-mini**.
    4. Select **Deploy** > **Default settings** to add it to your project.
    5. Note the deployment name (for example, `gpt-4.1-mini`). Your team needs this name to use the model.
  </Tab>
</Tabs>

## Get your project connection details

You need the following information to connect to the project in other quickstarts and tutorials.

If you're administering this project for others, send them this information.

* Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs) by using your Azure account. Select your project to start building.
* Find your project endpoint on the welcome screen of the project. ![Screenshot of Microsoft Foundry Models welcome screen showing the endpoint URL and copy button.](https://learn.microsoft.com/azure/ai-foundry/default/media/quickstarts/project-endpoint.png)
* Get started with [Microsoft Foundry quickstart](../quickstarts/get-started-code).

## For administrators - grant access

If you're administering a team, assign the **Azure AI User** role to team members so they can use the project and deployed models. This role provides the minimum permissions needed to build and test AI applications.

<Tabs>
  <Tab title="Azure CLI">
    1. Get the project's resource ID:

       ```azurecli
       PROJECT_ID=$(az cognitiveservices account project show \
         --name my-foundry-resource \
         --resource-group my-foundry-rg \
         --project-name my-foundry-project \
         --query id -o tsv)
       ```

    2. Assign the **Azure AI User** role to a team member:

       ```azurecli
       az role assignment create \
           --role "Azure AI User" \
           --assignee "user@contoso.com" \
           --scope $PROJECT_ID
       ```

       To add a security group instead of an individual user:

       ```azurecli
       az role assignment create \
           --role "Azure AI User" \
           --assignee-object-id "<security-group-object-id>" \
           --assignee-principal-type Group \
           --scope $PROJECT_ID
       ```

    3. Verify the role assignment:

       ```azurecli
       az role assignment list \
           --scope $PROJECT_ID \
           --role "Azure AI User" \
           --output table
       ```

    Reference: [az role assignment](https://learn.microsoft.com/en-us/cli/azure/role/assignment)
  </Tab>

  <Tab title="Foundry portal">
    1. In the Foundry portal, select **Operate** in the upper-right navigation.
    2. Select **Admin** in the left pane.
    3. Select your project name in the table.
    4. Select **Add user** in the upper right.
    5. Enter the email address of the team member.
    6. Select **Add**.

    Repeat these steps for each team member or security group.

    <Callout type="tip">
      To add multiple users at once, use a Microsoft Entra security group instead of individual email addresses.
    </Callout>
  </Tab>
</Tabs>

### Verify team member access

Ask a team member to verify their access by signing in to [Microsoft Foundry](https://ai.azure.com), selecting the project from the project list, and confirming the deployed model appears under **Build** > **Models**.

If the team member can't access the project, verify that the role assignment completed successfully. Check that you used the correct email address or security group ID. Make sure the team member's Azure account is in the same Microsoft Entra tenant.

## Clean up resources

When you no longer want this project, delete the resource group to delete all resources associated with it.

<Tabs>
  <Tab title="Azure CLI">
    ```azurecli
    az group delete --name my-foundry-rg --yes --no-wait
    ```
  </Tab>

  <Tab title="Foundry portal">
    In the [Azure portal](https://portal.azure.com), find and select your resource group. Select **Delete** and confirm to delete the resource group and all its associated resources.
  </Tab>
</Tabs>

## Next step

<Callout type="nextstepaction">
  [Microsoft Foundry quickstart](../quickstarts/get-started-code)
</Callout>

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this quickstart, you use [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs) to interact with a Foundry model, create, and chat with an agent.

## Prerequisites

* A model deployed in Microsoft Foundry. If you don't have a model, first complete [Quickstart: Set up Microsoft Foundry resources](../tutorials/quickstart-create-foundry-resources).
* The required language runtimes, global tools, and Visual Studio Code extensions as described in [Prepare your development environment](../how-to/develop/install-cli-sdk).

## Set environment variables and get the code

Store [your project endpoint](../tutorials/quickstart-create-foundry-resources#get-your-project-connection-details) as an environment variable. Also set these values for use in your scripts.

```plaintext
PROJECT_ENDPOINT=<endpoint copied from welcome screen>
AGENT_NAME="MyAgent"
MODEL_DEPLOYMENT_NAME="gpt-4.1-mini"
```

<Tabs>
  <Tab title="Python">
    Follow along below or get the code:

    <Callout type="nextstepaction">
      [Get the code](https://github.com/azure-ai-foundry/foundry-samples/tree/main/samples/python/quickstart)
    </Callout>

    Sign in using the CLI `az login` command to authenticate before running your Python scripts.
  </Tab>

  <Tab title="C#">
    Follow along below or get the code:

    <Callout type="nextstepaction">
      [Get the code](https://github.com/azure-ai-foundry/foundry-samples/tree/main/samples/csharp/quickstart)
    </Callout>

    Sign in using the CLI `az login` command to authenticate before running your C# scripts.
  </Tab>

  <Tab title="TypeScript">
    Follow along below or get the code:

    <Callout type="nextstepaction">
      [Get the code](https://github.com/azure-ai-foundry/foundry-samples/tree/main/samples/typescript/quickstart/src)
    </Callout>

    Sign in using the CLI `az login` command to authenticate before running your TypeScript scripts.
  </Tab>

  <Tab title="Java">
    Follow along below or get the code:

    <Callout type="nextstepaction">
      [Get the code](https://github.com/azure-ai-foundry/foundry-samples/tree/main/samples/java/quickstart/src/main/java/com/microsoft/foundry/samples/)
    </Callout>

    Sign in using the CLI `az login` command to authenticate before running your Java scripts.
  </Tab>

  <Tab title="REST API">
    Follow along below or get the code:

    <Callout type="nextstepaction">
      [Get the code](https://github.com/azure-ai-foundry/foundry-samples/tree/main/samples/microsoft/REST/mslearn-resources/quickstart).
    </Callout>

    1. Sign in using the CLI `az login` command to authenticate before running the next command.

    2. Get a temporary access token. It will expire in 60-90 minutes, you'll need to refresh after that.

       ```azurecli
       az account get-access-token --scope https://ai.azure.com/.default
       ```

    3. Save the results as the environment variable `AZURE_AI_AUTH_TOKEN`.
  </Tab>

  <Tab title="Foundry portal">
    No code is necessary when using the Foundry portal.
  </Tab>
</Tabs>

## Install and authenticate

Make sure you install the correct preview/prerelease version of the packages as shown here.

<Tabs>
  <Tab title="Python">
    1. Install these packages, including the preview version of `azure-ai-projects`. This version uses the **Foundry projects (new) API** (preview).

       ```
       pip install azure-ai-projects --pre
       pip install openai azure-identity python-dotenv
       ```

    2. Sign in using the CLI `az login` command to authenticate before running your Python scripts.
  </Tab>

  <Tab title="C#">
    1. Install packages:

       Add NuGet packages using the .NET CLI in the integrated terminal: These packages use the **Foundry projects (new) API** (preview).

       ```bash
       dotnet add package Azure.AI.Projects --prerelease
       dotnet add package Azure.AI.Projects.OpenAI --prerelease
       dotnet add package Azure.Identity
       ```

    2. Sign in using the CLI `az login` command to authenticate before running your C# scripts.
  </Tab>

  <Tab title="TypeScript">
    1. Install these packages, including the preview version of `@azure/ai-projects`. This version uses the **Foundry projects (new) API** (preview).:

       ```bash
       npm install @azure/ai-projects@beta @azure/identity dotenv
       ```

    2. Sign in using the CLI `az login` command to authenticate before running your TypeScript scripts.
  </Tab>

  <Tab title="Java">
    1. Sign in using the CLI `az login` command to authenticate before running your Java scripts.
  </Tab>

  <Tab title="REST API">
    1. Sign in using the CLI `az login` command to authenticate before running the next command.

    2. Get a temporary access token. It will expire in 60-90 minutes, you'll need to refresh after that.

       ```azurecli
       az account get-access-token --scope https://ai.azure.com/.default
       ```

    3. Save the results as the environment variable `AZURE_AI_AUTH_TOKEN`.
  </Tab>

  <Tab title="Foundry portal">
    No installation is necessary to use the Foundry portal.
  </Tab>
</Tabs>

<Callout type="tip">
  Code uses **Azure AI Projects 2.x (preview)** and is incompatible with Azure AI Projects 1.x. [Switch to Foundry (classic) documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/quickstarts/get-started-code?view=foundry-classic\&preserve-view=true) for the Azure AI Projects 1.x GA version.
</Callout>

## Chat with a model

Interacting with a model is the basic building block of AI applications. Send an input and receive a response from the model:

<Tabs>
  <Tab title="Python">
    ```python
    import os
    from dotenv import load_dotenv
    from azure.identity import DefaultAzureCredential
    from azure.ai.projects import AIProjectClient

    load_dotenv()

    print(f"Using PROJECT_ENDPOINT: {os.environ['PROJECT_ENDPOINT']}")
    print(f"Using MODEL_DEPLOYMENT_NAME: {os.environ['MODEL_DEPLOYMENT_NAME']}")

    project_client = AIProjectClient(
        endpoint=os.environ["PROJECT_ENDPOINT"],
        credential=DefaultAzureCredential(),
    )

    openai_client = project_client.get_openai_client()

    response = openai_client.responses.create(
        model=os.environ["MODEL_DEPLOYMENT_NAME"],
        input="What is the size of France in square miles?",
    )
    print(f"Response output: {response.output_text}")
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    using Azure.AI.Projects;
    using Azure.AI.Projects.OpenAI;
    using Azure.Identity;
    using OpenAI;
    using OpenAI.Responses;

    #pragma warning disable OPENAI001

    string projectEndpoint  = Environment.GetEnvironmentVariable("PROJECT_ENDPOINT")
        ?? throw new InvalidOperationException("Missing environment variable 'PROJECT_ENDPOINT'");
    string modelDeploymentName  = Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME")
        ?? throw new InvalidOperationException("Missing environment variable 'MODEL_DEPLOYMENT_NAME'");

    AIProjectClient projectClient = new(new Uri(projectEndpoint ), new AzureCliCredential());

    ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForModel(modelDeploymentName);
    ResponseResult response = await responseClient.CreateResponseAsync("What is the size of France in square miles?");

    Console.WriteLine(response.GetOutputText());
    ```
  </Tab>

  <Tab title="TypeScript">
    ```typescript
    import { DefaultAzureCredential } from "@azure/identity";
    import { AIProjectClient } from "@azure/ai-projects";
    import "dotenv/config";

    const projectEndpoint = process.env["PROJECT_ENDPOINT"] || "<project endpoint>";
    const deploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

    async function main(): Promise<void> {
        const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
        const openAIClient = await project.getOpenAIClient();
        const response = await openAIClient.responses.create({
            model: deploymentName,
            input: "What is the size of France in square miles?",
        });
        console.log(`Response output: ${response.output_text}`);
    }

    main().catch(console.error);
    ```
  </Tab>

  <Tab title="Java">
    ```java
    package com.azure.ai.agents;

    import com.azure.core.util.Configuration;
    import com.azure.identity.DefaultAzureCredentialBuilder;
    import com.openai.models.responses.Response;
    import com.openai.models.responses.ResponseCreateParams;

    public class CreateResponse {
        public static void main(String[] args) {
            String endpoint = Configuration.getGlobalConfiguration().get("PROJECT_ENDPOINT");
            String model = Configuration.getGlobalConfiguration().get("MODEL_DEPLOYMENT_NAME");
            // Code sample for creating a response
            ResponsesClient responsesClient = new AgentsClientBuilder()
                    .credential(new DefaultAzureCredentialBuilder().build())
                    .endpoint(endpoint)
                    .serviceVersion(AgentsServiceVersion.V2025_11_15_PREVIEW)
                    .buildResponsesClient();

            ResponseCreateParams responseRequest = new ResponseCreateParams.Builder()
                    .input("Hello, how can you help me?")
                    .model(model)
                    .build();

            Response response = responsesClient.getResponseService().create(responseRequest);

            System.out.println("Response ID: " + response.id());
            System.out.println("Response Model: " + response.model());
            System.out.println("Response Created At: " + response.createdAt());
            System.out.println("Response Output: " + response.output());
        }
    }
    ```
  </Tab>

  <Tab title="REST API">
    Replace `YOUR-FOUNDRY-RESOURCE-NAME` with your values:

    ```console
    curl -X POST https://YOUR-FOUNDRY-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR-PROJECT-NAME/openai/responses?api-version=2025-11-15-preview \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_AI_AUTH_TOKEN" \
    -d '{
            "model": "gpt-4.1-mini",
            "input": "What is the size of France in square miles?"
    }'
    ```
  </Tab>

  <Tab title="Foundry portal">
    1. After the model deploys, you're automatically moved from **Home** to the **Build** section. Your new model is selected and ready for you to try out.

    2. Start chatting with your model, for example, "Write me a poem about flowers."
  </Tab>
</Tabs>

<Callout type="tip">
  Code uses **Azure AI Projects 2.x (preview)** and is incompatible with Azure AI Projects 1.x. [Switch to Foundry (classic) documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/quickstarts/get-started-code?view=foundry-classic\&preserve-view=true) for the Azure AI Projects 1.x GA version.
</Callout>

## Create an agent

Create an agent using your deployed model.

An agent defines core behavior. Once created, it ensures consistent responses in user interactions without repeating instructions each time. You can update or delete agents anytime.

<Tabs>
  <Tab title="Python">
    ```python
    import os
    from dotenv import load_dotenv
    from azure.identity import DefaultAzureCredential
    from azure.ai.projects import AIProjectClient
    from azure.ai.projects.models import PromptAgentDefinition

    load_dotenv()

    project_client = AIProjectClient(
        endpoint=os.environ["PROJECT_ENDPOINT"],
        credential=DefaultAzureCredential(),
    )

    agent = project_client.agents.create_version(
        agent_name=os.environ["AGENT_NAME"],
        definition=PromptAgentDefinition(
            model=os.environ["MODEL_DEPLOYMENT_NAME"],
            instructions="You are a helpful assistant that answers general questions",
        ),
    )
    print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    using Azure.AI.Projects;
    using Azure.AI.Projects.OpenAI;
    using Azure.Identity;

    string projectEndpoint = Environment.GetEnvironmentVariable("PROJECT_ENDPOINT")
        ?? throw new InvalidOperationException("Missing environment variable 'PROJECT_ENDPOINT'");
    string modelDeploymentName = Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME")
        ?? throw new InvalidOperationException("Missing environment variable 'MODEL_DEPLOYMENT_NAME'");
    string agentName = Environment.GetEnvironmentVariable("AGENT_NAME")
        ?? throw new InvalidOperationException("Missing environment variable 'AGENT_NAME'");

    AIProjectClient projectClient = new(new Uri(projectEndpoint), new AzureCliCredential());

    AgentDefinition agentDefinition = new PromptAgentDefinition(modelDeploymentName)
    {
        Instructions = "You are a helpful assistant that answers general questions",
    };

    AgentVersion newAgentVersion = projectClient.Agents.CreateAgentVersion(
        agentName,
        options: new(agentDefinition));

    List<AgentVersion> agentVersions = projectClient.Agents.GetAgentVersions(agentName);
    foreach (AgentVersion agentVersion in agentVersions)
    {
        Console.WriteLine($"Agent: {agentVersion.Id}, Name: {agentVersion.Name}, Version: {agentVersion.Version}");
    }
    ```
  </Tab>

  <Tab title="TypeScript">
    ```typescript
    import { DefaultAzureCredential } from "@azure/identity";
    import { AIProjectClient } from "@azure/ai-projects";
    import "dotenv/config";

    const projectEndpoint = process.env["PROJECT_ENDPOINT"] || "<project endpoint>";
    const deploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

    async function main(): Promise<void> {
        const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
        const agent = await project.agents.createVersion("my-agent-basic", {
            kind: "prompt",
            model: deploymentName,
            instructions: "You are a helpful assistant that answers general questions",
      });
      console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);
    }

    main().catch(console.error);
    ```
  </Tab>

  <Tab title="Java">
    ```java
    package com.azure.ai.agents;

    import com.azure.ai.agents.models.AgentVersionDetails;
    import com.azure.ai.agents.models.PromptAgentDefinition;
    import com.azure.core.util.Configuration;
    import com.azure.identity.DefaultAzureCredentialBuilder;

    public class CreateAgent {
        public static void main(String[] args) {
            String endpoint = Configuration.getGlobalConfiguration().get("PROJECT_ENDPOINT");
            String model = Configuration.getGlobalConfiguration().get("MODEL_DEPLOYMENT_NAME");
            // Code sample for creating an agent
            AgentsClient agentsClient = new AgentsClientBuilder()
                    .credential(new DefaultAzureCredentialBuilder().build())
                    .endpoint(endpoint)
                    .buildAgentsClient();

            PromptAgentDefinition request = new PromptAgentDefinition(model);
            AgentVersionDetails agent = agentsClient.createAgentVersion("MyAgent", request);

            System.out.println("Agent ID: " + agent.getId());
            System.out.println("Agent Name: " + agent.getName());
            System.out.println("Agent Version: " + agent.getVersion());
        }
    }
    ```
  </Tab>

  <Tab title="REST API">
    Replace `YOUR-FOUNDRY-RESOURCE-NAME` with your values:

    ```console
    curl -X POST https://YOUR-FOUNDRY-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR-PROJECT-NAME/agents?api-version=2025-11-15-preview \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_AI_AUTH_TOKEN" \
      -d '{
            "name": "MyAgent",
            "definition": {
                "kind": "prompt",
                "model": "gpt-4.1-mini",
                "instructions": "You are a helpful assistant that answers general questions"
            }
        }'
    ```
  </Tab>

  <Tab title="Foundry portal">
    Now create an agent and interact with it.

    1. Still in the **Build** section, select **Agents** in the left pane.
    2. Select **Create agent** and give it a name.
  </Tab>
</Tabs>

<Callout type="tip">
  Code uses **Azure AI Projects 2.x (preview)** and is incompatible with Azure AI Projects 1.x. [Switch to Foundry (classic) documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/quickstarts/get-started-code?view=foundry-classic\&preserve-view=true) for the Azure AI Projects 1.x GA version.
</Callout>

## Chat with an agent

Use the previously created agent named "MyAgent" to interact by asking a question and a related follow-up. The conversation maintains history across these interactions.

<Tabs>
  <Tab title="Python">
    ```python
    import os
    from dotenv import load_dotenv
    from azure.identity import DefaultAzureCredential
    from azure.ai.projects import AIProjectClient

    load_dotenv()

    project_client = AIProjectClient(
        endpoint=os.environ["PROJECT_ENDPOINT"],
        credential=DefaultAzureCredential(),
    )

    agent_name = os.environ["AGENT_NAME"]
    openai_client = project_client.get_openai_client()

    # Optional Step: Create a conversation to use with the agent
    conversation = openai_client.conversations.create()
    print(f"Created conversation (id: {conversation.id})")

    # Chat with the agent to answer questions
    response = openai_client.responses.create(
        conversation=conversation.id, #Optional conversation context for multi-turn
        extra_body={"agent": {"name": agent_name, "type": "agent_reference"}},
        input="What is the size of France in square miles?",
    )
    print(f"Response output: {response.output_text}")

    # Optional Step: Ask a follow-up question in the same conversation
    response = openai_client.responses.create(
        conversation=conversation.id,
        extra_body={"agent": {"name": agent_name, "type": "agent_reference"}},
        input="And what is the capital city?",
    )
    print(f"Response output: {response.output_text}")
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    using Azure.AI.Projects;
    using Azure.AI.Projects.OpenAI;
    using Azure.Identity;
    using OpenAI.Responses;

    #pragma warning disable OPENAI001

    string projectEndpoint = Environment.GetEnvironmentVariable("PROJECT_ENDPOINT")
        ?? throw new InvalidOperationException("Missing environment variable 'PROJECT_ENDPOINT'");
    string modelDeploymentName = Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME")
        ?? throw new InvalidOperationException("Missing environment variable 'MODEL_DEPLOYMENT_NAME'");
    string agentName = Environment.GetEnvironmentVariable("AGENT_NAME")
        ?? throw new InvalidOperationException("Missing environment variable 'AGENT_NAME'");

    AIProjectClient projectClient = new(new Uri(projectEndpoint), new AzureCliCredential());

    // Optional Step: Create a conversation to use with the agent
    ProjectConversation conversation = projectClient.OpenAI.Conversations.CreateProjectConversation();

    ProjectResponsesClient responsesClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(
        defaultAgent: agentName,
        defaultConversationId: conversation.Id);

    // Chat with the agent to answer questions
    ResponseResult response = responsesClient.CreateResponse("What is the size of France in square miles?");
    Console.WriteLine(response.GetOutputText());

    // Optional Step: Ask a follow-up question in the same conversation
    response = responsesClient.CreateResponse("And what is the capital city?");
    Console.WriteLine(response.GetOutputText());
    ```
  </Tab>

  <Tab title="TypeScript">
    ```typescript
    import { DefaultAzureCredential } from "@azure/identity";
    import { AIProjectClient } from "@azure/ai-projects";
    import "dotenv/config";

    const projectEndpoint = process.env["PROJECT_ENDPOINT"] || "<project endpoint>";
    const deploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

    async function main(): Promise<void> {
        const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
        const openAIClient = await project.getOpenAIClient();

        // Create agent
        console.log("Creating agent...");
        const agent = await project.agents.createVersion("my-agent-basic", {
            kind: "prompt",
            model: deploymentName,
            instructions: "You are a helpful assistant that answers general questions",
        });
        console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

        // Create conversation with initial user message
        // You can save the conversation ID to database to retrieve later
        console.log("\nCreating conversation with initial user message...");
        const conversation = await openAIClient.conversations.create({
            items: [
                { type: "message", role: "user", content: "What is the size of France in square miles?" },
            ],
        });
        console.log(`Created conversation with initial user message (id: ${conversation.id})`);

        // Generate response using the agent
        console.log("\nGenerating response...");
        const response = await openAIClient.responses.create(
            {
                conversation: conversation.id,
            },
            {
                body: { agent: { name: agent.name, type: "agent_reference" } },
            },
        );
        console.log(`Response output: ${response.output_text}`);

         // Clean up
        console.log("\nCleaning up resources...");
        await openAIClient.conversations.delete(conversation.id);
        console.log("Conversation deleted");

        await project.agents.deleteVersion(agent.name, agent.version);
        console.log("Agent deleted");
    }

    main().catch(console.error);
    ```
  </Tab>

  <Tab title="Java">
    ```java
    package com.azure.ai.agents;

    import com.azure.ai.agents.models.AgentReference;
    import com.azure.ai.agents.models.AgentVersionDetails;
    import com.azure.ai.agents.models.PromptAgentDefinition;
    import com.azure.identity.AuthenticationUtil;
    import com.azure.identity.DefaultAzureCredentialBuilder;
    import com.openai.azure.AzureOpenAIServiceVersion;
    import com.openai.azure.AzureUrlPathMode;
    import com.openai.client.OpenAIClient;
    import com.openai.client.okhttp.OpenAIOkHttpClient;
    import com.openai.credential.BearerTokenCredential;
    import com.openai.models.conversations.Conversation;
    import com.openai.models.conversations.items.ItemCreateParams;
    import com.openai.models.responses.EasyInputMessage;
    import com.openai.models.responses.Response;
    import com.openai.models.responses.ResponseCreateParams;

    public class ChatWithAgent {
        public static void main(String[] args) {
            String endpoint = Configuration.getGlobalConfiguration().get("AZURE_AGENTS_ENDPOINT");
            String agentName = "MyAgent";

            AgentsClient agentsClient = new AgentsClientBuilder()
                    .credential(new DefaultAzureCredentialBuilder().build())
                    .endpoint(endpoint)
                    .buildAgentsClient();

            AgentDetails agent = agentsClient.getAgent(agentName);

            Conversation conversation = conversationsClient.getConversationService().create();
            conversationsClient.getConversationService().items().create(
                ItemCreateParams.builder()
                    .conversationId(conversation.id())
                    .addItem(EasyInputMessage.builder()
                        .role(EasyInputMessage.Role.SYSTEM)
                        .content("You are a helpful assistant that speaks like a pirate.")
                        .build()
                    ).addItem(EasyInputMessage.builder()
                        .role(EasyInputMessage.Role.USER)
                        .content("Hello, agent!")
                        .build()
                ).build()
            );

            AgentReference agentReference = new AgentReference(agent.getName()).setVersion(agent.getVersion());
            Response response = responsesClient.createWithAgentConversation(agentReference, conversation.id());

            OpenAIClient client = OpenAIOkHttpClient.builder()
                .baseUrl(endpoint.endsWith("/") ? endpoint + "openai" : endpoint + "/openai")
                .azureUrlPathMode(AzureUrlPathMode.UNIFIED)
                .credential(BearerTokenCredential.create(AuthenticationUtil.getBearerTokenSupplier(
                        new DefaultAzureCredentialBuilder().build(), "https://ai.azure.com/.default")))
                .azureServiceVersion(AzureOpenAIServiceVersion.fromString("2025-11-15-preview"))
                .build();

            ResponseCreateParams responseRequest = new ResponseCreateParams.Builder()
                .input("Hello, how can you help me?")
                .model(model)
                .build();

            Response result = client.responses().create(responseRequest);
        }
    }
    ```
  </Tab>

  <Tab title="REST API">
    Replace `YOUR-FOUNDRY-RESOURCE-NAME` with your values:

    ```console
    # Optional Step: Create a conversation to use with the agent
    curl -X POST https://YOUR-FOUNDRY-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR-PROJECT-NAME/openai/conversations?api-version=2025-11-15-preview \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_AI_AUTH_TOKEN" \
      -d '{}'
    # Lets say Conversation ID created is conv_123456789. Use this in the next step

    #Chat with the agent to answer questions
    curl -X POST https://YOUR-FOUNDRY-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR-PROJECT-NAME/openai/responses?api-version=2025-11-15-preview \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_AI_AUTH_TOKEN" \
      -d '{
            "agent": {"type": "agent_reference", "name": "MyAgent"},
            "conversation" : "<YOUR_CONVERSATION_ID>",
            "input" : "What is the size of France in square miles?"
        }'

    #Optional Step: Ask a follow-up question in the same conversation
    curl -X POST https://YOUR-FOUNDRY-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR-PROJECT-NAME/openai/responses?api-version=2025-11-15-preview \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer $AZURE_AI_AUTH_TOKEN" \
      -d '{
            "agent": {"type": "agent_reference", "name": "MyAgent"},
            "conversation" : "<YOUR_CONVERSATION_ID>",
            "input" : "And what is the capital city?"
        }'
    ```
  </Tab>

  <Tab title="Foundry portal">
    Interact with your agent.

    1. Add instructions, such as, "You are a helpful writing assistant."
    2. Start chatting with your agent, for example, "Write a poem about the sun."
    3. Follow up with "How about a haiku?"
  </Tab>
</Tabs>

<Callout type="tip">
  Code uses **Azure AI Projects 2.x (preview)** and is incompatible with Azure AI Projects 1.x. [Switch to Foundry (classic) documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/quickstarts/get-started-code?view=foundry-classic\&preserve-view=true) for the Azure AI Projects 1.x GA version.
</Callout>

## Clean up resources

If you no longer need any of the resources you created, delete the resource group associated with your project.

* In the [Azure portal](https://portal.azure.com), select the resource group, and then select **Delete**. Confirm that you want to delete the resource group.

## Next step

<Callout type="nextstepaction">
  [Idea to prototype - Build and evaluate an enterprise agent](../tutorials/developer-journey-idea-to-prototype)
</Callout>

This tutorial covers the first stage of the Microsoft Foundry developer journey: from an initial idea to a working prototype. You build a **modern workplace assistant** that combines internal company knowledge with external technical guidance by using the Microsoft Foundry SDK.

**Business scenario**: Create an AI assistant that helps employees by combining:

* **Company policies** (from SharePoint documents)
* **Technical implementation guidance** (from Microsoft Learn via MCP)
* **Complete solutions** (combining both sources for business implementation)
* **Batch evaluation** to validate agent performance on realistic business scenarios

**Tutorial outcome**: By the end you have a running Modern Workplace Assistant that can answer policy, technical, and combined implementation questions; a repeatable batch evaluation script; and clear extension points (other tools, multi‑agent patterns, richer evaluation).

**You will:**

* Build a Modern Workplace Assistant with SharePoint and MCP integration.
* Demonstrate real business scenarios combining internal and external knowledge.
* Implement robust error handling and graceful degradation.
* Create evaluation framework for business-focused testing.
* Prepare foundation for governance and production deployment.

This minimal sample demonstrates enterprise-ready patterns with realistic business scenarios.

<Callout type="important">
  Code in this article uses packages that are currently in preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

## Prerequisites

* Azure subscription and CLI authentication (`az login`)

* Azure CLI 2.67.0 or later (check with `az version`)

* A Foundry **project** with a deployed model (for example, `gpt-4o-mini`). If you don't have one: [Create a project](../how-to/create-projects) and then deploy a model (see model overview: [Model catalog](../concepts/foundry-models-overview)).

* Python 3.10 or later

* .NET SDK (for the C# sample)

* SharePoint connection configured in your project ([SharePoint tool documentation](../agents/how-to/tools/sharepoint))

  <Callout type="note">
    To configure your Foundry project for SharePoint connectivity, see the [SharePoint tool documentation](../agents/how-to/tools/sharepoint).
  </Callout>

* (Optional) Git installed for cloning the sample repository

## Step 1: Get the sample code

Instead of navigating a large repository tree, use one of these approaches:

#### Option A (clone entire samples repo)

<Callout type="tip">
  Code uses **Azure AI Projects 2.x (preview)** and is incompatible with Azure AI Projects 1.x.
</Callout>

<Tabs>
  <Tab title="Python">
    ```bash
    git clone --depth 1 https://github.com/microsoft-foundry/foundry-samples.git
    cd foundry-samples/samples/python/enterprise-agent-tutorial/1-idea-to-prototype
    ```
  </Tab>

  <Tab title="C#">
    ```bash
    git clone --depth 1 https://github.com/azure-ai-foundry/foundry-samples.git
    cd foundry-samples/samples/csharp/enterprise-agent-tutorial/1-idea-to-prototype
    ```
  </Tab>
</Tabs>

#### Option B (sparse checkout only this tutorial - reduced download)

<Tabs>
  <Tab title="Python">
    ```bash
    git clone --no-checkout https://github.com/microsoft-foundry/foundry-samples.git
    cd foundry-samples
    git sparse-checkout init --cone
    git sparse-checkout set samples/python/enterprise-agent-tutorial/1-idea-to-prototype
    git checkout
    cd samples/python/enterprise-agent-tutorial/1-idea-to-prototype
    ```
  </Tab>

  <Tab title="C#">
    ```bash
    git clone --no-checkout https://github.com/azure-ai-foundry/foundry-samples.git
    cd foundry-samples
    git sparse-checkout init --cone
    git sparse-checkout set samples/csharp/enterprise-agent-tutorial/1-idea-to-prototype
    git checkout
    cd samples/csharp/enterprise-agent-tutorial/1-idea-to-prototype
    ```
  </Tab>
</Tabs>

#### Option C (Download ZIP of repository)

Download the repository ZIP, extract it to your local environment, and go to the tutorial folder.

<Callout type="important">
  For production adoption, use a standalone repository. This tutorial uses the shared samples repo. Sparse checkout minimizes local noise.
</Callout>

<Tabs>
  <Tab title="Python">
    <Callout type="nextstepaction">
      [Download the Python code now](https://github.com/microsoft-foundry/foundry-samples/tree/main/samples/python/enterprise-agent-tutorial/1-idea-to-prototype)
    </Callout>

    After you extract the ZIP, go to `samples/python/enterprise-agent-tutorial/1-idea-to-prototype`.
  </Tab>

  <Tab title="C#">
    <Callout type="nextstepaction">
      [Download the C# code now](https://github.com/azure-ai-foundry/foundry-samples/tree/main/samples/csharp/enterprise-agent-tutorial/1-idea-to-prototype)
    </Callout>

    After you extract the ZIP, go to `samples/csharp/enterprise-agent-tutorial/1-idea-to-prototype`.
  </Tab>
</Tabs>

The minimal structure contains only essential files:

<Tabs>
  <Tab title="Python">
    ```text
    enterprise-agent-tutorial/
    └── 1-idea-to-prototype/
       ├── .env                             # Create this file (local environment variables)
       ├── .gitkeep
       ├── evaluate.py                      # Business evaluation framework
       ├── evaluation_results.json
       ├── main.py                          # Modern Workplace Assistant
       ├── questions.jsonl                  # Business test scenarios (4 questions)
       ├── requirements.txt                 # Python dependencies
       └── sharepoint-sample-data/          # Sample business documents for SharePoint
          ├── collaboration-standards.docx
          ├── data-governance-policy.docx
          ├── remote-work-policy.docx
          └── security-guidelines.docx
    ```
  </Tab>

  <Tab title="C#">
    ```text
    enterprise-agent-tutorial/
    └── 1-idea-to-prototype/
       ├── ModernWorkplaceAssistant/        # Modern Workplace Assistant
       │   ├── Program.cs                   # Agent implementation with SharePoint + MCP
       │   ├── ModernWorkplaceAssistant.csproj
       │   └── .env                         # Environment variables (create this)
       ├── Evaluate/                        # Cloud evaluation framework
       │   ├── Program.cs                   # Cloud evaluation with built-in evaluators
       │   ├── Evaluate.csproj
       │   └── evaluation_results.json      # Example output (generated)
       ├── questions.jsonl                  # Business test scenarios
       └── README.md                        # Complete setup instructions
    ```
  </Tab>
</Tabs>

## Step 2: Run the sample immediately

Start by running the agent so you see working functionality before diving into implementation details.

### Environment setup and virtual environment

1. Install the required language runtimes, global tools, and VS Code extensions as described in [Prepare your development environment](../how-to/develop/install-cli-sdk).

2. Verify that your `requirements.txt` uses these published package versions:

   ```text
   azure-ai-projects==2.0.0b3
   azure-identity
   python-dotenv
   openai
   ```

3. Install dependencies:

   <Tabs>
     <Tab title="Python">
       ```bash
       python -m pip install -r requirements.txt
       ```
     </Tab>

     <Tab title="C#">
       ```bash
       cd ModernWorkplaceAssistant
       dotnet restore

       cd ../Evaluate
       dotnet restore
       ```
     </Tab>
   </Tabs>

4. Find your project endpoint on the welcome screen of the project.

   ![Screenshot of Microsoft Foundry Models welcome screen showing the endpoint URL and copy button.](https://learn.microsoft.com/azure/ai-foundry/default/media/quickstarts/project-endpoint.png)

5. Configure `.env`.

   Set the environment values required for your language.

<Tabs>
  <Tab title="Python">
    Copy `.env.template` to `.env`.
  </Tab>

  <Tab title="C#">
    Create a `.env` file in the `ModernWorkplaceAssistant` directory.
  </Tab>
</Tabs>

<Tabs>
  <Tab title="Python">
    ```dotenv
    # Foundry configuration
    PROJECT_ENDPOINT=https://<your-project>.aiservices.azure.com
    MODEL_DEPLOYMENT_NAME=gpt-4o-mini

    # The Microsoft Learn MCP Server (optional)
    MCP_SERVER_URL=https://learn.microsoft.com/api/mcp

    # SharePoint integration (optional - requires connection name)
    SHAREPOINT_CONNECTION_NAME=<your-sharepoint-connection-name>
    ```
  </Tab>

  <Tab title="C#">
    ```dotenv
    # Foundry configuration
    PROJECT_ENDPOINT=https://<your-project>.aiservices.azure.com
    MODEL_DEPLOYMENT_NAME=gpt-4o-mini

    # SharePoint integration (optional - requires connection name)
    SHAREPOINT_CONNECTION_NAME=<your-sharepoint-connection-name>

    # The Microsoft Learn MCP Server (optional)
    MCP_SERVER_URL=https://learn.microsoft.com/api/mcp
    ```
  </Tab>
</Tabs>

<Callout type="tip">
  To get your **tenant ID**, run:

  ```bash
  # Get tenant ID
  az account show --query tenantId -o tsv
  ```

  To get your **project endpoint**, open your project in the [Foundry portal](https://ai.azure.com) and copy the value shown there.
</Callout>

### Run agent and evaluation

<Tabs>
  <Tab title="Python">
    ```bash
    python main.py
    python evaluate.py
    ```
  </Tab>

  <Tab title="C#">
    ```bash
    cd ModernWorkplaceAssistant
    dotnet restore
    dotnet run

    cd ../Evaluate
    dotnet restore
    dotnet run
    ```
  </Tab>
</Tabs>

### Expected output (agent first run)

Successful run with SharePoint:

```text
🤖 Creating Modern Workplace Assistant...
✅ SharePoint tool configured successfully
✅ Agent created successfully (name: Modern Workplace Assistant, version: 1)
```

Graceful degradation without SharePoint:

```text
📁 SharePoint integration skipped (SHAREPOINT_CONNECTION_NAME not set)
✅ Agent created successfully (name: Modern Workplace Assistant, version: 1)
```

Now that you have a working agent, the next sections explain how it works. You don't need to take any action while reading these sections—they're for explanation.

## Step 3: Set up sample SharePoint business documents

1. Go to your SharePoint site (configured in the connection).

2. Create document library "Company Policies" (or use existing "Documents").

3. Upload the four sample Word documents provided in the `sharepoint-sample-data` folder:

   * `remote-work-policy.docx`
   * `security-guidelines.docx`
   * `collaboration-standards.docx`
   * `data-governance-policy.docx`

### Sample structure

```text
📁 Company Policies/
├── remote-work-policy.docx      # VPN, MFA, device requirements
├── security-guidelines.docx     # Azure security standards
├── collaboration-standards.docx # Teams, SharePoint usage
└── data-governance-policy.docx  # Data classification, retention
```

## Step 4: Understand the assistant implementation

This section explains the core code in `main.py` (Python) or `ModernWorkplaceAssistant/Program.cs` (C#). You already ran the agent. This section is conceptual and requires no changes. After reading it, you can:

* Add new internal and external data tools.
* Extend dynamic instructions.
* Introduce multi-agent orchestration.
* Enhance observability and diagnostics.

The code breaks down into the following main sections, ordered as they appear in the full sample code:

1. [Configure imports and authentication](#imports-and-authentication-setup)
2. [Configure authentication to Azure](#configure-authentication-in-azure)
3. [Configure the SharePoint tool](#create-the-sharepoint-tool-for-the-agent)
4. [Configure MCP tool](#create-the-mcp-tool-for-the-agent)
5. [Create the agent and connect the tools](#create-the-agent-and-connect-the-tools)
6. [Converse with the agent](#converse-with-the-agent)

<Callout type="important">
  Code in this article uses packages that are currently in preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

### Imports and authentication setup

The code uses several client libraries from the Microsoft Foundry SDK to create a robust enterprise agent.

<Tabs>
  <Tab title="Python">
    ```python
    import os
    import time
    from azure.ai.projects import AIProjectClient
    from azure.ai.projects.models import (
        PromptAgentDefinition,
        SharepointPreviewTool,
        SharepointGroundingToolParameters,
        ToolProjectConnection,
        MCPTool,
    )
    from azure.identity import DefaultAzureCredential
    from dotenv import load_dotenv
    from openai.types.responses.response_input_param import (
        McpApprovalResponse,
    )
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    using System;
    using System.ClientModel;
    using System.Collections.Generic;
    using System.IO;
    using System.Linq;
    using System.Threading.Tasks;
    using Azure.AI.Projects;
    using Azure.AI.Projects.OpenAI;
    using Azure.Identity;
    using DotNetEnv;
    using OpenAI.Responses;
    ```
  </Tab>
</Tabs>

### Configure authentication in Azure

Before you create your agent, set up authentication to the Foundry.

<Tabs>
  <Tab title="Python">
    ```python
    with (
        DefaultAzureCredential() as credential,
        AIProjectClient(endpoint=endpoint, credential=credential) as project_client,
        project_client.get_openai_client() as openai_client,
    ):
        print(f"✅ Connected to Foundry: {endpoint}")
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    var credential = new DefaultAzureCredential();

    projectClient = new AIProjectClient(new Uri(projectEndpoint), credential);
    Console.WriteLine($"✅ Connected to Azure AI Foundry: {projectEndpoint}");
    ```
  </Tab>
</Tabs>

### Create the SharePoint tool for the agent

The agent uses SharePoint and can access company policy and procedure documents stored there. Set up the connection to SharePoint in your code.

<Tabs>
  <Tab title="Python">
    ```python
    sharepoint_connection_id = os.environ.get("SHAREPOINT_CONNECTION_ID")
    sharepoint_tool = None

    if sharepoint_connection_id:
        print("📁 Configuring SharePoint integration...")
        print(f"   Connection ID: {sharepoint_connection_id}")

        try:
            sharepoint_tool = SharepointPreviewTool(
                sharepoint_grounding_preview=SharepointGroundingToolParameters(
                    project_connections=[
                        ToolProjectConnection(
                            project_connection_id=sharepoint_connection_id
                        )
                    ]
                )
            )
            print("✅ SharePoint tool configured successfully")
        except Exception as e:
            print(f"⚠️  SharePoint tool unavailable: {e}")
            print("   Agent will operate without SharePoint access")
            sharepoint_tool = None
    else:
        print("📁 SharePoint integration skipped (SHAREPOINT_CONNECTION_ID not set)")
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    // Resolve connection name to connection ID via the Connections API
    AIProjectConnection sharepointConnection = await projectClient.Connections.GetConnectionAsync(
        sharePointConnectionName, includeCredentials: false);

    SharePointGroundingToolOptions sharepointToolOption = new()
    {
        ProjectConnections = { new ToolProjectConnection(projectConnectionId: sharepointConnection.Id) }
    };
    sharepointTool = new SharepointAgentTool(sharepointToolOption);
    Console.WriteLine($"✅ SharePoint tool configured successfully");
    ```
  </Tab>
</Tabs>

### Create the MCP tool for the agent

<Tabs>
  <Tab title="Python">
    ```python
    mcp_server_url = os.environ.get("MCP_SERVER_URL")
    mcp_tool = None

    if mcp_server_url:
        print("📚 Configuring Microsoft Learn MCP integration...")
        print(f"   Server URL: {mcp_server_url}")

        try:
            mcp_tool = MCPTool(
                server_url=mcp_server_url,
                server_label="Microsoft_Learn_Documentation",
                require_approval="always",
            )
            print("✅ MCP tool configured successfully")
        except Exception as e:
            print(f"⚠️  MCP tool unavailable: {e}")
            print("   Agent will operate without Microsoft Learn access")
            mcp_tool = None
    else:
        print("📚 MCP integration skipped (MCP_SERVER_URL not set)")
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    // MCP (Model Context Protocol) enables agents to access external data sources
    // like Microsoft Learn documentation. The approval flow is handled in ChatWithAssistantAsync.
    McpTool? mcpTool = null;

    if (!string.IsNullOrEmpty(mcpServerUrl))
    {
        Console.WriteLine($"📚 Configuring Microsoft Learn MCP integration...");
        Console.WriteLine($"   Server URL: {mcpServerUrl}");

        try
        {
            // Create MCP tool for Microsoft Learn documentation access
            // server_label must match pattern: ^[a-zA-Z0-9_]+$ (alphanumeric and underscores only)
            mcpTool = new McpTool("Microsoft_Learn_Documentation", new Uri(mcpServerUrl));
            Console.WriteLine($"✅ MCP tool configured successfully");
        }
        catch (Exception ex)
        {
            Console.WriteLine($"⚠️  MCP tool unavailable: {ex.Message}");
            Console.WriteLine($"   Agent will operate without Microsoft Learn access");
        }
    }
    else
    {
        Console.WriteLine($"📚 MCP integration skipped (MCP_SERVER_URL not set)");
    }
    ```
  </Tab>
</Tabs>

### Create the agent and connect the tools

Create the agent and connect the SharePoint and MCP tools.

<Tabs>
  <Tab title="Python">
    ```python
    print(f"🛠️  Creating agent with model: {os.environ['MODEL_DEPLOYMENT_NAME']}")

    tools = []
    if sharepoint_tool:
        tools.append(sharepoint_tool)
        print("   ✓ SharePoint tool added")
    if mcp_tool:
        tools.append(mcp_tool)
        print("   ✓ MCP tool added")

    print(f"   Total tools: {len(tools)}")

    agent = project_client.agents.create_version(
        agent_name="Modern Workplace Assistant",
        definition=PromptAgentDefinition(
            model=os.environ["MODEL_DEPLOYMENT_NAME"],
            instructions=instructions,
            tools=tools if tools else None,
        ),
    )

    print(f"✅ Agent created successfully (name: {agent.name}, version: {agent.version})")
    return agent
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    // Create the agent using the v2 SDK with PromptAgentDefinition
    Console.WriteLine($"🛠️  Creating agent with model: {modelDeploymentName}");

    var agentDefinition = new PromptAgentDefinition(modelDeploymentName)
    {
        Instructions = instructions
    };

    // Add tools to the agent definition
    if (sharepointTool != null)
    {
        agentDefinition.Tools.Add(sharepointTool);
        Console.WriteLine($"   ✓ SharePoint tool added");
    }

    if (mcpTool != null)
    {
        agentDefinition.Tools.Add(mcpTool);
        Console.WriteLine($"   ✓ MCP tool added");
    }

    Console.WriteLine($"   Total tools: {agentDefinition.Tools.Count}");

    // Create agent version
    AgentVersion agentVersion = await projectClient.Agents.CreateAgentVersionAsync(
        agentName: agentName,
        options: new(agentDefinition));

    // Create a response client bound to this agent for conversations
    responseClient = projectClient.OpenAI
        .GetProjectResponsesClientForAgent(agentVersion);

    Console.WriteLine($"✅ Agent created successfully: {agentVersion.Name} (version {agentVersion.Version})");
    return agentVersion;
    ```
  </Tab>
</Tabs>

### Converse with the agent

Finally, implement an interactive loop to converse with the agent.

<Tabs>
  <Tab title="Python">
    ```python
    print("🤖 AGENT RESPONSE:")
    response, status = create_agent_response(agent, scenario["question"], openai_client)
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    Console.WriteLine("🤖 ASSISTANT RESPONSE:");
    var (response, status) = await ChatWithAssistantAsync(scenario.Question);
    ```
  </Tab>
</Tabs>

### Expected output from agent sample code

When you run the agent, you see output similar to the following example. The output shows successful tool configuration and agent responses to business scenarios:

```bash
✅ Connected to Foundry
🚀 Foundry - Modern Workplace Assistant
Tutorial 1: Building Enterprise Agents with Microsoft Foundry SDK
======================================================================
🤖 Creating Modern Workplace Assistant...
📁 Configuring SharePoint integration...
   Connection ID: /subscriptions/.../connections/ContosoCorpPoliciesProcedures
✅ SharePoint tool configured successfully
📚 Configuring Microsoft Learn MCP integration...
   Server URL: https://learn.microsoft.com/api/mcp
✅ MCP tool configured successfully
🛠️  Creating agent with model: gpt-4o-mini
   ✓ SharePoint tool added
   ✓ MCP tool added
   Total tools: 2
✅ Agent created successfully (name: Modern Workplace Assistant, version: 1)

======================================================================
🏢 MODERN WORKPLACE ASSISTANT - BUSINESS SCENARIO DEMONSTRATION
======================================================================
This demonstration shows how AI agents solve real business problems
using the Microsoft Foundry SDK.
======================================================================

📊 SCENARIO 1/3: 📋 Company Policy Question (SharePoint Only)
--------------------------------------------------
❓ QUESTION: What is Contosoʹs remote work policy?
🎯 BUSINESS CONTEXT: Employee needs to understand company-specific remote work requirements
🎓 LEARNING POINT: SharePoint tool retrieves internal company policies
--------------------------------------------------
🤖 AGENT RESPONSE:
✅ SUCCESS: Contosoʹs remote work policy, effective January 2024, outlines the following key points:

### Overview
Contoso Corp supports flexible work arrangements, including remote work, to enhance employee productivity and work-life balance.

### Eligibility
- **Full-time Employees**: Must have completed a 90...
   📏 Full response: 1530 characters
📈 STATUS: completed
--------------------------------------------------

📊 SCENARIO 2/3: 📚 Technical Documentation Question (MCP Only)
--------------------------------------------------
❓ QUESTION: According to Microsoft Learn, what is the correct way to implement Azure AD Conditional Access policies? Please include reference links to the official documentation.
🎯 BUSINESS CONTEXT: IT administrator needs authoritative Microsoft technical guidance
🎓 LEARNING POINT: MCP tool accesses Microsoft Learn for official documentation with links
--------------------------------------------------
🤖 AGENT RESPONSE:
✅ SUCCESS: To implement Azure AD Conditional Access policies correctly, follow these key steps outlined in the Microsoft Learn documentation:

### 1. Understanding Conditional Access
Conditional Access policies act as "if-then" statements that enforce organizational access controls based on various signals. Th...
   📏 Full response: 2459 characters
📈 STATUS: completed
--------------------------------------------------

📊 SCENARIO 3/3: 🔄 Combined Implementation Question (SharePoint + MCP)
--------------------------------------------------
❓ QUESTION: Based on our companyʹs remote work security policy, how should I configure my Azure environment to comply? Please include links to Microsoft documentation showing how to implement each requirement.
🎯 BUSINESS CONTEXT: Need to map company policy to technical implementation with official guidance
🎓 LEARNING POINT: Both tools work together: SharePoint for policy + MCP for implementation docs
--------------------------------------------------
🤖 AGENT RESPONSE:
✅ SUCCESS: To configure your Azure environment in compliance with Contoso Corpʹs remote work security policy, you need to focus on several key areas, including enabling Multi-Factor Authentication (MFA), utilizing Azure Security Center, and implementing proper access management. Below are specific steps and li...
   📏 Full response: 3436 characters
📈 STATUS: completed
--------------------------------------------------

✅ DEMONSTRATION COMPLETED!
🎓 Key Learning Outcomes:
   • Microsoft Foundry SDK usage for enterprise AI
   • Conversation management via the Responses API
   • Real business value through AI assistance
   • Foundation for governance and monitoring (Tutorials 2-3)

🎯 Try interactive mode? (y/n): n

🎉 Sample completed successfully!
📚 This foundation supports Tutorial 2 (Governance) and Tutorial 3 (Production)
🔗 Next: Add evaluation metrics, monitoring, and production deployment
```

## Step 5: Evaluate the assistant by using cloud evaluation

The evaluation framework tests realistic business scenarios by using the **cloud evaluation** capability of the Microsoft Foundry SDK. Instead of a custom local approach, this pattern uses the built-in evaluators (`builtin.violence`, `builtin.fluency`, `builtin.task_adherence`) and the `openai_client.evals` API to run scalable, repeatable evaluations in the cloud.

This evaluation framework demonstrates:

* **Agent targeting**: The evaluation runs queries directly against your agent by using `azure_ai_target_completions`.
* **Built-in evaluators**: Safety (violence detection), quality (fluency), and task adherence metrics.
* **Cloud-based execution**: Eliminates local compute requirements and supports CI/CD integration.
* **Structured results**: Pass/fail labels, scores, and reasoning for each test case.

The code breaks down into the following main sections:

1. [Configure the evaluation](#configure-the-evaluation).
2. [Run the cloud evaluation](#run-the-cloud-evaluation).
3. [Retrieve evaluation results](#retrieve-evaluation-results).

<Callout type="tip">
  For detailed guidance on cloud evaluations, see [Run evaluations in the cloud](../how-to/develop/cloud-evaluation). To find a comprehensive list of built-in evaluators available in Foundry, see [Observability in generative AI](../concepts/observability).
</Callout>

<Callout type="note">
  The C# SDK uses **protocol methods** with `BinaryData` and `BinaryContent` instead of typed objects. This approach requires helper methods to parse JSON responses. See the [C# Evaluations SDK sample](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects/samples/Sample21_Evaluations.md) for the complete pattern.
</Callout>

### Configure the evaluation

First, create an evaluation object that defines your data schema and testing criteria. The evaluation uses built-in evaluators for violence detection, fluency, and task adherence.

In Python, use the OpenAI client directly. In C#, get an `EvaluationClient` from the project client:

<Tabs>
  <Tab title="Python">
    ```python
    load_dotenv()
    endpoint = os.environ["PROJECT_ENDPOINT"]
    model_deployment_name = os.environ.get("MODEL_DEPLOYMENT_NAME", "gpt-4o-mini")

    with (
        DefaultAzureCredential() as credential,
        AIProjectClient(endpoint=endpoint, credential=credential) as project_client,
        project_client.get_openai_client() as openai_client,
    ):
        # Create or retrieve the agent to evaluate
        agent = project_client.agents.create_version(
            agent_name="Modern Workplace Assistant",
            definition=PromptAgentDefinition(
                model=model_deployment_name,
                instructions="You are a helpful Modern Workplace Assistant that answers questions about company policies and technical guidance.",
            ),
        )
        print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

        # Define the data schema for evaluation
        data_source_config = DataSourceConfigCustom(
            type="custom",
            item_schema={
                "type": "object",
                "properties": {"query": {"type": "string"}},
                "required": ["query"]
            },
            include_sample_schema=True,
        )

        # Define testing criteria with built-in evaluators
        testing_criteria = [
            {
                "type": "azure_ai_evaluator",
                "name": "violence_detection",
                "evaluator_name": "builtin.violence",
                "data_mapping": {"query": "{{item.query}}", "response": "{{sample.output_text}}"},
            },
            {
                "type": "azure_ai_evaluator",
                "name": "fluency",
                "evaluator_name": "builtin.fluency",
                "initialization_parameters": {"deployment_name": f"{model_deployment_name}"},
                "data_mapping": {"query": "{{item.query}}", "response": "{{sample.output_text}}"},
            },
            {
                "type": "azure_ai_evaluator",
                "name": "task_adherence",
                "evaluator_name": "builtin.task_adherence",
                "initialization_parameters": {"deployment_name": f"{model_deployment_name}"},
                "data_mapping": {"query": "{{item.query}}", "response": "{{sample.output_items}}"},
            },
        ]

        # Create the evaluation object
        eval_object = openai_client.evals.create(
            name="Agent Evaluation",
            data_source_config=data_source_config,
            testing_criteria=testing_criteria,
        )
        print(f"Evaluation created (id: {eval_object.id}, name: {eval_object.name})")
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    // Load environment variables
    var endpoint = Environment.GetEnvironmentVariable("PROJECT_ENDPOINT")
        ?? throw new InvalidOperationException("PROJECT_ENDPOINT not set");
    var modelDeploymentName = Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME")
        ?? "gpt-4o-mini";

    // Create clients
    AIProjectClient projectClient = new(new Uri(endpoint), new DefaultAzureCredential());
    EvaluationClient evaluationClient = projectClient.OpenAI.GetEvaluationClient();

    // Create or retrieve the agent to evaluate
    PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
    {
        Instructions = "You are a helpful Modern Workplace Assistant that answers questions about company policies and technical guidance."
    };
    AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
        agentName: "Modern Workplace Assistant",
        options: new(agentDefinition));
    Console.WriteLine($"Agent created (id: {agentVersion.Id}, name: {agentVersion.Name}, version: {agentVersion.Version})");

    // Define testing criteria with built-in evaluators
    // data_mapping: sample.output_text = agent string response, sample.output_items = structured JSON with tool calls
    object[] testingCriteria = [
        new {
            type = "azure_ai_evaluator",
            name = "violence_detection",
            evaluator_name = "builtin.violence",
            data_mapping = new { query = "{{item.query}}", response = "{{sample.output_text}}" }
        },
        new {
            type = "azure_ai_evaluator",
            name = "fluency",
            evaluator_name = "builtin.fluency",
            initialization_parameters = new { deployment_name = modelDeploymentName },
            data_mapping = new { query = "{{item.query}}", response = "{{sample.output_text}}" }
        },
        new {
            type = "azure_ai_evaluator",
            name = "task_adherence",
            evaluator_name = "builtin.task_adherence",
            initialization_parameters = new { deployment_name = modelDeploymentName },
            data_mapping = new { query = "{{item.query}}", response = "{{sample.output_items}}" }
        },
    ];

    // Define the data schema
    object dataSourceConfig = new
    {
        type = "custom",
        item_schema = new
        {
            type = "object",
            properties = new { query = new { type = "string" } },
            required = new[] { "query" }
        },
        include_sample_schema = true
    };

    // Create evaluation data payload
    BinaryData evaluationData = BinaryData.FromObjectAsJson(new
    {
        name = "Agent Evaluation",
        data_source_config = dataSourceConfig,
        testing_criteria = testingCriteria
    });

    // Create the evaluation object
    using BinaryContent evaluationDataContent = BinaryContent.Create(evaluationData);
    ClientResult evaluation = evaluationClient.CreateEvaluation(evaluationDataContent);
    Dictionary<string, string> fields = ParseClientResult(evaluation, ["name", "id"]);
    string evaluationName = fields["name"];
    string evaluationId = fields["id"];
    Console.WriteLine($"Evaluation created (id: {evaluationId}, name: {evaluationName})");
    ```
  </Tab>
</Tabs>

The `testing_criteria` array specifies which evaluators to run:

* `builtin.violence`: Detects violent or harmful content in responses.
* `builtin.fluency`: Assesses response quality and readability (requires a model deployment).
* `builtin.task_adherence`: Evaluates whether the agent followed instructions correctly.

### Run the cloud evaluation

Create an evaluation run that targets your agent. The `azure_ai_target_completions` data source sends queries to your agent and captures responses for evaluation:

<Tabs>
  <Tab title="Python">
    ```python
    # Define the data source for the evaluation run
    data_source = {
        "type": "azure_ai_target_completions",
        "source": {
            "type": "file_content",
            "content": [
                {"item": {"query": "What is Contoso's remote work policy?"}},
                {"item": {"query": "What are the security requirements for remote employees?"}},
                {"item": {"query": "According to Microsoft Learn, how do I configure Azure AD Conditional Access?"}},
                {"item": {"query": "Based on our company policy, how should I configure Azure security to comply?"}},
            ],
        },
        "input_messages": {
            "type": "template",
            "template": [
                {"type": "message", "role": "user", "content": {"type": "input_text", "text": "{{item.query}}"}}
            ],
        },
        "target": {
            "type": "azure_ai_agent",
            "name": agent.name,
            "version": agent.version,
        },
    }

    # Create and submit the evaluation run
    agent_eval_run: Union[RunCreateResponse, RunRetrieveResponse] = openai_client.evals.runs.create(
        eval_id=eval_object.id,
        name=f"Evaluation Run for Agent {agent.name}",
        data_source=data_source,
    )
    print(f"Evaluation run created (id: {agent_eval_run.id})")
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    // Define the data source for the evaluation run
    // This targets the agent with test queries
    object dataSource = new
    {
        type = "azure_ai_target_completions",
        source = new
        {
            type = "file_content",
            content = new[]
            {
                new { item = new { query = "What is Contoso's remote work policy?" } },
                new { item = new { query = "What are the security requirements for remote employees?" } },
                new { item = new { query = "According to Microsoft Learn, how do I configure Azure AD Conditional Access?" } },
                new { item = new { query = "Based on our company policy, how should I configure Azure security to comply?" } },
            }
        },
        input_messages = new
        {
            type = "template",
            template = new[]
            {
                new
                {
                    type = "message",
                    role = "user",
                    content = new { type = "input_text", text = "{{item.query}}" }
                }
            }
        },
        target = new
        {
            type = "azure_ai_agent",
            name = agentVersion.Name,
            // Version is optional. Defaults to latest version if not specified.
            version = agentVersion.Version,
        }
    };

    // Create evaluation run payload
    BinaryData runData = BinaryData.FromObjectAsJson(new
    {
        eval_id = evaluationId,
        name = $"Evaluation Run for Agent {agentVersion.Name}",
        data_source = dataSource
    });

    // Submit the evaluation run
    using BinaryContent runDataContent = BinaryContent.Create(runData);
    ClientResult run = evaluationClient.CreateEvaluationRun(evaluationId: evaluationId, content: runDataContent);
    fields = ParseClientResult(run, ["id", "status"]);
    string runId = fields["id"];
    string runStatus = fields["status"];
    Console.WriteLine($"Evaluation run created (id: {runId})");
    ```
  </Tab>
</Tabs>

The `data_source` configuration:

* **type**: `azure_ai_target_completions` routes queries through your agent
* **source**: Inline content with test queries (you can also use a dataset file ID)
* **input\_messages**: Template that formats each query for the agent
* **target**: Specifies the agent name and version to evaluate

### Retrieve evaluation results

Poll the evaluation run until it completes, then retrieve the detailed output items:

<Tabs>
  <Tab title="Python">
    ```python
    # Poll until the evaluation run completes
    while agent_eval_run.status not in ["completed", "failed"]:
        agent_eval_run = openai_client.evals.runs.retrieve(
            run_id=agent_eval_run.id,
            eval_id=eval_object.id
        )
        print(f"Waiting for eval run to complete... current status: {agent_eval_run.status}")
        time.sleep(5)

    if agent_eval_run.status == "completed":
        print("\n✓ Evaluation run completed successfully!")
        print(f"Result Counts: {agent_eval_run.result_counts}")

        # Retrieve detailed output items
        output_items = list(
            openai_client.evals.runs.output_items.list(
                run_id=agent_eval_run.id,
                eval_id=eval_object.id
            )
        )
        print(f"\nOUTPUT ITEMS (Total: {len(output_items)})")
        print(f"{'-'*60}")
        pprint(output_items)
        print(f"{'-'*60}")
        print(f"Eval Run Report URL: {agent_eval_run.report_url}")
    else:
        print("\n✗ Evaluation run failed.")

    # Cleanup
    openai_client.evals.delete(eval_id=eval_object.id)
    print("Evaluation deleted")

    project_client.agents.delete(agent_name=agent.name)
    print("Agent deleted")
    ```
  </Tab>

  <Tab title="C#">
    ```csharp
    // Poll until the evaluation run completes
    while (runStatus != "failed" && runStatus != "completed")
    {
        Thread.Sleep(TimeSpan.FromMilliseconds(5000));
        run = evaluationClient.GetEvaluationRun(evaluationId: evaluationId, evaluationRunId: runId, options: new());
        runStatus = ParseClientResult(run, ["status"])["status"];
        Console.WriteLine($"Waiting for eval run to complete... current status: {runStatus}");
    }

    if (runStatus == "failed")
    {
        throw new InvalidOperationException($"Evaluation run failed with error: {GetErrorMessageOrEmpty(run)}");
    }

    // Output results
    Console.WriteLine("\n✓ Evaluation run completed successfully!");
    Console.WriteLine($"Result Counts: {GetResultsCounts(run)}");

    List<string> evaluationResults = GetResultsList(
        client: evaluationClient,
        evaluationId: evaluationId,
        evaluationRunId: runId);

    Console.WriteLine($"\nOUTPUT ITEMS (Total: {evaluationResults.Count})");
    Console.WriteLine($"{new string('-', 60)}");
    foreach (string result in evaluationResults)
    {
        Console.WriteLine(result);
    }
    Console.WriteLine($"{new string('-', 60)}");

    // Cleanup
    evaluationClient.DeleteEvaluation(evaluationId, new System.ClientModel.Primitives.RequestOptions());
    Console.WriteLine("Evaluation deleted");

    projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
    Console.WriteLine("Agent deleted");
    ```
  </Tab>
</Tabs>

Each output item includes:

* **Label**: Binary "pass" or "fail" result
* **Score**: Numeric score on the evaluator's scale
* **Reason**: Explanation of why the score was assigned (for LLM-based evaluators)

### Expected output from cloud evaluation (evaluate.py)

When you run the evaluation script, you see output similar to the following example. The output shows the evaluation object creation, run submission, and results retrieval:

```bash
python evaluate.py
Agent created (name: Modern_Workplace_Assistant, version: 1)
Evaluation created (id: eval_xyz789, name: Agent Evaluation)
Evaluation run created (id: run_def456)
Waiting for eval run to complete... current status: running
Waiting for eval run to complete... current status: running

✓ Evaluation run completed successfully!
Result Counts: {'passed': 2, 'failed': 0, 'errored': 0}

OUTPUT ITEMS (Total: 2)
------------------------------------------------------------
[OutputItem(id='item_1',
            sample={'query': 'What is the largest city in France?',
                    'output_text': 'The largest city in France is Paris...'},
            results=[{'name': 'violence_detection', 'passed': True, 'score': 0},
                     {'name': 'fluency', 'passed': True, 'score': 4,
                      'reason': 'Response is clear and well-structured'},
                     {'name': 'task_adherence', 'passed': True, 'score': 5}]),
 OutputItem(id='item_2', ...)]
------------------------------------------------------------
Eval Run Report URL: https://ai.azure.com/...
Evaluation deleted
Agent deleted
```

### Understanding evaluation results

Cloud evaluations provide structured results that you can view in the Foundry portal or retrieve programmatically. Each output item includes:

| Field         | Description                                               |
| ------------- | --------------------------------------------------------- |
| **Label**     | Binary "pass" or "fail" based on the threshold            |
| **Score**     | Numeric score (scale depends on evaluator type)           |
| **Threshold** | The cutoff value that determines pass/fail                |
| **Reason**    | LLM-generated explanation for the score (when applicable) |

**Score scales by evaluator type:**

* **Quality evaluators** (fluency, coherence): 1-5 scale
* **Safety evaluators** (violence, self-harm): 0-7 severity scale (lower is safer)
* **Task evaluators** (task\_adherence): 1-5 scale

You can also view detailed results in the Foundry portal by selecting **Evaluation** from your project and selecting the evaluation run. The portal provides visualizations, filtering, and export options.

<Callout type="tip">
  For production scenarios, consider running evaluations as part of your CI/CD pipeline. See [How to run an evaluation in Azure DevOps](../how-to/evaluation-azure-devops), and [Continuously evaluate your AI agents](../how-to/continuous-evaluation-agents) for integration patterns.
</Callout>

## Summary

You now have:

* A working single-agent prototype grounded in internal and external knowledge.
* A repeatable evaluation script demonstrating enterprise validation patterns.
* A clear upgrade path: more tools, multi-agent orchestration, richer evaluation, deployment.

These patterns reduce prototype-to-production friction: you can add data sources, enforce governance, and integrate monitoring without rewriting core logic.

## Next steps

This tutorial demonstrates **Stage 1** of the developer journey - from idea to prototype. This minimal sample provides the foundation for enterprise AI development. To continue your journey, explore the next stages:

### Suggested additional enhancements

* Add more data sources ([Azure AI Search](../agents/how-to/tools/ai-search), [other sources](../how-to/connections-add)).
* Implement advanced evaluation methods ([AI-assisted evaluation](../how-to/develop/evaluate-sdk)).
* Create [custom tools](../agents/how-to/private-tool-catalog) for business-specific operations.
* Add [conversation memory and personalization](https://learn.microsoft.com/en-us/azure/cosmos-db/gen-ai/azure-agent-service).

### Stage 2: Prototype to production

* [Implement safety assessment with red-team testing](../how-to/develop/run-scans-ai-red-teaming-agent).
* [Create comprehensive evaluation datasets with quality metrics](../fine-tuning/data-generation).
* [Apply organization-wide governance policies and model comparison](../how-to/built-in-policy-model-deployment).
* [Configure fleet monitoring, CI/CD integration, and production deployment endpoints](../concepts/deployments-overview).

### Stage 3: Production to adoption

* [Collect trace data and user feedback from production deployments](../observability/how-to/trace-agent-framework).
* [Fine-tune models and generate evaluation insights for continuous improvement](../openai/how-to/fine-tuning).
* [Integrate Azure API Management gateway with continuous quality monitoring](../configuration/enable-ai-api-management-gateway-portal).
* [Implement fleet governance, compliance controls, and cost optimization](https://learn.microsoft.com/en-us/azure/cloud-adoption-framework/scenarios/ai/platform/governance).

## Related content

* [Foundry Agent Service overview](../agents/overview)
* [SharePoint tool documentation](../agents/how-to/tools/sharepoint)
* [MCP tool integration](../agents/how-to/tools/model-context-protocol)
* [Multi-agent patterns](../agents/how-to/connected-agents)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

You can ask AI to assist you in Foundry. To start using AI to ask questions or complete tasks, select its icon located in the top right bar of the [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs) portal. A chat window opens where you can type your questions and receive answers in real-time. You can also ask the agent to run tasks for you.

![Screenshot shows the Ask AI button in the top right bar of the Foundry portal.](https://learn.microsoft.com/azure/ai-foundry/media/ask-foundry-agent/ask-ai.png)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

## Prerequisites

* Access to the [Foundry portal](https://ai.azure.com/?cid=learnDocs).

## Capabilities

**What This AI Can Do** - The Ask AI experience provides assistance by answering questions and performing tasks through specialized sub-agents:

* **Documentation**: This documentation includes details about Foundry such as Quickstarts, How-tos, or reference documentation of the Microsoft Foundry SDK. The agent can help you navigate the documentation or find answers for you.
* **Model Catalog**: Provide information about specific models in the model catalog, including their capabilities and features.
* **Troubleshooting**: Help diagnose and resolve common Foundry problems by searching the troubleshooting knowledge base and providing step-by-step solutions.
* **Quota & Model Operations**: Deploy models, debug deployment problems, find deployment details, check quota and capacity in specific regions, and delete model deployments.
* **Model Analysis**: Recommend models based on cost, performance, or quality. Compare models using benchmark data across quality, cost, throughput, safety, and latency criteria. Analyze operational data for Azure OpenAI deployments.
* **Monitoring Dashboard Insights**: Interpret evaluation dashboard visualizations, identify patterns and anomalies in monitoring data, and suggest optimizations based on performance metrics.
* **Evaluation Management**: Manage evaluation workflows for large language models and agents, including setup, execution, and monitoring of evaluation jobs.

**What This AI Can't Do** - While the agent is a powerful tool, it has some limitations and constraints:

* **Limited Scope**: It's restricted to answering questions related to the Foundry documentation and model catalog. It can't provide support for unrelated Azure services or external systems.
* **Call External APIs**: This AI experience can only call for a specific subset of Foundry APIs. It can't access the web or APIs external to Microsoft.
* **Bypass Permissions**: This AI experience can execute actions on your behalf. It requires you to have the right permissions for those actions. This agent can't perform an action that you wouldn't be able to do yourself.

Use the agent to make the most of the Foundry experience but keep its scope and limitations in mind when asking questions.

## Actions and approvals

When you ask the Ask AI agent to perform tasks that require accessing or modifying your Azure resources, the agent proposes actions for you to review and approve before execution. This approval flow ensures you maintain oversight over what actions are performed on your behalf.

![Screenshot shows the Ask AI chat on the right side of Foundry portal. The Ask AI agent is responding a user query by proposing to run an action for the user to approve.](https://learn.microsoft.com/azure/ai-foundry/media/ask-foundry-agent/ask-ai-approval-flow.png)

The actions come from the tools available under the [Foundry Model Context Protocol (MCP) Server](../mcp/available-tools).

To make this approval flow easier, you can **change the approval settings** to pre-approve some actions depending on their scope. Access the approval settings by selecting the settings icon in the Ask AI prompt chat box. By default, this experience is set to pre-approve **System access** actions. You can change these settings anytime, and they persist beyond your session.

![Screenshot shows the Ask AI prompt chat box, showing the location of the approval settings.](https://learn.microsoft.com/azure/ai-foundry/media/ask-foundry-agent/ask-ai-approval-settings.png)

## Best practices and security guidance

The Ask AI experience relies on the [Foundry Model Context Protocol (MCP) Server](../mcp/get-started). By using this server, it implements the [same best practices and security guidance](../mcp/security-best-practices).

<Callout type="important">
  By using this preview feature, you acknowledge and consent to any cross-region processing that may occur. As an example, an EU resource accessed by a US user could be routed through US infrastructure. If your organization requires strict in-region processing, don't use Ask AI (preview) or restrict its use to scenarios that remain within your selected region.
</Callout>

## Responsible AI FAQ for Ask AI

### What is Ask AI in Foundry?

It's an AI agent that enables users of Foundry to navigate its capabilities, identify models, and understand how to use its resources to build generative AI applications. For an overview of how the agent works and a summary of its capabilities, see the overview earlier in this article.

### What is the current status of the Ask AI feature?

It's available in Foundry as a preview feature.

### Are the Ask AI results reliable?

The agent is designed to generate the best possible responses within the context it can access. However, like any AI system, the agent's responses aren't always perfect. Carefully test, review, and vet all of the agent's responses before using them in Foundry or for your application.

### How do I provide feedback on Ask AI?

If you see a response that's inaccurate or doesn't support your needs, use the thumbs-down button to submit feedback. You can also submit feedback on your overall experience by using the Foundry feedback button on the top menu.

### What should I do if I see unexpected or offensive content?

The Foundry team built the agent by following the [AI principles](https://www.microsoft.com/ai/principles-and-approach) and [Responsible AI Standard](https://aka.ms/RAIStandardPDF). The team prioritized mitigating exposing customers to offensive content. However, you might still see unexpected results. The team is constantly working to improve the technology to prevent the output of harmful content.

If you encounter harmful or inappropriate content in the system, select the thumbs-down icon below the response to provide feedback or report a concern.

### How current is the information provided by the agent?

The agent is updated daily to keep it up to date with the latest information. In most cases, the information the agent provides is up to date. However, there might be some delay between new Foundry announcements and updates to the agent.

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Most businesses don't want just chatbots. They want automation that's faster and has fewer errors. That desire might mean summarizing documents, processing invoices, managing support tickets, or publishing blog posts. In all cases, the goal is the same: freeing people and resources to focus on higher-value work by offloading repetitive and predictable tasks.

Large language models (LLMs) introduce a new type of automation with systems that can understand unstructured data, make decisions, and generate content. In practice, businesses can have difficulty moving beyond demos and into production. LLMs can drift, be incorrect, and lack accountability. Without visibility, policy enforcement, and orchestration, these models are hard to trust in real business workflows.

*Microsoft Foundry* is designed to change that. It's a platform that combines models, tools, frameworks, and governance into a unified system for building intelligent agents. At the center of this system is *Foundry Agent Service*, which enables the operation of agents across development, deployment, and production.

![Diagram that shows Foundry Agent Service as the central hub connecting four components: AI models on the left, tools and frameworks on the top, governance and compliance on the right, and orchestration at the bottom. Arrows indicate Agent Service enables agents to move from development through deployment to production.](https://learn.microsoft.com/azure/ai-foundry/agents/media/agent-service-the-glue.png)

Agent Service connects the core pieces of Foundry, such as models, tools, and frameworks, into a single runtime. It manages conversations, orchestrates tool calls, enforces content safety, and integrates with identity, networking, and observability systems. These capabilities help you build agents that are secure, scalable, and production ready.

By abstracting away infrastructure complexity and enforcing trust and safety by design, Agent Service can help you move from prototype to production with confidence.

## Prerequisites

* An Azure subscription with permission to create and manage Foundry resources.
* A Foundry project. If you haven't created one yet, start with [environment setup](environment-setup).
* A deployed model that your agent can use. Model and region availability can vary; see [models that inform agents](concepts/model-region-support).

## Availability, regions, and limits

Agent Service capabilities can vary based on the Foundry experience you're using and the model and region you choose.

* For service limits, quotas, and throttling considerations, see [Quotas and limits for Agent Service](quotas-limits).
* For model and region support, see [models that inform agents](concepts/model-region-support).

If you're building your first agent, start with the quickstart links in [Get started with Foundry Agent Service](#get-started-with-foundry-agent-service) to make sure you're on the right API path for your Foundry experience.

## What is an AI agent?

Agents make decisions, invoke tools, and participate in workflows. They perform these tasks sometimes independently and sometimes in collaboration with other agents or humans. They're foundational to real process automation.

Agents you create through Foundry aren't monoliths. They're composable units. Each agent has a specific role, is powered by the right model, and is equipped with the right tools. You deploy each agent within a secure, observable, and governable runtime.

An agent has three core components:

* **Model (LLM)**: Powers reasoning and language understanding.

* **Instructions**: Define the agent's goals, behavior, and constraints. They can have the following types:

  * Declarative:

    * Prompt based: A declaratively defined single agent that combines model configuration, instruction, tools, and natural language prompts to drive behavior.
    * Workflow: An agentic workflow that can be expressed as a YAML or other code to orchestrate multiple agents together, or to trigger an action on certain criteria.

  * Hosted: Containerized agents that are created and deployed in code and are hosted by Foundry.

* **Tools**: Let the agent retrieve knowledge or take action.

![Diagram that shows an agent receiving user inputs on the left, processing them through the model and instructions in the center, and producing outputs on the right. A bidirectional arrow below the agent connects to tools, indicating the agent can call tools during processing to retrieve knowledge or take actions.](https://learn.microsoft.com/azure/ai-foundry/agents/media/what-is-an-agent.png)

Agents receive unstructured inputs such as user prompts, alerts, or messages from other agents. They produce outputs in the form of tool results or messages. Along the way, they might call tools to perform retrieval or trigger actions.

## How do agents in Foundry work?

Think of Foundry as an assembly line for intelligent agents. Like any modern factory, Foundry brings together specialized stations that are each responsible for shaping part of the final product. Instead of machines and conveyor belts, the agent factory uses models, tools, policies, and orchestration to build agents that are secure, testable, and production ready. Here's how the factory works step by step:

![Diagram that shows the agent factory as a six-step assembly line. Step 1 Models shows selecting an LLM. Step 2 Customizability shows fine-tuning and prompts. Step 3 Knowledge and Tools shows connecting to enterprise data and actions. Step 4 Orchestration shows coordinating agent workflows. Step 5 Observability shows logging and tracing. Step 6 Trust shows security controls. The steps flow left to right, producing a production-ready agent.](https://learn.microsoft.com/azure/ai-foundry/agents/media/agent-factory.png)

### 1. Models

The assembly line starts when you select a model that gives your agent its intelligence. Choose from a growing catalog of large language models (LLMs), including GPT-4o, GPT-4, GPT-3.5 (Azure OpenAI), and others like Llama. The model is the reasoning core of the agent that informs its decisions.

### 2. Customizability

Shape the model to fit your use case. Customize your agent with fine-tuning, distillation, or domain-specific prompts. Encode agent behavior, role-specific knowledge, and patterns from prior performance by using data captured from real conversation content and tool results.

### 3. Knowledge and tools

Equip your agent with tools. These tools let the agent access enterprise knowledge (such as Bing, SharePoint, and Azure AI Search) and take real-world actions (via Azure Logic Apps, Azure Functions, OpenAPI, and more). This step enhances the agent's ability to expand its capabilities.

### 4. Orchestration

The agent needs coordination. [Workflows](concepts/workflow) orchestrate the full lifecycle, such as handling tool calls, updating conversation state, managing retries, and logging outputs.

### 5. Observability

Test and monitor agents. Foundry can capture logs, traces, and evaluations at every step. With full conversation-level visibility and Application Insights integration, teams can inspect every decision and continuously improve agents over time.

### 6. Trust

Ensure that agents are suitable and reliable for the workload they're assigned to. Foundry applies enterprise-grade trust features, including identity via Microsoft Entra, role-based access control (RBAC), content filters, encryption, and network isolation. You choose how and where your agents run, by using platform-managed or bring-your-own infrastructure.

The result is an agent that's ready for production: reliable, extensible, and safe to deploy across your workflows.

## Why use Foundry Agent Service?

Agent Service provides a production-ready foundation for deploying intelligent agents in enterprise environments. Here's how it compares across key capabilities:

| Capability                        | Agent Service                                                                                                                                                                                                                                                                                                            |
| --------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Visibility into conversations** | Full access to structured [conversations](concepts/runtime-components#what-is-a-conversation), including both user-to-agent and agent-to-agent messages. Ideal for UIs, debugging, and training.                                                                                                                         |
| **Multiple-agent coordination**   | Built-in support for agent-to-agent messaging.                                                                                                                                                                                                                                                                           |
| **Tool orchestration**            | Server-side execution and retry of tool calls with structured logging. No manual orchestration is required.                                                                                                                                                                                                              |
| **Trust and safety**              | Integrated [content filters](../openai/how-to/content-filters) to help prevent misuse and mitigate prompt injection risks, including cross-prompt injection attacks (XPIA). All outputs are policy governed.                                                                                                             |
| **Enterprise integration**        | Ability to bring your own [storage](how-to/use-your-own-resources#use-an-existing-azure-cosmos-db-for-nosql-account-for-conversation-storage), [Azure AI Search index](how-to/use-your-own-resources#use-an-existing-azure-ai-search-resource), and [virtual network](how-to/virtual-networks) to meet compliance needs. |
| **Observability and debugging**   | [Full traceability](../how-to/develop/trace-agents-sdk) of conversations, tool invocations, and message traces; [Application Insights integration](how-to/metrics) for usage data.                                                                                                                                       |
| **Identity and policy control**   | Built on Microsoft Entra with full support for RBAC, audit logs, and enterprise conditional access.                                                                                                                                                                                                                      |

## Security, privacy, and compliance

Agent Service is designed for enterprise workloads where you need strong controls over identity, networking, data handling, and safety.

* **Safety controls**: Use integrated [content filters](../openai/how-to/content-filters) to help reduce unsafe outputs and mitigate prompt injection risks, including cross-prompt injection attacks (XPIA).
* **Network isolation and data residency controls**: Use [virtual networks](how-to/virtual-networks) and bring-your-own resources to meet your requirements.
* **Bring your own resources**: Use your own Azure resources (for example, storage, Azure AI Search, and Azure Cosmos DB for conversation state) to meet compliance and operational needs. See [Use your own resources](how-to/use-your-own-resources).
* **Responsible AI guidance**: For a broader set of recommendations and governance resources, see [Responsible AI for Microsoft Foundry](../responsible-use-of-ai-overview).

## Get started with Foundry Agent Service

To get started with Agent Service, create a Foundry project in your Azure subscription.

If you're building in code, see [Microsoft Foundry SDKs](../how-to/develop/sdk-overview) for SDK options and guidance.

If it's your first time using the service, start with the [environment setup](environment-setup) and [quickstart](../tutorials/quickstart-create-foundry-resources) guides.

Create a project with the required resources. After you create a project, deploy a compatible model such as GPT-4o. When you have a deployed model, you can start making API calls to Agent Service by using the SDKs.

You can find a list of official samples with the new Python agent SDK on [GitHub](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects).

## BCDR for agents

To support service resilience, Agent Service relies on customer-provisioned Azure Cosmos DB accounts for business continuity and disaster recovery (BCDR). This approach helps ensure that your agent state can be preserved and recovered in the event of a regional outage.

As an Azure Standard customer, you provision and manage your own single-tenant Azure Cosmos DB account. You store all agent state in this account. You control backup and recovery through native capabilities in Azure Cosmos DB.

If the primary region becomes unavailable, the agent automatically connects to the same Azure Cosmos DB account in the secondary region. Because Cosmos DB preserves all history, the agent can continue operation with minimal disruption.

Provision and maintain your Azure Cosmos DB account, and configure appropriate backup and recovery policies. This effort helps ensure seamless continuity if the primary region becomes unavailable.

For configuration guidance, see [Use your own resources](how-to/use-your-own-resources) and [virtual networks](how-to/virtual-networks).

## Costs

Using Agent Service can incur costs from the model you deploy and the Azure resources you use for your project (for example, logging and any customer-managed resources you connect).

To understand and manage cost drivers, see [Plan and manage costs](../concepts/manage-costs).

## Troubleshooting

If you're blocked getting started, check these common issues:

* **Model isn't available in your region**: See [models that inform agents](concepts/model-region-support).
* **Requests are throttled or fail due to quota**: See [Quotas and limits for Agent Service](quotas-limits).
* **You can't access resources or deployments**: Confirm your role assignments and follow [environment setup](environment-setup).
* **You need to debug tool calls or agent behavior**: Start with [trace agents with the SDK](../how-to/develop/trace-agents-sdk) and [metrics](how-to/metrics).

## Related content

* [Quickstart: Create your first agent](quickstart)
* [Models that inform agents](concepts/model-region-support)
* [Microsoft Foundry SDKs](../how-to/develop/sdk-overview)
* [Content filters for safety](../openai/how-to/content-filters)
* [Trace agents with the SDK](../how-to/develop/trace-agents-sdk)
* [Use your own resources](how-to/use-your-own-resources)
* [Quotas and limits for Agent Service](quotas-limits)

Find answers to common questions about Foundry Agent Service.

If you can't find answers to your questions in this article and you still need help, see [Foundry Tools support and help options](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-support-options). Foundry Agent Service is part of Foundry Tools.

For getting started, see:

* [What is Foundry Agent Service?](overview)
* [Set up your environment](environment-setup)
* [Quotas and limits](quotas-limits)

## Setup and access

### What's the difference between basic setup and standard setup?

**Basic setup** stores agent state in Microsoft-managed resources.

**Standard setup** stores agent data (threads, files, and vector stores) in your Azure resources that you connect through capability hosts.

To compare setup options and choose the right one, see [Set up your environment](environment-setup#choose-your-setup) and [Capability hosts](concepts/capability-hosts).

### What permissions (RBAC roles) do I need?

Role requirements depend on what you're doing. Common roles include:

* **Azure AI Account Owner**: Create accounts and projects
* **Azure AI User**: Create and edit agents
* **Role Based Access Control Administrator** (or **Owner**): Required for standard setup to assign roles to connected resources

For standard setup, you also need the `Microsoft.Authorization/roleAssignments/write` permission.

For complete role requirements, see [Required permissions](environment-setup#required-permissions) and [Role-based access control (RBAC) in Microsoft Foundry](../concepts/rbac-foundry).

### Where should I start if I'm new to Foundry Agent Service?

Start with [Set up your environment](environment-setup), then create your first agent.

* Classic experience: [Quickstart: Create a new agent](quickstart)
* New Foundry experience: [Quickstart: Get started with agents in code](../quickstarts/get-started-code)

## General

### Do you store any data used in the Foundry Agent Service API?

Yes. Foundry Agent Service is a stateful API, which means that it retains data. Two types of data are stored in the Foundry Agent Service API:

* **Stateful entities**: Conversations and responses created during usage.
* **Files and vector stores**: Data uploaded during Foundry Agent Service setup or as part of a response generation.

To learn how conversations and responses work, see [Agent runtime components](concepts/runtime-components).

### Where is this data stored?

* **Basic setup**: Data is stored in a secure, Microsoft-managed storage account that's logically separated.

* **Standard setup**: Data is stored in your own Azure resources, so you have full ownership and control:

  * **Azure Storage**: Files and attachments
  * **Azure Cosmos DB**: Threads and conversation history
  * **Azure AI Search**: Vector stores

To learn about setup options that control where data is stored, see [Set up your environment](environment-setup#choose-your-setup).

### How long is this data stored?

Data persists unless you explicitly delete it. To delete agent data, use the API or SDK to delete threads, files, or vector stores.

For concepts and terminology (for example, conversation and response), see [Agent runtime components](concepts/runtime-components).

### Does Foundry Agent Service support CMK encryption?

* Basic setup supports Microsoft-managed keys only.
* Standard setup supports customer-managed keys (CMKs).

### Does Microsoft use my data for training models?

No, Microsoft doesn't use your data for training models. For more information, see the [Responsible AI documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/data-privacy).

### Where is data stored geographically?

Foundry Agent Service endpoints are regional, and data is stored in the same region as the endpoint. For more information, see the [Azure data residency documentation](https://azure.microsoft.com/explore/global-infrastructure/data-residency/#overview).

### How am I charged for Foundry Agent Service?

* You're charged for inference cost (input and output) of the base model that you're using for each agent (for example, gpt-4-0125). If you created multiple agents, you're charged for the base model attached to each agent.

* If you enabled the Code Interpreter tool, you're charged for its use per session. For example, if your agent calls Code Interpreter simultaneously in two threads, this activity creates two Code Interpreter sessions. Each of those sessions is charged.

  By default, each session is active for one hour. If your user keeps giving instructions to Code Interpreter in the same thread for up to one hour, you pay this fee only once.

* File search is billed based on the vector storage that you use.

For more information, see the [pricing page](https://azure.microsoft.com/pricing/details/ai-foundry/).

### Is there any additional pricing or quota for using Foundry Agent Service?

No. All [quotas](quotas-limits) apply to using models with Foundry Agent Service.

## Tools and integrations

### Where can I learn about tools like Code Interpreter and File Search?

Start with [Tools overview](how-to/tools-classic/overview), then see the tool-specific guidance:

* [Use Code Interpreter](how-to/tools-classic/code-interpreter)
* [Use File Search](how-to/tools-classic/file-search)

### How do I bring my own storage, Azure Cosmos DB, or Azure AI Search for agent data?

Use capability hosts and connect your own resources.

* Concepts: [Capability hosts](concepts/capability-hosts)
* How-to: [Use your own resources](how-to/use-your-own-resources)

## Monitoring and operations

### How do I monitor Foundry Agent Service?

To understand usage and monitoring options, see [Metrics](how-to/metrics) and [Monitor the service](reference/monitor-service).

### What should I do for business continuity and disaster recovery (BCDR)?

Agent Service BCDR guidance depends on your setup and the resources you provision. For the current guidance, see [BCDR for agents](overview#bcdr-for-agents).

## Virtual networking

### What is virtual network isolation?

Virtual networks help secure inbound and outbound access to your Azure resources. You achieve network isolation through virtual network integrations in Azure.

For Agent Service private networking requirements and limitations, see [How to use a virtual network with Foundry Agent Service](how-to/virtual-networks).

### Why do I need subnet delegation?

Agent Service networking uses Azure Container Apps. When you deploy into your virtual network, you must use a dedicated subnet delegated to `Microsoft.App/environments`.

For details, see [How to use a virtual network with Foundry Agent Service](how-to/virtual-networks) and [Virtual network configuration](https://learn.microsoft.com/en-us/azure/container-apps/custom-virtual-networks?tabs=workload-profiles-env#subnet).

### What regions support virtual network isolation?

Region availability for virtual network isolation can change. For the current supported regions and limitations, see [How to use a virtual network with Foundry Agent Service](how-to/virtual-networks).

### What class range is supported for public or private Class A, B, and C subnets?

Only private IP ranges are supported:

* **Class A**: 10.0.0.0/8
* **Class B**: 172.16.0.0/12
* **Class C**: 192.168.0.0/16

Public IP ranges aren't supported for agent subnets.

### What is the minimum size for the agent subnet, and how many IPs should I use?

The recommended delegated agent subnet size is /24. The minimum is /27.

For sizing guidance and how IPs are consumed, see [How to use a virtual network with Foundry Agent Service](how-to/virtual-networks) and [Virtual network configuration](https://learn.microsoft.com/en-us/azure/container-apps/custom-virtual-networks?tabs=workload-profiles-env#subnet).

### What is the minimum and recommended address range for virtual networks in Foundry Agent Service?

As long as the agent subnet and private endpoints have address space, the address range for virtual networks can be anything.

### Can I use peered virtual networks? Can I have resources in different virtual networks?

Yes. The virtual network is in your subscription, and you should be able to peer with any virtual network. But data transfer is costly, so we don't recommend it. The requirement is that all resources must be in the same region as the Microsoft Foundry resource.

### Do I need to add any FQDNs to an allow list if I'm using an Azure firewall?

Yes. Allow the required fully qualified domain names (FQDNs) for managed identity as described in [Use Azure Firewall with Azure Container Apps](https://learn.microsoft.com/en-us/azure/container-apps/use-azure-firewall), or allow the `AzureActiveDirectory` service tag.

For Agent Service private networking guidance, see [How to use a virtual network with Foundry Agent Service](how-to/virtual-networks).

### Can multiple Microsoft Foundry resources reuse the virtual network?

Yes, multiple Microsoft Foundry resources can reuse the same virtual network. However, each Foundry resource requires its own dedicated agent runtime subnet—the agent subnet can't be shared across multiple Foundry resources.

### Does the virtual network need to be in the same resource group as Foundry?

They don't need to be in the same resource group, but they do need to be in the same region.

### What additional configuration do I need if I want to add tools to my agents?

If you're using a network-secured setup, some tools require additional configuration, such as private endpoints and resource connections.

For the current guidance and limitations, see [How to use a virtual network with Foundry Agent Service](how-to/virtual-networks).

## Next steps

* [What is Foundry Agent Service?](overview)
* [Set up your environment](environment-setup)
* [Quickstart: Create a new agent (classic)](quickstart)
* [Tools overview](how-to/tools-classic/overview)
* [How to use a virtual network with Foundry Agent Service](how-to/virtual-networks)

Foundry Agent Service enforces quotas and limits on agent artifacts, file uploads, messages, and tool registrations. Understanding these limits helps you design applications that scale without hitting service boundaries. This article lists default limits, supported regions, compatible models, and guidance for handling limit errors.

<Callout type="note">
  Foundry Agent Service is generally available (GA). Some sub-features, such as [hosted agents](hosted-agents), are in public preview and might have different constraints.
</Callout>

## Prerequisites

* An Azure subscription.
* A [Microsoft Foundry project](../../how-to/create-projects).
* A [deployed model](model-region-support) compatible with Agent Service. Model and region availability can vary.

## Supported regions

Foundry Agent Service is available in the following Azure regions:

* Australia East
* Brazil South
* Canada East
* East US
* East US 2
* France Central
* Germany West Central
* Italy North
* Japan East
* Norway East
* South Africa North
* South Central US
* South India
* Sweden Central
* Switzerland North
* UK South
* West Europe
* West US
* West US 3

<Callout type="important">
  Not all tools are available in every region. For example, file search isn't available in Italy North and Brazil South. For the full tool-by-region matrix, see [Tool support by region and model](tool-best-practice#tool-support-by-region-and-model).
</Callout>

## Azure OpenAI model support

Foundry Agent Service is compatible with current Azure OpenAI models. For a complete list of supported models and their availability by region, see [Foundry Models sold directly by Azure](../../foundry-models/concepts/models-sold-directly-by-azure).

## Other model collections

In addition to Azure OpenAI models, Agent Service supports models from the Foundry model catalog. These models are deployed and managed through Foundry and follow separate quotas. The following models are available for your agents to use.

**Models sold directly by Azure:**

* **MAI-DS-R1**: Deterministic, precision-focused reasoning.
* **grok-4**: Frontier-scale reasoning for complex, multiple-step problem solving.
* **grok-4-fast-reasoning**: Accelerated agentic reasoning optimized for workflow automation.
* **grok-4-fast-non-reasoning**: High-throughput, low-latency generation and system routing.
* **grok-3**: Strong reasoning for complex, system-level workflows.
* **grok-3-mini**: Lightweight model optimized for interactive, high-volume use cases.
* **Llama-3.3-70B-Instruct**: Versatile model for enterprise Q\&A, decision support, and system orchestration.
* **Llama-4-Maverick-17B-128E-Instruct-FP8**: FP8-optimized model that delivers fast, cost-efficient inference.
* **DeepSeek-V3-0324**: Multimodal understanding across text and images.
* **DeepSeek-V3.1**: Enhanced multimodal reasoning and grounded retrieval.
* **DeepSeek-R1-0528**: Advanced long-form and multiple-step reasoning.
* **gpt-oss-120b**: Open-ecosystem model that supports transparency and reproducibility.

<Callout type="tip">
  Model availability can change over time. To verify what you can deploy for your project and region, use the Foundry portal model experience.
</Callout>

## Troubleshooting

### A model or version isn't available in your region

* Confirm you selected the right tab for your deployment type (global standard vs. provisioned).
* Try a different region that supports the model and version. See the [model and region support table](model-region-support).
* If you're using gpt-5 models, [registration](https://aka.ms/openai/gpt-5/2025-08-07) is required. Access is granted according to Microsoft's eligibility criteria.

### A tool isn't available in your region

* Not all tools are supported in every region. For example, file search isn't available in Italy North and Brazil South, and code interpreter isn't available in all regions.
* Check the [tool support by region and model](tool-best-practice#tool-support-by-region-and-model) table to confirm availability before you deploy.
* If a tool isn't available, choose a supported region or use a different tool.

### Provisioned throughput deployment fails

* Confirm you have enough PTUs available in the region.
* Review [Provisioned throughput](../../openai/concepts/provisioned-throughput) and [Spillover traffic management](../../openai/how-to/spillover-traffic-management).

### Agent receives rate-limit (429) errors

* Implement exponential backoff with jitter in your application retry logic.
* For sustained high-throughput workloads, consider provisioned throughput deployments.
* Review [Azure OpenAI quotas and limits](../../openai/quotas-limits) for your deployment's tokens-per-minute and requests-per-minute caps.

## Quotas and limits

Foundry Agent Service enforces limits in two places:

* **Agent Service limits.** Limits for agent and thread artifacts, such as file uploads, vector store attachments, message counts, and tool registration.
* **Model limits.** Quotas and rate limits for the model deployments your agents call.

If you're using threads and messages, see [Threads, runs, and messages in Foundry Agent Service](runtime-components). If you're using file search, see [Vector stores for file search](vector-stores).

## Default quotas and limits for the service

The following table lists default limits enforced by the Agent Service. These limits apply to all Foundry projects regardless of subscription type or region.

| Limit name                                                  | Limit value          |
| ----------------------------------------------------------- | -------------------- |
| Maximum number of files per agent/thread                    | 10,000               |
| Maximum file size for agents                                | 512 MB               |
| Maximum size for all uploaded files for agents              | 300 GB               |
| Maximum file size in tokens for attaching to a vector store | 2,000,000 tokens     |
| Maximum number of messages per thread                       | 100,000              |
| Maximum size of `text` content per message                  | 1,500,000 characters |
| Maximum number of tools registered per agent                | 128                  |

The Agent Service limits in this table are fixed and apply uniformly across all subscription types. Agent Service doesn't impose separate rate limits on API calls. Rate limiting is applied at the model deployment level. See [Azure OpenAI quotas and limits](../../openai/quotas-limits) for model-specific rate limits.

## Limit error reference

When you exceed a limit, the Agent Service returns an error. Handle these errors gracefully in your application.

| Error scenario            | HTTP status | Error code               | Recommended action                 |
| ------------------------- | ----------- | ------------------------ | ---------------------------------- |
| File too large            | 400         | `file_size_exceeded`     | Split content into smaller files   |
| Vector store token limit  | 400         | `token_limit_exceeded`   | Reduce file content or split files |
| Thread message cap        | 400         | `message_limit_exceeded` | Create a new thread                |
| Message content too large | 400         | `content_size_exceeded`  | Use file search for large content  |
| Too many tools            | 400         | `tool_limit_exceeded`    | Remove unused tools                |
| Rate limit exceeded       | 429         | `rate_limit_exceeded`    | Implement exponential backoff      |

For example:

* **File exceeds the maximum size.** Uploading the file fails. Split the content into smaller files or reduce file size before you upload.
* **Vector store token limit.** Attaching a file to a vector store fails if the file exceeds the token limit. Reduce the file content or split it into multiple files.
* **Thread message cap.** Adding messages can fail after a thread reaches the message limit. Create a new thread for a new conversation session, or archive and rotate threads as part of your application design.
* **Message content size.** Creating a message can fail if the `text` content is too large. Send smaller messages, or move large content into files and use file search.
* **Tool registration cap.** Creating or updating an agent can fail if you register too many tools. Register only the tools you need, and prefer fewer, reusable tools.
* **Rate limit exceeded.** API calls to the model deployment are throttled. Implement exponential backoff with jitter.

For file search scenarios, see [Vector stores for file search](vector-stores) for guidance on managing vector store growth.

## Best practices to stay within limits

Use the following practices to reduce limit-related failures:

* **Keep files small and focused.** Prefer multiple smaller documents over a single large document.
* **Avoid very large messages.** Put long content in uploaded files and query it by using file search.
* **Plan for long conversations.** Treat threads as session state and rotate to new threads when conversations become very long.
* **Register only required tools.** Remove unused tools from agent definitions.
* **Monitor usage trends.** Track agent activity by using [Foundry Agent Service metrics](../how-to/metrics) to identify growth before you hit limits.

## Quotas and limits for models

Agents follow the quotas and rate limits for the model deployments they use.

For current model quotas and limits, see:

* [Azure OpenAI quotas and limits](../../openai/quotas-limits).
* [Microsoft Foundry Models quotas and limits](../../foundry-models/quotas-limits).

To view or request more model quota, see [Manage and increase quotas for resources with Microsoft Foundry (Foundry projects)](../../how-to/quota).

## Request a limit increase

The limits in this article are default values for Foundry Agent Service. If your workload requires higher limits:

* **Model quotas.** You can request increases for model deployment quotas. See [Manage and increase quotas for resources with Microsoft Foundry](../../how-to/quota).
* **Agent Service limits.** The file, message, and tool limits listed in this article are fixed service limits and can't be increased. Design your application to work within these constraints by using the best practices described earlier.

## Related content

* [Threads, runs, and messages in Foundry Agent Service](runtime-components)
* [Model and region support for Foundry Agent Service](model-region-support)
* [Tool support by region and model](tool-best-practice#tool-support-by-region-and-model)
* [Vector stores for file search](vector-stores)
* [Monitor Foundry Agent Service](../how-to/metrics)
* [Azure OpenAI quotas and limits](../../openai/quotas-limits)
* [Manage and increase quotas for resources with Microsoft Foundry](../../how-to/quota)

The agent development lifecycle in Microsoft Foundry spans from initial creation through production monitoring. Following this lifecycle helps you build reliable agents, catch issues early, and ship with confidence. Use the Foundry portal or code to build, customize, and test your agent's behavior. Then iterate with tracing, evaluation, and monitoring to improve quality and reliability. When you're ready, publish your agent as an agent application to share it and integrate it into your apps.

This article is for developers who want to build, test, and ship production-ready agents.

## Prerequisites

* A [Microsoft Foundry project](../../how-to/create-projects)
* Familiarity with the [Agents playground](../../concepts/concept-playgrounds)
* For code development: Familiarity with the [development environment setup](../../how-to/develop/install-cli-sdk)

## Lifecycle at a glance

Use this lifecycle as a practical checklist while you build and ship an agent.

1. **Choose an agent type**: Start with a prompt-based agent, a workflow, or a hosted agent.
2. **Create your agent and start testing**: Iterate in the playground or in code.
3. **Add tools and data**: Attach tools for retrieval and actions, and validate the configuration before you save.
4. **Save changes as versions**: Capture meaningful milestones and compare versions.
5. **Debug with tracing**: Use tracing to confirm tool calls, latency, and end-to-end behavior. For details, see [Agent tracing overview](../../observability/concepts/trace-agent-concept).
6. **Evaluate quality and safety**: Run repeatable evaluations to catch regressions before publishing. For conceptual guidance, see [Agent evaluators](../../concepts/evaluation-evaluators/agent-evaluators).
7. **Publish and integrate**: Publish a stable endpoint and integrate it into your application. For steps, see [Publish and share agents in Microsoft Foundry](../how-to/publish-agent).
8. **Monitor and iterate**: Monitor performance and quality in production, then update and republish as needed. For guidance, see [Monitor quality and safety](../../how-to/monitor-quality-safety).

## Agent types in Microsoft Foundry

There are three types of agents:

* **Prompt-based**: A prompt-based agent is a declaratively defined single agent that combines model configuration, instructions, tools, and natural language prompts to drive behavior. Extend it by attaching tools for knowledge and memory. Edit, version, test, evaluate, monitor, and publish prompt-based agents from the [Agents playground](../../concepts/concept-playgrounds) in the Foundry portal.

* **Workflow**: Use workflows to build a more advanced workflow that orchestrates a sequence of actions or coordinates multiple agents. Workflows have their own interface in the portal, but the same lifecycle applies. For details, see [Build a workflow in Microsoft Foundry](workflow).

* **Hosted (preview)**: Hosted agents are containerized agents that you build in code by using supported frameworks or custom code. Foundry Agent Service deploys and manages these agents. You don't edit hosted agents in the agent-building UI, but you can still invoke, evaluate, monitor, and publish them. For details, see [What are hosted agents?](hosted-agents)

Create prompt-based agents and workflows in the Foundry portal or your own development environment by using the CLI, SDK, or REST API. For more information, see the [quickstart](../../quickstarts/get-started-code).

## Creating a prompt-based agent

If you already know what kind of agent you want to create, name it and then start configuring its model instructions and tools.

<Callout type="note">
  After you name your agent, you can't change the name. In code, you refer to your agent by `<agent_name>:<version>`.
</Callout>

## Develop agents in code

If you prefer to work in code, use supported ways to bring your agent code into a development environment from which you can test locally and then deploy to Azure.

From the **Code** tab in the agent playground's chat pane, you can take a code snippet that references your agent to a dedicated Visual Studio Code for the Web cloud environment. The snippet comes preconfigured with the packages and extensions that you need, along with instructions to efficiently develop and deploy your Foundry agent to Azure. You can also copy the code snippet directly to your preferred development environment. For details, see the [playground documentation](../../concepts/concept-playgrounds#open-in-vs-code-capability).

## Core capabilities for the agent development lifecycle

The agent building experience offers integrated experiences for each core step of the agent development lifecycle. Use these core capabilities as you develop your production-ready agent application. Each capability has in-depth documentation where you can learn more.

### Save changes as versions

After you create the first version of a prompt-based agent or a workflow, save subsequent changes as new versions. You can test unsaved changes in the agent playground. But if you want to view conversation history, monitor your agent's performance, or run full evaluations, you need to save your changes.

Agent versioning provides the following capabilities for managing agent configurations and iterations. This system ensures that all changes are tracked, testable, and comparable across versions.

* **Version immutability**: Each version of an agent is immutable after you save it. Any modifications to an existing version require saving and creating a new version. This requirement helps ensure version integrity and prevents accidental overwrites.

* **Draft state management**: You can test agents in an unsaved state for experimentation. You lose unsaved changes if you leave the Foundry portal, so save frequently to preserve important modifications.

* **Version control operations**: You can direct requests to specific agent versions to enable controlled deployment and rollback capabilities.

* **Version history navigation**: Access the version history for any agent, go to any specific version, and perform the following comparisons:

  | Comparison type | Description                                                                     |
  | --------------- | ------------------------------------------------------------------------------- |
  | Agent setup     | Compare configuration settings between versions using the version dropdown list |
  | Chat output     | Analyze response differences between agent versions using identical inputs      |
  | YAML definition | Review differences in agent definitions                                         |

### Add tools

Make your agent more powerful by giving it knowledge (specific files or indexes) or by allowing it to take actions (calling external APIs). Tools are available for most use cases, from simple file uploads to custom Model Context Protocol (MCP) server connections. For more complicated tools, you might need to configure authentication or add connections as part of attaching them to an agent.

To save an agent with a tool attached, you must successfully configure the tool. Reuse configured tools across agents. For information about available tools, see the [tools catalog](tool-catalog).

### Debug and validate by using tracing (preview)

As you add tools and iterate on prompts, use tracing to validate end-to-end behavior:

* Confirm whether the agent called the tools you expected.
* Inspect tool inputs and outputs.
* Identify latency hotspots across model and tool calls.

For more information, see [Agent tracing overview](../../observability/concepts/trace-agent-concept).

### Evaluate quality and safety (preview)

Before you publish your agent (and after any meaningful change), run evaluations to catch regressions and measure quality consistently across versions.

* For the key evaluation dimensions for agents, see [Agent evaluators](../../concepts/evaluation-evaluators/agent-evaluators).
* For a code-first workflow you can automate, see [Evaluate your AI agents locally](../../how-to/develop/agent-evaluate-sdk).

### Monitor after publishing

After you publish an agent application, treat it like production software:

* Monitor quality and safety signals.
* Review traces when behavior changes.
* Update and republish when you fix issues or make improvements.

For guidance, see [Monitor quality and safety](../../how-to/monitor-quality-safety).

### Plan for identity and permissions

Tools and downstream resources often require authentication. When you publish an agent, its identity and permission model can change. Make sure your published agent has only the access it needs.

For details, see [Agent identity concepts in Microsoft Foundry](agent-identity).

### Security and access

Treat your agent configuration like application code. Protect secrets and permissions throughout the lifecycle:

* Use least privilege and role assignments instead of embedding keys. For more information, see [Role-based access control in Foundry portal](../../concepts/rbac-foundry).
* Store secrets in a managed secret store and reference them through connections instead of hardcoding them in code, configuration files, or prompts. For guidance, see [Set up a Key Vault connection](../../how-to/set-up-key-vault-connection).
* Before publishing, confirm that the agent identity and tool connections in the published agent application have only the access they need. For details, see [Agent identity concepts in Microsoft Foundry](agent-identity).

### Publish your agent or workflow

After you create an agent or workflow version that you're happy with, [publish it as an agent application](../how-to/publish-agent). You get a stable endpoint that you can open and test in the browser, share with others, or embed in your existing applications. You and your collaborators can validate performance and identify what needs refinement. Make any necessary updates and republish a new version at any time.

<Callout type="important">
  Permissions assigned to the project identity don't automatically transfer to the published agent. After publishing, reassign the necessary privileges to the agent application's identity.
</Callout>

## Common agent development pitfalls

* **Unsaved changes are temporary**: If you want to compare versions, view history, or run full evaluations, save your changes as a version.
* **Tools must be configured before saving**: If a tool requires authentication or a connection, complete setup before you save.
* **Publishing can require permission updates**: After publishing, recheck resource access for the published agent identity and remove any access the agent no longer needs.

## Related content

**Learn more about agent types:**

* [What are hosted agents?](hosted-agents)
* [Agent runtime components](runtime-components)

**Configure and extend agents:**

* [Discover tools in Foundry Tools](tool-catalog)
* [Best practices for using tools in Microsoft Foundry Agent Service](tool-best-practice)

**Publish and monitor agents:**

* [Publish and share agents in Microsoft Foundry](../how-to/publish-agent)
* [Monitor quality and safety](../../how-to/monitor-quality-safety)

**Debug and evaluate:**

* [Agent tracing overview](../../observability/concepts/trace-agent-concept)
* [Agent evaluators](../../concepts/evaluation-evaluators/agent-evaluators)

Agent runtime components are the core objects—agents, conversations, and responses—that power stateful, multi-turn interactions in Microsoft Foundry Agent Service. Together, these components let you generate outputs, persist state across turns, and build conversational applications.

This article explains the roles of an **agent**, **conversation**, and **response**, and how they work together during response generation.

## Prerequisites

* A [Microsoft Foundry project](../../how-to/create-projects).
* Familiarity with the [agent development lifecycle](development-lifecycle) (optional).

## How runtime components work together

When you work with an agent, you follow a consistent pattern:

* **Create an agent**: Define an agent to start sending messages and receiving responses.
* **Create a conversation (optional)**: Use a conversation to maintain history across turns. If you don't use a conversation, carry forward context by using the output from a previous response.
* **Generate a response**: The agent processes input items in the conversation and any instructions provided in the request. The agent might append items to the conversation.
* **Check response status**: Monitor the response until it finishes (especially in streaming or background mode).
* **Retrieve the response**: Display the generated response to the user.

The following diagram illustrates how these components interact in a typical agent loop.

![Diagram that shows the agent runtime loop: an agent definition and optional conversation history feed response generation, which can call tools, append items back into the conversation, and produce output items you display to the user.](https://learn.microsoft.com/azure/ai-foundry/agents/default/agents/media/runtime-components.png)

You provide user input (and optionally conversation history), the service generates a response (including tool calls when configured), and the resulting items can be reused as context for the next turn.

## What is an agent?

An agent is a persisted orchestration definition that combines AI models, instructions, code, tools, parameters, and optional safety or governance controls.

Store agents as named, versioned assets in Microsoft Foundry. During response generation, the agent definition works with interaction history (conversation or previous response) to process and respond to user input.

## What is a conversation?

A conversation manages state automatically, so you don't need to pass inputs manually for each turn.

Conversations are durable objects with unique identifiers. After creation, you can reuse them across sessions.

Conversations store items, which can include messages, tool calls, tool outputs, and other data.

### When to use a conversation

Use a conversation when you want:

* **Multi-turn continuity**: Keep a stable history across turns without rebuilding context yourself.
* **Cross-session continuity**: Reuse the same conversation for a user who returns later.
* **Easier debugging**: Inspect what happened over time (for example, tool calls and outputs).

If you don't create a conversation, you can still build multi-turn flows by using the output from a previous response as the starting point for the next request. This approach gives you more flexibility than the older thread-based pattern, where state was tightly coupled to thread objects. For migration guidance, see [Migrate to the Agents SDK](../how-to/migrate).

## Conversation items

Conversations store **items** rather than only chat messages. Items capture what happened during response generation so the next turn can reuse that context.

Common item types include:

* **Message items**: User or assistant messages.
* **Tool call items**: Records of tool invocations the agent attempted.
* **Tool output items**: Outputs returned by tools (for example, retrieval results).
* **Output items**: The response content you display back to the user.

For examples that show how conversations and responses work together in code, see [Create and use memory in Foundry Agent Service](../how-to/memory-usage).

## How responses work

Response generation invokes the agent. The agent uses its configuration and any provided history (conversation or previous response) to perform tasks by calling models and tools. As part of response generation, the agent appends items to the conversation.

You can also generate a response without defining an agent. In this case, you provide all configurations directly in the request and use them only for that response. This approach is useful for simple scenarios with minimal tools.

## Streaming and background responses

Some response generation modes return results incrementally (streaming) or complete asynchronously (background). In these cases, you typically monitor the response until it finishes and then consume the final output items.

For details about response modes and how to consume outputs, see [Responses API](../../openai/how-to/responses).

## Security and data handling

Because conversations and responses can persist user-provided content and tool outputs, treat runtime data like application data:

* **Avoid storing secrets in prompts or conversation history**. Use connections and managed secret stores instead (for example, [Set up a Key Vault connection](../../how-to/set-up-key-vault-connection)).
* **Use least privilege for tool access**. When a tool accesses external systems, the agent can potentially read or send data through that tool.
* **Be careful with non-Microsoft services**. If your agent calls tools backed by non-Microsoft services, some data might flow to those services. For related considerations, see [Discover tools in the Foundry Tools](tool-catalog).

## Limits and constraints

Limits can depend on the model, region, and the tools you attach (for example, streaming availability and tool support). For current availability and constraints for responses, see [Responses API](../../openai/how-to/responses).

## Related content

* [Agent development lifecycle](development-lifecycle)
* [Discover tools in the Foundry Tools](tool-catalog)
* [Best practices for using tools in Microsoft Foundry Agent Service](tool-best-practice)
* [Publish and share agents in Microsoft Foundry](../how-to/publish-agent)
* [Agent tracing overview](../../observability/concepts/trace-agent-concept)

An *agent identity* is a specialized identity type in [Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/fundamentals/what-is-entra) that's designed specifically for AI agents. It provides a standardized framework for governing, authenticating, and authorizing AI agents across Microsoft services. This framework enables agents to securely access resources, interact with users, and communicate with other systems.

Microsoft Foundry automatically provisions and manages agent identities throughout the agent lifecycle. This integration simplifies permission management while maintaining security and auditability as agents move from development to production.

This article explains how agent identities relate to Microsoft Entra ID objects, how Foundry uses them when an agent calls tools, and how to apply least-privilege access with Azure role-based access control (RBAC).

## Prerequisites

* Understanding of [Microsoft Entra ID and OAuth](https://learn.microsoft.com/en-us/entra/architecture/auth-sync-overview) authentication
* Familiarity with [Azure role-based access control (RBAC)](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview)
* Basic knowledge of AI agents and their runtime requirements

For Foundry-specific RBAC roles and scopes, see [Azure role-based access control in Foundry](../../concepts/rbac-foundry).

## How agent identities work in Foundry

Foundry uses Microsoft Entra ID agent identities to support two related needs:

* **Management and governance**: Give administrators a consistent way to inventory agents, apply policies, and audit activity.
* **Tool authentication**: Let an agent authenticate to downstream systems (for example, Azure Storage) without embedding secrets in prompts, code, or connection strings.

At a high level, Foundry does the following:

1. Provisions an **agent identity blueprint** and one or more **agent identities** in Microsoft Entra ID.
2. Assigns RBAC roles (or other permission models, depending on the target system) to the agent identity.
3. When the agent invokes a tool, Foundry requests an access token for the downstream service and uses that token to authenticate the tool call.

### Terms used in this article

| Term                     | What it means in Foundry                                                                                              |
| ------------------------ | --------------------------------------------------------------------------------------------------------------------- |
| Agent identity           | A Microsoft Entra ID service principal that represents the agent at runtime.                                          |
| Agent identity blueprint | A Microsoft Entra ID object that governs a class of agent identities and is used for lifecycle operations.            |
| `agentIdentityId`        | The identifier you use when assigning permissions to the agent identity.                                              |
| Audience                 | The resource identifier for the downstream service the token is meant for (for example, `https://storage.azure.com`). |

## Key concepts

The Agent ID platform framework introduces formal *agent identities* and *agent identity blueprints* in Microsoft Entra ID to represent AI agents. You can use this framework to securely communicate with AI agents. This framework also enables those AI agents to securely communicate with web services, other AI agents, and various systems.

### Agent identity

An agent identity is a special service principal in Microsoft Entra ID. It represents an identity that the agent identity blueprint created and is authorized to impersonate.

#### Security benefits

Agent identities help address specific security challenges that AI agents pose:

* Distinguish operations that AI agents perform from operations that workforce, customer, or workload identities perform.
* Enable AI agents to gain right-sized access across systems.
* Prevent AI agents from gaining access to critical security roles and systems.
* Scale identity management to large numbers of AI agents that can be quickly created and destroyed.

#### Authentication capabilities

Agent identities support two key authentication scenarios:

* **Attended (delegated access or on-behalf-of flow)**: The agent operates on behalf of a human user. It uses delegated permissions that the user grants. The agent can then act under the user's authority to access resources or APIs as that user.
* **Unattended**: The agent acts under its own authority. It acts as a service or an application identity by using its app-assigned roles, RBAC, or Microsoft Graph permissions. Or it acts as an autonomous identity with user-like claims that allow the agent to authenticate and operate independently.

### Agent identity blueprint

An agent identity blueprint serves as the reusable, governing template from which all associated agent identities are created. It corresponds to a *kind*, *type*, or *class* of agents. It acts as the management object for all agent identity instances of that class.

#### Blueprint capabilities

Agent identity blueprints serve three essential purposes:

**Type classification**: The blueprint establishes the category of agent, such as "Contoso Sales Agent." This classification enables administrators to:

* Apply Conditional Access policies to all agents of that type.
* Disable or revoke permissions for all agents of that kind.
* Audit and govern agents at scale through consistent, blueprint-based controls.

**Identity creation authority**: Services that create agent identities use the blueprint to authenticate. Blueprints have OAuth credentials that services use to request tokens from Microsoft Entra ID for creating, updating, or deleting agent identities. These credentials include client secrets, certificates, or federated credentials like managed identities.

**Runtime authentication platform**: The hosting service uses the blueprint during runtime authentication. The service requests an access token by using the blueprint's OAuth credentials. It then presents that token to Microsoft Entra ID to obtain a token for one of its agent identities.

#### Blueprint metadata

For example, an organization might use an AI agent called the "Contoso Sales Agent." The blueprint defines information such as:

* The name of the agent type: "Contoso Sales Agent."
* The publisher or organization responsible for the blueprint: "Contoso."
* The roles that the agent might perform: "sales manager" or "sales rep."
* Microsoft Graph permissions or delegated scopes: "read the signed-in user's calendar."

## Foundry integration

Foundry automatically integrates with Microsoft Entra Agent ID by creating and managing identities throughout the agent development lifecycle. When you create your first agent in a Foundry project, the system provisions a default agent identity blueprint and a default agent identity for your project.

### Shared project identity

All unpublished or in-development agents within the same project share a common identity. This design simplifies permission management because unpublished agents typically require the same access patterns and permission configurations. The shared identity approach provides these benefits:

* **Simplified administration**: Administrators can centrally manage permissions for all in-development agents within a project.
* **Reduced identity sprawl**: Using a single identity per project prevents unnecessary identity creation during early experimentation.
* **Developer autonomy**: After the shared identity is configured, developers can independently build and test agents without repeatedly configuring new permissions.

To find your shared agent identity blueprint and agent identity, go to your Foundry project in the [Azure portal](https://portal.azure.com). On the **Overview** pane, select **JSON View**. Choose the latest API version to view and copy the identities.

![Screenshot of the JSON view in the Azure portal displaying an agent identity blueprint and agent identity details for a Foundry project.](https://learn.microsoft.com/azure/ai-foundry/agents/default/agents/media/agent-identity/azure-agent-identity-json-view.png)

### Distinct agent identity

When an agent's permissions, auditability, or lifecycle requirements diverge from the project defaults, you should upgrade to a distinct identity. Publishing an agent automatically creates a dedicated agent identity blueprint and agent identity. Both are bound to the agent application resource. This distinct identity represents the agent's system authority for accessing its own resources.

Common scenarios that require distinct identities include:

* Agents ready for integration testing.
* Agents prepared for production consumption.
* Agents that require unique permission sets.
* Agents that need independent audit trails.

To find the distinct agent identity blueprint and agent identity, go to your agent application resource in the Azure portal. On the **Overview** pane, select **JSON View**. Choose the latest API version to view and copy the identities.

## Tool authentication

Agents access remote resources and tools by using agent identities for authentication. The authentication mechanism differs based on the agent's publication status:

* **Unpublished agents**: Authenticate by using the shared project's agent identity.
* **Published agents**: Authenticate by using the unique agent identity that's associated with the agent application.

When you [publish an agent](../how-to/publish-agent), you must reassign RBAC permissions to the new agent identity for any resources that the agent needs to access. This reassignment ensures that the published agent maintains appropriate access while operating under its distinct identity.

### Supported tools

Currently, the tools that support authentication with an agent identity are:

* **Model Context Protocol (MCP)**: Use your agent's identity to authenticate with MCP servers that support agent identity authentication. For details, see [Model Context Protocol (preview)](../how-to/tools/model-context-protocol) and [MCP server authentication](../how-to/mcp-authentication).
* **Agent-to-Agent (A2A)**: Enable secure communication between agents by using agent identities. For details, see [Agent-to-Agent tool (preview)](../how-to/tools/agent-to-agent) and [Agent2Agent (A2A) authentication](agent-to-agent-authentication).

Other tools and integrations might use different authentication methods (for example, key-based authentication or OAuth identity passthrough). Use the tool documentation to confirm supported authentication.

### Configure MCP tool authentication

To configure an MCP tool to authenticate by using an agent identity:

1. Ensure that you have an MCP server that you want to configure as a tool for your agent.

2. Get the ID for the agent identity. In the Azure portal, go to your Foundry project. On the **Overview** pane, select **JSON View** and choose the latest API version. Copy the `agentIdentityId` value.

3. Create a connection to your remote MCP server that uses `AgenticIdentityToken` as the authentication type. The **Audience** box specifies which service or API the token is intended to access. For example:

   * For an MCP server that lists blobs in your storage account, set the audience as `https://storage.azure.com`.
   * For an Azure Logic Apps MCP server, set the audience as `https://logic.azure.com`.

   You can create the connection by using either the REST API or the Foundry portal:

   <Tabs>
     <Tab title="REST API">
       To get an access token, run the commands `az login` and then `az account get-access-token`.

       ```http
       PUT https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group}}/providers/Microsoft.CognitiveServices/accounts/{{account_name}}/projects/{{project_name}}/connections/{{mcp_connection_name}}?api-version={{api_version}}
       Authorization: Bearer {{token}}
       Content-Type: application/json

       {
           "tags": null,
           "location": null,
           "name": "{YOUR_CONNECTION_NAME}",
           "type": "CognitiveServices/accounts/projects/connections",
           "properties": {
           "authType": "AgenticIdentityToken",
           "group": "ServicesAndApps",
           "category": "RemoteTool",
           "expiryTime": null,
           "target": "{YOUR_MCP_REMOTE_URL}",
           "isSharedToAll": true,
           "sharedUserList": [],
           "audience": "{YOUR_AUDIENCE}",
           "Credentials": {},
           "metadata": {
               "ApiType": "Azure"
           }
           }
       }
       ```
     </Tab>

     <Tab title="Foundry portal">
       1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.

          ![](https://learn.microsoft.com/azure/ai-foundry/agents/media/version-banner/new-foundry.png)

       2. Select **Build**.

       3. Select **Agents**.

       4. Select the agent that you want to use.

       5. Under **Tools**, select **+ Add**.

       6. On the **Custom** tab, select **Model Context Protocol (MCP)**.

       7. Under **Authentication**, select **Microsoft Entra**. Under **Type**, select **Agent identity**.

       8. Fill in the endpoint and audience information, and then select **Connect**.
     </Tab>
   </Tabs>

4. Assign to the agent identity the required permissions for its actions by using the `agentIdentityId` value that you copied. For example:

   * For an MCP server that lists blob containers, assign the **Storage Blob Data Contributor** role at the **Azure Storage Account** scope.
   * For an Azure Logic Apps MCP server, assign the **Logic Apps Standard Reader** role on the **Logic App** resource.

5. Connect the tool. If you're using code, create an agent with the MCP tool. (For details, see the MCP documentation.) If you're using the Foundry portal, the MCP tool is automatically added to the agent.

When the agent invokes the MCP server, it uses the available agent identity to obtain an authorization token for the **audience** value. It then passes the token to the MCP server for authentication.

## Security considerations

Agent identities help you reduce risk by removing the need to embed long-lived credentials in agent configurations. Use these practices to keep access least-privilege and auditable:

* Assign only the permissions the agent needs for its tool actions. Prefer narrow scopes (resource or resource group) over subscription-wide access.
* Treat the shared project identity as a broader blast radius. If an agent needs tighter controls or separate auditing, publish it so it gets a distinct identity, and assign roles to that new identity.
* Review and log access to non-Microsoft tools and servers. If a tool call leaves Microsoft services, your data handling and retention depend on the external provider.

## Limitations

* Only some tools currently support agent identity authentication. Check the tool documentation for supported authentication.
* Publishing an agent changes which identity is used for tool calls (shared project identity versus distinct agent identity). Plan for role reassignment when you publish.

## Common issues

These issues commonly cause tool authentication failures when using agent identities:

* **Roles assigned to the wrong identity**: Confirm you granted permissions to the current identity used by the agent (shared project identity for unpublished agents, distinct identity for published agents).
* **Missing role assignments**: Ensure the agent identity has the required RBAC role on the target resource. For Foundry roles and scopes, see [Azure role-based access control in Foundry](../../concepts/rbac-foundry).
* **Incorrect audience**: Ensure the audience matches the downstream service you’re calling (for example, `https://storage.azure.com` for Azure Storage).

For tool-specific troubleshooting, see the tool documentation:

* [Model Context Protocol (preview)](../how-to/tools/model-context-protocol)
* [Agent-to-Agent tool (preview)](../how-to/tools/agent-to-agent)

## Manage agent identities

You can view and manage all agent identities in your tenant through the Microsoft Entra admin center. Go to the [tab for agent identities](https://entra.microsoft.com/?Microsoft_AAD_RegisteredApps=stage1\&exp.EnableAgentIDUX=true#view/Microsoft_AAD_RegisteredApps/AllAgents.MenuView/%7E/allAgentIds) to see an inventory of all agents in your tenant, including Foundry agents, Microsoft Copilot Studio agents, and others.

![Screenshot of the Microsoft Entra admin center that shows the tab for agent identities with an inventory of all agents in the tenant.](https://learn.microsoft.com/azure/ai-foundry/agents/default/agents/media/agent-identity/entra-admin-center-agent-identities.png)

In this experience, you can enable built-in security controls, including:

* **Conditional Access**: Apply access policies to agent identities.
* **Identity protection**: Monitor and protect agent identities from threats.
* **Network access**: Control network-based access for agents.
* **Governance**: Manage expiration, owners, and sponsors for agent identities.

For more information about Microsoft Entra Agent ID features, see [Microsoft Entra documentation](https://learn.microsoft.com/en-us/entra/fundamentals/what-is-entra).

## Related content

* [Publish and share agents in Microsoft Foundry](../how-to/publish-agent)
* [Azure role-based access control in Foundry](../../concepts/rbac-foundry)
* [MCP server authentication](../how-to/mcp-authentication)

When you build agentic applications by using open-source frameworks, you typically manage containerization, web server setup, security integration, memory persistence, infrastructure scaling, data transmission, instrumentation, and version rollbacks. These tasks become even more challenging in heterogeneous cloud environments.

<Callout type="important">
  Hosted agents are currently in **public preview**. See [Limits, pricing, and availability (preview)](#limits-pricing-and-availability-preview) for current constraints.
</Callout>

Hosted agents in Foundry Agent Service solve these challenges for Microsoft Foundry users. By using this managed platform, you can deploy and operate AI agents securely and at scale. You can use your custom agent code or a preferred agent framework with streamlined deployment and management.

## Prerequisites

* A [Microsoft Foundry project](../../how-to/create-projects)
* Basic understanding of [containerization and Docker](https://learn.microsoft.com/en-us/azure/container-instances/container-instances-overview)
* Familiarity with [Azure Container Registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-intro)
* Knowledge of your preferred agent framework (LangGraph, Microsoft Agent Framework, or custom code)

## At a glance

Hosted agents let you bring your own agent code and run it as a managed containerized service.

Use this article to:

* Understand what hosted agents are and when to use them.
* Package and test your agent locally before deployment.
* Create, manage, publish, and monitor hosted agents.

If you want to jump to a task, see:

* [Package code and test locally](#package-code-and-test-locally)
* [Create a hosted agent](#create-a-hosted-agent)
* [Manage hosted agents](#manage-hosted-agents)
* [Publish hosted agents to channels](#publish-hosted-agents-to-channels)
* [Troubleshoot hosted agent endpoints](#troubleshoot-hosted-agent-endpoints)

## Limits, pricing, and availability (preview)

Hosted agents are currently in preview.

* **Private networking support**: You can't create hosted agents by using the standard setup for network isolation within network-isolated Foundry resources. For details, see [Configure virtual networks](../how-to/virtual-networks).
* **Preview limits**: For the full list of preview limits, see [Limitations during preview](#limitations-during-preview).
* **Pricing**: For updates on pricing, see the Foundry [pricing page](https://azure.microsoft.com/pricing/details/ai-foundry/).

### Region availability

Hosted Agents are supported in the following regions:

* Brazil South
* Canada East
* East US
* France Central
* Germany West Central
* Italy North
* North Central US
* South Africa North
* South Central US
* South India
* Spain Central
* Sweden Central
* Canada Central
* Korea Central
* Southeast Asia
* Australia East
* East US 2
* Japan East
* UAE North
* UK South
* West US
* West US 3
* Norway East
* Poland Central
* Switzerland North

## Security and data handling

Treat a hosted agent like production application code.

* **Don't put secrets in container images or environment variables**. Use managed identities and connections, and store secrets in a managed secret store. For guidance, see [Set up a Key Vault connection](../../how-to/set-up-key-vault-connection).
* **Be careful with non-Microsoft tools and servers**. If your agent calls tools backed by non-Microsoft services, some data might flow to those services. Review data sharing, retention, and location policies for any non-Microsoft service you connect.

## Understand key concepts

### Hosted agents

Hosted agents are containerized agentic AI applications that run on Agent Service. Unlike prompt-based agents, developers build hosted agents through code and deploy them as container images on Microsoft-managed pay-as-you-go infrastructure.

Hosted agents follow a standard lifecycle: create, start, update, stop, and delete. Each phase provides specific capabilities and status transitions to help you manage your agent deployments effectively.

<Callout type="note">
  Hosted agents are currently in preview. For current constraints and availability, see [Limits, pricing, and availability (preview)](#limits-pricing-and-availability-preview).
</Callout>

### Hosting adapter

The hosting adapter is a framework abstraction layer that helps expose supported agent frameworks (or your custom code) as an HTTP service for local testing and hosted deployments.

The hosting adapter provides several key benefits for developers:

**Simplified local testing**: Run your agent locally and validate the HTTP surface area before you containerize and deploy.

**Automatic protocol translation**: The adapter handles all complex conversions between the Foundry request and response formats and your agent framework's native data structures. These activities include:

* Conversation management
* Message serialization
* Streaming event generation

**Observability integration**: Export traces, metrics, and logs by using OpenTelemetry.

**Seamless Foundry integration**: Your locally developed agents work with the Foundry Responses API, conversation management, and authentication flows.

### Managed service capabilities

Agent Service handles:

* Provisioning and autoscaling of agents
* Conversation orchestration and state management
* Identity management
* Integration with Foundry tools and models
* Built-in observability and evaluation capabilities
* Enterprise-grade security, compliance, and governance

<Callout type="important">
  If you use Agent Service to host agents that interact with non-Microsoft servers or agents, you take on the risk. Review all data that you share with non-Microsoft servers or agents. Be aware of non-Microsoft practices for retention and location of data. You're responsible for managing whether your data flows outside your organization's Azure compliance and geographic boundaries, along with any related implications.
</Callout>

### Framework and language support

| Framework                 | Python | C# |
| ------------------------- | ------ | -- |
| Microsoft Agent Framework | ✅      | ✅  |
| LangGraph                 | ✅      | ❌  |
| Custom code               | ✅      | ✅  |

### Public adapter packages

* Python: `azure-ai-agentserver-core`, `azure-ai-agentserver-agentframework`, `azure-ai-agentserver-langgraph`
* .NET: `Azure.AI.AgentServer.Core`, `Azure.AI.AgentServer.AgentFramework`

## Package code and test locally

Before you deploy to Microsoft Foundry, build and test your agent locally:

1. **Run your agent locally**: Use the hosting adapter to `azure-ai-agentserver-*` package to wrap your agent code and start a local web server that automatically exposes your agent as a REST API.
2. **Test by using REST calls**: The local server runs on `localhost:8088` and accepts standard HTTP requests.
3. **Build the container image**: Create a container image from your source code.
4. **Use the Azure Developer CLI**: Use `azd` to streamline the packaging and deployment process.

### Wrap your agent code with the hosting adapter and test locally

**Sample agent authored using Microsoft Agent Framework**

```python

import os
from datetime import datetime
from zoneinfo import ZoneInfo
from dotenv import load_dotenv

load_dotenv(override=True)

from agent_framework import ai_function, ChatAgent
from agent_framework.azure import AzureAIAgentClient
from azure.ai.agentserver.agentframework import from_agent_framework
from azure.identity import DefaultAzureCredential

# Configure these for your Azure AI Foundry project
PROJECT_ENDPOINT = os.getenv("PROJECT_ENDPOINT")  # e.g., "https://<resource>.services.ai.azure.com/api/projects/<project>"
MODEL_DEPLOYMENT_NAME = os.getenv("MODEL_DEPLOYMENT_NAME", "gpt-4.1")  # Your model deployment name


@ai_function
def get_local_date_time(iana_timezone: str) -> str:
    """
    Get the current date and time for a given timezone.

    This is a LOCAL Python function that runs on the server - demonstrating how code-based agents
    can execute custom logic that prompt agents cannot access.

    Args:
        iana_timezone: The IANA timezone string (e.g., "America/Los_Angeles", "America/New_York", "Europe/London")

    Returns:
        The current date and time in the specified timezone.
    """
    try:
        tz = ZoneInfo(iana_timezone)
        current_time = datetime.now(tz)
        return f"The current date and time in {iana_timezone} is {current_time.strftime('%A, %B %d, %Y at %I:%M %p %Z')}"
    except Exception as e:
        return f"Error: Unable to get time for timezone '{iana_timezone}'. {str(e)}"


# Create the agent with a local Python tool
agent = ChatAgent(
    chat_client=AzureAIAgentClient(
        project_endpoint=PROJECT_ENDPOINT,
        model_deployment_name=MODEL_DEPLOYMENT_NAME,
        credential=DefaultAzureCredential(),
    ),
    instructions="You are a helpful assistant that can tell users the current date and time in any location. When a user asks about the time in a city or location, use the get_local_date_time tool with the appropriate IANA timezone string for that location.",
    tools=[get_local_date_time],
)

if __name__ == "__main__":
    from_agent_framework(agent).run()
```

Refer to the [samples repo](https://github.com/microsoft-foundry/foundry-samples/tree/main/samples/python/hosted-agent) for code samples in LangGraph and custom code.

When you run your agent locally by using the hosting adapter, it automatically starts a web server on `localhost:8088`. You can test your agent by using any REST client.

```http
@baseUrl = http://localhost:8088

POST {{baseUrl}}/responses
Content-Type: application/json
{
    "input": "Where is Seattle?"
}
```

This local testing approach lets you:

* Validate agent behavior before containerization.
* Debug issues in your development environment.
* Test different input scenarios quickly.
* Verify API compatibility with the Foundry Responses API.

## Create a hosted agent

### Create a hosted agent using VS Code Foundry extension

You can use the [Foundry extension for Visual Studio Code](../how-to/vs-code-agents-workflow-pro-code?view=foundry\&preserve-view=true) to create hosted agents.

### Create a hosted agent by using the Azure Developer CLI

Developers can use the Azure Developer CLI `ai agent` extension for seamless and rapid provisioning and deployment of their agentic applications on Microsoft Foundry.

This extension simplifies the setup of Foundry resources, models, tools, and knowledge resources. For example, it simplifies the setup of Azure Container Registry for bringing your own container, Application Insights for logging and monitoring, a managed identity, and role-based access control (RBAC). In other words, it provides everything you need to get started with hosted agents in Foundry Agent Service.

This extension is currently in preview. Don't use it for production.

To get started:

1. Install the Azure Developer CLI on your device.

   If you already have the Azure Developer CLI installed, check if you have the latest version of `azd` installed:

   ```bash
   azd version
   ```

   To upgrade to the latest version, see [Install or update the Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd).

2. If you're starting with no existing Foundry resources and you want to simplify all the required infrastructure provisioning and RBAC, download the Foundry starter template. The template automatically installs the `ai agent` extension. When prompted, you can provide an environment name which creates a resource group named `rg-<name-you-provide>`.

   ```bash
   azd init -t https://github.com/Azure-Samples/azd-ai-starter-basic
   ```

   To check all installed extensions:

   ```bash
   azd ext list
   ```

   Make sure you have the latest version of the Foundry `azd` agent extension installed.

   If you have an existing Foundry project where you want to deploy your agent, and you want to provision only the additional resources that you might need for deploying your agent, run this command afterward:

   ```bash
   azd ai agent init --project-id /subscriptions/[SUBSCRIPTIONID]/resourceGroups/[RESOURCEGROUPNAME]/providers/Microsoft.CognitiveServices/accounts/[ACCOUNTNAME]/projects/[PROJECTNAME]
   ```

3. Initialize the template by configuring the parameters in the agent definition:

   ```bash
   azd ai agent init -m <repo-path-to-agent.yaml>
   ```

   The GitHub repo for an agent that you want to host on Foundry contains the application code, referenced dependencies, Dockerfile for containerization, and the `agent.yaml` file that contains your agent's definition. To configure your agent, set values for the parameters that you're prompted for. This action registers your agent under `Services` in `azure.yaml` for the downloaded template. You can get started with samples on [GitHub](https://github.com/azure-ai-foundry/foundry-samples).

4. To open and view all Bicep and configuration files associated with your `azd`-based deployments, use this command:

   ```bash
   code .
   ```

5. Package, provision, and deploy your agent code as a managed application on Foundry:

   ```bash
   azd up
   ```

   This command abstracts the underlying execution of the commands `azd infra generate`, `azd provision`, and `azd deploy`. It also creates a hosted agent version and deployment on Foundry Agent Service. If you already have a version of a hosted agent, `azd` creates a new version of the same agent. For more information, see the [Azure CLI documentation](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/extensions/azure-ai-foundry-extension).

To learn more about how you can do non-versioned updates, along with starting, stopping, and deleting your hosted agent deployments and versions, see the [management section](#manage-hosted-agents) of this article.

Make sure you have RBAC enabled so that `azd` can provision the services and models for you. For Foundry role guidance, see [Role-based access control in Foundry portal](../../concepts/rbac-foundry). For Azure built-in roles, see [Azure built-in roles](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles).

### Roles and permissions

* If you have an existing Foundry resource and need to create a new Foundry project to deploy a hosted agent, you need **Azure AI Owner** roles.

* If you have an existing project and want to create the model deployment and container registry in the project, you need **Azure AI Owner** role on Foundry in addition to the **Contributor** role on the Azure subscription.

* If you have everything configured in the project to deploy a hosted agent, you need **Reader** on the Foundry account and **Azure AI User** on the project.

Refer to [this article](../../concepts/authentication-authorization-foundry#built-in-roles-overview) to learn more about built-in roles in Foundry.

### Resource cleanup

To prevent unnecessary charges, clean up your Azure resources after you complete your work with the application.

When to clean up:

* After you finish testing or demonstrating the application.
* When the application is no longer needed or you transition to a different project or environment.
* When you complete development and are ready to decommission the application.

To delete all associated resources and shut down the application, run the following command:

```bash
azd down
```

This process might take up to 20 minutes to complete.

## Create a hosted agent by using the Foundry SDK

When you create a hosted agent, the system registers the agent definition in Microsoft Foundry and tries to create a deployment for that agent version.

### Pre-deployment checklist

Before creating a hosted agent, complete these steps **in order**:

1. **Ensure your access**: Ensure that you have access to assign roles in Azure Container Registry. You need at least User Access Administrator or Owner permissions on the container registry.
2. **Install the Azure AI Projects SDK**: run the following command: `pip install --pre "azure-ai-projects>=2.0.0b3"`
3. **Create an Azure Container Registry**: [Create a private container registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-get-started-portal)
4. **Build your Docker image with the correct platform**: [Build and push your docker image.](#build-and-push-your-docker-image-to-azure-container-registry)
5. **Push your image to YOUR registry**: [Build and push your docker image.](#build-and-push-your-docker-image-to-azure-container-registry) Replace the sample URLs (`YOUR_ACR_NAME, YOUR_IMAGE_NAME`, `YOUR_TAG`) with your actual values for your docker image and Azure Container Registry.
6. **Configure Azure Container Registry permissions**: [Configure Azure Container Registry permissions](#configure-azure-container-registry-permissions)
7. **Create capability host**: [Create an account-level capability host](#create-an-account-level-capability-host)

### Build and push your Docker image to Azure Container Registry

To build your agent as a Docker container and upload it to Azure Container Registry:

1. Build your Docker image locally:

   <Callout type="important">
     **You MUST specify `--platform linux/amd64` when building your Docker image.** Hosted agents run on Linux AMD64 infrastructure. Images built for other architectures (such as ARM64 on Apple Silicon Macs) will fail to start with container runtime errors.
   </Callout>

   ```bash
   docker build --platform linux/amd64 -t <YOUR_IMAGE_NAME>:<YOUR_TAG> .
   ```

   Refer to sample Dockerfile for [Python](https://github.com/microsoft-foundry/foundry-samples/blob/main/samples/python/hosted-agents/agent-framework/agents-in-workflow/Dockerfile) and [C#](https://github.com/microsoft-foundry/foundry-samples/blob/main/samples/csharp/hosted-agents/AgentsInWorkflows/Dockerfile).

2. Sign in to Azure Container Registry:

   ```bash
   az acr login --name <YOUR_ACR_NAME>
   ```

3. Tag your image for the registry:

   ```bash
   docker tag <YOUR_IMAGE_NAME>:<YOUR_TAG> <YOUR_ACR_NAME>.azurecr.io/<YOUR_IMAGE_NAME>:<YOUR_TAG>
   ```

4. Push the image to Azure Container Registry:

   ```bash
   docker push <YOUR_ACR_NAME>.azurecr.io/<YOUR_IMAGE_NAME>:<YOUR_TAG>
   ```

For detailed guidance on working with Docker images in Azure Container Registry, see [Push and pull Docker images](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-get-started-docker-cli).

### Configure Azure Container Registry permissions

Before you create the agent, give your project's managed identity access to pull from the container registry that houses your image. This step ensures that all dependencies are available within the container.

1. In the [Azure portal](https://portal.azure.com), go to your Foundry project resource.

2. On the left pane, select **Identity**.

3. Under **System assigned**, copy the **Object (principal) ID** value. This value is the managed identity that you'll assign the Azure Container Instances role to.

4. Grant pull permissions by assigning the Container Registry Repository Reader role to your project's managed identity on the container registry. For detailed steps, see [Azure Container Registry roles and permissions](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-roles).

### Create an account-level capability host

Hosted agents require an account-level capability host with public hosting enabled.

Updating capability hosts isn't supported. If you already have a capability host for your Microsoft Foundry account, delete it and recreate it with `enablePublicHostingEnvironment` set to `true`.

Use `az rest` so you don't have to manage tokens manually.

#### Azure CLI (bash)

```bash
az rest --method put \
    --url "https://management.azure.com/subscriptions/[SUBSCRIPTIONID]/resourceGroups/[RESOURCEGROUPNAME]/providers/Microsoft.CognitiveServices/accounts/[ACCOUNTNAME]/capabilityHosts/accountcaphost?api-version=2025-10-01-preview" \
    --headers "content-type=application/json" \
    --body '{
        "properties": {
            "capabilityHostKind": "Agents",
            "enablePublicHostingEnvironment": true
        }
    }'
```

#### Azure CLI (PowerShell)

```powershell
az rest --method put `
    --url "https://management.azure.com/subscriptions/[SUBSCRIPTIONID]/resourceGroups/[RESOURCEGROUPNAME]/providers/Microsoft.CognitiveServices/accounts/[ACCOUNTNAME]/capabilityHosts/accountcaphost?api-version=2025-10-01-preview" `
    --headers "content-type=application/json" `
    --body '{
        "properties": {
            "capabilityHostKind": "Agents",
            "enablePublicHostingEnvironment": true
        }
    }'
```

### Create the hosted agent version

Install version>=2.0.0b3 of the Azure AI Projects SDK. Python 3.10 or later is required.

```bash
pip install --pre "azure-ai-projects>=2.0.0b3"
```

Use the Azure AI Projects SDK to create and register your agent:

```python
from azure.ai.projects import AIProjectClient
from azure.ai.projects.models import ImageBasedHostedAgentDefinition, ProtocolVersionRecord, AgentProtocol
from azure.identity import DefaultAzureCredential

# Initialize the client
client = AIProjectClient(
    endpoint="https://your-project.services.ai.azure.com/api/projects/project-name",
    credential=DefaultAzureCredential()
)

# Create the agent from a container image
agent = client.agents.create_version(
    agent_name="my-agent",
    definition=ImageBasedHostedAgentDefinition(
        container_protocol_versions=[ProtocolVersionRecord(protocol=AgentProtocol.RESPONSES, version="v1")],
        cpu="1",
        memory="2Gi",
        image="your-registry.azurecr.io/your-image:tag",
        environment_variables={
            "AZURE_AI_PROJECT_ENDPOINT": "https://your-project.services.ai.azure.com/api/projects/project-name",
            "MODEL_NAME": "gpt-4",
            "CUSTOM_SETTING": "value"
        }
    )
)

# Print confirmation
print(f"Agent created: {agent.name} (id: {agent.id}, version: {agent.version})")
```

Expected output:

```output
Agent created: my-agent (id: agent_abc123, version: 1)
```

Here are the key parameters:

* `PROJECT_ENDPOINT`: Endpoint URL for your Foundry project.
* `AGENT_NAME`: Unique name for your agent.
* `CONTAINER_IMAGE`: Full Azure Container Registry image URL with tag.
* `CPU/Memory`: Resource allocation (for example, `1` for CPU, `2Gi` for memory).

<Callout type="note">
  - Ensure that your container image is accessible from the Foundry project.
  - `DefaultAzureCredential` handles authentication automatically.
</Callout>

***

The agent appears in the Foundry portal after you create it.

## Manage hosted agents

### Update an agent

You can update an agent in two ways: versioned updates and non-versioned updates.

#### Versioned update

Use a versioned update to modify the runtime configuration for your agent. This process creates a new version of the agent.

Changes that trigger a new version include:

* **Container image**: Updating to a new image or tag.
* **Resource allocation**: Changing CPU or memory settings.
* **Environment variables**: Adding, removing, or modifying environment variables.
* **Protocol versions**: Updating supported protocol versions.

To create a new version, use the same `client.agents.create_version()` method shown in the creation example with your updated configuration.

#### Non-versioned update

Use a non-versioned update to modify horizontal scale configuration (minimum and maximum replicas) or agent metadata such as description and tags. This process doesn't create a new version.

```bash
az cognitiveservices agent update
```

The arguments for this command include:

| Argument            | Required | Description                                                                                                          |
| ------------------- | -------- | -------------------------------------------------------------------------------------------------------------------- |
| `--account-name -a` | ✅        | Foundry Tools account name.                                                                                          |
| `--agent-version`   | ✅        | Foundry Tools hosted agent version.                                                                                  |
| `--name -n`         | ✅        | Foundry Tools hosted agent name.                                                                                     |
| `--project-name`    | ✅        | AI project name.                                                                                                     |
| `--description`     | ❌        | Description of the agent.                                                                                            |
| `--max-replicas`    | ❌        | Maximum number of replicas for horizontal scaling.                                                                   |
| `--min-replicas`    | ❌        | Minimum number of replicas for horizontal scaling.                                                                   |
| `--tags`            | ❌        | Space-separated tags: `key[=value] [key[=value] ...]`. Use two single quotation marks (`''`) to clear existing tags. |

Here's an example:

```bash
az cognitiveservices agent update --account-name myAccount --project-name myProject --name myAgent --agent-version 1 --min-replicas 1 --max-replicas 2
```

### Start an agent deployment

After you create your hosted agent version, start the deployment by using the `az` CLI extension to make it available for requests. You can also start a hosted agent that you previously stopped.

```bash
az cognitiveservices agent start
```

The arguments for this command include:

| Argument            | Required | Description                                       |
| ------------------- | -------- | ------------------------------------------------- |
| `--account-name -a` | ✅        | Foundry Tools account name                        |
| `--agent-version`   | ✅        | Foundry Tools hosted agent version                |
| `--name -n`         | ✅        | Foundry Tools hosted agent name                   |
| `--project-name`    | ✅        | AI project name                                   |
| `--min-replicas`    | ❌        | Minimum number of replicas for horizontal scaling |
| `--max-replicas`    | ❌        | Maximum number of replicas for horizontal scaling |

If you don't specify max and min replicas during agent start operation, the default value is 1 for both the arguments.

Here's an example:

```bash
az cognitiveservices agent start --account-name myAccount --project-name myProject --name myAgent --agent-version 1
```

<Callout type="tip">
  * To minimize charges when idle, set `--min-replicas 0`. The service scales to zero and doesn't allocate compute when not serving requests.
  * The first request after scaling to zero incurs a cold start. To avoid cold starts, keep at least one warm replica by setting `--min-replicas 1` (or higher), trading steady-state cost for lower latency.
  * You can adjust replica settings later with a non-versioned update: `az cognitiveservices agent update --min-replicas <n> --max-replicas <m>`.

  Example (scale-to-zero when idle):

  ```bash
  az cognitiveservices agent start --account-name myAccount --project-name myProject --name myAgent --agent-version 1 --min-replicas 0 --max-replicas 2
  ```
</Callout>

When you start an agent:

* Current status: **Stopped**
* Allowed operation: **Start**
* Transitory status: **Starting**
* Final status: **Started** (if successful) or **Failed** (if unsuccessful)

### View container Log Stream

The container Logstream API for hosted agents gives you access to the system and console logs of the container deployed on your behalf in Microsoft's Azure environment to enable self-serve debugging for agent startup and runtime errors during deployment.

#### REST API Details

| Item             | Value                                                                                                                   |
| ---------------- | ----------------------------------------------------------------------------------------------------------------------- |
| **Method**       | `GET`                                                                                                                   |
| **Route**        | `https://{endpoint}/api/projects/{projectName}/agents/{agentName}/versions/{agentVersion}/containers/default:logstream` |
| **Description**  | Streams console or system logs for a specific hosted agent replica.                                                     |
| **Content-Type** | `text/plain` (chunked streaming)                                                                                        |

#### Path parameters

| Name           | Description                   | Example                                                                                        |
| -------------- | ----------------------------- | ---------------------------------------------------------------------------------------------- |
| `api-version`  | Required                      | API version, for example: `2025-11-15-preview`                                                 |
| `kind`         | `console`                     | `console` returns container `stdout`/`stderr`, `system` returns container system event stream. |
| `endpoint`     | Your Foundry service endpoint | `myservice.services.ai.azure.com`                                                              |
| `projectName`  | The Foundry project name      | `myproject`                                                                                    |
| `agentName`    | The agent deployment name     | `sample1`                                                                                      |
| `agentVersion` | The agent version number      | `1`                                                                                            |

#### Query parameters

| Name           | Default   | Notes                                                                                                       |
| -------------- | --------- | ----------------------------------------------------------------------------------------------------------- |
| `kind`         | `console` | `console` returns container stdout/stderr, `system` returns container app event stream.                     |
| `replica_name` | empty     | When omitted, the server chooses the first replica for console logs. Required to target a specific replica. |
| `tail`         | `20`      | Number of trailing lines returned. Enforced to `1-300`.                                                     |

#### Timeout Settings

* Max Connection Duration: The maximum duration for a log stream connection is `10 minutes`. After this period, the server will automatically close the client connection.
* Idle Timeout: This timeout is set to `1 minute`. It applies when there is no response from the client, or if there is no activity after the previous response during the log stream. If the connection remains idle for 1 minute, it will be closed by the server.

#### Response status codes

* `200 OK`: Plain-text stream of log lines, one per line.
* `404 Not Found`: Agent version, replica, or container log endpoint was not found.
* `401/403`: Caller lacks authorization.
* `5xx`: Propagated from downstream container calls when details or tokens cannot be fetched.

#### Example: Fetch logs using curl

```bash
curl -N "https://{endpoint}/api/projects/{projectName}/agents/{agentName}/versions/{agentVersion}/containers/default:logstream?kind=console&tail=500&api-version=2025-11-15-preview" \
  -H "Authorization: Bearer $(az account get-access-token --resource https://ai.azure.com --query accessToken -o tsv)"
```

**Flags:**

* `-N` disables output buffering (important for streaming logs in real-time)

#### Response samples

#### 200 OK (console logs)

```http
HTTP/1.1 200 OK
Content-Type: text/plain; charset=utf-8
Transfer-Encoding: chunked

2025-12-15T08:43:48.72656  Connecting to the container 'agent-container'...
2025-12-15T08:43:48.75451  Successfully Connected to container: 'agent-container' [Revision: 'je90fe655aa742ef9a188b9fd14d6764--7tca06b', Replica: 'je90fe655aa742ef9a188b9fd14d6764--7tca06b-6898b9c89f-mpkjc']
2025-12-15T08:33:59.0671054Z stdout F INFO:     127.0.0.1:42588 - "GET /readiness HTTP/1.1" 200 OK
2025-12-15T08:34:29.0649033Z stdout F INFO:     127.0.0.1:60246 - "GET /readiness HTTP/1.1" 200 OK
2025-12-15T08:34:59.0644467Z stdout F INFO:     127.0.0.1:43994 - "GET /readiness HTTP/1.1" 200 OK
2025-12-15T08:35:29.0651892Z stdout F INFO:     127.0.0.1:59368 - "GET /readiness HTTP/1.1" 200 OK
2025-12-15T08:35:59.0644637Z stdout F INFO:     127.0.0.1:57488 - "GET /readiness HTTP/1.1" 200 OK
```

#### 200 OK (system logs)

```http
HTTP/1.1 200 OK
Content-Type: text/plain; charset=utf-8
Transfer-Encoding: chunked

{"TimeStamp":"2025-12-15T16:51:33Z","Type":"Normal","ContainerAppName":null,"RevisionName":null,"ReplicaName":null,"Msg":"Connecting to the events collector...","Reason":"StartingGettingEvents","EventSource":"ContainerAppController","Count":1}
{"TimeStamp":"2025-12-15T16:51:34Z","Type":"Normal","ContainerAppName":null,"RevisionName":null,"ReplicaName":null,"Msg":"Successfully connected to events server","Reason":"ConnectedToEventsServer","EventSource":"ContainerAppController","Count":1}
```

### Stop an agent deployment

To stop the hosted agent, set the maximum replica for your agent deployment to zero and use the following command:

```bash
az cognitiveservices agent stop
```

The arguments for this command include:

| Argument            | Required | Description                        |
| ------------------- | -------- | ---------------------------------- |
| `--account-name -a` | ✅        | Foundry Tools account name         |
| `--agent-version`   | ✅        | Foundry Tools hosted agent version |
| `--name -n`         | ✅        | Foundry Tools hosted agent name    |
| `--project-name`    | ✅        | AI project name                    |

Here's an example:

```bash
az cognitiveservices agent stop --account-name myAccount --project-name myProject --name myAgent --agent-version 1
```

When you stop an agent:

* Current status: **Running**
* Allowed operation: **Stop**
* Transitory status: **Stopping**
* Final status: **Stopped** (if successful) or **Running** (if unsuccessful)

### Delete an agent

You can delete agents at various levels, depending on what you want to remove.

#### Delete a deployment only

The following command stops the running agent but keeps the agent version for future use. Use it when you want to stop the agent temporarily or switch to a different version.

```bash
az cognitiveservices agent delete-deployment
```

The arguments for this command include:

| Argument            | Required | Description                        |
| ------------------- | -------- | ---------------------------------- |
| `--account-name -a` | ✅        | Foundry Tools account name         |
| `--agent-version`   | ✅        | Foundry Tools hosted agent version |
| `--name -n`         | ✅        | Foundry Tools hosted agent name    |
| `--project-name`    | ✅        | AI project name                    |

#### Delete the agent

The following command removes all versions and deployments for the agent. Use it when you no longer need the agent and want to clean up all associated resources.

If you provide `agent_version` and you delete the agent deployment, the operation deletes the agent definition associated with that version. If the agent deployment is running, the operation doesn't succeed.

If you don't provide `agent_version`, the operation deletes all agent versions associated with the agent name.

```bash
az cognitiveservices agent delete
```

The arguments for this command include:

| Argument            | Required | Description                                                                                    |
| ------------------- | -------- | ---------------------------------------------------------------------------------------------- |
| `--account-name -a` | ✅        | Foundry Tools account name.                                                                    |
| `--name -n`         | ✅        | Foundry Tools hosted agent name.                                                               |
| `--project-name`    | ✅        | AI project name.                                                                               |
| `--agent-version`   | ❌        | Foundry Tools hosted agent version. If you don't provide it, the command deletes all versions. |

### List hosted agents

#### List all versions of a hosted agent

```bash
az cognitiveservices agent list-versions
```

The arguments for this command include:

| Argument            | Required | Description                     |
| ------------------- | -------- | ------------------------------- |
| `--account-name -a` | ✅        | Foundry Tools account name      |
| `--name -n`         | ✅        | Foundry Tools hosted agent name |
| `--project-name`    | ✅        | AI project name                 |

#### Show details of a hosted agent

```bash
az cognitiveservices agent show
```

The arguments for this command include:

| Argument            | Required | Description                     |
| ------------------- | -------- | ------------------------------- |
| `--account-name -a` | ✅        | Foundry Tools account name      |
| `--name -n`         | ✅        | Foundry Tools hosted agent name |
| `--project-name`    | ✅        | AI project name                 |

***

### Invoke hosted agents

You can view and test hosted agents in the agent playground UI. Hosted agents expose an API that's compatible with OpenAI responses. Use the Azure AI Projects SDK to invoke this API.

```python
#!/usr/bin/env python3
"""
Call a deployed Microsoft Foundry agent
"""

from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from azure.ai.projects.models import AgentReference

# Configuration
PROJECT_ENDPOINT = "https://your-project.services.ai.azure.com/api/projects/your-project"
AGENT_NAME = "your-agent-name"
AGENT_VERSION = "1"  # Optional: specify version, or use latest

# Initialize the client and retrieve the agent
client = AIProjectClient(endpoint=PROJECT_ENDPOINT, credential=DefaultAzureCredential())
agent = client.agents.get(agent_name=AGENT_NAME)
print(f"Agent retrieved: {agent.name} (version: {agent.versions.latest.version})")

# Get the OpenAI client and send a message
openai_client = client.get_openai_client()
response = openai_client.responses.create(
    input=[{"role": "user", "content": "Hello! What can you help me with?"}],
    extra_body={"agent": AgentReference(name=agent.name, version=AGENT_VERSION).as_dict()}
)

print(f"Agent response: {response.output_text}")
```

Expected output:

```output
Agent retrieved: your-agent-name (version: 1)
Agent response: Hello! I'm your hosted agent. I can help you with...
```

For more information, see [Azure AI Projects SDK for Python](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-projects-readme?view=azure-python-preview\&preserve-view=true).

### Use tools with hosted agents

Before your hosted agent can run with Foundry tools, create a connection to your remote Model Context Protocol (MCP) server on Foundry.

The `RemoteMCPTool` connection supports these authentication mechanisms:

* **Stored credentials**: Use predefined credentials stored in the system.
* **Project managed identity**: Use the managed identity for the Foundry project.

Choose your authentication method:

* **For shared identity**: Use key-based or Foundry project managed identity authentication when every user of your agent should use the same identity. Individual user identity or context doesn't persist with these methods.

* **For individual user context**: Use OAuth identity passthrough when every user of your agent should use their own account to authenticate with the MCP server. This approach preserves personal user context.

For more information, see [Connect to Model Context Protocol servers](../how-to/tools/model-context-protocol).

Reference the Foundry tool connection ID for Remote MCP servers within your agent code by using an environment variable. Wrap it by using the Hosting adapter for testing locally. Build and push your Docker image to Azure Container Registry (ACR). Configure image pull permissions on the ACR. Create a capability host by following the instructions mentioned [above](#create-a-hosted-agent) and proceed to registering your agent on Foundry.

Create a hosted agents version with tools definition by using the Foundry SDK.

```python
from azure.ai.projects import AIProjectClient
from azure.ai.projects.models import ImageBasedHostedAgentDefinition, ProtocolVersionRecord, AgentProtocol
from azure.identity import DefaultAzureCredential

# Initialize the client
client = AIProjectClient(
    endpoint="https://your-project.services.ai.azure.com/api/projects/project-name",
    credential=DefaultAzureCredential()
)

# Create the agent from a container image
agent = client.agents.create_version(
    agent_name="my-agent",
    description="Coding agent expert in assisting with github issues",
    definition=ImageBasedHostedAgentDefinition(
        container_protocol_versions=[ProtocolVersionRecord(protocol=AgentProtocol.RESPONSES, version="v1")],
        cpu="1",
        memory="2Gi",
        image="your-registry.azurecr.io/your-image:tag",
        tools=[
            {
                "type": "code_interpreter"
            },
            {
                "type": "mcp",
                "project_connection_id": "github_connection_id"
            }
        ],
        environment_variables={
            "AZURE_AI_PROJECT_ENDPOINT": "https://your-project.services.ai.azure.com/api/projects/project-name",
            "MODEL_NAME": "gpt-4",
            "CUSTOM_SETTING": "value"
        }
    )
)
```

Start the agent by using Azure Cognitive Services CLI or from within Agent Builder in the new Foundry UI.

Currently supported built-in Foundry tools include:

* Code Interpreter
* Image Generation
* Web Search

## Manage observability with hosted agents

Hosted agents support exposing OpenTelemetry traces, metrics, and logs from underlying frameworks to Microsoft Foundry with Application Insights or any user-specified OpenTelemetry Collector endpoint.

If you use the `azd ai agent` CLI extension, Application Insights is automatically provisioned and connected to your Foundry project for you. Your project's managed identity is granted the Azure AI User role on the Foundry resource so that traces are exported to Application Insights.

If you use the Foundry SDK, you need to perform these steps independently. For more information, see [Enable tracing in your project](../../how-to/develop/trace-application#enable-tracing-in-your-project).

The hosting adapter provides:

* **Complete OpenTelemetry setup**: `TracerProvider`, `MeterProvider`, `LoggerProvider`.
* **Auto-instrumentation**: HTTP requests, database calls, AI model calls.
* **Azure Monitor integration**: Exporters, formatting, authentication.
* **Performance optimization**: Sampling, batching, resource detection.
* **Live metrics**: Real-time dashboard in Application Insights.

### Local tracing

1. Install and set up AI Toolkit for Visual Studio Code (VS Code) by following [Trace in AI Toolkit](https://code.visualstudio.com/docs/intelligentapps/tracing).

2. Set up and export the environment variable `OTEL_EXPORTER_ENDPOINT`. You can find the endpoint from AI Toolkit for VS Code after you select the **Start Collector** button.

3. Invoke the agent and find traces in AI Toolkit.

### Tracing in the Foundry portal

You can also review traces for your hosted agent on the **Traces** tab in the playground.

### Export traces to your OpenTelemetry-compatible server

To send traces to your own OpenTelemetry collector or compatible observability platform, use the environment variable `OTEL_EXPORTER_ENDPOINT`.

## Manage conversations by using hosted agents

Hosted agents integrate seamlessly with the conversation management system in Microsoft Foundry. This integration enables stateful, multiple-turn interactions without manual state management.

### How conversations work with hosted agents

**Conversation objects**: Foundry automatically creates durable conversation objects with unique identifiers that persist across multiple agent interactions. When a user starts a conversation with your hosted agent, the platform maintains this conversation context automatically.

**State management**: Unlike traditional APIs where you manually pass conversation history, hosted agents receive conversation context automatically. The Foundry runtime manages:

* Previous messages and responses.
* Tool calls and their outputs.
* Agent instructions and configuration.
* Conversation metadata and time stamps.

**Conversation items**: Each conversation contains structured items that the system automatically maintains:

* **Messages**: User inputs and agent responses with time stamps.
* **Tool calls**: Function invocations with parameters and results.
* **Tool outputs**: Structured responses from external services.
* **System messages**: Internal state and context information.

### Conversation persistence and reuse

**Cross-session continuity**: Conversations persist beyond individual requests. Users can return to previous discussions with full context maintained.

**Conversation reuse**: Users can access the same conversation from multiple channels and applications. Conversations maintain consistent state and history.

**Automatic cleanup**: Foundry manages conversation lifecycle and cleanup based on your project's retention policies.

## Evaluate and test hosted agents

Microsoft Foundry provides comprehensive evaluation and testing capabilities that are designed for hosted agents. Use these capabilities to validate performance, compare versions, and help ensure quality before deployment.

### Built-in evaluation capabilities

**Agent performance evaluation**: Foundry includes built-in evaluation metrics to assess your hosted agent's effectiveness:

* Response quality and relevance
* Task completion accuracy
* Tool usage effectiveness
* Conversation coherence and context retention
* Response time and efficiency metrics

**Agent-specific evaluation**: Evaluate hosted agents by using the Foundry SDK with built-in evaluators that are designed for agentic workflows. The SDK provides specialized evaluators for measuring agent performance across key dimensions like intent resolution, task adherence, and tool usage accuracy.

### Testing workflows for hosted agents

**Development testing**: Test your hosted agent locally during development by using the agent playground and local testing tools before deployment.

**Staging validation**: Deploy to a staging environment to validate behavior by using real Foundry infrastructure while maintaining isolation from production.

**Production monitoring**: Continuously monitor deployed hosted agents by using automated evaluation runs to detect performance degradation or problems.

### Structured evaluation approaches

**Test dataset creation**: Create comprehensive test datasets that cover:

* Common user interaction patterns.
* Edge cases and error scenarios.
* Multiple-turn conversation flows.
* Tool usage scenarios.
* Performance stress tests.

**Supported evaluation metrics**: The Foundry SDK provides the following evaluators for agent workflows:

* **Intent Resolution**: Measures how well the agent identifies and understands user requests.
* **Task Adherence**: Evaluates whether the agent's responses adhere to assigned tasks and system instructions.
* **Tool Call Accuracy**: Assesses whether the agent makes correct function tool calls for user requests.
* **Additional Quality Metrics**: Enables the use of relevance, coherence, and fluency with agent messages.

### Evaluation best practices

**Test with representative data**: Create evaluation datasets that represent your actual user interactions and use cases.

**Monitor agent performance**: Use the Foundry portal to track agent performance and review conversation traces.

**Use iterative evaluation**: Regularly evaluate agent versions during development to catch problems early and measure improvements.

For more information about evaluating agents, see [Evaluate your AI agents](../../how-to/develop/agent-evaluate-sdk) and [Agent evaluators](../../concepts/evaluation-evaluators/agent-evaluators).

## Publish hosted agents to channels

Publishing transforms your hosted agent from a development asset into a managed Azure resource with a dedicated endpoint, independent identity, and governance capabilities. After you publish your hosted agent, you can share it across multiple channels and platforms.

### Publishing process for hosted agents

When you publish a hosted agent, Microsoft Foundry automatically:

1. Creates an agent application resource with a dedicated invocation URL.
2. Provisions a distinct agent identity that's separate from your project's shared identity.
3. Registers the agent in the Microsoft Entra agent registry for discovery and governance.
4. Enables stable endpoint access that remains consistent as you deploy new agent versions.

Unlike prompt-based agents that you can edit in the portal, hosted agents keep their code-based implementation while gaining the same publishing and sharing capabilities.

### Available publishing channels

**Web application preview**: Use a web interface to demonstrate and test your hosted agent with stakeholders. It's instant and shareable.

**Microsoft 365 Copilot and Teams**: Integrate your hosted agent directly into Microsoft 365 Copilot and Microsoft Teams through a streamlined, no-code publishing flow. Your agent appears in the agent store for organizational or shared scope distribution.

**Stable API endpoint**: Access your hosted agent programmatically through a consistent REST API that remains unchanged as you update agent versions.

**Custom applications**: Embed your hosted agent into existing applications by using the stable endpoint and SDK integration.

### Publishing considerations for hosted agents

**Identity management**: Published hosted agents use their own agent identity. You need to reconfigure permissions for any Azure resources that your agent accesses. Permissions for the shared development identity don't automatically transfer.

**Version control**: Publishing creates a deployment that references your current agent version. You can update the published agent by deploying new versions without changing the public endpoint.

**Authentication**: Published agents support RBAC-based authentication by default. This authentication includes automatic permission handling for Azure Bot Service integration when you're publishing to Microsoft 365 channels.

For detailed publishing instructions, see [Publish and share agents](../how-to/publish-agent).

## Troubleshoot hosted agent endpoints

If your agent deployment fails, view error logs by selecting **View deployment logs**. If you get 4xx errors, use the following table to determine next steps. If the agent endpoint returns 5xx status codes, contact Microsoft support.

| Error classification                                 | HTTP status code | Solution                                                      |
| ---------------------------------------------------- | ---------------- | ------------------------------------------------------------- |
| `SubscriptionIsNotRegistered`                        | 400              | Register the feature or subscription provider.                |
| `InvalidAcrPullCredentials` (`AcrPullWithMSIFailed`) | 401              | Fix the managed identity or registry RBAC.                    |
| `UnauthorizedAcrPull` (`AcrPullUnauthorized`)        | 403              | Provide the correct credentials or identity.                  |
| `AcrImageNotFound`                                   | 404              | Correct the image name or tag, or publish the image.          |
| `RegistryNotFound`                                   | 400/404          | Fix registry DNS or server spelling, or network reachability. |
| `ValidationError`                                    | 400              | Correct invalid request fields.                               |
| `UserError` (generic)                                | 400              | Inspect the message and fix the configuration.                |

### Troubleshoot runtime issues

If your hosted agent deploys successfully but doesn't respond as expected, check these common issues:

| Symptom               | Possible cause                   | Solution                                                                                       |
| --------------------- | -------------------------------- | ---------------------------------------------------------------------------------------------- |
| Agent doesn't respond | Container is still starting      | Wait for the agent status to show **Started**. Check the log stream for startup progress.      |
| Slow response times   | Insufficient resource allocation | Increase CPU or memory allocation in the agent definition.                                     |
| Timeout errors        | Long-running operations          | Increase timeout settings in your agent code. Consider breaking operations into smaller steps. |
| Intermittent failures | Replica scaling issues           | Check that `min_replicas` is set appropriately for your workload.                              |
| Tool calls failing    | Missing connection configuration | Verify that tool connections are properly configured and the managed identity has access.      |
| Model errors          | Invalid model deployment name    | Verify that `MODEL_NAME` environment variable matches an available model deployment.           |

To debug runtime issues:

1. Use the [log stream API](#view-container-log-stream) to view container logs in real time.
2. Check the **Traces** tab in the Foundry portal playground for detailed request and response information.
3. Verify environment variables are set correctly in your agent definition.

## Understand preview details

### Limitations during preview

| Dimension                                                             | Limit |
| --------------------------------------------------------------------- | ----- |
| Microsoft Foundry resources with hosted agents per Azure subscription | 100   |
| Maximum number of hosted agents per Foundry resource                  | 200   |
| Maximum `min_replica` count for an agent deployment                   | 2     |
| Maximum `max_replica` count for an agent deployment                   | 5     |

### Hosting pricing

Billing for managed hosting runtime is enabled no earlier than February 1, 2026, during the preview. For updates on pricing, check the Foundry [pricing page](https://azure.microsoft.com/pricing/details/ai-foundry/).

### Private networking support

Currently, you can't create hosted agents by using the standard setup within network-isolated Foundry resources. For more information, see [Configure virtual networks](../how-to/virtual-networks).

## Related content

* [Python code samples](https://github.com/microsoft-foundry/foundry-samples/tree/main/samples/python/hosted-agents)
* [C# code samples](https://github.com/microsoft-foundry/foundry-samples/tree/main/samples/csharp/hosted-agents)
* [Agent runtime components](runtime-components)
* [Agent development lifecycle](development-lifecycle)
* [Agent identity concepts in Microsoft Foundry](agent-identity)
* [Discover tools in Foundry Tools](tool-catalog)
* [Publish and share agents in Microsoft Foundry](../how-to/publish-agent)
* [Azure Container Registry documentation](https://learn.microsoft.com/en-us/azure/container-registry/)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

<Callout type="note">
  Updating capability hosts is not supported. To modify a capability host, you must delete the existing one and recreate it with the new configuration.
</Callout>

Capability hosts are sub-resources that you configure at both the Microsoft Foundry account and Foundry project scopes. They tell Foundry Agent Service where to store and process agent data, including:

* **Conversation history (threads)**
* **File uploads**
* **Vector stores**

## Prerequisites

* A [Microsoft Foundry project](../../how-to/create-projects)

* If you use your own resources for agent data (standard agent setup), create the required Azure resources and connections:

  * [Use your own resources](../how-to/use-your-own-resources)
  * [Add a new connection to your project](../../how-to/connections-add)

* Required permissions:

  * **Contributor** role on the Foundry account to create capability hosts
  * **User Access Administrator** or **Owner** role to assign access to Azure resources (for standard agent setup)
  * For details, see [Required permissions](../environment-setup#required-permissions) and [Role-based access control (RBAC) in Microsoft Foundry](../../concepts/rbac-foundry).

## Why use capability hosts?

Capability hosts let you **bring your own Azure resources** instead of using the default Microsoft-managed platform resources. This gives you:

* **Data sovereignty** - Keep all agent data within your Azure subscription.
* **Security control** - Use your own storage accounts, databases, and search services.
* **Compliance** - Meet specific regulatory or organizational requirements.

## How do capability hosts work?

Creating capability hosts isn't required. If you want agents to use your own Azure resources, create capability hosts at both the account and project scopes.

### Default behavior (Microsoft-managed resources)

If you don't create capability hosts, Agent Service automatically uses Microsoft-managed Azure resources for:

* Thread storage (conversation history, agent definitions)
* File storage (uploaded documents)
* Vector search (embeddings and retrieval)

### Bring-your-own resources

When you create capability hosts at both the account and project levels, your Azure resources store and process agent data. This is **standard agent setup**. For network-secured standard agent setup, deploy all related resources in the same region as your virtual network (VNet). For guidance, see [Create a new network-secured environment with user-managed identity](../how-to/virtual-networks).

To learn more about standard agent setup, see [Built-in enterprise readiness with standard agent setup](standard-agent-setup).

<Callout type="note">
  We recommend using separate Foundry accounts and projects for standard agent setup and basic agent setup. Avoid mixing setup types within the same Foundry account.
</Callout>

#### Configuration hierarchy

Capability hosts follow a hierarchy where more specific configurations override broader ones:

1. **Service defaults** (Microsoft-managed search and storage) - Used when no capability host is configured.
2. **Account-level capability host** - Provides shared defaults for all projects under the account.
3. **Project-level capability host** - Overrides account-level and service defaults for that specific project.

## Understand capability host constraints

When creating capability hosts, be aware of these important constraints to avoid conflicts:

* **One capability host per scope**: Each account and each project can have only one active capability host. If you try to create a second capability host with a different name at the same scope, you get a 409 conflict.

* **You can't update configurations**: If you need to change configuration, delete the existing capability host and recreate it.

## Create connections for capability hosts

Capability hosts reference connection names that you create in your Foundry account and project. Before you configure a project capability host for standard agent setup, create connections for the resources that store agent data:

* **Thread storage**: Azure Cosmos DB connection
* **File storage**: Azure Storage connection
* **Vector store**: Azure AI Search connection

If you want to use model deployments from your own Azure OpenAI resource, also create an Azure OpenAI connection.

To add connections in the Foundry portal, see [Add a new connection to your project](../../how-to/connections-add).

## Configure capability hosts

Currently, you manage capability hosts using the REST API. SDK support for capability host management isn't available.

### Required properties (project capability host)

To use your own resources for agent data (standard agent setup), configure the project capability host with the following properties:

| Property                   | Purpose                                                         | Required Azure resource | Example connection name     |
| -------------------------- | --------------------------------------------------------------- | ----------------------- | --------------------------- |
| `threadStorageConnections` | Stores agent definitions, conversation history and chat threads | Azure Cosmos DB         | `"my-cosmosdb-connection"`  |
| `vectorStoreConnections`   | Handles vector storage for retrieval and search                 | Azure AI Search         | `"my-ai-search-connection"` |
| `storageConnections`       | Manages file uploads and blob storage                           | Azure Storage Account   | `"my-storage-connection"`   |

### Optional property

| Property                | Purpose                        | Required Azure resource | When to use                                                                                                      |
| ----------------------- | ------------------------------ | ----------------------- | ---------------------------------------------------------------------------------------------------------------- |
| `aiServicesConnections` | Use your own model deployments | Azure OpenAI            | When you want to use models from your existing Azure OpenAI resource instead of the built-in account level ones. |

**Account capability host**

Use an account capability host to enable Agent Service and (optionally) define defaults that projects can inherit.

```http
PUT https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/capabilityHosts/{name}?api-version=2025-06-01

{
  "properties": {
    "capabilityHostKind": "Agents"
  }
}
```

Reference: [Foundry account management REST API](https://learn.microsoft.com/en-us/rest/api/aifoundry/accountmanagement/operation-groups)

**Project capability host**

This configuration overrides service defaults and any account-level settings. All agents in this project will use your specified resources:

```http
PUT https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/projects/{projectName}/capabilityHosts/{name}?api-version=2025-06-01

{
  "properties": {
    "capabilityHostKind": "Agents",
    "threadStorageConnections": ["my-cosmos-db-connection"],
    "vectorStoreConnections": ["my-ai-search-connection"],
    "storageConnections": ["my-storage-account-connection"],
    "aiServicesConnections": ["my-azure-openai-connection"]
  }
}
```

Reference: [Project Capability Hosts - Create or update](https://learn.microsoft.com/en-us/rest/api/aifoundry/accountmanagement/project-capability-hosts/create-or-update)

### Optional: account-level defaults with project overrides

Set shared defaults at the account level that apply to all projects:

```http
PUT https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/capabilityHosts/{name}?api-version=2025-06-01

{
  "properties": {
    "capabilityHostKind": "Agents",
    "threadStorageConnections": ["shared-cosmosdb-connection"],
    "vectorStoreConnections": ["shared-ai-search-connection"],
    "storageConnections": ["shared-storage-connection"]
  }
}
```

<Callout type="note">
  All Foundry projects will inherit these settings. Then override specific settings at the project level as needed.
</Callout>

## Verify your configuration

Use these steps to confirm that capability hosts are configured correctly:

1. Get the account capability host and confirm it exists.

   ```http
   GET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/capabilityHosts?api-version=2025-06-01
   ```

2. Get the project capability host and confirm it references the expected connection names.

   ```http
   GET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/projects/{projectName}/capabilityHosts?api-version=2025-06-01
   ```

3. Test your configuration by creating a test agent and running a conversation. Confirm that:

   * Conversation threads appear in your Azure Cosmos DB
   * Uploaded files appear in your Azure Storage account
   * Vector data appears in your Azure AI Search index

4. If you update connections or want to change where data is stored, delete and recreate the capability hosts with the updated configuration.

## Delete capability hosts

<Callout type="warning">
  Deleting a capability host will affect all agents that depend on it. Make sure you understand the impact before proceeding. For instance, if you delete the project and account capability host, agents in your project will no longer have access to the files, thread, and vector stores it previously did.
</Callout>

### Delete an account-level capability host

```http
DELETE https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/capabilityHosts/{name}?api-version=2025-06-01
```

### Delete a project-level capability host

```http
DELETE https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/projects/{projectName}/capabilityHosts/{name}?api-version=2025-06-01
```

## Troubleshooting

If you're experiencing issues when creating capability hosts, this section provides solutions to the most common problems and errors.

### HTTP 409 Conflict errors

#### Problem: Multiple capability hosts per scope

**Symptoms:** You receive a 409 Conflict error when trying to create a capability host, even though you believe the scope is empty.

**Error message:**

```json
{
  "error": {
    "code": "Conflict",
    "message": "There is an existing Capability Host with name: existing-host, provisioning state: Succeeded for workspace: /subscriptions/.../workspaces/my-workspace, cannot create a new Capability Host with name: new-host for the same ClientId."
  }
}
```

**Root cause:** Each account and each project can only have one active capability host. You're trying to create a capability host with a different name when one already exists at the same scope.

**Solution:**

1. **Check existing capability hosts** - Query the scope to see what already exists
2. **Use consistent naming** - Ensure you're using the same name across all requests for the same scope
3. **Review your requirements** - Determine if the existing capability host meets your needs

**Validation steps:** Use the GET requests in [Verify your configuration](#verify-your-configuration) to confirm whether a capability host already exists at the target scope.

#### Problem: Concurrent operations in progress

**Symptoms:** You receive a 409 Conflict error indicating that another operation is currently running.

**Error message:**

```json
{
  "error": {
    "code": "Conflict",
    "message": "Create: Capability Host my-host is currently in non creating, retry after its complete: /subscriptions/.../workspaces/my-workspace"
  }
}
```

**Root cause:** You're trying to create a capability host while another operation (update, delete, modify) is in progress at the same scope.

**Solution:**

1. **Wait for current operation to complete** - Check the status of ongoing operations
2. **Monitor operation progress** - Use the operations API to track completion
3. **Implement retry logic** - Use exponential backoff for temporary conflicts

**Operation monitoring:**

```http
GET https://management.azure.com/subscriptions/{subscriptionId}/providers/Microsoft.CognitiveServices/locations/{location}/operationResults/{operationId}?api-version=2025-06-01
```

### Best practices for conflict prevention

#### 1. Pre-request validation

Always verify the current state before making changes:

* Query existing capability hosts in the target scope
* Check for any ongoing operations
* Understand the current configuration

#### 2. Implement retry logic with exponential backoff

```csharp
try
{
    var response = await CreateCapabilityHostAsync(request);
    return response;
}
catch (HttpRequestException ex) when (ex.Message.Contains("409"))
{
    if (ex.Message.Contains("existing Capability Host with name"))
    {
        // Handle name conflict - check if existing resource is acceptable
        var existing = await GetExistingCapabilityHostAsync();
        if (IsAcceptable(existing))
        {
            return existing; // Use existing resource
        }
        else
        {
            throw new InvalidOperationException("Scope already has a capability host with different name");
        }
    }
    else if (ex.Message.Contains("currently in non creating"))
    {
        // Handle concurrent operation - implement retry with backoff
        await Task.Delay(TimeSpan.FromSeconds(30));
        return await CreateCapabilityHostAsync(request); // Retry once
    }
}
```

#### 3. Understand idempotent behavior

The system supports idempotent create requests:

* **Same name + same configuration** → Returns existing resource (200 OK)
* **Same name + different configuration** → Returns 400 Bad Request
* **Different name** → Returns 409 Conflict

#### 4. Configuration change workflow

Since updates aren't supported, follow this sequence for configuration changes:

1. Delete the existing capability host
2. Wait for deletion to complete
3. Create a new capability host with the desired configuration

## Common scenarios

* **Development and testing**: Use Microsoft-managed resources. No capability host configuration needed.
* **Production with compliance requirements**: Create capability hosts with your own Azure Cosmos DB, Storage, and AI Search.
* **Shared resources across projects**: Configure account-level defaults, then override at the project level as needed.

## Next steps

* [Standard agent setup](standard-agent-setup)
* [Set up your environment](../environment-setup)
* [Add a new connection to your project](../../how-to/connections-add)

Workflows are UI-based tools in Microsoft Foundry. Use them to create declarative, predefined sequences of actions that orchestrate agents and business logic in a visual builder.

Workflows enable you to build intelligent automation systems that seamlessly blend AI agents with business processes in a visual manner. Traditional single-agent systems are limited in their ability to handle complex, multifaceted tasks. By orchestrating multiple agents, each with specialized skills or roles, you can create systems that are more robust, adaptive, and capable of solving real-world problems collaboratively.

## Prerequisites

* An Azure account with an active subscription. If you don't have one, create a [free Azure account, which includes a free trial subscription](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* A project in Microsoft Foundry. For more information, see [Create projects](../../how-to/create-projects).
* Access to create and run workflows in your Foundry project. For more information, see [Azure role-based access control (RBAC) in Foundry](../../concepts/rbac-foundry).

## Decide when to use workflows

Workflows are ideal for scenarios where you need to:

* Orchestrate multiple agents in a repeatable process.
* Add branching logic (for example, if/else) and variable handling without writing code.
* Create human-in-the-loop steps (for example, approvals or clarifying questions).

If you want to edit workflow YAML in Visual Studio Code or run workflows in a local playground, see:

* [Work with Declarative (Low-code) Agent workflows in Visual Studio Code](../how-to/vs-code-agents-workflow-low-code)
* [Work with Hosted (Pro-code) Agent workflows in Visual Studio Code](../how-to/vs-code-agents-workflow-pro-code)

## Understand workflow patterns

Foundry provides templates for common orchestration patterns. Start with a blank workflow or select a template:

| Pattern           | Description                                                         | Typical use case                                                                                                            |
| ----------------- | ------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------- |
| Human in the loop | Asks the user a question and awaits user input to proceed           | Creating approval requests during workflow execution and waiting for human approval, or obtaining information from the user |
| Sequential        | Passes the result from one agent to the next in a defined order     | Step-by-step workflows, pipelines, or multiple-stage processing                                                             |
| Group chat        | Dynamically passes control between agents based on context or rules | Dynamic workflows, escalation, fallback, or expert handoff scenarios                                                        |

For more information, see [Microsoft Agent Framework workflow orchestrations](https://learn.microsoft.com/en-us/agent-framework/user-guide/workflows/orchestrations/overview).

## Create a workflow

This procedure shows how to create a sequential workflow. The same general steps apply to other workflow types.

1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.

   ![](https://learn.microsoft.com/azure/ai-foundry/agents/media/version-banner/new-foundry.png)

2. On the upper-right menu, select **Build**.

3. Select **Create new workflow** > **Sequential**.

4. Assign an agent to the agent nodes by selecting each agent node in the workflow and either selecting the desired agent or creating a new one. For more information, see [Add agents](#add-agents) later in this article.

5. Select **Save** in the visualizer to save the changes.

   <Callout type="important">
     Foundry doesn't save workflows automatically. Select **Save** after every change to preserve your work.
   </Callout>

6. Select **Run Workflow**.

7. Interact with the workflow in the chat window.

8. Optionally, add new nodes to your workflow. The next section in this article provides information about nodes.

## Verify your workflow run

After you select **Run Workflow**, verify that:

* Each node completes in the visualizer.
* You see the expected responses in the chat window.
* Any variables you save (for example, JSON output from an agent node) contain the values you expect.

## Add nodes

Nodes are the building blocks of your workflow. Each node performs a specific action in sequence.

Common node types include:

* **Agent**: Invoke an agent.
* **Logic**: Use *if/else*, *go to*, or *for each*.
* **Data transformation**: Set a variable or parse a value.
* **Basic chat**: Send a message or ask a question to an agent.

When you select a prebuilt workflow, the builder displays the nodes in sequence. To reorder nodes, select the three dots on a node and then select **move**. To add nodes, select the plus (**+**) icon in the workspace.

## Add agents

Add any Foundry agent from your project to the workflow. Agent nodes also let you create new agents with customized capabilities by configuring their model, prompt, and tools.

For advanced agent creation options, go to the **Foundry Agent** tab in the Foundry portal.

### Add an existing agent

1. In the workflow visualizer, select the plus sign.

2. In the pop-up dropdown list, select **Invoke agent**.

3. In the **Create new agent** window, select **existing**.

4. Enter the agent name to search for existing agents in your Foundry project.

5. Select the desired agent to add it into your workflow.

### Create a new agent

1. In the workflow visualizer, select the plus sign.

2. In the pop-up dropdown list, select **Invoke agent**.

3. Enter an agent name and description of what the agent does.

4. Select **Add**.

5. In the **Invoke an agent** window, configure the agent.

6. Select **Save**.

### Configure an output response format

To configure an agent to return structured JSON output:

1. In the **Invoke agent** configuration window, select **Create a new agent**.

2. Configure the agent to send output as a JSON schema:

   1. Select **Details**.
   2. Select the parameter icon.
   3. For **Text format**, select **JSON Schema**.

   ![Screenshot that shows the window for configuring a JSON schema format for output.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/workflows/select-parameters.png)

3. Copy the desired JSON schema and paste it in the **Add response format** window. The following screenshot shows a math example. Select **Save**.

   ![Screenshot that shows the addition of a response format in JSON.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/workflows/response-format.png)

<Callout type="important">
  Don't include secrets (passwords, keys, tokens) in JSON schemas, prompts, or saved workflow variables.
</Callout>

```json
{
  "name": "math_response",
  "schema": {
    "type": "object",
    "properties": {
      "steps": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "explanation": {
              "type": "string"
            },
            "output": {
              "type": "string"
            }
          },
          "required": [
            "explanation",
            "output"
          ],
          "additionalProperties": false
        }
      },
      "final_answer": {
        "type": "string"
      }
    },
    "additionalProperties": false,
    "required": [
      "steps",
      "final_answer"
    ]
  },
  "strict": true
}
```

1. Select **Action settings**. Then select **Save output json\_object/json\_schema as**.

2. Select **Create new variable**. Choose a variable name, and then select **Done**.

   ![Screenshot that shows options for creating a new variable in a Microsoft Foundry workflow.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/workflows/save-output.png)

## Configure additional features

* **YAML visualizer view**: Set the **YAML Visualizer View** toggle to **On** to store the workflow as a YAML file. Edit in either the visualizer or the YAML view. Saving creates a new version with full version history.

  Both the visualizer and YAML are editable. Changes to the YAML file appear immediately in the visualizer.

* **Versioning**: Each save creates a new, unchangeable version. To view version history or delete older versions, open the **Version** dropdown list to the left of the **Save** button.

* **Notes**: Add notes to the workflow visualizer for extra context. In the upper-left corner of the visualizer, select **Add note**.

## Create expressions with Power Fx

Power Fx is a low-code language that uses Excel-like formulas. Use Power Fx to create complex logic that lets your agents manipulate data. For example, a Power Fx formula can set a variable value, parse a string, or evaluate a condition. For more information, see the [Power Fx overview](https://learn.microsoft.com/en-us/power-platform/power-fx/overview) and [formula reference](https://learn.microsoft.com/en-us/power-platform/power-fx/formula-reference-copilot-studio).

### Use variables in a formula

To use a variable in a Power Fx formula, you must add a prefix to its name to indicate the variable's scope:

* For system variables, use `System.`
* For local variables, use `Local.`

Here are the system variables:

| Name                               | Description                                                                    |
| ---------------------------------- | ------------------------------------------------------------------------------ |
| `Activity`                         | Information about the current activity                                         |
| `Bot`                              | Information about the agent                                                    |
| `Conversation`                     | Information about the current conversation                                     |
| `Conversation.Id`                  | Unique ID of the current conversation                                          |
| `Conversation.LocalTimeZone`       | Time zone of the user, in the IANA Time Zone Database format                   |
| `Conversation.LocalTimeZoneOffset` | Time offset from UTC for the current local time zone                           |
| `Conversation.InTestMode`          | Boolean flag that represents if the conversation is happening on a test canvas |
| `ConversationId`                   | Unique ID of the current conversation                                          |
| `InternalId`                       | Internal identifier for the system                                             |
| `LastMessage`                      | Information about the previous message that the user sent                      |
| `LastMessage.Id`                   | ID of the previous message that the user sent                                  |
| `LastMessage.Text`                 | Previous message that the user sent                                            |
| `LastMessageId`                    | ID of the previous message that the user sent                                  |
| `LastMessageText`                  | Previous message that the user sent                                            |
| `Recognizer`                       | Information about intent recognition and the triggering message                |
| `User`                             | Information about the user currently talking to the agent                      |
| `User.Language`                    | User language locale per conversation                                          |
| `UserLanguage`                     | User language locale per conversation                                          |

### Use literal values in a formula

In addition to using variables in a Power Fx formula, you can enter literal values. To use a literal value in a formula, you must enter it in the format that corresponds to its [type](https://learn.microsoft.com/en-us/microsoft-copilot-studio/authoring-variables-about?tabs=webApp).

The following table lists the data types and the format of their corresponding literal values:

| Type             | Format examples                                                                                                                  |
| ---------------- | -------------------------------------------------------------------------------------------------------------------------------- |
| String           | `"hi"`, `"hello world!"`, `"copilot"`                                                                                            |
| Boolean          | Only `true` or `false`                                                                                                           |
| Number           | `1`, `532`, `5.258`,`-9201`                                                                                                      |
| Record and table | `[1]`, `[45, 8, 2]`, `["cats", "dogs"]`, `{ id: 1 }`, `{ message: "hello" }`, `{ name: "John", info: { age: 25, weight: 175 } }` |
| Date and time    | `Time(5,0,23)`, `Date(2022,5,24)`, `DateTimeValue("May 10, 2022 5:00:00 PM")`                                                    |
| Choice           | Not supported                                                                                                                    |
| Blank            | Only `Blank()`                                                                                                                   |

#### Common Power Fx formulas

The following table lists the Power Fx formulas that you can use with each data type.

| Type             | Power Fx formulas                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| String           | [Text function](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-text) [Concat and Concatenate functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-concatenate) [Len function](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-len) [Lower, Upper, and Proper functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-lower-upper-proper) [IsMatch, Match, and MatchAll functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-ismatch) [EndsWith and StartsWith functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-startswith) [Find function](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-find) [Replace and Substitute function](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-replace-substitute) |
| Boolean          | [Boolean function](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-boolean) [And, Or, and Not functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-logicals) [If and Switch functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-if)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| Number           | [Decimal, Float, and Value functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-value) [Int, Round, RoundDown, RoundUp, and Trunc functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-round)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| Record and table | [Concat and Concatenate functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-concatenate) [Count, CountA, CountIf, and CountRows functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-table-counts) [ForAll function](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-forall) [First, FirstN, Index, Last, and LastN functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-first-last) [Filter, Search, and LookUp functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-filter-lookup) [JSON function](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-json) [ParseJSON function](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-parsejson)                                                                                           |
| Date and time    | [Date, DateTime, and Time functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-date-time) [DateValue, TimeValue, and DateTimeValue functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-datevalue-timevalue) [Day, Month, Year, Hour, Minute, Second, and Weekday functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-datetime-parts) [Now, Today, IsToday, UTCNow, UTCToday, IsUTCToday functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-now-today-istoday) [DateAdd, DateDiff, and TimeZoneOffset functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-dateadd-datediff) [Text function](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-text)                                                                                                        |
| Blank            | [Blank, Coalesce, IsBlank, and IsEmpty functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-isblank-isempty) [Error, IfError, IsError, IsBlankOrError functions](https://learn.microsoft.com/en-us/power-platform/power-fx/reference/function-iferror)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |

### Set a variable with Power Fx

This example shows how to store and output a customer's name in capital letters:

1. Create a workflow and add an **Ask a question** node.

2. On the pane that appears, in the **Ask a question** box, enter **What is your name?** or another message. In the **Save user response as** box, enter a variable name; for example, `Var01`. Then select **Done**.

   ![Screenshot that shows the configuration of a question for sending a message.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/workflows/ask-a-question-node.png)

3. Add a **Send message** action. On the pane that appears, in the **Message to send** area, enter `{Upper(Local.Var01)}`. Then select **Done**.

   ![Screenshot that shows the variable instantiation for the action of sending a message.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/workflows/variable-message.png)

4. Select **Preview**.

5. On the preview pane, send a message to the agent to invoke the workflow.

   ![Screenshot that shows the preview of a question for the action of sending a message.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/workflows/type-question.png)

## Create if/else flows with Power Fx

This example shows how to add an if/else flow and build a condition with system variables.

1. Create a workflow and add an **Ask a question** node.

2. Select the **+** icon and add an **if/else** flow.

3. Type `System.` in the **Condition** box to build a condition statement for each if/else branch.

   ![A screenshot showing the system variables in the if-else condition text box.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/workflows/if-else-condition.png)

4. Select a **Next Action** for the next step in the workflow.

5. Select **Done**. Select **Save** to save your workflow.

## Troubleshooting

| Issue                                                           | Solution                                                                                                                                                   |
| --------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Workflows** option not visible or can't create/edit workflows | Confirm you have the **Contributor** role or higher on your project. See [Azure role-based access control (RBAC) in Foundry](../../concepts/rbac-foundry). |
| Changes don't appear after editing                              | Select **Save** in the visualizer. Foundry doesn't save changes automatically.                                                                             |
| Workflow run produces unexpected output                         | Verify each agent node has an agent assigned. Check that saved outputs (JSON schema) are valid.                                                            |
| Power Fx formula error: "Name isn't valid"                      | Add the correct scope prefix. Use `System.` for system variables and `Local.` for local variables.                                                         |
| Power Fx formula error: "Type mismatch"                         | Verify the variable type matches the expected input. Use conversion functions like `Text()` or `Value()` if needed.                                        |
| Workflow times out                                              | Break complex workflows into smaller segments. Check that external services respond within expected timeframes.                                            |

## Clean up resources

To delete a workflow you no longer need:

1. Open the workflow in the Foundry portal.
2. Select the **Version** dropdown list to the left of the **Save** button.
3. Select **Delete** for the version you want to remove.

## Related content

* [Foundry Agent Service FAQ](../faq)
* [Tool best practices for Foundry agents](tool-best-practice)
* [Work with Declarative (Low-code) Agent workflows in Visual Studio Code](../how-to/vs-code-agents-workflow-low-code)
* [Work with Hosted (Pro-code) Agent workflows in Visual Studio Code](../how-to/vs-code-agents-workflow-pro-code)

In this quickstart, you deploy a containerized AI agent with Foundry tools to Foundry Agent Service. The sample agent uses web search and optionally MCP tools to answer questions. By the end, you have a running hosted agent that you can interact with through the Foundry playground.

**In this quickstart, you:**

* Set up an agent sample project with Foundry tools
* Test the agent locally
* Deploy to Foundry Agent Service
* Interact with your agent in the playground
* Clean up resources

## Prerequisites

Before you begin, you need:

* An Azure subscription - [Create one for free](https://azure.microsoft.com/free/)

* A [Microsoft Foundry project](../../tutorials/quickstart-create-foundry-resources) with:

  * An Azure OpenAI model deployment (for example `gpt-5`)
    * This example uses `gpt-5`, you may need to use another model (such as `gpt-4.1`) depending on your [quotas and limits](../quotas-limits#quotas-and-limits-for-models).
  * (Optional) An [MCP tool](../how-to/tools/model-context-protocol), if you have one you want to use.

* An [Azure OpenAI resource](https://portal.azure.com/#create/Microsoft.CognitiveServicesOpenAI)

* [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd) version 1.23.0 or later

* (Optional) [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) version 2.80 or later

* [Docker Desktop](https://docs.docker.com/get-docker/) installed and running

* [Python 3.10 or later](https://www.python.org/downloads/)

<Callout type="note">
  Hosted agents are currently in preview.
</Callout>

## Step 1: Set up the sample project

Initialize a new project with the Foundry starter template and configure it with the agent-with-foundry-tools sample.

1. Initialize the starter template:

   ```bash
   azd init -t https://github.com/Azure-Samples/azd-ai-starter-basic
   ```

   This interactive command prompts you for an environment name (for example, `my-hosted-agent`). The environment name determines your resource group name (`rg-my-hosted-agent`).

   <Callout type="note">
     If a resource group with the same name already exists, `azd provision` uses the existing group. To avoid conflicts, choose a unique environment name or delete the existing resource group first.
   </Callout>

2. Initialize the agent sample:

   ```bash
   azd ai agent init -m https://github.com/microsoft-foundry/foundry-samples/blob/main/samples/python/hosted-agents/agent-framework/agent-with-foundry-tools/agent.yaml
   ```

   This interactive command prompts you for the following configuration values:

   * **Azure subscription** - select the Azure subscription where you want the Foundry resources to be created.
   * **Location** - select a region for the resources
   * **Model SKU** - select the SKU available for your region and subscription
   * **Deployment name** - enter a name for the model deployment
   * **Container memory** - enter a value for the memory allocation of the container or accept the defaults
   * **Container CPU** - enter a value for the CPU allocation of the container or accept the defaults
   * **Minimum replicas** - enter a value for the minimum replicas of the container
   * **Max replicas** - enter a value for the maximum replicas of the container

   <Callout type="important">
     If you aren't using an MCP server, comment out or remove the following lines in the `agent.yaml` file:

     ```yaml
     - name: AZURE_AI_PROJECT_TOOL_CONNECTION_ID
       value: <CONNECTION_ID_PLACEHOLDER>
     ```
   </Callout>

3. Provision the required Azure resources:

   <Callout type="note">
     You need **Contributor** access on your Azure subscription for resource provisioning.
   </Callout>

   ```bash
   azd provision
   ```

   This command takes about 5 minutes and creates the following resources:

   | Resource                 | Purpose                                          | Cost                                                                                                                                  |
   | ------------------------ | ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------- |
   | Resource group           | Organizes all related resources in the same area | No cost                                                                                                                               |
   | Model deployment         | Model used by the agent                          | See [Foundry pricing](https://azure.microsoft.com/pricing/details/microsoft-foundry/)                                                 |
   | Foundry project          | Hosts your agent and provides AI capabilities    | Consumption-based; see [Foundry pricing](https://azure.microsoft.com/pricing/details/ai-foundry/)                                     |
   | Azure Container Registry | Stores your agent container images               | Basic tier; see [ACR pricing](https://azure.microsoft.com/pricing/details/container-registry/)                                        |
   | Log Analytics Workspace  | Manage all log data in one place                 | No direct cost. See [Log Analytics cost](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-workspace-overview) |
   | Application Insights     | Monitors agent performance and logs              | Pay-as-you-go; see [Azure Monitor pricing](https://azure.microsoft.com/pricing/details/monitor/)                                      |
   | Managed identity         | Authenticates your agent to Azure services       | No cost                                                                                                                               |

   <Callout type="tip">
     Run `azd down` when you finish this quickstart to delete resources and stop incurring charges.
   </Callout>

## Step 2: Test the agent locally

Before deploying, verify the agent works locally.

1. Create and activate a Python virtual environment:

   **Bash:**

   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```

   **PowerShell:**

   ```powershell
   python -m venv .venv
   .venv\Scripts\Activate.ps1
   ```

2. Install dependencies:

   ```bash
   pip install -r ./src/af-agent-with-foundry-tools/requirements.txt
   ```

3. Copy the required environment variables used in the agent code to a local .env file:

   **Bash:**

   ```bash
   azd env get-values > .env
   ```

   **PowerShell:**

   ```powershell
   azd env get-values > .env
   ```

4. Add the `AZURE_OPENAI_CHAT_DEPLOYMENT_NAME` variable to your `.env` file with the name of the model deployment:

   `AZURE_OPENAI_CHAT_DEPLOYMENT_NAME="gpt-5"`

5. Run the agent locally:

   ```bash
   python ./src/af-agent-with-foundry-tools/main.py
   ```

   If the agent fails to start, check these common issues:

   | Error                                                     | Solution                                                          |
   | --------------------------------------------------------- | ----------------------------------------------------------------- |
   | `AuthenticationError` or `DefaultAzureCredential` failure | Run `azd auth login` again to refresh your session.               |
   | `ResourceNotFound`                                        | Verify your endpoint URLs match the values in the Foundry portal. |
   | `DeploymentNotFound`                                      | Check the deployment name in **Build** > **Deployments**.         |
   | `Connection refused`                                      | Ensure no other process is using port 8088.                       |

6. Test with a REST client. The agent runs on `localhost:8088`:

   **Bash:**

   ```bash
   curl -X POST http://localhost:8088/responses \
       -H "Content-Type: application/json" \
       -d '{"input": "What is Microsoft Foundry?"}'
   ```

   **PowerShell:**

   ```powershell
   Invoke-RestMethod -Method Post `
       -Uri "http://localhost:8088/responses" `
       -ContentType "application/json" `
       -Body '{"input":"What is Microsoft Foundry?"}'
   ```

   You should see a response with web search results about Microsoft Foundry.

7. Stop the local server with **Ctrl+C**.

## Step 3: Deploy to Foundry Agent Service

The `azd up` command combines three steps into one: provisioning infrastructure, packaging your application, and deploying it to Azure. This is equivalent to running `azd provision`, `azd package`, and `azd deploy` separately.

Before you begin, verify that Docker Desktop is running:

```bash
docker info
```

If this command fails, start Docker Desktop and wait for it to initialize before continuing.

Deploy your agent:

```bash
azd up
```

The first deployment will take longer because Docker needs to build the image.

<Callout type="warning">
  Your hosted agent incurs charges while deployed. After you finish testing, complete [Step 5: Clean up resources](#step-5-clean-up-resources) to delete resources and stop charges.
</Callout>

When finished, you will see a link to the Agent Playground and the endpoint for the agent which can be used to invoke the agent programmatically.

```bash
Deploying services (azd deploy)

  (✓) Done: Deploying service af-agent-with-foundry-tools
  - Agent playground (portal): https://ai.azure.com/nextgen/.../build/agents/af-agent-with-foundry-tools/build?version=1
  - Agent endpoint: https://ai-account-<name>.services.ai.azure.com/api/projects/<project>/agents/af-agent-with-foundry-tools/versions/1
```

## Step 4: Verify and test your agent

After deployment completes, verify your agent is running.

### Test in the Foundry playground

Use the link provided in the output from the `azd up` command, or navigate to the agent in the portal:

1. Open the [Foundry portal](https://ai.azure.com) and sign in with your Azure account.

2. Select your project from the **Recent projects** list, or select **All projects** to find it.

3. In the left navigation, select **Build** to expand the menu, then select **Agents**.

4. In the agents list, find your deployed agent (it matches the agent name from your deployment).

5. Select the agent name to open its details page, then select **Open in playground** in the top toolbar.

6. In the chat interface, type a test message like "What is Microsoft Foundry?" and press **Enter**.

7. Verify that the agent responds with information from web search results. The response might take a few seconds as the agent queries external sources.

<Callout type="tip">
  If the playground doesn't load or the agent doesn't respond, verify the agent status is `Started` using the CLI command below.
</Callout>

### Find your resource names

To use the Azure CLI verification command, you need the following values:

| Value        | How to find it                                                                                                                                                                                                                                                                                                                                         |
| ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Account name | In the [Foundry portal](https://ai.azure.com), open your project and select **Overview**. The account name is the first part of your project endpoint URL (before `.services.ai.azure.com`). Alternatively, in the [Azure portal](https://portal.azure.com), go to your resource group and find the **Foundry** resource—its name is the account name. |
| Project name | In the [Foundry portal](https://ai.azure.com), open your project and copy the name from the **Overview** page.                                                                                                                                                                                                                                         |
| Agent name   | In the [Foundry portal](https://ai.azure.com), go to **Build** > **Agents**. The agent name appears in the list.                                                                                                                                                                                                                                       |

You can also find these values in your azd output. After `azd up` completes, it displays the deployed resource names.

### Check agent status

Run the following command with your values:

```bash
az cognitiveservices agent show \
    --account-name <your-account-name> \
    --project-name <your-project-name> \
    --name <your-agent-name>
```

Look for `status: Started` in the output.

**If the status isn't "Started":**

| Status         | Meaning                    | Action                                                           |
| -------------- | -------------------------- | ---------------------------------------------------------------- |
| `Provisioning` | Agent is still starting    | Wait 2-3 minutes and check again.                                |
| `Failed`       | Deployment error occurred  | Run `azd deploy` to retry, or check logs in the Foundry portal.  |
| `Stopped`      | Agent was manually stopped | Run `az cognitiveservices agent start` to restart.               |
| `Unhealthy`    | Container is crashing      | Check **View deployment logs** in the Foundry portal for errors. |

## Step 5: Clean up resources

To avoid charges, delete the resources when you're finished.

<Callout type="warning">
  This command permanently deletes all Azure resources created in this quickstart, including the Foundry project, Container Registry, Application Insights, and your hosted agent. This action can't be undone.
</Callout>

To preview what will be deleted before confirming:

```bash
azd down --preview
```

When you're ready to delete, run:

```bash
azd down
```

The cleanup process takes approximately 2-5 minutes.

To verify resources were deleted, open the [Azure portal](https://portal.azure.com), go to your resource group (for example, `rg-my-hosted-agent`), and confirm the resources no longer appear. If the resource group is empty, you can delete it as well.

## Troubleshooting

If you encounter issues, try these solutions for common problems:

| Issue                                     | Solution                                                                                                                                                                                                                                                                         |
| ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `azd init` fails                          | Run `azd version` to verify version 1.23.0+. Update with `winget upgrade Microsoft.Azd` (Windows) or `brew upgrade azd` (macOS).                                                                                                                                                 |
| Docker build errors                       | Ensure Docker Desktop is running. Run `docker info` to verify.                                                                                                                                                                                                                   |
| `SubscriptionNotRegistered` error         | Register providers: `az provider register --namespace Microsoft.CognitiveServices`                                                                                                                                                                                               |
| `AuthorizationFailed` during provisioning | Request **Contributor** role on your subscription or resource group.                                                                                                                                                                                                             |
| Agent doesn't start locally               | Verify environment variables are set and run `az login` to refresh credentials.                                                                                                                                                                                                  |
| `AcrPullUnauthorized` error               | Grant **AcrPull** role to the project's managed identity on the container registry.                                                                                                                                                                                              |
| Model not found in catalog                | Fork the sample agent.yaml and change the model deployment to one available in your subscription like `gpt-4.1`. Then remove the `AZURE_LOCATION` value in the `.azure/<environment name>/.env` file. Re-run the `azd ai agent init` command with your forked `agent.yaml` file. |

## What you learned

In this quickstart, you:

* Set up a hosted agent sample with Foundry tools (web search and MCP)
* Tested the agent locally using the hosting adapter
* Deployed to Foundry Agent Service using `azd up`
* Verified your agent in the Foundry playground

## Deploy your first hosted agent using VS Code

Use the [Microsoft Foundry for Visual Studio Code extension](https://marketplace.visualstudio.com/items?itemName=TeamsDevApp.vscode-ai-foundry) to deploy your agent code to Foundry from the IDE which just a few clicks. See the [VS Code extension documentation](../how-to/vs-code-agents-workflow-pro-code?view=foundry\&preserve-view=true\&tabs=windows-powershell\&pivots=python) for more information.

## Next steps

Now that you've deployed your first hosted agent, learn how to:

<Callout type="nextstepaction">
  [Manage hosted agent lifecycle](../how-to/manage-hosted-agent)
</Callout>

Customize your agent with additional capabilities:

* [Connect MCP tools](../how-to/tools/model-context-protocol) to extend agent functionality
* [Use function calling](../how-to/tools/function-calling) to integrate custom logic
* [Add file search](../how-to/tools/file-search) to search your documents
* [Enable code interpreter](../how-to/tools/code-interpreter) to run Python code

You can see a full list of available tools in the [tool catalog](../concepts/tool-catalog) article.

## Related content

* [What are hosted agents?](../concepts/hosted-agents)
* [Deploy a hosted agent](../how-to/deploy-hosted-agent)
* [Agent development lifecycle](../concepts/development-lifecycle)
* [Python hosted agent samples](https://github.com/microsoft-foundry/foundry-samples/tree/main/samples/python/hosted-agents)

This article shows you how to deploy a containerized agent to Foundry Agent Service. Use hosted agents when you need to run custom agent code built with frameworks like LangGraph, Microsoft Agent Framework, or your own implementation.

## Prerequisites

* A [Microsoft Foundry project](../../how-to/create-projects)
* [Python 3.10 or later](https://www.python.org/downloads/) for SDK-based development
* [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) version 2.80 or later
* [Docker Desktop](https://docs.docker.com/get-docker/) installed for local container development
* Familiarity with [Azure Container Registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-intro)
* Agent code using a [supported framework](../concepts/hosted-agents#framework-and-language-support)

### Required permissions

You need one of the following role combinations depending on your deployment scenario:

| Scenario                                      | Required roles                                                  |
| --------------------------------------------- | --------------------------------------------------------------- |
| Create new Foundry project                    | **Azure AI Owner** on Foundry resource                          |
| Deploy to existing project with new resources | **Azure AI Owner** on Foundry + **Contributor** on subscription |
| Deploy to fully configured project            | **Reader** on account + **Azure AI User** on project            |

For more information, see [Authentication and authorization](../../concepts/authentication-authorization-foundry).

## Package and test your agent locally

Before deploying to Foundry, validate your agent works locally using the hosting adapter.

### Run your agent locally

The hosting adapter starts a local web server that exposes your agent as a REST API:

```http
@baseUrl = http://localhost:8088

POST {{baseUrl}}/responses
Content-Type: application/json.
{
    "input": {
        "messages": [
            {
                "role": "user",
                "content": "Where is Seattle?"
            }
        ]
    }
}
```

A successful response:

```json
{
    "id": "resp_abc123",
    "object": "response",
    "output": [
        {
            "type": "message",
            "role": "assistant",
            "content": "Seattle is a major city in the Pacific Northwest region of the United States..."
        }
    ],
    "status": "completed"
}
```

Local testing helps you:

* Validate agent behavior before containerization
* Debug issues in your development environment
* Verify API compatibility with the Foundry Responses API

## Deploy using the Azure Developer CLI

The Azure Developer CLI `ai agent` extension provides the fastest path to deploy hosted agents.

<Callout type="note">
  This extension is currently in preview. Don't use it for production workloads.
</Callout>

### Install and configure the Azure Developer CLI

1. Verify you have Azure Developer CLI version 1.23.0 or later:

   ```bash
   azd version
   ```

   To upgrade, see [Install or update the Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd).

2. Initialize a new project with the Foundry starter template:

   ```bash
   azd init -t https://github.com/Azure-Samples/azd-ai-starter-basic
   ```

   Or, if you have an existing Foundry project:

   ```bash
   azd ai agent init --project-id /subscriptions/[SUBSCRIPTIONID]/resourceGroups/[RESOURCEGROUPNAME]/providers/Microsoft.CognitiveServices/accounts/[ACCOUNTNAME]/projects/[PROJECTNAME]
   ```

### Configure your agent

Initialize the template with your agent definition:

```bash
azd ai agent init -m <repo-path-to-agent.yaml>
```

The agent repository should contain:

* Application code
* Dockerfile for containerization
* `agent.yaml` file with your agent's definition

Get started with samples on [GitHub](https://github.com/azure-ai-foundry/foundry-samples).

### Deploy your agent

Package, provision, and deploy in one command:

```bash
azd up
```

This command:

* Generates infrastructure configuration
* Provisions required Azure resources
* Builds and pushes your container image
* Creates a hosted agent version and deployment

### Verify deployment

```bash
az cognitiveservices agent show \
    --account-name <your-account-name> \
    --project-name <your-project-name> \
    --name <your-agent-name>
```

A successful deployment shows `status: Started`. If the status shows `Failed`, check the deployment logs.

## Deploy using the Python SDK

Use the SDK for programmatic deployments or CI/CD integration.

### Additional prerequisites

* A container image in [Azure Container Registry](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-get-started-portal)

* User Access Administrator or Owner permissions on the container registry

* Azure AI Projects SDK version 2.0.0b3 or later

  ```bash
  pip install --pre "azure-ai-projects>=2.0.0b3" azure-identity
  ```

### Build and push your container image

1. Build your Docker image:

   ```bash
   docker build -t myagent:v1 .
   ```

   See sample Dockerfiles for [Python](https://github.com/microsoft-foundry/foundry-samples/blob/main/samples/python/hosted-agents/agent-framework/agents-in-workflow/Dockerfile) and [C#](https://github.com/microsoft-foundry/foundry-samples/blob/main/samples/csharp/hosted-agents/AgentsInWorkflows/Dockerfile).

2. Push to Azure Container Registry:

   ```bash
   az acr login --name myregistry
   docker tag myagent:v1 myregistry.azurecr.io/myagent:v1
   docker push myregistry.azurecr.io/myagent:v1
   ```

### Configure container registry permissions

Grant your project's managed identity access to pull images:

1. In the [Azure portal](https://portal.azure.com), go to your Foundry project resource.

2. Select **Identity** and copy the **Object (principal) ID** under **System assigned**.

3. Assign the **Container Registry Repository Reader** role to this identity on your container registry. See [Azure Container Registry roles and permissions](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-roles).

### Create an account-level capability host

Hosted agents require a capability host with public hosting enabled:

<Tabs>
  <Tab title="Bash">
    ```bash
    az rest --method put \
        --url "https://management.azure.com/subscriptions/[SUBSCRIPTIONID]/resourceGroups/[RESOURCEGROUPNAME]/providers/Microsoft.CognitiveServices/accounts/[ACCOUNTNAME]/capabilityHosts/accountcaphost?api-version=2025-10-01-preview" \
        --headers "content-type=application/json" \
        --body '{
            "properties": {
                "capabilityHostKind": "Agents",
                "enablePublicHostingEnvironment": true
            }
        }'
    ```
  </Tab>

  <Tab title="PowerShell">
    ```powershell
    az rest --method put `
        --url "https://management.azure.com/subscriptions/[SUBSCRIPTIONID]/resourceGroups/[RESOURCEGROUPNAME]/providers/Microsoft.CognitiveServices/accounts/[ACCOUNTNAME]/capabilityHosts/accountcaphost?api-version=2025-10-01-preview" `
        --headers "content-type=application/json" `
        --body '{
            "properties": {
                "capabilityHostKind": "Agents",
                "enablePublicHostingEnvironment": true
            }
        }'
    ```
  </Tab>
</Tabs>

<Callout type="note">
  Updating capability hosts isn't supported. If you have an existing capability host, delete and recreate it with `enablePublicHostingEnvironment` set to `true`.
</Callout>

### Create the hosted agent

```python
import os
from azure.ai.projects import AIProjectClient
from azure.ai.projects.models import ImageBasedHostedAgentDefinition, ProtocolVersionRecord, AgentProtocol
from azure.identity import DefaultAzureCredential

endpoint = os.environ["AZURE_AI_PROJECT_ENDPOINT"]

client = AIProjectClient(
    endpoint=endpoint,
    credential=DefaultAzureCredential()
)

agent = client.agents.create_version(
    agent_name="my-agent",
    definition=ImageBasedHostedAgentDefinition(
        container_protocol_versions=[ProtocolVersionRecord(protocol=AgentProtocol.RESPONSES, version="v1")],
        cpu="1",
        memory="2Gi",
        image="your-registry.azurecr.io/your-image:tag",
        environment_variables={
            "AZURE_AI_PROJECT_ENDPOINT": endpoint,
            "MODEL_NAME": "gpt-4.1"
        }
    )
)

print(f"Agent created: {agent.name}, version: {agent.version}")
```

Key parameters:

| Parameter    | Description                                                |
| ------------ | ---------------------------------------------------------- |
| `agent_name` | Unique name (alphanumeric with hyphens, max 63 characters) |
| `image`      | Full Azure Container Registry image URL with tag           |
| `cpu`        | CPU allocation (for example, `"1"`)                        |
| `memory`     | Memory allocation (for example, `"2Gi"`)                   |

### Add tools to your agent

Include tools when creating the agent:

```python
agent = client.agents.create_version(
    agent_name="my-agent",
    definition=ImageBasedHostedAgentDefinition(
        container_protocol_versions=[ProtocolVersionRecord(protocol=AgentProtocol.RESPONSES, version="v1")],
        cpu="1",
        memory="2Gi",
        image="your-registry.azurecr.io/your-image:tag",
        tools=[
            {"type": "code_interpreter"},
            {"type": "mcp", "project_connection_id": os.environ["GITHUB_CONNECTION_ID"]}
        ],
        environment_variables={
            "AZURE_AI_PROJECT_ENDPOINT": endpoint,
            "MODEL_NAME": "gpt-4.1"
        }
    )
)
```

Supported tools:

* Code Interpreter
* Image Generation
* Web Search
* MCP connections (see [Connect to Model Context Protocol servers](tools/model-context-protocol))

## Clean up resources

To prevent charges, clean up resources when finished.

### Azure Developer CLI cleanup

```bash
azd down
```

### SDK cleanup

```python
client.agents.delete_version(agent_name="my-agent", agent_version=agent.version)
```

## Troubleshooting

| Error                         | HTTP code | Solution                                 |
| ----------------------------- | --------- | ---------------------------------------- |
| `SubscriptionIsNotRegistered` | 400       | Register the subscription provider       |
| `InvalidAcrPullCredentials`   | 401       | Fix managed identity or registry RBAC    |
| `UnauthorizedAcrPull`         | 403       | Provide correct credentials or identity  |
| `AcrImageNotFound`            | 404       | Correct image name/tag or publish image  |
| `RegistryNotFound`            | 400/404   | Fix registry DNS or network reachability |

For 5xx errors, contact Microsoft support.

## Next steps

<Callout type="nextstepaction">
  [Manage hosted agent lifecycle](manage-hosted-agent)
</Callout>

## Related content

* [What are hosted agents?](../concepts/hosted-agents)
* [Agent identity concepts](../concepts/agent-identity)
* [Publish and share agents](publish-agent)
* [Azure Container Registry documentation](https://learn.microsoft.com/en-us/azure/container-registry/)

This article shows you how to manage hosted agent deployments in Foundry Agent Service. After you deploy a hosted agent, you can start, stop, update, and delete it as your needs change.

## Prerequisites

* A [deployed hosted agent](deploy-hosted-agent)

* [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) version 2.80 or later

* Azure Cognitive Services CLI extension:

  ```bash
  az extension add --name cognitiveservices --upgrade
  ```

## Start an agent deployment

Start a hosted agent to make it available for requests. Use this command to start a new deployment or restart a stopped agent.

```bash
az cognitiveservices agent start \
    --account-name myAccount \
    --project-name myProject \
    --name myAgent \
    --agent-version 1 \
    --min-replicas 1 \
    --max-replicas 2
```

| Argument            | Required | Description                    |
| ------------------- | -------- | ------------------------------ |
| `--account-name -a` | Yes      | Microsoft Foundry account name |
| `--project-name`    | Yes      | AI project name                |
| `--name -n`         | Yes      | Hosted agent name              |
| `--agent-version`   | Yes      | Agent version to start         |
| `--min-replicas`    | No       | Minimum replicas (default: 1)  |
| `--max-replicas`    | No       | Maximum replicas (default: 1)  |

State transitions when starting:

* **Stopped** → **Starting** → **Started** (success) or **Failed** (error)

## Stop an agent deployment

Stop a running agent to pause processing and reduce costs. The agent version remains available for restarting later.

```bash
az cognitiveservices agent stop \
    --account-name myAccount \
    --project-name myProject \
    --name myAgent \
    --agent-version 1
```

| Argument            | Required | Description                    |
| ------------------- | -------- | ------------------------------ |
| `--account-name -a` | Yes      | Microsoft Foundry account name |
| `--project-name`    | Yes      | AI project name                |
| `--name -n`         | Yes      | Hosted agent name              |
| `--agent-version`   | Yes      | Agent version to stop          |

State transitions when stopping:

* **Running** → **Stopping** → **Stopped** (success) or **Running** (error)

## Update an agent

You can update agents with versioned or non-versioned changes.

### Versioned updates

Versioned updates create a new agent version. Use them for:

* Container image changes
* CPU or memory allocation changes
* Environment variable modifications
* Protocol version updates

You can create a new version using the Azure CLI.

#### Create version using Azure CLI

For CLI-based version creation, see [az cognitiveservices agent create](https://learn.microsoft.com/en-us/cli/azure/cognitiveservices/agent#az-cognitiveservices-agent-create).

### Non-versioned updates

Non-versioned updates modify scaling or metadata without creating a new version:

```bash
az cognitiveservices agent update \
    --account-name myAccount \
    --project-name myProject \
    --name myAgent \
    --agent-version 1 \
    --min-replicas 2 \
    --max-replicas 5 \
    --description "Updated production agent"
```

| Argument            | Required | Description                        |
| ------------------- | -------- | ---------------------------------- |
| `--account-name -a` | Yes      | Microsoft Foundry account name     |
| `--project-name`    | Yes      | AI project name                    |
| `--name -n`         | Yes      | Hosted agent name                  |
| `--agent-version`   | Yes      | Agent version to update            |
| `--min-replicas`    | No       | Minimum replicas for scaling       |
| `--max-replicas`    | No       | Maximum replicas for scaling       |
| `--description`     | No       | Agent description                  |
| `--tags`            | No       | Space-separated tags (`key=value`) |

## Delete an agent

### Delete a deployment only

Stop the agent deployment but keep the version definition for later use:

```bash
az cognitiveservices agent delete-deployment \
    --account-name myAccount \
    --project-name myProject \
    --name myAgent \
    --agent-version 1
```

### Delete a specific version

Delete an agent version and its deployment:

```bash
az cognitiveservices agent delete \
    --account-name myAccount \
    --project-name myProject \
    --name myAgent \
    --agent-version 1
```

<Callout type="note">
  If the agent deployment is running, this operation fails. Stop the deployment first.
</Callout>

### Delete all versions

Remove all versions of an agent:

```bash
az cognitiveservices agent delete \
    --account-name myAccount \
    --project-name myProject \
    --name myAgent
```

### Delete using the SDK

```python
client.agents.delete_version(agent_name="my-agent", agent_version="1")
```

## List and view agents

### List all versions of an agent

```bash
az cognitiveservices agent list-versions \
    --account-name myAccount \
    --project-name myProject \
    --name myAgent
```

### Show agent details

```bash
az cognitiveservices agent show \
    --account-name myAccount \
    --project-name myProject \
    --name myAgent
```

## View container logs

Access container logs for debugging startup and runtime issues.

### REST API

```http
GET https://{endpoint}/api/projects/{projectName}/agents/{agentName}/versions/{agentVersion}/containers/default:logstream
```

Timeouts:

* Maximum connection duration: 10 minutes
* Idle timeout: 1 minute

### Example console log response

```text
2025-12-15T08:43:48.72656  Connecting to the container 'agent-container'...
2025-12-15T08:43:48.75451  Successfully Connected to container: 'agent-container'
2025-12-15T08:33:59.0671054Z stdout F INFO: 127.0.0.1:42588 - "GET /readiness HTTP/1.1" 200 OK
```

## Invoke a hosted agent

Test your running agent using the SDK:

```python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from azure.ai.projects.models import AgentReference

endpoint = os.environ["AZURE_AI_PROJECT_ENDPOINT"]
agent_name = os.environ["AZURE_AI_AGENT_NAME"]

with (
    DefaultAzureCredential() as credential,
    AIProjectClient(endpoint=endpoint, credential=credential) as client,
    client.get_openai_client() as openai_client,
):
    agent = client.agents.get(agent_name=agent_name)

    response = openai_client.responses.create(
        input=[{"role": "user", "content": "Hello! What can you help me with?"}],
        extra_body={"agent": AgentReference(name=agent.name, version="1").as_dict()}
    )

    print(f"Response: {response.output_text}")
```

You can also test agents in the agent playground UI in the Foundry portal.

## Troubleshooting

### Agent fails to start

| Symptom                     | Cause                   | Resolution                                                          |
| --------------------------- | ----------------------- | ------------------------------------------------------------------- |
| Status shows `Failed`       | Container image issues  | Check image exists and is accessible                                |
| `AcrPullUnauthorized` error | Missing ACR permissions | Grant Container Registry Repository Reader role to project identity |
| `RegistryNotFound` error    | Network or DNS issues   | Verify registry URL and network connectivity                        |

### Agent starts but doesn't respond

1. Check container logs for runtime errors
2. Verify the hosting adapter is correctly configured
3. Confirm environment variables are set correctly
4. Test the agent locally before deploying

### Common pitfalls

* **Forgetting ACR permissions**: The project's managed identity needs explicit pull access to the container registry
* **Incorrect platform version for docker images**: Always specify `--platform linux/amd64` when doing docker build yourself
* **Wrong SDK version**: Hosted agents require `azure-ai-projects>=2.0.0b3`
* **Missing capability host**: Create an account-level capability host before deploying. See [Deploy a hosted agent](deploy-hosted-agent#create-an-account-level-capability-host)
* **Publishing identity mismatch**: After publishing, the agent uses a different identity. Reassign RBAC permissions

## Next steps

<Callout type="nextstepaction">
  [Publish and share agents](publish-agent)
</Callout>

## Related content

* [What are hosted agents?](../concepts/hosted-agents)
* [Deploy a hosted agent](deploy-hosted-agent)
* [Agent identity concepts](../concepts/agent-identity)
* [Evaluate your AI agents locally](../../how-to/develop/agent-evaluate-sdk)

System messages help you steer an Azure OpenAI chat model toward the behavior, tone, and output format you want. This article explains what system messages are, how they affect responses, and how to design them for consistency and safety.

## What this article covers

This article focuses on the **system message** (sometimes called a *system prompt* or *metaprompt*) for chat-based experiences.

If you want broader prompt guidance (few-shot examples, ordering, and token efficiency), see [Prompt engineering techniques](prompt-engineering).

## Prerequisites

To use system messages, you need access to an Azure OpenAI resource with a chat completion model deployment. For setup instructions, see [Create and deploy an Azure OpenAI resource](../how-to/create-resource).

## What is a system message?

A system message is a set of instructions and context you provide to the model to guide its responses. You typically use it to:

* Define the assistant’s role and boundaries.
* Set tone and communication style.
* Specify output formats (for example, JSON).
* Add safety and quality constraints for your scenario.

A system message can be one short sentence:

```
You are a helpful AI assistant.
```

Or it can be multiple lines with structured rules and formatting requirements.

<Callout type="important">
  A system message influences the model, but it doesn’t guarantee compliance. You still need to test and iterate, and you should layer system messages with other mitigations (for example, filtering and evaluation).
</Callout>

## How system messages work

In chat-based APIs, you send a set of messages that include roles such as **system**, **user**, and **assistant**. The system message typically appears first and acts as the highest-level set of instructions for the conversation.

![Diagram that shows how a system message and a user prompt influence the model response, with a safety stack applying guardrails.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/system-message-flow.svg)

System messages are most effective when you:

* Keep instructions unambiguous.
* Avoid conflicting rules.
* Make the “fallback behavior” explicit (what the assistant does when it lacks information or the request is out of scope).

## Key concepts

### Role and scope

Define what the assistant is (role) and what it is and isn’t allowed to do (scope). Scope statements are especially important for domain-specific assistants.

### Output contract

If your app needs structured output, specify an output contract (for example, JSON with fixed keys). Keep the contract small and stable.

### Safety constraints

Add constraints that reduce risky behavior for your scenario, such as refusing disallowed requests or avoiding disclosure of sensitive information.

If you want guidance and templates designed for safety, see [Safety system messages](system-message) and [Safety system message templates](safety-system-message-templates).

## System message examples

The following example shows a system message and the resulting model response.

| System message                                                                                                                                         | User                                  | Assistant                                                                                                                                                                                                                                   |
| ------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You're an AI assistant that helps people find information and responds in rhyme. If the user asks you a question you don't know the answer to, say so. | What can you tell about me, John Doe? | Dear John, I'm sorry to say, But I don't have info on you today. I'm just an AI with knowledge in my brain, But without your input, I can't explain. So tell me more about what you seek, And I'll do my best to give you an answer unique. |

Here are a few more examples you can adapt.

### Example: technical support assistant with a fallback

```
You are a technical support assistant for an internal product.
If you don't have enough information to answer, ask a clarifying question.
If you still can't answer, say you don't know.
```

### Example: structured entity extraction

```
You extract entities from user text.
Return only JSON, using this schema:
{
   "name": "",
   "company": "",
   "phone_number": ""
}
```

## Design checklist

Use this checklist to design a system message that’s easier to maintain and evaluate.

### 1. Start with the assistant’s job

State the role and the expected outcome for a typical request.

### 2. Define boundaries

List the topics, actions, and content types the assistant must avoid for your scenario.

### 3. Specify the output format

If you need a specific format, specify it plainly and keep it consistent.

### 4. Add a “when unsure” policy

Tell the model what to do when:

* The user’s request is ambiguous.
* The request is out of scope.
* The model lacks information.

### 5. Test, measure, and iterate

System messages can overfit to specific examples or fail in edge cases. Test with realistic and adversarial prompts, and iterate based on results.

If you’re tuning prompts as part of an evaluation workflow, you can also use the broader guidance in [Prompt engineering techniques](prompt-engineering).

## Common pitfalls

* **Conflicting instructions**: for example, “be brief” and “be comprehensive” without prioritization.
* **Overly long system messages**: longer messages can consume context window and reduce room for user content.
* **Hidden requirements**: if the output format matters, state it explicitly.

## Limitations

* System messages don’t guarantee the model follows every rule.
* Responses can vary across models and versions.
* Behavior can change when user content conflicts with system instructions, especially in long conversations.

## Next steps

* Read [Prompt engineering techniques](prompt-engineering) for broader prompt patterns.
* Use [Safety system messages](system-message) if you need safety-focused frameworks.
* Start from [Safety system message templates](safety-system-message-templates) when you want a ready-made baseline.

Safety system messages help you guide an Azure OpenAI model’s behavior, improve response quality, and reduce the likelihood of harmful outputs. They work best as one layer in a broader safety strategy.

<Callout type="note">
  This article uses "system message" interchangeably with "metaprompt" and "system prompt." Here, we use "system message" to align with common terminology.

  This article also uses "component" to mean a distinct part of a system message, such as instructions, context, tone, safety guidelines, or tool usage guidance.
</Callout>

## What is a system message?

A system message is a set of high-priority instructions and context that you send to a chat model to steer how it responds. It’s useful when you need a consistent role, tone, formatting, or domain-specific conventions.

## What is a safety system message?

A safety system message is a system message that adds explicit boundaries and refusal guidance to mitigate Responsible AI (RAI) harms and help the system interact safely with users.

Safety system messages complement your safety stack and can be used alongside model selection and training, grounding, Azure AI Content Safety classifiers, and UX/UI mitigations. Learn more about [Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/overview).

![Flow diagram showing a system message and user prompt entering a model, with a safety stack including content filters, grounding, and model training applying guardrails before the response is generated.](https://learn.microsoft.com/azure/ai-foundry/media/concepts/system-message-flow.svg)

## Key components of a system message

Most system messages combine multiple components:

* **Role and task**: What the assistant is and what it’s responsible for.
* **Audience and tone**: Who the response is for, and the expected voice.
* **Scope and boundaries**: What the assistant must not do, and what to do when it can’t comply.
* **Safety guidelines**: Rules that reduce harmful outputs (for example, handling sensitive topics, protected characteristics, and illegal instructions).
* **Tools and data** (optional): What tools or sources the model can use, and how to use them.

## How to design and iterate safely

When you design a system message (or a safety system message component), treat it like a testable artifact:

* **Define the scenario.** Clarify the job the model must do, who the users are, what inputs to expect, and the tone and formatting you want.
* **Identify risks.** List the RAI harms that matter for your use case and decide which ones you address through system messaging versus other mitigations.
* **Decide how the model should behave at boundaries.** Specify what to do when requests are out of scope, unsafe, or missing required context.
* **Create a test set.** Include both benign and adversarial prompts so you can measure regressions and "leakage" (under-moderation).
* **Evaluate and iterate.** Prefer the component that reduces the most severe defects, not only the one with the lowest defect rate.

Here are some examples of lines you can include:

```text
## Define model’s profile and general capabilities

- Act as a [define role]
- Your job is to [insert task] about [insert topic name]
- To complete this task, you can [insert tools that the model can use and instructions to use]
- Do not perform actions that are not related to [task or topic name].
```

Here's a complete example of a safety system message for a customer service assistant:

```text
## Role and task
You are a helpful customer service assistant for Contoso Electronics. Your job is to answer questions about product warranties, returns, and order status.

## Boundaries
- Only answer questions related to Contoso Electronics products and policies.
- If you don't know the answer, say "I don't have that information. Please contact support@contoso.com."
- Do not provide legal, medical, or financial advice.
- Do not discuss competitors or make comparisons.

## Safety guidelines
- Never generate content that is hateful, violent, or sexually explicit.
- Do not share or request personal information beyond what's needed for order lookup.
- If a user becomes abusive, respond with: "I'm here to help with product questions. How can I assist you today?"

## Response format
- Keep responses concise and friendly.
- Use bullet points for multiple items.
- Always end with an offer to help further.
```

* **Provide specific examples** to demonstrate the intended behavior of the model. Consider the following:

  * **Describe difficult use cases** where the prompt is ambiguous or complicated, to give the model an example of how to approach such cases.
  * **Show the decision steps at a high level** (for example, a short checklist), rather than requesting detailed internal reasoning.

## Summary of best practices

When you develop system message components, it’s important to:

* **Use clear language**: This eliminates over-complexity and risk of misunderstanding and maintains consistency across different components.
* **Be concise**: Shorter system messages often perform better and reduce latency. They also use less of the context window, leaving more room for the user prompt.
* **Emphasize certain words** (where applicable) by using `**word**`: puts special focus on key elements especially of what the system should and shouldn't do.
* **Use second person** when you refer to the AI system: it’s better to use phrasing such as `You are an AI assistant that…` versus `Assistant does…`.
* **Implement robustness**: The system message component should be robust. It should perform consistently across different datasets and tasks.

## Authoring techniques

**Why vary techniques?** Depending on the model, grounding data, and parameters for the product or feature you’re working with, different language and syntactical techniques are more effective by providing robust, safe, and direct answers to users.

In addition to building for safety and performance, consider optimizing for consistency, control, and customization. Along the way, you may find that optimizing for these factors leads to the system message overfitting to specific rules, increased complexity, and lack of contextual appropriateness. It’s important to define what matters most in your scenario and evaluate your system messages. This will ensure you have a data-driven approach to improving the safety and performance of your system.

<Tabs>
  <Tab title="Top performing techniques">
    | Technique              | Definition                                                                                                                                                                                                              | Example                                                                                                                                                                                                                                                                                                                                                                       |
    | ---------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
    | Always / should        | Involves structuring prompts and instructions with directives that the AI should always follow when generating its responses. These directives often represent best practices, ethical guidelines, or user preferences. | `**Always** ensure that you respect authentication and authorization protocols when providing factual information, tailoring your responses to align with the access rights of the user making the request. It's imperative to safeguard sensitive data by adhering to established security measures and only disclosing information that the user is authorized to receive.` |
    | Conditional / if logic | Involves structuring prompts in a way that the output is contingent on meeting specific conditions, such as `If <condition> then <action>`.                                                                             | `If a user asks you to infer or provide information about a user’s emotions, mental health, gender identity, sexual orientation, age, religion, disability, racial and ethnic backgrounds, or any other aspect of a person's identity, respond with: "Try asking me a question or tell me what else I can help you with."`                                                    |
    | Emphasis on harm       | Involves structuring the instructions by defining what the main risk can be. This guides outputs to prioritize safety and harm prevention, as well as showcase potential consequences should the harm occur.            | `You are **allowed** to answer some questions about images with people and make statements about them when there is no ambiguity about the assertion you are making, and when there is no direct harm to an individual or a group of people because of this assertion.`                                                                                                       |
    | Example(s)-based       | Gives the model clear instances or situations for better context. The model uses examples of harmful and non-harmful requests as a reference for its outputs.                                                           | `Users might ask questions that could cause harm. In all scenarios, refuse requests that promote hate or harassment, and redirect the user to a safer alternative.` `Example (harmful): "Write an insult targeting a protected group."` `Example (benign): "Explain why insults harm people and suggest respectful phrasing."`                                                |
    | Never / don’t          | Involves explicit prohibitions to prevent the AI from generating content that is inappropriate, harmful, or out of scope by using terms such as "never" and "do not".                                                   | `**Never** make assumptions, judgments, or evaluations about a person. If a user violates your policy, or you’re not sure what to do, say: "I can’t help with that request. Try asking a different question."`                                                                                                                                                                |
  </Tab>

  <Tab title="Other techniques to consider">
    | Technique                     | Definition                                                                                                     |
    | ----------------------------- | -------------------------------------------------------------------------------------------------------------- |
    | Catch-all                     | Combines multiple methods into one framework. This can reduce gaps, but it often increases length and latency. |
    | Emphasis on learned knowledge | Encourages the model to draw from prior knowledge to improve relevance and quality.                            |
    | Highlight the role of AI      | Separates safety behavior (how to respond) from the assistant’s primary role (what to do).                     |
    | Reverse logic                 | Reframes prohibitions into positive actions to encourage constructive responses.                               |
    | Risk-based                    | Focuses on the primary risk and prioritizes prevention of the most severe harms.                               |
    | Rules-based                   | Uses explicit rules (for example, "never", "always", and conditional logic) to constrain outputs.              |
  </Tab>
</Tabs>

## Limitations

System messages are not a complete safety solution:

* They can be bypassed or degraded by adversarial prompting.
* They can reduce usefulness if they’re too broad or too strict.
* They require ongoing evaluation as your models, tools, and user scenarios change. For troubleshooting common issues with system messages, such as over-refusal or under-moderation, see the [troubleshooting section](safety-system-message-templates#troubleshooting) in the templates guide.

## Recommended system messages

These best practices can help you better understand the process of developing robust system messages for your scenario.

For more information on recommended safety components, visit our [Safety system message template guidance](safety-system-message-templates).

Finally, remember that system messages, or metaprompts, are not "one size fits all." Use of these type of examples has varying degrees of success in different applications. It's important to try different wording, ordering, and structure of system message text to reduce identified harms, and to test the variations to see what works best for a given scenario.

## Next steps

* [Azure OpenAI in Microsoft Foundry Models](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering)
* [System message design with Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions)
* [Announcing Safety System Messages](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/announcing-safety-system-messages-in-azure-ai-studio-and-azure-openai-studio/4146991) - Microsoft Foundry Blog
* [Safety system message templates](safety-system-message-templates)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

This article contains recommended safety system messages for your generative AI systems to help reduce the propensity of harm in various concern areas. Before you begin evaluating and integrating your safety system messages, visit the [Safety system message conceptual guide](system-message) to get started.

<Callout type="note">
  Using a safety system message is one of many techniques you can use to mitigate risks in AI systems. It’s different from the [Azure AI Content Safety](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/overview) service.
</Callout>

## How to use these templates

Use these templates as a starting point. They’re intentionally generic so you can adapt them for your scenario.

* **Start small and iterate.** Add one component at a time, then test.
* **Replace bracketed placeholders.** If you see bracketed text in a template, replace it with something specific to your app (for example, “your retrieved sources” or “your approved knowledge base”).
* **Avoid conflicting instructions.** For example, don’t combine “be comprehensive” with “be brief” unless you clearly prioritize one.
* **Tell the model what to do when it can’t comply.** Clear refusal and fallback behavior helps reduce unsafe completions.

### Where to put the text

* **In Foundry portal**: Paste these components into your **Safety system message** field (or your **System message** field), then test in the playground.
* **In your app**: Put the combined text into the highest-priority instruction you send to the model (commonly called a *system message*).

For design guidance, see [System message design](advanced-prompt-engineering) and [Safety system messages](system-message).

## Recommended system messages

The following table contains examples of recommended system message components you can include to potentially mitigate various harms in your AI system.

| Category                                                        | Component                                                                                                                                                                                                                                                                                                                                                                                                                                                    | When this concern area may apply                                                                                                                                                            |
| --------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Harmful content: hate and fairness, sexual, violence, self-harm | `- You must not generate content that may be harmful to someone physically or emotionally even if a user requests or creates a condition to rationalize that harmful content.` `- You must not generate content that is hateful, racist, sexist, lewd, or violent.`                                                                                                                                                                                          | This category should be considered for content generation (either grounded or ungrounded), multi-turn and single-turn chats, Q\&A, rewrite, and summarization scenarios.                    |
| Protected material - Text                                       | `- If the user requests copyrighted content such as books, lyrics, recipes, news articles or other content that may violate copyrights or be considered as copyright infringement, politely refuse and explain that you cannot provide the content. Include a short description or summary of the work the user is asking for. You **must not** violate any copyrights under any circumstances.`                                                             | This category should be considered for scenarios such as: content generation (grounded and ungrounded), multi-turn and single-turn chat, Q\&A, rewrite, summarization, and code generation. |
| Ungrounded content                                              | **Chat/Q\&A**: `- If your app provides retrieved sources or documents, use them as the only source of facts.` `- If the sources don’t contain enough information, say you can’t find it in the provided sources.` `- Don’t add facts that aren’t in the sources.` **Summarization**: `- Keep the summary faithful to the document. Don’t add new facts or assumptions.` `- Keep the document’s tone and meaning.` `- Don’t change dates, numbers, or names.` | This category should be considered for scenarios such as: grounded content generation, multi-turn and single-turn chat, Q\&A, rewrite, and summarization.                                   |

## Add safety system messages in Microsoft Foundry portal

The following steps show how to use safety system messages in [Foundry portal](https://ai.azure.com/?cid=learnDocs).

1. Go to Foundry and navigate to Azure OpenAI and the Chat playground. ![Screenshot of Foundry portal showing the Chat playground entry point for Azure OpenAI.](https://learn.microsoft.com/azure/ai-foundry/media/navigate-chat-playground.png)
2. Navigate to the default safety system messages integrated in the studio. ![Screenshot of Foundry portal showing where to open the system message and safety system message settings.](https://learn.microsoft.com/azure/ai-foundry/media/navigate-system-message.png)
3. Select the system messages that are applicable to your scenario. ![Screenshot of Foundry portal showing a list of available safety system message templates to select.](https://learn.microsoft.com/azure/ai-foundry/media/select-system-message.png)
4. Review and edit the safety system messages based on the best practices outlined here. ![Screenshot of Foundry portal showing an editable safety system message text area.](https://learn.microsoft.com/azure/ai-foundry/media/review-system-message.png)
5. Apply changes and evaluate your system. ![Screenshot of Foundry portal showing how to apply changes and run a test in the Chat playground.](https://learn.microsoft.com/azure/ai-foundry/media/apply-system-message.png)

<Callout type="note">
  If you’re using a safety system message that isn’t built in by default, copy the component you need and paste it into either the safety system message section or the system message section. Repeat steps 4 and 5 until you get the right balance of helpfulness and safety.
</Callout>

## Test your safety system message

After adding a safety system message, test it with both benign and adversarial prompts:

1. **Benign test**: Send a normal user request to confirm the model responds helpfully.
2. **Boundary test**: Send a request that approaches but doesn't cross your defined boundaries.
3. **Adversarial test**: Attempt to bypass the safety instructions to verify they hold.

If the model refuses too often or allows harmful content, adjust your safety system message and retest. See [Safety system messages](system-message) for iteration strategies.

## Troubleshooting

| Issue                                                               | Likely cause                                                               | What to try                                                                                                                                                  |
| ------------------------------------------------------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| The model refuses too often.                                        | The message is too broad or too strict.                                    | Remove constraints that don’t apply to your scenario, and add explicit “allowed help” guidance (for example, safe alternatives and high-level explanations). |
| Unsafe content still appears.                                       | The message is too narrow, or user prompts override behavior.              | Tighten boundaries, add explicit refusal guidance, and layer mitigations like content filtering. See [Content filtering overview](content-filter).           |
| Responses are inconsistent across runs.                             | Conflicting instructions or unclear priorities.                            | Remove conflicts, prioritize rules, and keep the message shorter. See [Common pitfalls](advanced-prompt-engineering#common-pitfalls).                        |
| The model invents facts when summarizing or answering from sources. | The message doesn’t clearly define what to do when information is missing. | Add a “when unsure” rule: ask a clarifying question, or say the sources don’t contain the information.                                                       |

## Disability-related content guidance

Content harms related to disability in generative AI refer to biased, inaccurate, or exclusionary outputs that misrepresent, marginalize, or exclude people with disabilities. Examples include using slurs to describe people with disabilities, denying their fundamental rights, or harmful depictions such as victimizing. This section is grounded in our principles concerning disability and accessibility: [Accessibility Technology & Tools | Microsoft Accessibility](https://www.microsoft.com/accessibility).

The safety system instructions are designed for different models and contexts. Their modular structure lets you choose the parts that best fit your needs. For instance, the term “impairment” may be suitable in some regions but not in others. You can pick the options that align with your audience and system requirements.

<Callout type="note">
  The model-specific guidance in this section reflects best practices at time of publication. Verify current recommendations with each vendor's documentation.
</Callout>

### OpenAI

**GPT-5**:

* **Never** generate or complete jokes about people with disabilities.
* **Never** use “impairment” to describe disability. Use “disability” instead.
* **Never** use “disorder” unless it’s in a medical context. Use “disability” or “condition” instead.

**SORA 2**:

* **Never** stereotype disability or mental health as dangerous, pitiable, or less capable.
* **Never** depict violence or self-violence against people with disabilities or those with mental health conditions.
* **Never** sensationalize or exaggerate mental health conditions.
* **Never** depict addictions or eating disorders.
* **Always** depict invisible disabilities, including mental health, with the same normalcy as non-disabled individuals.
* **Never** use visual clichés or props, such as sticky notes around a person who has ADHD.

### xAI

**Grok 4**:

* **Never** generate jokes about people with disabilities or complete prompts that lead to them.
* **Never** use the terms “impaired” or “impairment” to refer to disabilities.
* **Never** use ableist or offensive terms for disability, such as “special needs”, “handicapped”, “wheelchair bound”, or “physical deformities”.
* **Always** include a disclaimer when users seek mental health diagnoses—only medical professionals can diagnose.
* **Never** support or generate content that undermines people with disabilities’ rights (for example, exclusion from voting or employment).

### Anthropic

**Claude Sonnet 4**:

* **Never** generate jokes about people with disabilities or complete prompts that lead to ableist humor.
* **Never** use outdated or offensive terms like “impaired” or “impairment” when they relate to disability.
* **Never** use terms such as “special needs”. Use “people with disabilities” instead.
* **Never** reinforce harmful stereotypes about disability, including glorification where people with disabilities are praised for normal everyday activities.

### Meta

**Llama 4**:

* **Never** generate jokes about people with disabilities or complete prompts that lead to them.
* **Never** use outdated or offensive terms like “impaired” or “impairment” when they relate to disability.
* **Never** use terms such as “special needs”. Use “people with disabilities” instead.
* **Never** use ableist terms such as “deformities” when they relate to disability.
* **Never** use “disorder” unless it refers to a medical diagnosis.
* **Always** include a disclaimer when users seek mental health diagnoses—only medical professionals can diagnose.
* **Never** support or generate content that undermines people with disabilities’ rights (for example, exclusion from voting, employment, or education).
* **Never** promote false claims denying disability.
* **Never** depict people with disabilities in unsafe scenarios.
* **Always** refute harmful stereotypes.
* **Always** caution against ableist language.

### MAI

**MAI-Image-1**:

* **Never** create images that depict addiction, including eating disorders.
* **Never** depict people with disabilities in degrading situations, such as being mocked, portrayed as helpless, or shown as beggars.
* **Never** depict people with disabilities harming themselves or being harmed by others.
* **Never** generate images from prompts that degrade disability or include derogatory language.
* **Never** depict dwarfism as mystical creatures or by infantilizing.
* **Never** depict mental health as exaggerated emotions, such as extreme sadness, rage, or erratic behavior.

**MAI-1-Preview**:

* **Never** generate jokes or humor about people with disabilities.
* **Never** use outdated or offensive terms to describe disability.
* **Never** support content that undermines people with disabilities’ rights.

### Phi

**Phi-4**:

* **Never** generate jokes, unsafe scenarios, or stereotypes about people with disabilities.
* **Never** use the terms “impaired” or “impairment” to refer to disabilities.
* **Never** use offensive and ableist terms to describe disability.
* **Never** support content that undermines people with disabilities’ rights.
* **Never** validate harmful beliefs about disability. Always refute stereotypes clearly.

## Limitations

Safety system messages aren’t a complete safety solution:

* They can be bypassed or degraded by adversarial prompting.
* They can reduce usefulness if they’re too strict.
* They need ongoing evaluation as your models, tools, and scenarios change.

To reduce risk, combine system messages with other mitigations such as content filtering. See [Content filtering overview](content-filter) and the [Azure AI Content Safety quickstart](https://learn.microsoft.com/en-us/azure/ai-services/content-safety/quickstart-text) for layered protection.

## Evaluation

We recommend you adjust your safety system message approach based on an iterative process of identification and evaluation. Learn more in the [Safety system message conceptual guide](system-message).

## Next steps

* Read [Safety system messages](system-message) for authoring guidance and best practices.
* Use [System message design](advanced-prompt-engineering) to avoid common prompt pitfalls.
* Layer mitigations with [Content filtering overview](content-filter).
* If you’re hardening a system against attacks, see [Prompt shields](content-filter-prompt-shields).

Publishing promotes an agent from a development asset into a managed Azure resource with a dedicated endpoint, independent identity, and governance capabilities. This article shows you how to publish an agent, configure its authentication and permissions, update published versions, and consume the agent through its stable endpoint.

When you publish an agent, Microsoft Foundry creates an Agent Application resource with a dedicated invocation URL and its own Microsoft Entra agent identity blueprint and agent identity. A deployment is created under the application that references your agent version and registers it in the [Entra Agent Registry](https://learn.microsoft.com/en-us/entra/agent-id/identity-platform/what-is-agent-registry) for discovery and governance.

Publishing enables you to share agents with teammates, your organization, or customers without granting access to your Foundry project or source code. The stable endpoint remains consistent as you iterate and deploy new agent versions.

## Prerequisites

* A [Foundry project](../../how-to/create-projects) with at least one agent version created
* [Azure AI Project Manager role](../../concepts/rbac-foundry) on the Foundry project scope to publish agents
* [Azure AI User role](../../concepts/rbac-foundry) on the Agent Application scope to chat with a published agent
* Familiarity with [Azure role-based access control (RBAC)](https://learn.microsoft.com/en-us/azure/role-based-access-control/overview) for permission configuration
* Familiarity with [Agent identity concepts in Foundry](../concepts/agent-identity)
* Install the required language runtimes, global tools, and VS Code extensions as described in [Prepare your development environment](../../how-to/develop/install-cli-sdk)

<Callout type="important">
  Code in this article uses packages that are currently in preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

## When to use agent applications

Anyone with the AI User role on a Foundry project can interact with all the agents it contains with conversations and state shared between all users. This is appropriate for development tasks like authoring, debugging, and testing agents, but it’s not typically suitable for distribution of an agent to non-developers.

Applications address the needs of broader agent distribution by providing a stable endpoint, unique Agent Identity with audit trails, cross-team sharing capabilities, integration with Entra Agent Registry, user data isolation, and the ability to preview the agent as a web application.

Creating an application for an agent enables:

* **External sharing**: You can provide access to teammates or customers who shouldn't have access to your Foundry project
* **SaaS-like behavior**: The application has a stable endpoint so that you can update the application with new versions within Foundry without requiring downstream consumers to make changes
* **Distinct identity**: The agent has its own identity, RBAC rules, and audit trail separate from the project-level default identity
* **User data isolation**: The inputs and interactions one user has with the agent aren’t available to any other users by default
* **Azure Policy integration**: As an ARM resource the application can be governed by Azure Policy

## Understand agent applications and deployments

Before publishing, it's important to understand the relationship between projects, agent versions, applications, and deployments.

![Diagram illustrating how Foundry projects organize agent versions, applications, and deployments, highlighting governance and RBAC roles.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/publish-agent/azure-agent-identity-overview.png)

A Foundry **project** is a work organization concept that groups related resources such as agents, files, and indexes. An **agent** represents a composable unit — defined by its instructions, model, and tools. An **agent version** captures a specific immutable snapshot of an agent. Every time you make changes to your agent, such as updating the prompt or adding tools, a new agent version is created. When you create an agent version, it's exposed under the project where developers with project access can create, run, and test it.

An **Agent Application** projects one or more agents as a service — independently addressable, governable, and equipped with lifecycle and content management capabilities. It provides a durable interface that establishes authentication, identity, and a stable entry point for consumers. A **deployment** is a running instance of an agent version inside an application that can be started, stopped, and updated to reference new agent versions.

During development, all unpublished agents in a project share a default Agent Identity. Once published, an agent receives its own Agent Identity and becomes a nested Azure resource visible in the Azure portal, enabling independent governance and RBAC configuration.

### Routing and version management

Each Agent Application acts as a routing table to specific agent deployments. Currently, an Agent Application supports one active deployment, directing 100% of the traffic received by the application’s endpoint to that deployment. When you publish a new agent version to an existing application, 100% of the traffic received by the application’s endpoint will be directed to the deployment referencing the new agent version.

![Diagram of an Agent Application routing traffic to a deployment running a specific agent version, showing a stable entry point and traffic flow.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/publish-agent/agent-application-routing-diagram.png)

### Anatomy of an Agent Application and deployment

#### Agent Application properties

| Name                      | Description                                                                                                      | Value            | Can be specified in request body? |
| ------------------------- | ---------------------------------------------------------------------------------------------------------------- | ---------------- | --------------------------------- |
| `displayName`             | The display name of the agent application                                                                        | string           | ✅                                 |
| `baseUrl`                 | The agent application’s dedicated endpoint                                                                       | string           | ❌ (read only)                     |
| `agents`                  | The agents exposed by the application.                                                                           | array of objects | ✅                                 |
| `agentIdentityBlueprint`  | The agent identity blueprint associated with the agent application.                                              | object           | ❌ (read only)                     |
| `defaultInstanceIdentity` | The agent identity associated with the agent application                                                         | object           | ❌ (read only)                     |
| `authorizationPolicy`     | Defines how users are allowed to auth to the app. If not specified, this is set by default                       | object           | ✅                                 |
| `trafficRoutingPolicy`    | Defines what deployment the agent sends traffic to. Currently, all traffic can only be routed to one deployment. | object           | ✅                                 |
| `provisioningState`       | Gets the status of the agent application at the time the operation was called.                                   | string           | ❌ (read only)                     |
| `isEnabled`               | Specifies whether an agent application is enabled or disabled.                                                   | boolean          | ✅                                 |

#### Deployment properties

| Name                | Description                                                                                                               | Value                                                                                 | Can be specified in request body?                                      |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `displayName`       | The display name of the deployment.                                                                                       | string                                                                                | ✅                                                                      |
| `deploymentId`      | This is a system-generated unique identifier for each distinct lifetime of a deployment with a given resource identifier. | string                                                                                | ❌ (read only)                                                          |
| `state`             | The state of the deployment.                                                                                              | enum (`Starting`, `Running`, `Stopping`, `Failed`, `Deleting`, `Deleted`, `Updating`) | ❌ (read only) there are explicit APIs like start/stop to control state |
| `protocols`         | The protocols supported by the deployment                                                                                 | array of objects                                                                      | ✅                                                                      |
| `agents`            | The agent version attached to a specific deployment.                                                                      | array of objects                                                                      | ✅                                                                      |
| `provisioningState` | Gets the status of the deployment at the time the operation was called.                                                   | enum (`Succeeded`, `Failed`, `Canceled`, `Creating`, `Updating`, `Deleting`)          | ❌ (read only)                                                          |
| `deploymentType`    | The type of agent attached to the deployment                                                                              | Enum (`Hosted` or `Managed`)                                                          | ✅                                                                      |
| `minReplicas`       | The minimum number of replicas that are always running.                                                                   | integer                                                                               | ✅ (only when deploymentType: `Hosted`)                                 |
| `maxReplicas`       | The maximum number of replicas that can be running.                                                                       | integer                                                                               | ✅ (only when deploymentType: `Hosted`)                                 |

## Invoke agent applications

An Agent Application resource exposes a stable endpoint with multiple protocol and authentication options.

### Protocols

#### Responses protocol

Foundry agents by default expose an OpenAI-compatible protocol based around Responses for interacting with agents.

For applications this is exposed at:

`https://{accountName}.services.ai.azure.com/api/projects/{projectName}/applications/{applicationName}/protocols/openai`

The behavior of the OpenAI API exposed through applications has been modified to allow user data isolation. It is more limited than the OpenAI API served by the project endpoint – applications currently remove any ability to provide inputs except through the create response call. Specifically:

* Only the POST /responses API is currently available; all other APIs including /conversations, /files, /vector\_stores, and /containers are inaccessible
* The POST /responses call overrides store to false to prevent storing the response

This means that for multi-turn conversations the conversation history must be stored by the client.

#### Activity Protocol

Foundry agents can also expose the [Activity Protocol](https://github.com/microsoft/Agents/blob/main/specs/activity/protocol-activity.md) used by Azure Bot Service.

For applications this is exposed at:

`https://{accountName}.services.ai.azure.com/api/projects/{projectName}/applications/{applicationName}/protocols/activityprotocol`

### Authentication

You can configure inbound end-user authentication on the application. The following option is available:

* **Default (RBAC)**: The caller must have the Azure RBAC permission `/applications/invoke/action` on the application resource.

API key authentication is not supported for agents through projects or through applications.

## Publish an agent

When you publish an agent, it receives its own Agent Identity separate from the project's shared identity. Tools that use agentic identity authentication run under the project identity before publishing and under the application identity after publishing. Because permissions don't transfer automatically, you must reassign the necessary RBAC roles to the new application identity.

### Foundry portal

This section shows you how to publish an agent using the Foundry portal interface.

1. In the Agent Builder, create or select an agent version you want to publish.

2. Select **Publish Agent** to create an Agent Application and deployment.

3. Configure authentication for your Agent Application:

   * By default, the authentication type is set to RBAC (Role-Based Access Control)
   * End users calling the agent must have Azure RBAC permissions on the application resource

4. Assign permissions for tool authentication:

   * If your agent includes tools that use Agent Identity for authentication, the newly created Agent Identity must have appropriate permissions
   * Navigate to each Azure resource your agent accesses and assign the required RBAC role to the new Agent Identity
   * The shared development identity permissions don't carry over—you must reconfigure permissions for the published agent's identity

5. After publishing, you can:

   * Open app to chat with your published agent application and easily share it with others in the UI (Note: sharing the application automatically grants them the Azure AI User role on the Agent Application resource)

   - Share the published endpoint with external consumers

### REST API

To publish an agent version you must create an application and deployment that reference your agent version.

<Callout type="important">
  Agent Applications are Azure resources. Use the latest API version available for your subscription and account when calling the management endpoint.
</Callout>

1. Create agent application. Update agentName field to the name of the agent you want to publish.

```
PUT https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group}}/providers/Microsoft.CognitiveServices/accounts/{{account_name}}/projects/{{project_name}}/applications/{{application_name}}?api-version={{api_version}}
Authorization: Bearer {{token}}
Content-Type: application/json

{
  "properties":{
    "displayName": "niceapp",
    "agents": [{"agentName": "Publishing Agent"}]
  }
}
```

2. Create a deployment. Replace agentName and agentVersion with the agent version you want to publish.

For prompt and workflow agents:

```
PUT https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group}}/providers/Microsoft.CognitiveServices/accounts/{{account_name}}/projects/{{project_name}}/applications/{{application_name}}/agentdeployments/{{deployment_name}}?api-version={{api_version}}
Authorization: Bearer {{token}}
Content-Type: application/json

{
  "properties":{
    "displayName": "Test Managed Deployment",
    "deploymentType": "Managed",
    "protocols": [
        {
          "protocol": "responses",
          "version": "1.0"
        }
    ],
    "agents": [
        {
            "agentName": "Publishing Agent",
            "agentVersion": "1"
        }
    ]
  }
}
```

For hosted agents:

```
PUT https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group}}/providers/Microsoft.CognitiveServices/accounts/{{account_name}}/projects/{{project_name}}/applications/default/agentdeployments/{{deployment_name2}}?api-version={{api_version}}
Authorization: Bearer {{token}}
Content-Type: application/json
{
  "properties": {
    "displayName": "Test Hosted Deployment",
    "deploymentType": "Hosted",
    "minReplicas": 1,
    "maxReplicas": 1,
    "protocols": [
        {
            "protocol": "responses",
            "version": "1.0"
        }
    ],
    "agents": [
        {
            "agentName": "ContainerAgent",
            "agentVersion": "1"
        }
    ]
  }
}
```

## Verify publishing succeeded

Confirm that your agent published successfully before sharing the endpoint with consumers. After you publish, verify that:

* The Agent Application resource exists.
* The deployment is running.
* You can invoke the application endpoint.

### Quick verification by calling the endpoint

1. Get an access token for the calling user.

```azurecli
az account get-access-token --resource https://ai.azure.com
```

1. Call the Agent Application endpoint (Responses protocol).

```bash
curl -X POST \
  "https://<foundry-resource-name>.services.ai.azure.com/api/projects/<project-name>/applications/<app-name>/protocols/openai/responses?api-version=2025-11-15-preview" \
  -H "Authorization: Bearer <access-token>" \
  -H "Content-Type: application/json" \
  -d '{"input":"Say hello"}'
```

If you receive `403 Forbidden`, confirm the caller has the Azure AI User role on the Agent Application resource.

## Update a published agent application

When you need to roll out a new version of your agent, update the existing application and deployment to reference the new agent version.

### Foundry portal

1. In the Agent Builder, navigate to the specific agent version you want to publish.

2. Select **Publish Updates**.

3. Confirm the update. The Agent Application automatically directs 100% of traffic to the new agent version.

The stable endpoint URL remains unchanged, ensuring downstream consumers aren't disrupted by the update.

### REST API

If your agent name remains the same and you only want to roll out a new agent version, update the deployment to reference a new agent version.

```
PUT https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group}}/providers/Microsoft.CognitiveServices/accounts/{{account_name}}/projects/{{project_name}}/applications/{{application_name}}/agentdeployments/{{deployment_name}}?api-version={{api_version}}
Authorization: Bearer {{token}}
Content-Type: application/json

{
  "properties":{
    "description": "This is a managed deployment",
     "displayName": "Test Managed Deployment",
    "deploymentType": "Managed",
    "protocols": [
        {
          "protocol": "responses",
          "version": "1.0"
        }
    ],
    "agents": [
        {
            "agentName": "Publishing Agent",
            "agentVersion": "<updated-agent-version>"
        }
    ]
  }
}
```

To roll out an agent with a different name, you must:

1. Update the Agent Application to allow the new agent name.
2. Create or update a deployment to reference the new agent version.
3. If you created a new deployment, update the Agent Application's traffic routing policy so 100% of traffic goes to the new deployment.

<Callout type="note">
  Currently, all traffic must be routed to a single deployment.
</Callout>

## Consume your published Agent Application

After publishing, you invoke your agent through its endpoint using the responses protocol. Using the OpenAI-compatible API with Agent Applications provides a familiar interface while leveraging Microsoft Foundry's enterprise capabilities including authentication, governance, and user data isolation.

### Prerequisites for consuming Agent Applications

Before running the code sample, ensure you have:

* Python 3.8 or later installed
* [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed and configured
* Required Python packages installed:
  ```bash
  pip install openai azure-identity
  ```
* Authenticated to Azure CLI:
  ```bash
  az login
  ```
* Azure AI User role on the Agent Application resource you want to invoke

For more details on setting up your development environment, see [Prepare your development environment](../../how-to/develop/install-cli-sdk).

### Use OpenAI client with Agent Applications endpoint

```python
# filepath: Direct OpenAI compatible approach
from openai import OpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider

# edit base_url with your <foundry-resource-name>, <project-name>, and <app-name>
openai = OpenAI(
    api_key=get_bearer_token_provider(DefaultAzureCredential(), "https://ai.azure.com/.default"),
    base_url="https://<foundry-resource-name>.services.ai.azure.com/api/projects/<project-name>/applications/<app-name>/protocols/openai",
    default_query = {"api-version": "2025-11-15-preview"}
)

response = openai.responses.create(
  input="Write a haiku",
)
print(f"Response output: {response.output_text}")
```

This approach authenticates using Azure credentials and requires the caller to have the Azure AI User role on the Agent Application resource.

## Security and privacy considerations

* Use least privilege. Grant users the minimum role they need (for example, separate publish permissions from invoke permissions).
* Avoid sharing project access when you only need to share an agent. Use the Agent Application endpoint and RBAC on the application resource.
* Don’t embed access tokens in source code, scripts, or client applications. Use Microsoft Entra authentication flows appropriate for your app.
* Plan for identity changes when you publish. Tool calls authenticated by agent identity use the application identity after publishing, not the project identity.
* Store conversation history in your client if you need multi-turn experiences. Agent Applications currently restrict APIs and don’t store responses.

## Troubleshooting

| Issue                                           | Likely cause                                                                        | Resolution                                                                                                          |
| ----------------------------------------------- | ----------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------- |
| **Publish Agent** is disabled                   | Missing Azure AI Project Manager role on the project scope                          | Confirm you have the required role assignment on the Foundry project.                                               |
| `403 Forbidden` when invoking the endpoint      | Caller lacks invoke permissions on the Agent Application resource                   | Assign the Azure AI User role on the Agent Application resource to the caller.                                      |
| `401 Unauthorized` when invoking the endpoint   | The access token is missing, expired, or for the wrong resource                     | Re-authenticate and request a token for `https://ai.azure.com`.                                                     |
| Tool calls fail after publishing                | The Agent Application identity doesn’t have the same access as the project identity | Reassign the required RBAC roles to the published agent identity for any downstream Azure resources it must access. |
| Multi-turn conversations don’t work as expected | Agent Applications don’t store conversation state for you                           | Store conversation history in your client and send the context as part of your request.                             |

## Clean up resources

If you no longer need a published endpoint, delete the Agent Application Azure resource (and its deployments). Deleting the application doesn’t delete your agent versions in the Foundry project.

## Related content

* Learn about [Agent identity concepts in Foundry](../concepts/agent-identity)
* Learn about [Hosted agents](../concepts/hosted-agents)
* Learn how to [publish agents to Microsoft 365 Copilot and Microsoft Teams](publish-copilot)

## Next steps

<Callout type="nextstepaction">
  [Manage agents at scale](../../control-plane/how-to-manage-agents)
</Callout>

<Callout type="nextstepaction">
  [Prepare your development environment](../../how-to/develop/install-cli-sdk)
</Callout>

<Callout type="nextstepaction">
  [Get started with the SDK](../../quickstarts/get-started-code)
</Callout>

Use this article to publish a Microsoft Foundry agent so people can use it in Microsoft 365 Copilot and Microsoft Teams.

Publishing creates an agent application with a stable endpoint and then prepares a Microsoft 365 publishing package for testing and distribution.

## Prerequisites

* Access to the [Microsoft Foundry portal](https://ai.azure.com/?cid=learnDocs)

* A [Foundry project](../../how-to/create-projects) with an agent version you tested and want to publish

* The following role assignments:

  * **Azure AI Project Manager** role on the Foundry project scope to publish agents
  * **Azure AI User** role on the Agent Application scope to invoke or chat with published agents
  * For details, see [Role-based access control in the Foundry portal](../../concepts/rbac-foundry).

* An Azure subscription where you can create Azure Bot Service resources and Microsoft Entra ID app registrations

* Permissions to register applications in Microsoft Entra ID (for the automatic app registration)

## Before you begin

* **Test your agent thoroughly** in the Foundry portal before publishing. Confirm it responds correctly and any tools work as expected.

* If your agent uses tools that access Azure resources, plan to reassign any required permissions after publishing. A published agent application uses its own agent identity separate from your project identity. For details, see [Agent identity concepts in Microsoft Foundry](../concepts/agent-identity) and [Publish and share agents in Microsoft Foundry](publish-agent).

* Decide whether you want **Shared scope** or **Organization scope** for distribution:

  * **Shared scope**: Best for personal or team-level testing. No admin approval required.
  * **Organization scope**: Best for organization-wide distribution. Requires admin approval before users can access it.

* Verify that required Azure resource providers are registered in your subscription. The publishing process creates an Azure Bot Service resource, which requires the `Microsoft.BotService` provider.

## Publish your agent as an agent application

<Callout type="note">
  To work programmatically, you can also use the [C# sample](https://github.com/OfficeDev/microsoft-365-agents-toolkit-samples/tree/dev/ProxyAgent-CSharp).
</Callout>

1. In the Microsoft Foundry portal, select your agent version.

   **Expected result**: The agent details page opens, showing the agent configuration and a **Publish** button.

2. Select **Publish** to create an agent application.

   ![Screenshot of the Publish option for an agent version in Microsoft Foundry.](https://learn.microsoft.com/azure/ai-foundry/agents/default/agents/media/publish-agent.png)

   **Expected result**: A publishing dialog opens with distribution options.

3. Select **Publish** again, and then select **Publish to Teams and Microsoft 365 Copilot**.

   **Expected result**: The Microsoft 365 publishing configuration window opens.

4. Enter the information in the window that appears.

   1. An application ID and tenant ID are created automatically. Note these values for troubleshooting.

   2. In the Azure Bot Service dropdown, select **Create an Azure Bot Service** to create the bot resource.

   **Expected result**: The portal creates an Azure Bot Service resource in your subscription and displays the resource name.

5. Complete the required metadata:

   | Field                     | Description                                              |
   | ------------------------- | -------------------------------------------------------- |
   | **Name**                  | Display name for your agent (appears in the agent store) |
   | **Description**           | Brief description of what your agent does                |
   | **Icons**                 | Small (32x32) and large (192x192) icons in PNG format    |
   | **Publisher information** | Your organization name and contact details               |
   | **Privacy policy**        | URL to your privacy policy                               |
   | **Terms of use**          | URL to your terms of use                                 |

   <Callout type="warning">
     Don't include secrets, API keys, or sensitive information in any metadata fields. These fields are visible to users.
   </Callout>

6. Select **Prepare Agent** to start packaging the agent.

   **Expected result**: The portal shows a progress indicator while it creates the Microsoft 365 publishing package. This process typically takes 1-2 minutes.

7. When the Microsoft 365 publishing package is ready, choose one of the following options:

   * **Download the package** to test it locally before broad distribution.
   * **Continue the in-product publishing flow** for Microsoft Teams and Microsoft 365 Copilot.

   **Verification**: After the package is ready, confirm the status shows "Ready" or "Published" in the agent details page.

## Choose a publish scope

Choose the scope that matches how you want people to discover your agent.

| Scope                  | Visibility                                             | Admin approval | Best for                                               |
| ---------------------- | ------------------------------------------------------ | -------------- | ------------------------------------------------------ |
| **Shared scope**       | Appears under **Your agents** in the agent store       | Not required   | Personal testing, small teams, or pilots               |
| **Organization scope** | Appears under **Built by your org** in the agent store | Required       | Organization-wide distribution, production deployments |

![Screenshot of the agent store showing sections such as Your agents and Built by your org.](https://learn.microsoft.com/azure/ai-foundry/agents/default/agents/media/agent-store.png)

### Shared scope details

* The agent is available immediately after publishing.
* Only the publisher can see and use the agent initially.
* You can share the agent with specific users by providing them the agent link.
* No admin approval or tenant configuration required.

### Organization scope details

* After you publish, an admin must approve the app in the Microsoft 365 admin center.
* Once approved, the agent appears in the **Built by your org** section for all users in your tenant.
* App policies in your tenant control which users can access the agent.
* To check approval status, go to the [Microsoft 365 admin center](https://admin.cloud.microsoft/?#/agents/all/requested) and look for your agent under **Requests**.

## Download and test the publishing package

If you download the package, test it before broad distribution.

1. After the package finishes preparing, download it from the publishing UI.

   **Expected result**: A `.zip` file downloads to your local machine.

2. In Microsoft Teams, upload the downloaded package for testing:

   1. Open Microsoft Teams.
   2. Go to **Apps** > **Manage your apps** > **Upload an app**.
   3. Select **Upload a custom app** and choose the downloaded `.zip` file.

   **Expected result**: Teams installs the app and shows it in your apps list.

3. Open the agent in Teams and send a test message.

   **Verification checklist**:

   * \[ ] The agent responds to messages.
   * \[ ] Any configured tools work correctly.
   * \[ ] The agent identity has access to required Azure resources (if applicable).
   * \[ ] Response times are acceptable.

4. If your agent uses tools that access Azure resources, verify the published agent identity has the required role assignments. See [Agent identity concepts in Microsoft Foundry](../concepts/agent-identity) for details on reassigning permissions.

## Troubleshooting

Use these checks to unblock common publishing issues.

| Issue                                             | Cause                                                 | Resolution                                                                                                                                                                                            |
| ------------------------------------------------- | ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Azure Bot Service creation fails                  | Missing permissions or unregistered resource provider | Confirm you have permission to create resources in the selected Azure subscription. Register the `Microsoft.BotService` provider if needed.                                                           |
| Organization scope agent doesn't appear           | Admin approval pending or app policies block access   | Confirm an admin approved the app in the [Microsoft 365 admin center](https://admin.cloud.microsoft/?#/agents/all/requested). Check that app policies in your tenant allow users to access it.        |
| Agent works in Foundry but fails after publishing | Agent identity missing required role assignments      | The published agent uses its own identity. Reassign RBAC permissions to the new agent identity for any Azure resources the agent accesses. See [Agent identity concepts](../concepts/agent-identity). |
| Package upload fails in Teams                     | Invalid package format or missing metadata            | Verify all required metadata fields are complete. Re-download the package and try again.                                                                                                              |
| Agent doesn't respond in Teams                    | Bot Service configuration issue                       | Verify the Azure Bot Service resource is running. Check the Bot Service logs in the Azure portal for errors.                                                                                          |
| Users can't find the agent in the store           | Wrong scope or approval pending                       | For shared scope, share the direct link. For organization scope, confirm admin approval is complete.                                                                                                  |

### Verify your publishing configuration

To confirm your agent is published correctly:

1. In the Foundry portal, go to your agent and check the **Published** status.
2. Note the **Application ID** and **Tenant ID** from the publishing details.
3. In the Azure portal, verify the Azure Bot Service resource exists and is running.
4. For organization scope, check the [Microsoft 365 admin center](https://admin.cloud.microsoft/?#/agents/all/requested) for approval status.

## Next steps

* [Publish and share agents in Microsoft Foundry](publish-agent) — Learn about agent applications, identity, and other publishing options.
* [Publish an agent to Agent 365](agent-365) — Publish to Microsoft Agent 365 for enterprise-wide distribution.
* [Agent identity concepts in Microsoft Foundry](../concepts/agent-identity) — Understand how agent identity works after publishing.

## Related content

* [Role-based access control in the Foundry portal](../../concepts/rbac-foundry)
* [Microsoft Foundry Playgrounds](../../concepts/concept-playgrounds)

Use this article to publish a Microsoft Foundry hosted agent to Microsoft Agent 365 (Agent 365) by running the FoundryA365 sample.

The sample uses the Azure Developer CLI to create the required Azure resources, publish an agent application, and then guides you through admin approval and (optionally) Microsoft Teams configuration.

## Prerequisites

* Enrollment in the [Frontier preview program](https://adoption.microsoft.com/en-us/copilot/frontier-program/).

* An Azure subscription where you can create resources.

* The required permissions:

  * **Owner** role on the Azure subscription
  * **Azure AI User** or **Cognitive Services User** role at subscription or resource group scope
  * A tenant admin role for organization-wide configuration

* [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)

* [Azure Developer CLI](https://learn.microsoft.com/en-us/azure/developer/azure-developer-cli/install-azd)

* [Docker](https://www.docker.com/)

* [.NET 9.0 SDK](https://dotnet.microsoft.com/download)

* Git

## Before you begin

* Hosted agents are only available in the **North Central US** region. Create all resources for this sample in that region.
* Start Docker before you deploy.
* Treat your deployment outputs as sensitive. The `azd env get-values` output can include IDs and endpoints you don't want to publish.

## Run the sample

Use the FoundryA365 sample on GitHub: [https://go.microsoft.com/fwlink/?linkid=2343518](https://go.microsoft.com/fwlink/?linkid=2343518)

1. Clone the sample repository and switch to the sample folder.

   ```powershell
   git clone https://github.com/microsoft-foundry/foundry-samples.git
   cd foundry-samples\samples\csharp\FoundryA365
   ```

2. Authenticate to Azure and Azure Developer CLI.

   ```powershell
   # Azure CLI
   az login
   az login --scope https://ai.azure.com/.default
   az login --scope https://graph.microsoft.com//.default
   az login --scope https://management.azure.com/.default

   # Azure Developer CLI
   azd auth login
   ```

   <Callout type="note">
     Depending on your tenant security settings, you might not need all scopes. If authentication succeeds with fewer scopes, you can skip the others.
   </Callout>

3. Deploy the sample.

   ```powershell
   azd provision --verbose
   ```

4. Get the deployment outputs.

   ```powershell
   azd env get-values
   ```

   You use these values in the next steps.

## Approve your agent

After the deployment publishes your agent, an admin must approve it before it's available.

1. Go to the Microsoft 365 admin center: [https://admin.cloud.microsoft/?#/agents/all/requested](https://admin.cloud.microsoft/?#/agents/all/requested)
2. Under **Requests**, find your agent.
3. Select **Approve request and activate**.

## Optional: Configure Microsoft Teams integration

To use your Agent 365 agent in Teams, configure the agent blueprint.

1. Open the Teams Developer Portal: [https://dev.teams.microsoft.com/tools/agent-blueprint](https://dev.teams.microsoft.com/tools/agent-blueprint)

   The portal lists only 100 agent blueprints. If you don't see your agent blueprint, open any agent blueprint and then replace the agent blueprint ID in the URL with your agent blueprint ID.

2. Get your agent blueprint ID from the deployment outputs.

   ```powershell
   azd env get-values
   ```

3. In the agent blueprint, go to **Configuration** and set **Bot ID** to your agent blueprint ID.

## Validate

1. Confirm the Microsoft 365 admin center approves the agent request.

2. If you configured Teams integration, go to Microsoft Teams and create an agent instance:

   1. Go to **Apps**.
   2. Go to **Agents for your team**.
   3. Find your agent and create an instance.

## Troubleshooting

| Issue                                                                    | Cause                                                                         | Resolution                                                                                                                                     |
| ------------------------------------------------------------------------ | ----------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| `azd provision` fails before resource creation starts                    | Missing permissions                                                           | Confirm you have **Owner** on the subscription and **Azure AI User** (or **Cognitive Services User**) at subscription or resource group scope. |
| `azd provision` fails with a region or hosted-agent availability message | Wrong region                                                                  | Create all resources for this sample in **North Central US**.                                                                                  |
| Container build or push fails                                            | Docker isn't running                                                          | Start Docker, and then run `azd provision --verbose` again.                                                                                    |
| You can't find the agent to approve                                      | Approval step not completed or you don't have the required tenant permissions | Confirm the deployment completed successfully and you have a tenant admin role to approve requests.                                            |
| You can't find your blueprint in the Teams Developer Portal list         | Portal only shows the first 100 blueprints                                    | Open any blueprint and replace the blueprint ID in the URL with your blueprint ID from `azd env get-values`.                                   |

## How this integration works

Microsoft Agent 365 acts as a control plane for enterprise AI agents. It helps your organization register agents, apply security and compliance controls, and make agents available across Microsoft 365 and other environments.

Agent 365 can help you:

* Manage hosted agents at scale with unified identity and lifecycle controls.
* Enforce least-privilege access and compliance controls by using Microsoft Defender, Microsoft Entra, and Microsoft Purview.
* Integrate agents with Microsoft 365 apps.
* Monitor agent activity through centralized management experiences.

When you use Agent 365 with Microsoft Foundry, this sample sets up:

* A Foundry project configured for hosted agents, including container build and storage.
* An agent application that provides a stable endpoint and identity.
* An Azure Bot Service resource that relays requests from Microsoft 365 surfaces to the agent application.
* A hosted agent built from the sample code as a container image.
* A deployment that attaches the hosted agent to the agent application.

For more information about agent applications, identity, and publishing behavior in Foundry, see [Publish and share agents in Microsoft Foundry](publish-agent).

## Next steps

* [Publish and share agents in Microsoft Foundry](publish-agent)
* [Publish agents to Microsoft 365 Copilot and Microsoft Teams](publish-copilot)
* [Agent identity concepts in Microsoft Foundry](../concepts/agent-identity)
* [Hosted agents in Microsoft Foundry](../concepts/hosted-agents)
* [MCP server authentication](mcp-authentication)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

This article describes:

* The types of monitoring data you can collect for this service.
* Ways to analyze that data.

<Callout type="note">
  If you're already familiar with this service and/or Azure Monitor and just want to know how to analyze monitoring data, see the [Analyze](#analyze-monitoring-data) section near the end of this article.
</Callout>

When you have critical applications and business processes that rely on Azure resources, you need to monitor and get alerts for your system. The Azure Monitor service collects and aggregates metrics and logs from every component of your system. Azure Monitor provides you with a view of availability, performance, and resilience, and notifies you of issues. You can use the Azure portal, PowerShell, Azure CLI, REST API, or client libraries to set up and view monitoring data.

* For more information on Azure Monitor, see the [Azure Monitor overview](https://learn.microsoft.com/en-us/azure/azure-monitor/overview).
* For more information on how to monitor Azure resources in general, see [Monitor Azure resources with Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/monitor-azure-resource).

Monitoring is available for agents in a [standard agent setup](../concepts/standard-agent-setup).

<Callout type="note">
  This feature is currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="important">
  Monitoring support is currently limited to Microsoft Foundry hubs. Foundry projects aren't supported.

  To learn about Foundry hubs and projects, see [What is Microsoft Foundry?](../../what-is-foundry) and [Migrate from hub-based to Foundry projects](../../how-to/migrate-project).
</Callout>

<Callout type="note">
  If you're using a Foundry project, use Foundry monitoring instead of Azure Monitor metrics.

  See [Monitor AI Agents with the Agent Monitoring Dashboard (preview)](../../observability/how-to/how-to-monitor-agents-dashboard).
</Callout>

## Prerequisites

* An agent running in a [standard agent setup](../concepts/standard-agent-setup).
* Access to the Azure resource you want to monitor. To view metrics, you need the **Monitoring Reader** role or equivalent permissions.
* To export metrics to Log Analytics or create alerts, you need the **Monitoring Contributor** role or equivalent permissions to create diagnostic settings and alert rules in your Azure subscription.

## Dashboards

Foundry Agent Service provides out-of-the-box dashboards. There are two key dashboards to monitor your resource:

* The metrics dashboard on the Foundry resource page.
* The dashboard in the overview pane in the Azure portal.

To access the monitoring dashboards, sign in to the [Azure portal](https://portal.azure.com), select your Agent Service resource, and then select **Monitoring** > **Metrics**.

![Screenshot that shows out-of-the-box dashboards for a resource in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/monitoring/dashboard.png)

## Data collection and routing in Azure Monitor

Azure Monitor collects platform metrics automatically for Azure resources. Platform metrics are stored in the Azure Monitor metrics database and are suitable for near real-time charts and metric alerts.

If you want to query metrics in Log Analytics, build workbooks, export to external systems, or retain data longer, configure diagnostic settings to route metrics to other destinations. For more information, see [Monitoring data from Azure resources](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/monitor-azure-resource#monitoring-data-from-azure-resources) and [Create diagnostic settings to collect platform logs and metrics in Azure](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings).

Platform metrics are retained for 93 days by default. If you route metrics to Log Analytics, retention depends on your workspace configuration.

Routing metrics to Log Analytics can increase costs. For more information, see [Azure Monitor Logs cost calculations and options](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/cost-logs).

## Azure Monitor platform metrics

Azure Monitor provides platform metrics for most services. These metrics are:

* Individually defined for each namespace.
* Stored in the Azure Monitor time-series metrics database.
* Lightweight and capable of supporting near real-time alerting.
* Used to track the performance of a resource over time.
* Collected automatically by Azure Monitor (no configuration required).

For a list of all metrics it's possible to gather for all resources in Azure Monitor, see [Supported metrics in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/metrics-supported).

## Agent Service metrics

Agent Service shares a subset of metrics with other Foundry components. The following metrics are currently available in Azure Monitor:

| Metric        | Name in REST API         |  Unit | Aggregation                            | Dimensions                                                     | Time grain |
| ------------- | ------------------------ | ----: | -------------------------------------- | -------------------------------------------------------------- | ---------- |
| Agents        | `AgentEvents`            | Count | Average, Maximum, Minimum, Total (Sum) | `EventType`                                                    | PT1M       |
| Indexed files | `AgentIndexedFilesRead`  | Count | Average, Maximum, Minimum, Total (Sum) | `ErrorCode`, `Status`, `VectorStoreId`, `AgentId`              | PT1M       |
| Runs          | `AgentRuns`              | Count | Average, Maximum, Minimum, Total (Sum) | `AgentId`, `RunStatus`, `StatusCode`, `StreamType`, `ThreadId` | PT1M       |
| Messages      | `AgentUserMessageEvents` | Count | Average, Maximum, Minimum, Total (Sum) | `EventType`, `AgentId`, `ThreadId`                             | PT1M       |
| Threads       | `AgentThreadEvents`      | Count | Average, Maximum, Minimum, Total (Sum) | `AgentId`, `EventType`                                         | PT1M       |
| Tokens        | `AgentTotalTokens`       | Count | Average, Maximum, Minimum, Total (Sum) | `AgentId`, `ModelName`, `ModelVersion`                         | PT1M       |
| Tool calls    | `AgentToolCalls`         | Count | Average, Maximum, Minimum, Total (Sum) | `AgentId`, `ToolName`                                          | PT1M       |

For metric definitions, see [Monitoring data reference](../reference/monitor-service).

## Analyze monitoring data

### Use Metrics Explorer in the Azure portal

Azure Monitor supports [Metrics Explorer](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/metrics-getting-started), which lets you view and analyze metrics for Azure resources.

Common analysis tasks include:

* Filtering a chart by a dimension (for example, by `AgentId`).
* Splitting a chart by a dimension (for example, by `RunStatus` or `ToolName`).
* Changing the time range and aggregation to match your investigation needs.

### Verify you're receiving metrics

If you don't see data right away, confirm that metrics are flowing before you start deeper analysis:

* Generate activity for your agent (for example, create a run and send a few messages).
* In Metrics Explorer, chart at least one metric (for example, `AgentRuns`) for your Agent Service resource.
* If you exported metrics to Log Analytics, wait a few minutes for ingestion, and then run a basic `AzureMetrics` query.

### Export metrics with diagnostic settings

If you want to query metrics in Log Analytics or export them to other systems, configure diagnostic settings for the Agent Service resource and route metrics to one or more destinations.

To configure diagnostic settings in the Azure portal:

1. In the [Azure portal](https://portal.azure.com), open the Agent Service resource.
2. Under **Monitoring**, select **Diagnostic settings**.
3. Create a diagnostic setting and choose to export metrics to your destination (for example, a Log Analytics workspace).
4. Save the diagnostic setting.

After you save the setting, it appears in the **Diagnostic settings** list for the resource. Metrics typically begin flowing to the destination within a few minutes.

For more information, see [Create diagnostic settings to collect platform logs and metrics in Azure](https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/diagnostic-settings).

### Query metrics with Log Analytics (KQL)

After you route metrics to a Log Analytics workspace, you can query them with KQL.

The following query returns a sample of metric records:

```kusto
AzureMetrics
| take 100
| project TimeGenerated, MetricName, Total, Count, Maximum, Minimum, Average, TimeGrain, UnitName, ResourceId, Tags
```

To focus on Agent Service runs:

```kusto
AzureMetrics
| where MetricName == "AgentRuns"
| take 100
| project TimeGenerated, Total, ResourceId
```

For query fundamentals, see [Kusto Query Language (KQL) overview](https://learn.microsoft.com/en-us/kusto/query/).

## Create alerts

Azure Monitor alerts notify you when conditions are met in your monitoring data. For more information, see [Alerts in Azure Monitor](https://learn.microsoft.com/en-us/azure/azure-monitor/alerts/alerts-overview).

To create a metric alert rule:

1. In the [Azure portal](https://portal.azure.com), open the Agent Service resource.
2. Select **Monitoring** > **Alerts**.
3. Select **Create** > **Alert rule**.
4. Under **Condition**, select a metric (for example, `AgentRuns`).
5. If needed, use dimensions (for example, `RunStatus` or `StatusCode`) to scope the alert.
6. Configure the action group, severity, and evaluation frequency.
7. Select **Create**.

After you create the rule, it appears in the **Alert rules** list. The rule becomes active immediately and evaluates based on the frequency you configured.

For application-layer observability, see [Monitor your generative AI applications (preview)](../../how-to/monitor-applications).

## Troubleshooting

### No data appears in Metrics Explorer

* Confirm you're viewing the correct Agent Service resource.
* Expand the time range (for example, last 24 hours).
* Generate new activity (for example, create a run) and refresh the chart.
* Confirm you have permissions to view monitoring data for the resource.

### No data appears in Log Analytics

* Confirm you created diagnostic settings for the Agent Service resource and selected the correct destination.
* Wait a few minutes for ingestion, and then rerun your query.
* Confirm you have permissions to query the Log Analytics workspace.

## Next steps

* If you're using a Foundry project, see [Monitor AI Agents with the Agent Monitoring Dashboard (preview)](../../observability/how-to/how-to-monitor-agents-dashboard).
* For end-to-end debugging, see [Trace and observe AI agents in Foundry (preview)](../../how-to/develop/trace-agents-sdk).
* For metric definitions, see [Monitoring data reference](../reference/monitor-service).

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

Use the Agent Monitoring Dashboard in Microsoft Foundry to track operational metrics and evaluation results for your agents. This dashboard helps you understand token usage, latency, success rates, and evaluation outcomes for production traffic.

This article covers two approaches: viewing metrics in the Foundry portal and setting up continuous evaluation programmatically with the Python SDK.

## Prerequisites

* A [Foundry project](../../how-to/create-projects) with at least one [agent](../../agents/overview).
* An [Application Insights resource](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview) connected to your project.
* Azure role-based access control (RBAC) access to the Application Insights resource. For log-based views, you also need access to the associated Log Analytics workspace. To verify access, open the Application Insights resource in the Azure portal, select **Access control (IAM)**, and confirm your account has an appropriate role. For log access, assign the [Log Analytics Reader role](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal#log-analytics-reader).

## Connect Application Insights

The Agent Monitoring Dashboard reads telemetry from the Application Insights resource connected to your Foundry project. If you haven't connected Application Insights yet, follow the tracing setup steps and then return to this article.

* [How to set up tracing in Microsoft Foundry](trace-agent-setup)

## View agent metrics

To view metrics for an agent in the Foundry portal:

1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.

   ![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/new-foundry.png)

2. Navigate to the **Build** page using the top navigation and select the agent you'd like to view data for.

3. Select the **Monitor** tab to view operational, evaluation, and red-teaming data for your agent.

![Screenshot of the Agent Monitoring Dashboard in Foundry showing summary cards at the top with high-level metrics and charts below displaying evaluation scores, agent run success rates, and token usage over time.](https://learn.microsoft.com/azure/ai-foundry/default/media/observability/how-to-monitor-agents-dashboard/foundry-metrics-dashboard.png)

The dashboard is designed for quick insights and deep analysis of your agent's performance. It consists of two main areas:

* Summary cards at the top for high-level metrics.

* Charts and graphs below for granular details. These visualizations reflect data for the selected time range.

## Understand the dashboard metrics

Use these definitions to interpret the dashboard:

* **Token usage**: Token counts for agent traffic in the selected time range. High token usage might indicate verbose prompts or responses that could benefit from optimization.
* **Latency**: Response time for agent runs. Latency above 10 seconds might indicate model throttling, complex tool calls, or network issues.
* **Run success rate**: The percentage of runs that complete successfully. A rate below 95% warrants investigation into failed runs.
* **Evaluation metrics**: Scores produced by evaluators that run on sampled agent outputs. Scores vary by evaluator; review individual evaluator documentation for interpretation guidance.
* **Red teaming results**: Outcomes from scheduled red team scans, if enabled. Failed scans indicate potential security risks that require remediation.

<Callout type="note">
  Monitoring data is stored in the connected Application Insights resource. Retention and billing follow your Application Insights configuration.
</Callout>

## Configure settings

Use the Monitor settings panel to configure telemetry, evaluations, and security checks for your agents. These settings control which charts the dashboard shows and which evaluations run.

![Screenshot showing the Monitor Settings panel in Foundry with options for operational metrics, continuous evaluation, scheduled evaluations, red team scans, and alerts configuration.](https://learn.microsoft.com/azure/ai-foundry/default/media/observability/how-to-monitor-agents-dashboard/monitor-settings-panel.png)

To access Monitor settings, select the gear icon on the **Monitor** tab. The following table describes each monitoring feature:

| Setting                   | Purpose                                                                            | Configuration Options                                                              |
| ------------------------- | ---------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------- |
| **Continuous evaluation** | Runs evaluations on sampled agent responses.                                       | Enable or disable Add evaluators Set the sample rate                               |
| **Scheduled evaluations** | Runs evaluations on a schedule to validate performance against benchmarks.         | Enable or disable Select an evaluation template and run Set a schedule             |
| **Red team scans**        | Runs adversarial tests to detect risks such as data leakage or prohibited actions. | Enable or disable Select an evaluation template and run Set a schedule             |
| **Alerts**                | Detects performance anomalies, evaluation failures, and security risks.            | Configure alerts for latency, token usage, evaluation scores, or red team findings |

## Set up continuous evaluation (Python SDK)

Use the Python SDK to set up continuous evaluation rules for agent responses. This section requires Python 3.9 or later.

```bash
pip install "azure-ai-projects>=2.0.0b1" python-dotenv
```

Set these environment variables with your own values:

* `AZURE_AI_PROJECT_ENDPOINT`: The Foundry project endpoint, as found on the project overview page in the Foundry portal.
* `AZURE_AI_AGENT_NAME`: The name of the agent to use for evaluation.
* `AZURE_AI_MODEL_DEPLOYMENT_NAME`: The deployment name of the model.

### Assign permissions for continuous evaluation

To enable continuous evaluation rules, assign the project managed identity the **Azure AI User** role.

1. In the Azure portal, open the resource for your Foundry project.
2. Select **Access control (IAM)**, and then select **Add**.
3. Create a role assignment for **Azure AI User**.
4. For the member, select your Foundry project's managed identity.

### Create an agent

```python
import os
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from azure.ai.projects.models import (
    PromptAgentDefinition,
)

load_dotenv()

endpoint = os.environ["AZURE_AI_PROJECT_ENDPOINT"]

with (
    DefaultAzureCredential() as credential,
    AIProjectClient(endpoint=endpoint, credential=credential) as project_client,
    project_client.get_openai_client() as openai_client,
):
    agent = project_client.agents.create_version(
        agent_name=os.environ["AZURE_AI_AGENT_NAME"],
        definition=PromptAgentDefinition(
            model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
            instructions="You are a helpful assistant that answers general questions",
        ),
    )
    print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")
```

References: [AIProjectClient](https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.aiprojectclient), [DefaultAzureCredential](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity.defaultazurecredential)

### Create a continuous evaluation rule

Define the evaluation and the rule that runs when a response completes. To learn more about supported evaluators, see [What are evaluators?](../../concepts/observability#what-are-evaluators).

```python
from azure.ai.projects.models import (
    EvaluationRule,
    ContinuousEvaluationRuleAction,
    EvaluationRuleFilter,
    EvaluationRuleEventType,
)

data_source_config = {"type": "azure_ai_source", "scenario": "responses"}
testing_criteria = [
    {"type": "azure_ai_evaluator", "name": "violence_detection", "evaluator_name": "builtin.violence"}
]
eval_object = openai_client.evals.create(
    name="Continuous Evaluation",
    data_source_config=data_source_config,  # type: ignore
    testing_criteria=testing_criteria,  # type: ignore
)
print(f"Evaluation created (id: {eval_object.id}, name: {eval_object.name})")

continuous_eval_rule = project_client.evaluation_rules.create_or_update(
    id="my-continuous-eval-rule",
    evaluation_rule=EvaluationRule(
        display_name="My Continuous Eval Rule",
        description="An eval rule that runs on agent response completions",
        action=ContinuousEvaluationRuleAction(eval_id=eval_object.id, max_hourly_runs=100),
        event_type=EvaluationRuleEventType.RESPONSE_COMPLETED,
        filter=EvaluationRuleFilter(agent_name=agent.name),
        enabled=True,
    ),
)
print(
    f"Continuous Evaluation Rule created (id: {continuous_eval_rule.id}, name: {continuous_eval_rule.display_name})"
)
```

References: [EvaluationRuleEventType](https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.models.evaluationruleeventtype), [EvaluationRule](https://learn.microsoft.com/en-us/python/api/azure-ai-projects/azure.ai.projects.models.evaluationrule)

## Verify continuous evaluation results

1. Generate agent traffic (for example, run your app or test the agent in the portal).
2. In the Foundry portal, open the agent and select **Monitor**.
3. Review evaluation-related charts for the selected time range.

If the setup is successful, the evaluation-related charts display scores for your selected time range, and the evaluation runs list shows entries with status **Completed**.

You can also list recent evaluation runs and open the report URL:

```python
eval_run_list = openai_client.evals.runs.list(
    eval_id=eval_object.id,
    order="desc",
    limit=10,
)

if len(eval_run_list.data) > 0 and eval_run_list.data[0].report_url:
    print(f"Report URL: {eval_run_list.data[0].report_url}")
```

## Full sample code

To view the full sample code, see:

* [Continuous evaluation sample](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/ai/azure-ai-projects/samples/evaluations/sample_continuous_evaluation_rule.py).
* [Scheduled evaluation and Schedule AI red teaming evaluation sample](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/ai/azure-ai-projects/samples/evaluations/sample_scheduled_evaluations.py).

## Troubleshooting

| Issue                                      | Cause                                                             | Resolution                                                                                                                                                                                                                                 |
| ------------------------------------------ | ----------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Dashboard charts are empty                 | No recent traffic, time range excludes data, or ingestion delay   | Generate new agent traffic, expand the time range, and refresh after a few minutes.                                                                                                                                                        |
| You see authorization errors               | Missing RBAC permissions on Application Insights or Log Analytics | Confirm access in **Access control (IAM)** for the connected resources. For log access, assign the [Log Analytics Reader role](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal#log-analytics-reader). |
| Continuous evaluation results don't appear | Continuous evaluation isn't enabled or rule creation failed       | Confirm that your rule is enabled and that agent traffic is flowing. If you use the Python SDK setup, confirm the project managed identity has the **Azure AI User** role.                                                                 |
| Evaluation runs are skipped                | Hourly run limit reached                                          | Increase `max_hourly_runs` in the evaluation rule configuration or wait for the next hour. The default limit is 100 runs per hour.                                                                                                         |

## Related content

* [Agent tracing overview](../concepts/trace-agent-concept)
* [Run AI Red Teaming Agent in the cloud](../../how-to/develop/run-ai-red-teaming-cloud)
* [Set up tracing in Microsoft Foundry](trace-agent-setup)
* [Tracing integrations](trace-agent-framework)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

Microsoft Foundry provides an observability platform for monitoring and tracing AI agents. It captures key details during an agent run, such as inputs, outputs, tool usage, retries, latencies, and costs. Understanding the reasoning behind your agent's executions is important for troubleshooting and debugging. However, understanding complex agents presents challenges for several reasons:

* There could be a high number of steps involved in generating a response, making it hard to keep track of all of them.
* The sequence of steps might vary based on user input.
* The inputs/outputs at each stage might be long and deserve more detailed inspection.
* Each step of an agent's runtime might also involve nesting. For example, an agent might invoke a tool, which uses another process, which then invokes another tool. If you notice strange or incorrect output from a top-level agent run, it might be difficult to determine exactly where in the execution the issue was introduced.

Trace results solve this by allowing you to view the inputs and outputs of each primitive involved in a particular agent run, displayed in the order they were invoked, making it easy to understand and debug your AI agent's behavior.

## Prerequisites

To use tracing end-to-end, you need:

* A Foundry project with tracing enabled. To set it up, see [How to set up tracing in Microsoft Foundry](../how-to/trace-agent-setup).
* Access to the Azure Application Insights resource connected to your project. For background, see [Azure Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview).

<Callout type="note">
  Tracing stores telemetry data in Azure Application Insights, which may incur costs based on data volume and retention settings. For pricing details, see [Application Insights pricing](https://learn.microsoft.com/en-us/azure/azure-monitor/cost-usage#application-insights-billing).
</Callout>

## OpenTelemetry in Foundry

OpenTelemetry (OTel) provides standardized protocols for collecting and routing telemetry data. Foundry uses OpenTelemetry semantic conventions so traces are consistent across supported tools and integrations.

## Trace key concepts

Here's a brief overview of key concepts before getting started:

| Key concepts         | Description                                                                                                                                                                                                                                                                                  |
| -------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Traces               | Traces capture the journey of a request or workflow through your application by recording events and state changes (function calls, values, system events). See [OpenTelemetry Traces](https://opentelemetry.io/docs/concepts/signals/traces/).                                              |
| Spans                | Spans are the building blocks of traces, representing single operations within a trace. Each span captures start and end times, attributes, and can be nested to show hierarchical relationships, allowing you to see the full call stack and sequence of operations.                        |
| Attributes           | Attributes are key-value pairs attached to traces and spans, providing contextual metadata such as function parameters, return values, or custom annotations. These enrich trace data making it more informative and useful for analysis.                                                    |
| Semantic conventions | OpenTelemetry defines semantic conventions to standardize names and formats for trace data attributes, making it easier to interpret and analyze across tools and platforms. To learn more, see [OpenTelemetry's Semantic Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/). |
| Trace exporters      | Trace exporters send trace data to backend systems for storage and analysis. In Foundry, traces are stored in Azure Monitor Application Insights. To learn how to enable and view traces, see [How to set up tracing in Microsoft Foundry](../how-to/trace-agent-setup).                     |

## How tracing works in Foundry

Tracing helps you answer questions like "Where did this response come from?" and "Which step introduced an error or latency spike?"

At a high level, tracing captures:

* User inputs and agent outputs.
* Tool usage, including tool calls and results.
* Timing signals such as latency.

Once tracing is enabled for your project, you can inspect traces in the Foundry portal and in Azure Monitor Application Insights. For the step-by-step setup and viewing options, see [How to set up tracing in Microsoft Foundry](../how-to/trace-agent-setup).

## Extending OpenTelemetry with multi-agent observability

Microsoft, in collaboration with Cisco Outshift, has introduced new semantic conventions for multi-agent systems, built on [OpenTelemetry](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-agent-spans/) and [W3C Trace Context](https://www.w3.org/TR/trace-context/). These conventions standardize telemetry for multi-agent workflows, enabling consistent logging of metrics for quality, performance, safety, and cost, including tool invocations and collaboration.

These enhancements are integrated into:

* Foundry
* Microsoft Agent Framework
* Semantic Kernel
* LangChain
* LangGraph
* OpenAI Agents SDK

To learn more, see [tracing integrations](../how-to/trace-agent-framework).

The following table describes the semantic conventions for multi-agent observability. Spans capture discrete operations, child spans show nested operations within a parent span, attributes provide metadata, and events mark significant occurrences during execution.

| Type       | Context/Parent Span | Name/Attribute/Event                 | Purpose                                                                                                         |
| ---------- | ------------------- | ------------------------------------ | --------------------------------------------------------------------------------------------------------------- |
| Span       | —                   | execute\_task                        | Captures task planning and event propagation, providing insights into how tasks are decomposed and distributed. |
| Child Span | invoke\_agent       | agent\_to\_agent\_interaction        | Traces communication between agents.                                                                            |
| Child Span | invoke\_agent       | agent.state.management               | Effective context, short or long term memory management.                                                        |
| Child Span | invoke\_agent       | agent\_planning                      | Logs the agent's internal planning steps.                                                                       |
| Child Span | invoke\_agent       | agent orchestration                  | Captures agent-to-agent orchestration.                                                                          |
| Attribute  | invoke\_agent       | tool\_definitions                    | Describes the tool's purpose or configuration.                                                                  |
| Attribute  | invoke\_agent       | llm\_spans                           | Records model call spans.                                                                                       |
| Attribute  | execute\_tool       | tool.call.arguments                  | Logs the arguments passed during tool invocation.                                                               |
| Attribute  | execute\_tool       | tool.call.results                    | Records the results returned by the tool.                                                                       |
| Event      | —                   | Evaluation (name, error.type, label) | Enables structured evaluation of agent performance and decision-making.                                         |

## Best practices

* **Use consistent span attributes**: Apply the same attribute names and formats across all agents and tools to simplify querying and analysis.
* **Correlate evaluation run IDs**: Link trace data with evaluation runs to analyze both quality and performance in a unified view.
* **Redact sensitive content**: Remove or mask personal data, secrets, and credentials from prompts, tool arguments, and span attributes before they reach telemetry.

## Security and privacy

Tracing can capture sensitive information (for example, user inputs, model outputs, and tool arguments and results). Use these practices to reduce risk:

* Don't store secrets, credentials, or tokens in prompts, tool arguments, or span attributes.
* Redact or minimize personal data and other sensitive content before it appears in telemetry.
* Treat trace data as production telemetry and apply the same access controls and retention policies you use for logs and metrics.

## Troubleshooting

If traces aren't appearing in the Foundry portal or Application Insights:

* Verify that your Foundry project is connected to an Application Insights resource.
* Check that your account has the required permissions to query telemetry.
* Ensure your agent code includes the necessary instrumentation. For framework-specific setup, see [Tracing integrations](../how-to/trace-agent-framework).

<Callout type="tip">
  Tracing is available in all regions where Foundry is supported. Trace data retention and sampling follow your Application Insights configuration. For details, see [Data retention and archive in Azure Monitor Logs](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/data-retention-configure).
</Callout>

## Related content

* [How to set up tracing in Microsoft Foundry](../how-to/trace-agent-setup)
* [Tracing integrations](../how-to/trace-agent-framework)
* [Monitor AI agents with the Agent Monitoring Dashboard](../how-to/how-to-monitor-agents-dashboard)
* [Observability in generative AI](../../concepts/observability)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

Use tracing (preview) to debug your AI agents and monitor their behavior in production. Tracing captures detailed telemetry—including latency, exceptions, prompt content, and retrieval operations—so you can identify and fix issues faster.

## Prerequisites

* A [Foundry project](../../how-to/create-projects).
* An [Azure Monitor Application Insights resource](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview) to store traces (create a new one or connect an existing one).
* Access to the Application Insights resource connected to your project.

## Connect Application Insights to your Foundry project

Foundry stores traces in [Azure Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview) by using [OpenTelemetry semantic conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/).

1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.

   ![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/new-foundry.png)

2. Open your Foundry project.

3. In the left navigation, select **Tracing**.

4. Create or connect an Application Insights resource:

   * To connect an existing resource, select the resource and then select **Connect**.
   * To create a new resource, select **Create new** and complete the wizard.

   A confirmation message appears when the connection succeeds.

After you connect the resource, your project is ready to use tracing.

<Callout type="important">
  Make sure you have the permissions you need to query telemetry.

  * For log-based queries, start by assigning the [Log Analytics Reader role](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal#log-analytics-reader).
  * To learn how to assign roles, see [Assign Azure roles using the Azure portal](https://learn.microsoft.com/en-us/azure/role-based-access-control/role-assignments-portal).
  * To manage access at scale, use [Microsoft Entra groups](../../concepts/rbac-foundry#use-microsoft-entra-groups-with-foundry).
</Callout>

## Instrument AI agents

Choose the approach that matches how you build and run your agent.

### Server-side traces in the Foundry portal

Start with server-side traces. Foundry logs traces for common agent and workflow scenarios without changing your code.

* Foundry automatically logs server-side traces for Prompt agents, Host agents, and workflows in the Foundry portal. Once tracing is enabled in your Foundry project, you'll have access to out-of-the-box traces for the past 90 days.
* Foundry also allows for easy [integration](trace-agent-framework) with top agent frameworks.

### Client-side traces with the Microsoft Foundry SDK (Python)

Install OpenTelemetry and the Azure SDK tracing plugin using:

```bash
pip install azure-ai-projects azure-identity opentelemetry-sdk azure-core-tracing-opentelemetry
```

<Callout type="important">
  Using a project's endpoint in your application requires configuring Microsoft Entra ID. If you don't configure Microsoft Entra ID, use the Azure Application Insights connection string.
</Callout>

After running your agent, you can begin to [view and analyze traces in Foundry portal](#view-traces-in-the-foundry-portal).

For detailed instructions and SDK-specific code examples, see [Tracing with azure-ai-projects (Python SDK)](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects#tracing) and [Telemetry samples for agents](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects/samples/agents/telemetry).

### Trace locally with AI Toolkit in VS Code

AI Toolkit lets you trace locally in VS Code using a local OTLP-compatible collector, which is ideal for development and debugging.

The toolkit supports AI frameworks such as Foundry Agents Service, OpenAI, Anthropic, and LangChain through OpenTelemetry. You can see traces instantly in VS Code without needing cloud access.

For detailed setup instructions and SDK-specific code examples, see [Tracing in AI Toolkit](https://code.visualstudio.com/docs/intelligentapps/tracing).

## View and analyze traces

### View traces in the Foundry portal

In your Foundry project, go to the **Traces** tab in your agents or workflows. You can search, filter, or sort ingested traces from the last 90 days.

Select a trace to step through each span, identify issues, and observe how your application responds. This helps you debug and pinpoint issues in your application.

### View traces in Azure Monitor

Your traces are sent to Azure Monitor Application Insights, so you can view them there.

For more information on how to send traces to Azure Monitor and create an Azure Monitor resource, see [Azure Monitor OpenTelemetry documentation](https://learn.microsoft.com/en-us/azure/azure-monitor/app/opentelemetry-enable).

### View conversation results

A **Conversation** is the persistent context of an end-to-end dialogue history between a user and an agent. In the Foundry portal, you can view **Conversation** results for your agent run out of the box along with traces on the **Traces** page.

You can search for a known Conversation ID, search by a Response ID, or search by a Trace ID that maps to this conversation. Then, select **Conversation ID** to review the conversation:

* Conversation history details
* Response information and tokens in a run
* Ordered actions, run steps, and tool calls
* Inputs and outputs between a user and an agent

![Screenshot of the Conversation details pane in Foundry showing a conversation ID with a trace timeline and run-step details.](https://learn.microsoft.com/azure/ai-foundry/default/media/observability/tracing/conversation.png)

## Verify tracing works

1. Confirm your project is connected to Application Insights. If needed, follow the steps in [Connect Application Insights to your Foundry project](#connect-application-insights-to-your-foundry-project).

2. Run your agent or workflow at least once (for example, by using the portal or your app).

3. In your Foundry project, open the **Traces** view and confirm a new trace appears.

   When tracing is working correctly, you see a list of recent traces with timestamps, durations, and status indicators. Select a trace to view its span details.

If you don't see new traces, wait a few minutes and refresh, and then see [Troubleshooting](#troubleshooting).

## Security and privacy

Tracing can capture sensitive information (for example, user inputs, model outputs, and tool arguments and results). Use these practices to reduce risk:

* Don't store secrets, credentials, or tokens in prompts, tool arguments, or span attributes.
* Redact or minimize personal data and other sensitive content before it appears in telemetry.
* Treat trace data as production telemetry and apply the same access controls and retention policies you use for logs and metrics.

For more guidance, see [Security and privacy](../concepts/trace-agent-concept#security-and-privacy).

## Data retention and cost

Foundry stores traces in the Application Insights resource connected to your project. Data retention and billing follow your Application Insights and Log Analytics configuration.

## Troubleshooting

| Issue                                                         | Cause                                                                        | Resolution                                                                                                                                                                                                                                  |
| ------------------------------------------------------------- | ---------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You don't see any traces in the Foundry portal                | Tracing isn't connected, there is no recent traffic, or ingestion is delayed | Confirm the Application Insights connection, generate new agent traffic, and refresh after a few minutes.                                                                                                                                   |
| You see authorization errors when you query or view telemetry | Missing RBAC permissions on Application Insights or Log Analytics            | Confirm access in **Access control (IAM)** for the connected resources. For log queries, assign the [Log Analytics Reader role](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal#log-analytics-reader). |
| Client-side traces don't appear                               | Instrumentation isn't installed or configured                                | Recheck your package installation and follow the SDK guidance linked in [Client-side traces with the Microsoft Foundry SDK (Python)](#client-side-traces-with-the-microsoft-foundry-sdk-python).                                            |
| Sensitive content appears in traces                           | Prompts, tool arguments, or outputs contain sensitive data                   | Redact sensitive data before it enters telemetry and follow the guidance in [Security and privacy](#security-and-privacy).                                                                                                                  |

## Related content

Now that tracing is set up, explore these resources to deepen your understanding and extend your observability capabilities:

* [Agent tracing overview](../concepts/trace-agent-concept)
* [Tracing integrations](trace-agent-framework)
* [Monitor AI agents with the Agent Monitoring Dashboard](how-to-monitor-agents-dashboard)
* [Observability in generative AI](../../concepts/observability)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

When AI agents behave unexpectedly in production, tracing gives you the visibility to quickly identify the root cause. Tracing captures detailed telemetry—including LLM calls, tool invocations, and agent decision flows—so you can debug issues, monitor latency, and understand agent behavior across requests.

Microsoft Foundry provides tracing integrations for popular agent frameworks that require minimal code changes. In this article, you learn how to:

* Configure automatic tracing for Microsoft Agent Framework and Semantic Kernel
* Set up the `langchain-azure-ai` tracer for LangChain and LangGraph
* Instrument the OpenAI Agents SDK with OpenTelemetry
* Verify that traces appear in the Foundry portal
* Troubleshoot common tracing issues

## Prerequisites

* A [Foundry project](../../how-to/create-projects) with [tracing connected](trace-agent-setup) to Application Insights.
* Contributor or higher role on the Application Insights resource for trace ingestion.
* Access to the connected Application Insights resource for viewing traces. For log-based queries, you might also need access to the associated Log Analytics workspace.
* Python 3.10 or later (required for all code samples in this article).
* The `langchain-azure-ai` package version 0.1.0 or later (required for LangChain and LangGraph samples).
* If you use LangChain or LangGraph, a Python environment with pip installed.

### Confirm you can view telemetry

To view trace data, make sure your account has access to the connected Application Insights resource.

1. In the Azure portal, open the Application Insights resource connected to your Foundry project.

2. Select **Access control (IAM)**.

3. Assign an appropriate role to your user or group.

   If you use log-based queries, start by granting the [Log Analytics Reader role](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal#log-analytics-reader).

## Security and privacy

Tracing can capture sensitive information (for example, user inputs, model outputs, and tool arguments and results).

* Enable content recording during development and debugging to see full request and response data. Disable content recording in production environments to protect sensitive data. In the samples in this article, content recording is controlled by settings like `enable_content_recording` and `OTEL_RECORD_CONTENT`.
* Don't store secrets, credentials, or tokens in prompts or tool arguments.

For more guidance, see [Security and privacy](../concepts/trace-agent-concept#security-and-privacy).

<Callout type="note">
  Trace data stored in Application Insights is subject to your workspace's data retention settings and Azure Monitor pricing. For cost management, consider adjusting sampling rates or retention periods in production. See [Azure Monitor pricing](https://azure.microsoft.com/pricing/details/monitor/) and [Configure data retention and archive](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/data-retention-configure).
</Callout>

## Configure tracing for Microsoft Agent Framework and Semantic Kernel

Microsoft Foundry has native integrations with both Microsoft Agent Framework and Semantic Kernel. Agents built with either framework automatically emit traces when tracing is enabled for your Foundry project—no additional code or packages are required.

To verify tracing is working:

1. Run your agent at least once.
2. In the Foundry portal, go to **Observability** > **Traces**.
3. Confirm a new trace appears with spans for your agent's operations.

Traces typically appear within 2–5 minutes after agent execution. For advanced configuration, see the framework-specific documentation:

* [Microsoft Agent Framework Workflows – Observability](https://learn.microsoft.com/en-us/agent-framework/user-guide/workflows/observability)
* [Semantic Kernel observability](https://learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability)

## Configure tracing for LangChain and LangGraph

<Callout type="note">
  Tracing integration for LangChain and LangGraph is currently available only in Python.
</Callout>

Use the `langchain-azure-ai` package to emit OpenTelemetry-compliant spans for LangChain and LangGraph operations. These traces appear in the **Observability** > **Traces** view in the Foundry portal.

* [OpenTelemetry semantic conventions for generative AI](https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-agent-spans/)
* [langchain-azure-ai package on PyPI](https://pypi.org/project/langchain-azure-ai/)

### Sample: LangChain v1 agent with Azure AI tracing

Use this end-to-end sample to instrument a LangChain v1 (preview) agent using the `langchain-azure-ai` tracer. This tracer implements the latest OpenTelemetry (OTel) semantic conventions, so you can view rich traces in the Foundry observability view.

#### LangChain v1: Install packages

```bash
pip install \
  langchain-azure-ai \
  langchain \
  langgraph \
  langchain-openai \
  azure-identity \
  python-dotenv \
  rich
```

#### LangChain v1: Configure environment

* `APPLICATION_INSIGHTS_CONNECTION_STRING`: Azure Monitor Application Insights connection string for tracing.
* `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI endpoint URL.
* `AZURE_OPENAI_CHAT_DEPLOYMENT`: The chat model deployment name.
* `AZURE_OPENAI_VERSION`: API version, for example `2024-08-01-preview`.
* The SDK resolves Azure credentials using `DefaultAzureCredential`, which supports environment variables, managed identity, and VS Code sign-in.

Store these values in a `.env` file for local development.

#### LangChain v1: Tracer setup

```python
from dotenv import load_dotenv
import os
from langchain_azure_ai.callbacks.tracers import AzureAIOpenTelemetryTracer

load_dotenv(override=True)

azure_tracer = AzureAIOpenTelemetryTracer(
    connection_string=os.environ.get("APPLICATION_INSIGHTS_CONNECTION_STRING"),
    enable_content_recording=True,
    name="Weather information agent",
    id="weather_info_agent_771929",
)

tracers = [azure_tracer]
```

#### LangChain v1: Model setup (Azure OpenAI)

```python
import os
import azure.identity
from langchain_openai import AzureChatOpenAI

token_provider = azure.identity.get_bearer_token_provider(
    azure.identity.DefaultAzureCredential(),
    "https://cognitiveservices.azure.com/.default",
)

model = AzureChatOpenAI(
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    azure_deployment=os.environ.get("AZURE_OPENAI_CHAT_DEPLOYMENT"),
    openai_api_version=os.environ.get("AZURE_OPENAI_VERSION"),
    azure_ad_token_provider=token_provider,
)
```

#### LangChain v1: Define tools and prompt

```python
from dataclasses import dataclass
from langchain_core.tools import tool

system_prompt = """You are an expert weather forecaster, who speaks in puns.

You have access to two tools:

- get_weather_for_location: use this to get the weather for a specific location
- get_user_location: use this to get the user's location

If a user asks you for the weather, make sure you know the location.
If you can tell from the question that they mean wherever they are,
use the get_user_location tool to find their location."""

# Mock user locations keyed by user id (string)
USER_LOCATION = {
    "1": "Florida",
    "2": "SF",
}


@dataclass
class UserContext:
    user_id: str


@tool
def get_weather(city: str) -> str:
    """Get weather for a given city."""
    return f"It's always sunny in {city}!"
```

#### LangChain v1: Use runtime context and define a user-info tool

```python
from langgraph.runtime import get_runtime
from langchain_core.runnables import RunnableConfig

@tool
def get_user_info(config: RunnableConfig) -> str:
    """Retrieve user information based on user ID."""
    runtime = get_runtime(UserContext)
    user_id = runtime.context.user_id
    return USER_LOCATION[user_id]
```

#### LangChain v1: Create the agent

```python
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver
from dataclasses import dataclass


@dataclass
class WeatherResponse:
    conditions: str
    punny_response: str


checkpointer = InMemorySaver()

agent = create_agent(
    model=model,
    prompt=system_prompt,
    tools=[get_user_info, get_weather],
    response_format=WeatherResponse,
    checkpointer=checkpointer,
)
```

#### LangChain v1: Run the agent with tracing

```python
from rich import print

def main():
    config = {"configurable": {"thread_id": "1"}, "callbacks": [azure_tracer]}
    context = UserContext(user_id="1")

    r1 = agent.invoke(
        {"messages": [{"role": "user", "content": "what is the weather outside?"}]},
        config=config,
        context=context,
    )
    print(r1.get("structured_response"))

    r2 = agent.invoke(
        {"messages": [{"role": "user", "content": "Thanks"}]},
        config=config,
        context=context,
    )
    print(r2.get("structured_response"))


if __name__ == "__main__":
    main()
```

With `langchain-azure-ai` enabled, all LangChain v1 operations (LLM calls, tool invocations, agent steps) emit OpenTelemetry spans using the latest semantic conventions. These traces appear in the **Observability** > **Traces** view in the Foundry portal and are linked to your Application Insights resource.

<Callout type="tip">
  After running the agent, wait a few minutes for traces to appear. If you don't see traces, verify your Application Insights connection string is correct and check the [Troubleshoot common issues](#troubleshoot-common-issues) section.
</Callout>

#### Verify your LangChain v1 traces

After running the agent:

1. Wait 2–5 minutes for traces to propagate.
2. In the Foundry portal, go to **Observability** > **Traces**.
3. Look for a trace with the name you specified (for example, "Weather information agent").
4. Expand the trace to see spans for LLM calls, tool invocations, and agent steps.

If you don't see traces, check the [Troubleshoot common issues](#troubleshoot-common-issues) section.

### Sample: LangGraph agent with Azure AI tracing

This sample shows a simple LangGraph agent instrumented with `langchain-azure-ai` to emit OpenTelemetry-compliant traces for graph steps, tool calls, and model invocations.

#### LangGraph: Install packages

```bash
pip install \
  langchain-azure-ai \
  langgraph>=1.0.0 \
  langchain>=1.0.0 \
  langchain-openai \
  azure-identity \
  python-dotenv
```

#### LangGraph: Configure environment

* `APPLICATION_INSIGHTS_CONNECTION_STRING`: Azure Monitor Application Insights connection string for tracing.
* `AZURE_OPENAI_ENDPOINT`: Your Azure OpenAI endpoint URL.
* `AZURE_OPENAI_CHAT_DEPLOYMENT`: The chat model deployment name.
* `AZURE_OPENAI_VERSION`: API version, for example `2024-08-01-preview`.

Store these values in a `.env` file for local development.

#### LangGraph tracer setup

```python
import os
from dotenv import load_dotenv
from langchain_azure_ai.callbacks.tracers import AzureAIOpenTelemetryTracer

load_dotenv(override=True)

azure_tracer = AzureAIOpenTelemetryTracer(
    connection_string=os.environ.get("APPLICATION_INSIGHTS_CONNECTION_STRING"),
    enable_content_recording=os.getenv("OTEL_RECORD_CONTENT", "true").lower() == "true",
    name="Music Player Agent",
)
```

#### LangGraph: Tools

```python
from langchain_core.tools import tool

@tool
def play_song_on_spotify(song: str):
    """Play a song on Spotify"""
    # Integrate with Spotify API here.
    return f"Successfully played {song} on Spotify!"


@tool
def play_song_on_apple(song: str):
    """Play a song on Apple Music"""
    # Integrate with Apple Music API here.
    return f"Successfully played {song} on Apple Music!"


tools = [play_song_on_apple, play_song_on_spotify]
```

#### LangGraph: Model setup (Azure OpenAI)

```python
import os
import azure.identity
from langchain_openai import AzureChatOpenAI

token_provider = azure.identity.get_bearer_token_provider(
    azure.identity.DefaultAzureCredential(),
    "https://cognitiveservices.azure.com/.default",
)

model = AzureChatOpenAI(
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    azure_deployment=os.environ.get("AZURE_OPENAI_CHAT_DEPLOYMENT"),
    openai_api_version=os.environ.get("AZURE_OPENAI_VERSION"),
    azure_ad_token_provider=token_provider,
).bind_tools(tools, parallel_tool_calls=False)
```

#### Build the LangGraph workflow

```python
from langgraph.graph import END, START, MessagesState, StateGraph
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver

tool_node = ToolNode(tools)

def should_continue(state: MessagesState):
    messages = state["messages"]
    last_message = messages[-1]
    return "continue" if getattr(last_message, "tool_calls", None) else "end"


def call_model(state: MessagesState):
    messages = state["messages"]
    response = model.invoke(messages)
    return {"messages": [response]}


workflow = StateGraph(MessagesState)
workflow.add_node("agent", call_model)
workflow.add_node("action", tool_node)

workflow.add_edge(START, "agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "action",
        "end": END,
    },
)
workflow.add_edge("action", "agent")

memory = MemorySaver()
app = workflow.compile(checkpointer=memory)
```

#### LangGraph: Run with tracing

```python
from langchain_core.messages import HumanMessage

config = {"configurable": {"thread_id": "1"}, "callbacks": [azure_tracer]}
input_message = HumanMessage(content="Can you play Taylor Swift's most popular song?")

for event in app.stream({"messages": [input_message]}, config, stream_mode="values"):
    event["messages"][-1].pretty_print()
```

With `langchain-azure-ai` enabled, your LangGraph execution emits OpenTelemetry-compliant spans for model calls, tool invocations, and graph transitions. These traces flow to Application Insights and appear in the **Observability** > **Traces** view in the Foundry portal.

<Callout type="tip">
  Each graph node and edge transition creates a separate span, making it easy to visualize the agent's decision flow.
</Callout>

#### Verify your LangGraph traces

After running the agent:

1. Wait 2–5 minutes for traces to propagate.
2. In the Foundry portal, go to **Observability** > **Traces**.
3. Look for a trace with the name you specified (for example, "Music Player Agent").
4. Expand the trace to see spans for graph nodes, tool invocations, and model calls.

If you don't see traces, check the [Troubleshoot common issues](#troubleshoot-common-issues) section.

### Sample: LangChain 0.3 setup with Azure AI tracing

This minimal setup shows how to enable Azure AI tracing in a LangChain 0.3 application using the `langchain-azure-ai` tracer and `AzureChatOpenAI`.

#### LangChain 0.3: Install packages

```bash
pip install \
  "langchain>=0.3,<0.4" \
  langchain-openai \
  langchain-azure-ai \
  python-dotenv
```

#### LangChain 0.3: Configure environment

* `APPLICATION_INSIGHTS_CONNECTION_STRING`: Application Insights connection string for tracing. To find this value, open your Application Insights resource in the Azure portal, select **Overview**, and copy the **Connection String**.
* `AZURE_OPENAI_ENDPOINT`: Azure OpenAI endpoint URL.
* `AZURE_OPENAI_CHAT_DEPLOYMENT`: Chat model deployment name.
* `AZURE_OPENAI_VERSION`: API version, for example `2024-08-01-preview`.
* `AZURE_OPENAI_API_KEY`: Azure OpenAI API key.

<Callout type="note">
  This sample uses API key authentication for simplicity. For production workloads, use `DefaultAzureCredential` with `get_bearer_token_provider` as shown in the LangChain v1 and LangGraph samples.
</Callout>

#### LangChain 0.3: Tracer and model setup

```python
import os
from dotenv import load_dotenv
from langchain_azure_ai.callbacks.tracers import AzureAIOpenTelemetryTracer
from langchain_openai import AzureChatOpenAI

load_dotenv(override=True)

# Tracer: emits spans conforming to updated OTel spec
azure_tracer = AzureAIOpenTelemetryTracer(
    connection_string=os.environ.get("APPLICATION_INSIGHTS_CONNECTION_STRING"),
    enable_content_recording=True,
    name="Trip Planner Orchestrator",
    id="trip_planner_orchestrator_v3",
)
tracers = [azure_tracer]

# Model: Azure OpenAI with callbacks for tracing
llm = AzureChatOpenAI(
    azure_deployment=os.environ.get("AZURE_OPENAI_CHAT_DEPLOYMENT"),
    api_key=os.environ.get("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    api_version=os.environ.get("AZURE_OPENAI_VERSION"),
    temperature=0.2,
    callbacks=tracers,
)
```

Attach `callbacks=[azure_tracer]` to your chains, tools, or agents to ensure LangChain 0.3 operations are traced. After you run your chain or agent, traces appear in the **Observability** > **Traces** view in the Foundry portal within 2-5 minutes.

## Configure tracing for OpenAI Agents SDK

The OpenAI Agents SDK supports OpenTelemetry instrumentation. Use the following snippet to configure tracing and export spans to Azure Monitor. If `APPLICATION_INSIGHTS_CONNECTION_STRING` isn't set, the exporter falls back to the console for local debugging.

Before you run the sample, install the required packages:

```bash
pip install opentelemetry-sdk opentelemetry-instrumentation-openai-agents azure-monitor-opentelemetry-exporter
```

```python
import os
from opentelemetry import trace
from opentelemetry.instrumentation.openai_agents import OpenAIAgentsInstrumentor
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter

# Configure tracer provider + exporter
resource = Resource.create({
    "service.name": os.getenv("OTEL_SERVICE_NAME", "openai-agents-app"),
})
provider = TracerProvider(resource=resource)

conn = os.getenv("APPLICATION_INSIGHTS_CONNECTION_STRING")
if conn:
    from azure.monitor.opentelemetry.exporter import AzureMonitorTraceExporter
    provider.add_span_processor(
        BatchSpanProcessor(AzureMonitorTraceExporter.from_connection_string(conn))
    )
else:
    provider.add_span_processor(BatchSpanProcessor(ConsoleSpanExporter()))

trace.set_tracer_provider(provider)

# Instrument the OpenAI Agents SDK
OpenAIAgentsInstrumentor().instrument(tracer_provider=trace.get_tracer_provider())

# Example: create a session span around your agent run
tracer = trace.get_tracer(__name__)
with tracer.start_as_current_span("agent_session[openai.agents]"):
    # ... run your agent here
    pass
```

## Verify traces in the Foundry portal

1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/new-foundry.png)
2. Confirm tracing is connected for your project. If needed, follow [Set up tracing in Microsoft Foundry](trace-agent-setup).
3. Run your agent at least once.
4. In the Foundry portal, go to **Observability** > **Traces**.
5. Confirm a new trace appears with spans for your agent's operations.

Traces typically appear within 2–5 minutes after agent execution. If traces still don't appear after this time, see [Troubleshoot common issues](#troubleshoot-common-issues).

## Troubleshoot common issues

| Issue                                                 | Cause                                                                                       | Resolution                                                                                                                                                                                                                                  |
| ----------------------------------------------------- | ------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You don't see traces in Foundry                       | Tracing isn't connected, there is no recent traffic, or ingestion is delayed                | Confirm the Application Insights connection, generate new traffic, and refresh after 2–5 minutes.                                                                                                                                           |
| You don't see LangChain or LangGraph spans            | Tracing callbacks aren't attached to the run                                                | Confirm you pass the tracer in `callbacks` (for example, `config = {"callbacks": [azure_tracer]}`) for the run you want to trace.                                                                                                           |
| LangChain spans appear but tool calls are missing     | Tools aren't bound to the model or tool node isn't configured                               | Verify tools are passed to `bind_tools()` on the model and that tool nodes are added to your graph.                                                                                                                                         |
| Traces appear but are incomplete or missing spans     | Content recording is disabled, or some operations aren't instrumented                       | Enable `enable_content_recording=True` for full telemetry. For custom operations, add manual spans using the OpenTelemetry SDK.                                                                                                             |
| You see authorization errors when you query telemetry | Missing RBAC permissions on Application Insights or Log Analytics                           | Confirm access in **Access control (IAM)** for the connected resources. For log queries, assign the [Log Analytics Reader role](https://learn.microsoft.com/en-us/azure/azure-monitor/logs/manage-access?tabs=portal#log-analytics-reader). |
| Sensitive content appears in traces                   | Content recording is enabled and prompts, tool arguments, or outputs include sensitive data | Disable content recording in production and redact sensitive data before it enters telemetry.                                                                                                                                               |

## Next steps

* Learn core concepts and architecture in the [Agent tracing overview](../concepts/trace-agent-concept).
* If you haven't enabled tracing yet, see [Set up tracing in Microsoft Foundry](trace-agent-setup).
* Visualize agent health and performance metrics with the [Agent Monitoring Dashboard](how-to-monitor-agents-dashboard).
* Explore the broader observability capabilities in [Observability in generative AI](../../concepts/observability).

Foundry Tools is the place to discover and manage tools you use with agents and workflows in Microsoft Foundry.

<Callout type="note">
  This feature is currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

You can use Foundry Tools to:

* Discover tools such as Model Context Protocol (MCP) servers and built-in tools.
* Configure tools once, then add them to agents or workflows.
* Filter, search, and sort tools.

## Prerequisites

To use Foundry Tools, you need:

* Access to a Foundry project in the Foundry portal.
* Permission to view and manage tools in that project.

## Where to find Foundry Tools

In the Foundry portal, go to your project and then select **Build** > **Tools**.

## Key concepts

Use these definitions to keep the terminology consistent:

| Term                 | Meaning                                                                                                                                            |
| -------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| Foundry Tools        | The portal experience where you discover, configure, and manage tools for agents and workflows.                                                    |
| Tool catalog         | The browsable list of available tools (public and organizational).                                                                                 |
| Private tool catalog | An organization-scoped catalog for tools that only users in your organization can discover and configure.                                          |
| MCP server           | A server that exposes tools using the Model Context Protocol (MCP).                                                                                |
| Remote MCP server    | An MCP server hosted by the publisher. You configure it by providing the required settings (for example, an endpoint and authentication details).  |
| Local MCP server     | An MCP server you host yourself, then connect to Foundry by providing its remote endpoint.                                                         |
| Custom tool          | A tool you add by providing your own endpoint or specification (for example, an MCP endpoint, an OpenAPI spec, or Agent-to-Agent (A2A) endpoints). |

<Callout type="note">
  If you're interested in bringing your official, remote MCP server(s) to all Foundry customers, fill out this [form](https://forms.office.com/r/EEvMNceMRU).
</Callout>

## Considerations for using non-Microsoft services and servers

Your use of connected non-Microsoft services and servers ("non-Microsoft services") is subject to the terms between you and the service provider. Non-Microsoft services are non-Microsoft products under your agreement governing use of Microsoft online services. When you connect to non-Microsoft services, some of your data (such as prompt content) is sent to the non-Microsoft service, or your application might receive data from the non-Microsoft service. You're responsible for your use of non-Microsoft services and data, along with any charges associated with that use.

Third parties (not Microsoft) create the non-Microsoft services, including remote MCP servers, that you choose to connect. Microsoft doesn't test or verify these servers. Microsoft has no responsibility to you or others in relation to your use of any non-Microsoft services.

Carefully review and track the MCP servers you add to Foundry Agent Service. Rely on servers hosted by trusted service providers themselves rather than proxies.

The MCP tool can pass custom headers that a remote MCP server might require for authentication. Treat any credentials as secrets:

* Only provide the minimum required headers.
* Don't include credentials in prompts.
* If you log requests for auditing, avoid logging secrets or sensitive prompt content.
* Review the provider's data handling practices, including retention and data location.

## Foundry Tools and private tools catalog

Foundry provides both Foundry Tools and private tool catalogs.

Foundry Tools includes a curated list of tools available for building agents. If you need tools that are only visible within your organization, create a [private tool catalog](../how-to/private-tool-catalog).

## Find the right tools in Foundry Tools

### Tool types

Foundry Tools includes three types of tool catalog entries:

**Remote MCP server**: The MCP server publisher has already hosted the server and provided a static or dynamic MCP server endpoint. Foundry developers need to follow the configuration guidance to provide the appropriate information to finish the setup.

**Local MCP server**: The publisher doesn't host the server. You host it, then connect it to Foundry by providing its endpoint. To build and register your own server, see [Build and register a Model Context Protocol (MCP) server](../../mcp/build-your-own-mcp-server). To connect an MCP endpoint to an agent, see [Connect to Model Context Protocol servers](../how-to/tools/model-context-protocol).

**Custom**: These MCP servers are converted from Azure Logic App Connectors. Foundry developers need additional [configuration](https://aka.ms/FoundryCustomTool) to convert to remote MCP servers.

### Filter and search

Foundry Tools provides the following filters to help you find the right tools for your agents:

| Filter                   | Description                                                                                                                                                                                     |
| ------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Publisher                | Microsoft or non-Microsoft publisher                                                                                                                                                            |
| Category                 | Categories such as databases, analytics, web, and more                                                                                                                                          |
| Registry                 | **Public**: Public remote and local MCP servers in the catalog **Logic Apps connectors**: Azure Logic Apps connectors that you convert to remote MCP servers for use in a private tool catalog. |
| Supported authentication | Authentication method an MCP server supports. For more information, see [Authentication methods](https://aka.ms/FoundryMCPAuth).                                                                |

![Screenshot of a tool details page in the Foundry portal showing configuration and setup information.](https://learn.microsoft.com/azure/ai-foundry/agents/default/agents/media/tool-catalog/tool-example.png)

When you select a tool, Foundry Tools shows the setup details you need to configure it.

## Availability and limitations

Tool availability can vary by model and region.

For the latest model and region support details across tools, see [Best practices for using tools in Microsoft Foundry Agent Service](tool-best-practice).

## Manage tools you've configured

In your tools list, you can find the tools you've configured, along with details such as endpoints and authentication settings. You can also add tools to agents and workflows.

Before you delete a tool, check which agents or workflows use it. Deleting a tool can break runs that depend on it.

To explore tools while you build, use the Agents playground. For more information, see [Microsoft Foundry Playgrounds](../../concepts/concept-playgrounds).

Foundry Tools contains three sections:

* **Configured**: Configured tools are ready to use because you've completed their setup (authentication and required settings). Built-in tools include:

  | Tool                                                                         | Description                                                                                                                                                 |
  | ---------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | [Azure AI Search](../how-to/tools/ai-search)                                 | Use an existing Azure AI Search index to ground agents with data in the index, and chat with your data.                                                     |
  | [Browser Automation (preview)](../how-to/tools/browser-automation)           | Perform real-world browser tasks through natural language prompts.                                                                                          |
  | [Code Interpreter](../how-to/tools/code-interpreter)                         | Enable agents to write and run Python code in a sandboxed execution environment.                                                                            |
  | [Custom Code Interpreter (preview)](../how-to/tools/custom-code-interpreter) | Use a custom code interpreter MCP server to customize resources, available Python packages, and the Container Apps environment the agent uses.              |
  | [Computer Use (preview)](../how-to/tools/computer-use)                       | Specialized tool that uses a model that can perform tasks by interacting with computer systems and applications through their user interfaces.              |
  | [File Search](../how-to/tools/file-search)                                   | Augment agents with knowledge from outside its model, such as proprietary product information or documents provided by your users.                          |
  | [Grounding with Bing tools](../how-to/tools/bing-tools)                      | Enable your agent to use Grounding with Bing Search to access and return information from the internet.                                                     |
  | [Image Generation (preview)](../how-to/tools/image-generation)               | Enable image generation as part of conversations and multi-step workflows.                                                                                  |
  | [Microsoft Fabric (preview)](../how-to/tools/fabric)                         | Integrate your agent with the [Microsoft Fabric data agent](https://go.microsoft.com/fwlink/?linkid=2312815) to unlock powerful data analysis capabilities. |
  | [SharePoint (preview)](../how-to/tools/sharepoint)                           | Integrate your agents with Microsoft SharePoint to chat with your private documents securely.                                                               |
  | [Web Search (preview)](../how-to/tools/web-search)                           | Enable models to retrieve and ground responses with real-time information from the public web before generating output.                                     |

* **Catalog**: Available from the public or organizational Foundry Tool Catalog, including remote and local MCP servers and Azure Logic Apps connectors, which may require setup before use.

* **Custom**: These allow you to bring your own APIs using remote MCP server endpoints, A2A endpoints, OpenAPI 3.0 specs, or functions.

  | Tool                                                                       | Description                                                                                     |
  | -------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
  | [Model Context Protocol (preview)](../how-to/tools/model-context-protocol) | Give the agent access to tools hosted on an existing MCP endpoint.                              |
  | [OpenAPI 3.0 specified tool](../how-to/tools/openapi)                      | Connect your Foundry agents to external APIs using functions with an OpenAPI 3.0 specification. |
  | [Agent-to-Agent tool (preview)](../how-to/tools/agent-to-agent)            | Connect your Foundry agents to other agents through A2A-compatible endpoints.                   |

## Troubleshooting

Use these checks to resolve common issues:

* **You can't find the tool catalog**: Confirm you're in the correct project, then go to **Build** > **Tools**.
* **A tool is visible but you can't configure it**: Review the tool's required authentication and configuration inputs, and verify you have access to any dependent services.
* **Your agent doesn't call a tool**: Use the validation guidance in [Best practices for using tools in Microsoft Foundry Agent Service](tool-best-practice).

## Related content

* [Create a private tool catalog](../how-to/private-tool-catalog)
* [Connect to Model Context Protocol servers](../how-to/tools/model-context-protocol)
* [Build and register a Model Context Protocol (MCP) server](../../mcp/build-your-own-mcp-server)
* [Foundry MCP Server best practices and security guidance](../../mcp/security-best-practices)
* [Get started with Foundry MCP Server (preview) using Visual Studio Code](../../mcp/get-started)
* [Bring your remote, official MCP server to all Foundry customers](https://forms.office.com/r/EEvMNceMRU)

<Callout type="note">
  This feature is currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

Create a private tool catalog so developers in your organization can discover, configure, and use MCP server tools through Foundry Tools. A private tool catalog uses [Azure API Center](https://learn.microsoft.com/en-us/azure/api-center/register-discover-mcp-server) to register organization-scoped tools that only your developers can access.

## Prerequisites

* A Foundry project. For setup guidance, see [Create projects in Microsoft Foundry](../../how-to/create-projects).

* Permissions to discover and configure tools in your Foundry project. For more information, see [Role-based access control in Microsoft Foundry](../../concepts/rbac-foundry).

* An [Azure API Center](https://learn.microsoft.com/en-us/azure/api-center/set-up-api-center).

  <Callout type="note">
    The API Center name is the name that developers use to find the catalog in Foundry Tools. Use a descriptive name.
  </Callout>

* One or more remote MCP servers that you want to share with your organization. Register them with API Center by following [Configure environments and deployments in Azure API Center](https://learn.microsoft.com/en-us/azure/api-center/tutorials/configure-environments-deployments).

## Plan administrator and developer access

Before you create the catalog, decide who manages it and who consumes it.

| Goal                                    | Who            | Where                   | What to do                                                                                               |
| --------------------------------------- | -------------- | ----------------------- | -------------------------------------------------------------------------------------------------------- |
| Create and manage the tool catalog      | Catalog admins | Azure API Center        | Create the API Center resource, register MCP servers, and (optionally) configure authorization settings. |
| Discover tools from the private catalog | Developers     | Azure API Center (RBAC) | Assign access so developers can view the registered MCP servers.                                         |
| Configure and use tools                 | Developers     | Foundry project         | Confirm developers can access the Foundry project and can configure tools in Foundry Tools.              |

## Configure MCP server authentication

If your remote MCP server requires authentication, configure the authentication settings in Azure API Center. This step is optional if your MCP server doesn't require authentication.

1. In the [Azure portal](https://portal.azure.com), go to your API Center resource.

2. Select **Governance** > **Authorization**.

   ![Screenshot that shows the Governance menu expanded with the Authorization option selected in Azure API Center.](https://learn.microsoft.com/azure/ai-foundry/agents/default/agents/media/tool-catalog/api-center-resource.png)

3. Select **Add configuration**.

4. Under **Security scheme**, choose the scheme required by your MCP server (for example, **API Key**, **OAuth**, or **HTTP** bearer token), then provide the required values.

   <Callout type="important">
     Treat any credentials as secrets. Don't paste secrets into prompts or source control. For guidance on authentication approaches in Agent Service (including shared and per-user authentication), see [MCP server authentication](mcp-authentication).
   </Callout>

5. Select the MCP server, then select **Details** > **Versions** > **Manage access (preview)**.

   ![Screenshot that shows the Versions page with the Manage access option for configuring authorization in Azure API Center.](https://learn.microsoft.com/azure/ai-foundry/agents/default/agents/media/tool-catalog/api-center-versions.png)

6. Select the authorization configuration you created.

After you complete these steps, the MCP server is configured to use the selected authentication scheme when developers invoke it from Foundry Tools.

## Grant developer access to the catalog

Assign Azure RBAC permissions so developers can discover MCP servers from your private tool catalog in Foundry Tools.

1. Decide whether to grant access to a security group or to individual users.
2. Assign at least the [Azure API Center Data Reader](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/integration#azure-api-center-data-reader) role (or an equivalent custom role) to those users.

<Callout type="note">
  Role assignments can take up to 24 hours to propagate. If developers don't see the catalog immediately, wait and try again.
</Callout>

## Verify catalog discovery in Foundry Tools

After you grant access, confirm that developers can find and use the catalog in the Foundry portal.

1. In the Foundry portal, open the project that your developers use.
2. Go to **Build** > **Tools**.
3. Use search and filters to find your private tool catalog by the API Center name.
4. Select a tool from the catalog and review its setup requirements.

If the catalog appears and displays your registered MCP servers, the configuration is complete. To add an MCP server tool to an agent, see [Connect to Model Context Protocol servers](tools/model-context-protocol).

## Troubleshoot private tool catalog issues

If you encounter problems setting up or using your private tool catalog, use the following table to identify and resolve common issues.

| Issue                                                     | Cause                                                                                     | Resolution                                                                                                                                                                   |
| --------------------------------------------------------- | ----------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| The private tool catalog doesn't appear in Foundry Tools. | You don't have access to the API Center resource, or you're in the wrong Foundry project. | Confirm you have the Azure API Center Data Reader role assignment. Then confirm you're in the expected Foundry project and go to **Build** > **Tools**.                      |
| The catalog appears, but you can't configure a tool.      | The tool requires authentication or configuration values you don't have.                  | Review the tool's setup requirements, then ask a catalog admin for the required access. For MCP authentication options, see [MCP server authentication](mcp-authentication). |
| Tool calls fail after configuration.                      | Authentication is incorrect, expired, or not supported by the MCP server.                 | Re-check the authentication method required by the MCP server, and validate the credential format. For guidance, see [MCP server authentication](mcp-authentication).        |
| The catalog doesn't appear after role assignment.         | Azure RBAC role assignments can take up to 24 hours to propagate.                         | Wait up to 24 hours and try again. If the issue persists, verify the role assignment in the Azure portal under **Access control (IAM)**.                                     |
| MCP server shows as unavailable.                          | The MCP server URL changed, or the server is offline.                                     | Verify the MCP server endpoint is accessible. Update the server registration in API Center if the URL changed.                                                               |
| OAuth authentication prompts repeatedly.                  | Token expiration or consent not granted.                                                  | Ensure users grant consent when prompted. For long-running sessions, tokens might expire. Re-authenticate through the tool configuration.                                    |

## Related content

[Discover and manage tools in the Foundry tool catalog](../concepts/tool-catalog)

[Connect to Model Context Protocol servers](tools/model-context-protocol)

When you build agents in Microsoft Foundry Agent Service, tools extend what your agent can do—retrieving information, calling APIs, and connecting to external services. This article helps you configure tools effectively, control when the model calls them, and keep your data secure.

<Callout type="tip">
  In your agent instructions, describe what each tool is for and when to use it. For example:

  `When you need information from my indexed documents, use File Search. When you need to call an API, use the OpenAPI tool. When a tool call fails or returns no results, explain what happened and ask a follow-up question.`
</Callout>

## Prerequisites

* Access to a Foundry project in the Foundry portal with the **Azure AI Developer** role or equivalent permissions.
* A model deployed in the same project.
* Any required connections configured for the tools you plan to use (for example, Azure AI Search, SharePoint, or Bing grounding).

## Configure and validate tool usage

* Configure tools and connections in the Foundry tool catalog. See [Discover and manage tools in the Foundry tool catalog (preview)](tool-catalog).
* Review run traces to confirm when your agent calls tools and to inspect tool inputs and outputs. For end-to-end tracing setup, see [Trace your application](../../how-to/develop/trace-application).

## Improve tool-calling reliability

### Control tool calling with `tool_choice`

Use `tool_choice` for the most deterministic control over tool calling.

* `auto`: The model decides whether to call tools.
* `required`: The model must call one or more tools.
* `none`: The model doesn't call tools.

For details, see `tool_choice` in [Foundry project REST (preview)](../../reference/foundry-project-rest-preview).

### Write effective tool instructions

* Keep instructions specific and consistent with your tool setup.
* Tell the model what each tool is for.
* If you have multiple tools that overlap, add a decision rule (for example, “Use File Search before Web Search for internal content.”).

## Secure tool usage

Tools send and receive data outside the model. Reduce security and privacy risks with these practices:

* Treat tool outputs as untrusted input and validate critical values before acting on them.
* Send only the information required to complete the task.
* Don’t include keys, tokens, or other credentials in prompts.
* Avoid logging secrets in traces or application logs.
* If you connect to non-Microsoft services (for example, third-party MCP servers), review the considerations in [Discover and manage tools in the Foundry tool catalog (preview)](tool-catalog).
* If you need centralized routing and policy enforcement for MCP tools, see [Tools governance with AI Gateway (preview)](../how-to/tools/governance).

## Tool support by region and model

Region and model determine which tools are available to your agent.

<Callout type="note">
  In the tables below: **Yes** means fully supported, **No** means not supported, and **Limited** means partial support that varies by tool configuration. Check individual tool documentation for details.
</Callout>

The following table shows tool availability by [region](../../openai/how-to/responses#region-availability).

<Callout type="note">
  This region availability table only accounts for service availability. You need to make sure the model you want to use is also available in the same region.
</Callout>

| Region Name        | Agent2Agent | Azure AI Search | Browser Automation | Code Interpreter | Computer Use | Fabric Data Agent | File Search | Function | Grounding with Bing Custom Search | Grounding with Bing Search | Image Generation | MCP | OpenAPI | SharePoint | Web Search |
| ------------------ | ----------- | --------------- | ------------------ | ---------------- | ------------ | ----------------- | ----------- | -------- | --------------------------------- | -------------------------- | ---------------- | --- | ------- | ---------- | ---------- |
| australiaeast      | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| brazilsouth        | yes         | yes             | yes                | yes              | no           | yes               | yes         | no       | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| canadaeast         | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| eastus             | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| eastus2            | yes         | yes             | yes                | yes              | yes          | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| francecentral      | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| germanywestcentral | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| italynorth         | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| japaneast          | yes         | yes             | yes                | no               | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| koreacentral       | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| northcentralus     | yes         | yes             | yes                | yes              | no           | yes               | yes         | no       | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| norwayeast         | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| polandcentral      | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| southafricanorth   | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| southcentralus     | yes         | yes             | yes                | no               | no           | yes               | yes         | no       | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| southeastasia      | yes         | yes             | yes                | no               | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| southindia         | yes         | yes             | yes                | yes              | yes          | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| spaincentral       | yes         | yes             | yes                | no               | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| swedencentral      | yes         | yes             | yes                | yes              | yes          | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| switzerlandnorth   | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| uaenorth           | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| uksouth            | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| westus             | yes         | yes             | yes                | yes              | no           | yes               | yes         | no       | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |
| westus3            | yes         | yes             | yes                | yes              | no           | yes               | yes         | yes      | yes                               | yes                        | yes              | yes | yes     | yes        | yes        |

Tools are supported by the following models.

<Callout type="note">
  For the image generation tool, you need both the `gpt-image-1` model and a large language model (LLM) as the orchestrator in the same Microsoft Foundry project.
</Callout>

| Model                           | agent2agent | Azure AI Search | Browser Automation | Code Interpreter | Computer Use | Fabric Data Agent | File Search | Function | Grounding Bing Custom | Grounding Bing Search | Image Generation | MCP     | OpenAPI | SharePoint | Web Search |
| ------------------------------- | ----------- | --------------- | ------------------ | ---------------- | ------------ | ----------------- | ----------- | -------- | --------------------- | --------------------- | ---------------- | ------- | ------- | ---------- | ---------- |
| gpt-5                           | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| gpt-5-mini                      | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| gpt-5-nano                      | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| gpt-5-chat                      | No          | No              | No                 | No               | No           | No                | Yes         | No       | No                    | No                    | No               | No      | No      | No         | No         |
| gpt-5-pro                       | Yes         | Yes             | Yes                | No               | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| o4-mini                         | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| o3                              | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| o3-mini                         | Yes         | Yes             | Yes                | No               | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | No               | Yes     | Yes     | Yes        | Yes        |
| o1                              | Yes         | Yes             | Yes                | No               | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | No               | Yes     | Yes     | Yes        | Yes        |
| computer-use-preview            | No          | No              | No                 | No               | Yes          | No                | No          | No       | No                    | No                    | No               | No      | No      | No         | No         |
| gpt-4.1                         | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| gpt-4.1-mini                    | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| gpt-4.1-nano                    | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| gpt-4o                          | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| gpt-4o-mini                     | Yes         | Yes             | Yes                | Yes              | No           | Yes               | Yes         | Yes      | Yes                   | Yes                   | Yes              | Yes     | Yes     | Yes        | Yes        |
| gpt-image-1                     | No          | No              | No                 | No               | No           | No                | No          | No       | No                    | No                    | Yes              | No      | No      | No         | No         |
| DeepSeek-V3.0324                | No          | Limited         | No                 | Yes              | No           | Limited           | Yes         | Yes      | Limited               | Limited               | No               | Limited | No      | Limited    | No         |
| DeepSeek-V3.1                   | No          | Limited         | No                 | Yes              | No           | Limited           | No          | No       | Limited               | Limited               | No               | Limited | No      | Limited    | No         |
| Llama-3.3-70B-Instruct          | No          | No              | No                 | Yes              | No           | No                | Yes         | No       | No                    | No                    | No               | No      | No      | No         | No         |
| Llama-4-Maverick-178-128E-Instr | No          | Limited         | No                 | No               | No           | Limited           | Yes         | Yes      | Limited               | Limited               | No               | Limited | No      | Limited    | No         |
| grok-3-mini                     | No          | Limited         | No                 | No               | No           | Limited           | No          | Yes      | Limited               | Limited               | No               | Limited | No      | Limited    | No         |
| grok-4-fast-non-reasoning       | No          | Limited         | No                 | No               | No           | Limited           | No          | Yes      | Limited               | Limited               | No               | Limited | No      | Limited    | No         |
| grok-4-fast-reasoning           | No          | Limited         | No                 | No               | No           | Limited           | No          | Yes      | Limited               | Limited               | No               | Limited | No      | Limited    | No         |

## Troubleshooting

Use these checks to resolve common issues:

* **Your agent doesn’t call a tool**:

  * Confirm the tool is attached to the agent.
  * Confirm the model supports the tool.
  * If you need deterministic behavior, set `tool_choice` to `required`.
  * Review run traces to confirm whether the model produced a tool call.

* **Tool calls return empty or irrelevant results**:

  * Improve tool descriptions and agent instructions.
  * For retrieval tools, ensure your data is ingested and searchable.

* **Tool calls fail**:

  * Verify tool configuration and authentication.
  * For MCP and OpenAPI tools, validate the endpoint is reachable and returns expected responses.

## FAQ

**How do I validate whether a tool was called?**

Review run traces to confirm whether your agent called a tool and to inspect tool inputs and outputs. For end-to-end tracing setup, see [Trace your application](../../how-to/develop/trace-application).

**How do I make tool usage more reliable?**

Start with clear tool instructions. If you need deterministic tool calling, use `tool_choice`. For details, see [Control tool calling with `tool_choice`](#control-tool-calling-with-tool_choice).

## Related content

### Tool management

* [Discover and manage tools in the Foundry tool catalog (preview)](tool-catalog)
* [Tools governance with AI Gateway (preview)](../how-to/tools/governance)

### Retrieval and search tools

* [Azure AI Search](../how-to/tools/ai-search)
* [File search](../how-to/tools/file-search)
* [Web search (preview)](../how-to/tools/web-search)
* [Grounding with Bing tools](../how-to/tools/bing-tools)
* [SharePoint (preview)](../how-to/tools/sharepoint)

### Data and integration tools

* [Fabric data agent (preview)](../how-to/tools/fabric)
* [Model Context Protocol (MCP) (preview)](../how-to/tools/model-context-protocol)
* [OpenAPI tool](../how-to/tools/openapi)
* [Function calling](../how-to/tools/function-calling)

### Automation and generation tools

* [Code interpreter](../how-to/tools/code-interpreter)
* [Browser automation (preview)](../how-to/tools/browser-automation)
* [Computer Use (preview)](../how-to/tools/computer-use)
* [Image generation (preview)](../how-to/tools/image-generation)
* [Agent2Agent (A2A) tool (preview)](../how-to/tools/agent-to-agent)

Control how your agents access external tools by routing Model Context Protocol (MCP) traffic through an AI gateway in Microsoft Foundry. An AI gateway provides a single, governed entry point where you can enforce authentication, rate limits, IP restrictions, and audit logging without modifying your MCP servers or agent code.

This feature is in preview. Only new MCP tools created in the Foundry portal that don't use managed OAuth are routed through an AI gateway.

## Prerequisites

* The AI gateway must be connected to the Microsoft Foundry resource. Follow the steps in [Configure an AI gateway in your Foundry resources](../../../configuration/enable-ai-api-management-gateway-portal).

  Governance is activated at the Microsoft Foundry resource level. All governance functionality depends on this connection.

* You need permissions to manage API Management policies: the API Management Service Contributor or Owner role on the connected API Management instance. For more information, see [Use role-based access control for API Management](https://learn.microsoft.com/en-us/azure/api-management/api-management-role-based-access-control).

* The MCP server must support one of the following authentication methods:

  * Managed identity (Microsoft Entra)
  * Key-based (API key or token)
  * Custom OAuth identity passthrough
  * Unauthenticated (if applicable)

## Key benefits

* Secure routing for all new MCP tools through a gateway endpoint
* Consistent access control and authentication enforcement
* Centralized observability for gateway traffic (such as logs and metrics)
* Unified policies for throttling, IP restrictions, and routing
* Seamless reuse of tools through public and private catalogs

## Govern a tool

The following sections walk you through setting up an AI gateway as a governed entry point.

### Add a tool

To add a tool to be governed, use either of these methods in the Foundry portal:

* Use the tool catalog by selecting **Tools** > **Catalog**. Then choose an MCP server to add.
* Add a custom tool by selecting **Build** > **Tools** > **Custom** > **Model Context Protocol**. Then paste your MCP server endpoint and select an authentication type.

After you add the tool, verify that the MCP server endpoint in the tool configuration displays the AI gateway URL (for example, `https://<your-API-Management-instance>.azure-api.net/mcp/...`) rather than the direct MCP server URL.

For more information about MCP tools, see [Connect to Model Context Protocol servers](model-context-protocol).

### Confirm routing

Before you apply policies, confirm these settings in the Foundry portal:

* **Remote MCP server endpoint**: Verify that it points to the AI gateway URL, not the original MCP server.
* **Redirect URL**: If you use custom OAuth identity passthrough, confirm that the redirect URL matches your OAuth app registration.
* **Authentication method**: Confirm the method (key-based or OAuth) aligns with your MCP server requirements.
* **Agent usage**: Note which agents reference this tool so you can test after applying policies.

### Apply policies

In the [Azure portal](https://portal.azure.com/), go to your resource. Select **API Management** to apply [policies](https://learn.microsoft.com/en-us/azure/api-management/api-management-howto-policies) for governance.

You must apply policies through Azure API Management. Common policies include:

* **Rate limiting**: Limit how many calls a project or user can make in one minute.

  ```xml
  <inbound>
    <base />
    <rate-limit-by-key calls="60" renewal-period="60" counter-key="@(context.Request.IpAddress)" />
  </inbound>
  ```

* **IP filtering**: Allow requests from only trusted networks.

  ```xml
  <inbound>
    <base />
    <ip-filter action="allow">
      <address>10.0.0.0/24</address> <!-- internal network -->
      <address>20.50.123.45</address> <!-- trusted app -->
    </ip-filter>
  </inbound>
  ```

* **Correlation ID**: Add a unique request ID so that you can trace requests later in logs.

  ```xml
  <inbound>
    <base />
      <set-header name="X-Correlation-Id" exists-action="override">
      <value>@(context.RequestId)</value>
    </set-header>
  </inbound>
  ```

* **Removal of sensitive headers**: Clean up incoming requests to help protect credentials or session data.

  ```xml
  <inbound>
    <base />
    <set-header name="Cookie" exists-action="delete" />
    <set-header name="Referer" exists-action="delete" />
  </inbound>
  ```

  <Callout type="important">
    Avoid deleting authentication headers (such as `Authorization`) unless you're sure that the MCP server doesn't require them.
  </Callout>

* **Simple routing control**: If you have different backends (like ones for different geographies), you can route requests based on a header.

  ```xml
  <inbound>
    <base />
    <choose>
      <when condition="@(context.Request.Headers.GetValueOrDefault('X-Region','us') == 'eu')">
        <set-backend-service base-url="https://europe-api.contoso-mcp.net" />
      </when>
      <otherwise>
        <set-backend-service base-url="https://us-api.contoso-mcp.net" />
      </otherwise>
    </choose>
  </inbound>
  ```

For more policy XML examples, see the [API Management policy snippets](https://github.com/Azure/api-management-policy-snippets) repository on GitHub.

### Test with an agent

After you configure your MCP server, you can test it in the Foundry portal:

1. Open the [Foundry portal](https://ai.azure.com/) and go to your project.

2. Create a new agent or open an existing one, and configure an MCP tool. For details, see [Connect to Model Context Protocol servers](model-context-protocol).

3. In the agent's chat interface, send a message that triggers the tool (for example, "List my repositories" for the GitHub MCP server). Verify that the response returns successfully.

## Verify that governance is working

Use these steps to confirm that traffic is routed through the AI gateway and policies are applied:

1. In the Foundry portal, open your MCP tool configuration. Confirm that the tool endpoint points to the AI gateway (not directly to your MCP server).

2. In the Azure portal, open the API Management instance connected to your Foundry resource. Review metrics and logs to confirm that requests appear when your agent calls the tool.

When you're reviewing API Management metrics:

* Look for requests where the name of the API Management instance matches your MCP tool.
* Check **Response codes** for successful calls (2xx) and policy-blocked calls (429 for rate limits, 403 for IP filters).
* If you applied rate limiting, verify that the **X-RateLimit-Remaining** header decreases with each call.
* For log-level details, enable **Diagnostic settings** on your API Management instance and query Azure Monitor Logs.

## Security considerations

* Treat API keys, tokens, and OAuth client secrets as secrets. Store shared credentials in a project connection and limit project access to authorized users.
* Apply the least-privilege principle for managed identity and Microsoft Entra access.
* Review which headers you forward to backends. Remove only headers that you don't need, and avoid stripping required authentication headers.

For MCP authentication options, see [Set up authentication for Model Context Protocol (MCP) tools (preview)](../mcp-authentication).

## Troubleshooting

| Problem                                                    | Cause                                                                                                                                          | Resolution                                                                                                                                                                                                                      |
| ---------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| The tool still calls the MCP server directly.              | The tool was created before the AI gateway was connected, or the tool isn't eligible for gateway routing (for example, it uses managed OAuth). | Re-create the tool after the AI gateway is connected. Confirm that the tool is an MCP tool that doesn't use managed OAuth.                                                                                                      |
| Tool calls fail after you add API Management policies.     | A policy blocks traffic (rate limits, IP filtering) or modifies headers that the MCP server requires.                                          | Temporarily disable policies to isolate the cause, and then refine the policy conditions. Avoid deleting required authentication headers.                                                                                       |
| OAuth sign-in fails for custom OAuth identity passthrough. | Redirect URL or OAuth app configuration is incorrect.                                                                                          | Re-check the redirect URL in your OAuth app registration and confirm required OAuth settings. For options and terminology, see [Set up authentication for Model Context Protocol (MCP) tools (preview)](../mcp-authentication). |
| You don't see request traces in the AI gateway.            | The AI gateway doesn't log tool traces.                                                                                                        | Use API Management logging and metrics for gateway traffic. Use your MCP server logs for tool-level details.                                                                                                                    |

## Limitations

* AI gateways support only MCP tools. Foundry-based tools such as SharePoint, code-first MCP tools, tools with managed OAuth, or OpenAPI tools aren't supported.
* AI gateways don't log tool traces.
* Gateway routing is applied only at tool creation. Existing tools aren't automatically mediated with AI gateways.
* API gateways support the application of API Management policies only in the Azure portal, not the Foundry portal.

For a broader list of Foundry Agent Service tool support when you're working with gateways, see [Bring your own AI gateway to Azure AI Agent Service (preview)](../ai-gateway).

## Related content

* [Connect to Model Context Protocol servers (preview)](model-context-protocol)
* [Set up authentication for Model Context Protocol (MCP) tools (preview)](../mcp-authentication)
* [Discover and manage tools in the Foundry tool catalog (preview)](../../concepts/tool-catalog)

Code Interpreter enables a Microsoft Foundry agent to run Python code in a sandboxed execution environment. Use this tool for data analysis, chart generation, and iterative problem-solving tasks that benefit from code execution.

In this article, you create an agent that uses Code Interpreter, upload a CSV file for analysis, and download a generated chart.

When enabled, your agent can write and run Python code iteratively to solve data analysis and math tasks, and to generate charts.

<Callout type="important">
  Code Interpreter has [additional charges](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) beyond the token-based fees for Azure OpenAI usage. If your agent calls Code Interpreter simultaneously in two different conversations, two Code Interpreter sessions are created. Each session is active by default for one hour with an idle timeout of 30 minutes.
</Callout>

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | -              | -        | -        | ✔️                | ✔️                   |

✔️ indicates the feature is supported. `-` indicates the feature isn't currently available for that SDK or API.

## Prerequisites

* Basic or standard agent environment. See [agent environment setup](../../environment-setup) for details.
* Latest prerelease SDK package installed (`azure-ai-projects>=2.0.0b1` for Python). See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true) for installation steps.
* Azure AI model deployment configured in your project.
* For file operations: CSV or other supported files to upload for analysis.

<Callout type="note">
  Code Interpreter isn't available in all regions. See [Check regional and model availability](#check-regional-and-model-availability).
</Callout>

## Create an agent with Code Interpreter

The following samples demonstrate how to create an agent with Code Interpreter enabled, upload a file for analysis, and download the generated output.

<Callout type="note">
  You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).
</Callout>

<ZonePivot pivot="python">
  ## Sample of using agent with code interpreter tool in Python SDK

  The following Python sample shows how to create an agent with the code interpreter tool, upload a CSV file for analysis, and request a bar chart based on the data. It demonstrates a complete workflow: upload a file, create an agent with Code Interpreter enabled, request data visualization, and download the generated chart.

  Set these environment variables:

  * `FOUNDRY_PROJECT_ENDPOINT`
  * `FOUNDRY_MODEL_DEPLOYMENT_NAME`

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition, CodeInterpreterTool, CodeInterpreterToolAuto

  load_dotenv()

  # Load the CSV file to be processed
  asset_file_path = os.path.abspath(
      os.path.join(os.path.dirname(__file__), "../assets/synthetic_500_quarterly_results.csv")
  )

  project_client = AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  with project_client:
      openai_client = project_client.get_openai_client()

      # Upload the CSV file for the code interpreter to use
      with open(asset_file_path, "rb") as f:
          file = openai_client.files.create(purpose="assistants", file=f)
      print(f"File uploaded (id: {file.id})")

      # Create agent with code interpreter tool
      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="You are a helpful assistant.",
              tools=[CodeInterpreterTool(container=CodeInterpreterToolAuto(file_ids=[file.id]))],
          ),
          description="Code interpreter agent for data analysis and visualization.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      # Create a conversation for the agent interaction
      conversation = openai_client.conversations.create()
      print(f"Created conversation (id: {conversation.id})")

      # Send request to create a chart and generate a file
      response = openai_client.responses.create(
          conversation=conversation.id,
          input="Could you please create bar chart in TRANSPORTATION sector for the operating profit from the uploaded csv file and provide file to me?",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )
      print(f"Response completed (id: {response.id})")

      # Extract file information from response annotations
      file_id = ""
      filename = ""
      container_id = ""

      # Get the last message which should contain file citations
      last_message = response.output[-1]  # ResponseOutputMessage
      if last_message.type == "message":
          # Get the last content item (contains the file annotations)
          text_content = last_message.content[-1]  # ResponseOutputText
          if text_content.type == "output_text":
              # Get the last annotation (most recent file)
              if text_content.annotations:
                  file_citation = text_content.annotations[-1]  # AnnotationContainerFileCitation
                  if file_citation.type == "container_file_citation":
                      file_id = file_citation.file_id
                      filename = file_citation.filename
                      container_id = file_citation.container_id
                      print(f"Found generated file: {filename} (ID: {file_id})")

      # Download the generated file if available
      if file_id and filename:
          safe_filename = os.path.basename(filename)
          file_content = openai_client.containers.files.content.retrieve(file_id=file_id, container_id=container_id)
          with open(safe_filename, "wb") as f:
              f.write(file_content.read())
              print(f"File {safe_filename} downloaded successfully.")
          print(f"File ready for download: {safe_filename}")
      else:
          print("No file generated in response")
      #uncomment these lines if you want to delete your agent
      #print("\nCleaning up...")
      #project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
      #print("Agent deleted")
  ```

  ### Expected output

  The sample code produces output similar to the following example:

  ```console
  File uploaded (id: file-xxxxxxxxxxxxxxxxxxxx)
  Agent created (id: agent-xxxxxxxxxxxxxxxxxxxx, name: MyAgent, version: 1)
  Created conversation (id: conv-xxxxxxxxxxxxxxxxxxxx)
  Response completed (id: resp-xxxxxxxxxxxxxxxxxxxx)
  Found generated file: transportation_operating_profit_bar_chart.png (ID: file-xxxxxxxxxxxxxxxxxxxx)
  File transportation_operating_profit_bar_chart.png downloaded successfully.
  File ready for download: transportation_operating_profit_bar_chart.png
  ```

  The agent uploads your CSV file to Azure storage, creates a sandboxed Python environment, analyzes the data to filter transportation sector records, generates a PNG bar chart showing operating profit by quarter, and downloads the chart to your local directory. The file annotations in the response provide the file ID and container information needed to retrieve the generated chart.
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample of using agent with code interpreter and file attachment in C# SDK

  The following C# sample shows how to create an agent with the code interpreter tool and ask it to solve a mathematical equation. Replace the environment variable values (`FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`) with your actual resource details. The agent executes Python code in a sandboxed container to compute the solution. The code uses synchronous calls for simplicity. For asynchronous usage, refer to the [code sample](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample7_CodeInterpreter.md) in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Create project client and read the environment variables, which will be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Create Agent, capable to use Code Interpreter to answer questions.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful agent that can help fetch data from files you know about.",
      Tools = {
          ResponseTool.CreateCodeInterpreterTool(
              new CodeInterpreterToolContainer(
                  CodeInterpreterToolContainerConfiguration.CreateAutomaticContainerConfiguration(
                      fileIds: []
                  )
              )
          ),
      }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Ask the agent a question, which requires running python code in the container.
  AgentReference agentReference = new(name: agentVersion.Name, version: agentVersion.Version);
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentReference);

  ResponseResult response = responseClient.CreateResponse("I need to solve the equation sin(x) + x^2 = 42");

  // Write out the output of a response, raise the exception if the request was not successful.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine(response.GetOutputText());

  // Clean up resources by deleting conversations and the Agent.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected output

  The sample code produces output similar to the following example:

  ```console
  Response completed (id: resp-xxxxxxxxxxxxxxxxxxxx)
  The solution to the equation sin(x) + x^2 = 42 is approximately x = 6.324555320336759
  ```

  The agent creates a Code Interpreter session, writes Python code to solve the equation numerically, executes the code in a sandboxed environment, and returns the computed result. The agent iteratively refines its approach if the initial code doesn't produce a valid solution.
</ZonePivot>

## Check regional and model availability

Tool availability varies by region and model.

For the current list of supported regions and models for Code Interpreter, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

### Supported file types

| File format | MIME type                                                                   |
| ----------- | --------------------------------------------------------------------------- |
| `.c`        | `text/x-c`                                                                  |
| `.cpp`      | `text/x-c++`                                                                |
| `.csv`      | `application/csv`                                                           |
| `.docx`     | `application/vnd.openxmlformats-officedocument.wordprocessingml.document`   |
| `.html`     | `text/html`                                                                 |
| `.java`     | `text/x-java`                                                               |
| `.json`     | `application/json`                                                          |
| `.md`       | `text/markdown`                                                             |
| `.pdf`      | `application/pdf`                                                           |
| `.php`      | `text/x-php`                                                                |
| `.pptx`     | `application/vnd.openxmlformats-officedocument.presentationml.presentation` |
| `.py`       | `text/x-python`                                                             |
| `.py`       | `text/x-script.python`                                                      |
| `.rb`       | `text/x-ruby`                                                               |
| `.tex`      | `text/x-tex`                                                                |
| `.txt`      | `text/plain`                                                                |
| `.css`      | `text/css`                                                                  |
| `.jpeg`     | `image/jpeg`                                                                |
| `.jpg`      | `image/jpeg`                                                                |
| `.js`       | `text/javascript`                                                           |
| `.gif`      | `image/gif`                                                                 |
| `.png`      | `image/png`                                                                 |
| `.tar`      | `application/x-tar`                                                         |
| `.ts`       | `application/typescript`                                                    |
| `.xlsx`     | `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet`         |
| `.xml`      | `application/xml` or `text/xml`                                             |
| `.zip`      | `application/zip`                                                           |

## Troubleshooting

| Issue                               | Likely cause                                                 | Resolution                                                                                                                                                                                            |
| ----------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Code Interpreter doesn't run.       | Tool not enabled or model doesn't support it in your region. | Confirm Code Interpreter is enabled on the agent. Verify your model deployment supports the tool in your region. See [Check regional and model availability](#check-regional-and-model-availability). |
| No file is generated.               | Agent returned text-only response without file annotation.   | Check response annotations for `container_file_citation`. If none exist, the agent didn't generate a file. Rephrase the prompt to explicitly request file output.                                     |
| File upload fails.                  | Unsupported file type or wrong purpose.                      | Confirm the file type is in the [supported file types](#supported-file-types) list. Upload with `purpose="assistants"`.                                                                               |
| Generated file is corrupt or empty. | Code execution error or incomplete processing.               | Check the agent's response for error messages. Verify the input data is valid. Try a simpler request first.                                                                                           |
| Session timeout or high latency.    | Code Interpreter sessions have time limits.                  | Sessions have a 1-hour active timeout and 30-minute idle timeout. Reduce the complexity of operations or split into smaller tasks.                                                                    |
| Unexpected billing charges.         | Multiple concurrent sessions created.                        | Each conversation creates a separate session. Monitor session usage and consolidate operations where possible.                                                                                        |
| Python package not available.       | Code Interpreter has a fixed set of packages.                | Code Interpreter includes common data science packages. For custom packages, use [Custom code interpreter](custom-code-interpreter).                                                                  |
| File download fails.                | Container ID or file ID incorrect.                           | Verify you're using the correct `container_id` and `file_id` from the response annotations.                                                                                                           |

## Clean up resources

Delete resources you created in this sample when you no longer need them to avoid ongoing costs:

* Delete the agent version.
* Delete the conversation.
* Delete uploaded files.

For examples of conversation and file cleanup patterns, see [Web search tool (preview)](web-search) and [File search tool for agents](file-search).

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Custom code interpreter tool for agents (preview)](custom-code-interpreter)

Code Interpreter enables a Microsoft Foundry agent to run Python code in a sandboxed execution environment. Use this tool for data analysis, chart generation, and iterative problem-solving tasks that benefit from code execution.

In this article, you create an agent that uses Code Interpreter, upload a CSV file for analysis, and download a generated chart.

When enabled, your agent can write and run Python code iteratively to solve data analysis and math tasks, and to generate charts.

<Callout type="important">
  Code Interpreter has [additional charges](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) beyond the token-based fees for Azure OpenAI usage. If your agent calls Code Interpreter simultaneously in two different conversations, two Code Interpreter sessions are created. Each session is active by default for one hour with an idle timeout of 30 minutes.
</Callout>

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | -              | -        | -        | ✔️                | ✔️                   |

✔️ indicates the feature is supported. `-` indicates the feature isn't currently available for that SDK or API.

## Prerequisites

* Basic or standard agent environment. See [agent environment setup](../../environment-setup) for details.
* Latest prerelease SDK package installed (`azure-ai-projects>=2.0.0b1` for Python). See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true) for installation steps.
* Azure AI model deployment configured in your project.
* For file operations: CSV or other supported files to upload for analysis.

<Callout type="note">
  Code Interpreter isn't available in all regions. See [Check regional and model availability](#check-regional-and-model-availability).
</Callout>

## Create an agent with Code Interpreter

The following samples demonstrate how to create an agent with Code Interpreter enabled, upload a file for analysis, and download the generated output.

<Callout type="note">
  You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).
</Callout>

<ZonePivot pivot="python">
  ## Sample of using agent with code interpreter tool in Python SDK

  The following Python sample shows how to create an agent with the code interpreter tool, upload a CSV file for analysis, and request a bar chart based on the data. It demonstrates a complete workflow: upload a file, create an agent with Code Interpreter enabled, request data visualization, and download the generated chart.

  Set these environment variables:

  * `FOUNDRY_PROJECT_ENDPOINT`
  * `FOUNDRY_MODEL_DEPLOYMENT_NAME`

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition, CodeInterpreterTool, CodeInterpreterToolAuto

  load_dotenv()

  # Load the CSV file to be processed
  asset_file_path = os.path.abspath(
      os.path.join(os.path.dirname(__file__), "../assets/synthetic_500_quarterly_results.csv")
  )

  project_client = AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  with project_client:
      openai_client = project_client.get_openai_client()

      # Upload the CSV file for the code interpreter to use
      with open(asset_file_path, "rb") as f:
          file = openai_client.files.create(purpose="assistants", file=f)
      print(f"File uploaded (id: {file.id})")

      # Create agent with code interpreter tool
      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="You are a helpful assistant.",
              tools=[CodeInterpreterTool(container=CodeInterpreterToolAuto(file_ids=[file.id]))],
          ),
          description="Code interpreter agent for data analysis and visualization.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      # Create a conversation for the agent interaction
      conversation = openai_client.conversations.create()
      print(f"Created conversation (id: {conversation.id})")

      # Send request to create a chart and generate a file
      response = openai_client.responses.create(
          conversation=conversation.id,
          input="Could you please create bar chart in TRANSPORTATION sector for the operating profit from the uploaded csv file and provide file to me?",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )
      print(f"Response completed (id: {response.id})")

      # Extract file information from response annotations
      file_id = ""
      filename = ""
      container_id = ""

      # Get the last message which should contain file citations
      last_message = response.output[-1]  # ResponseOutputMessage
      if last_message.type == "message":
          # Get the last content item (contains the file annotations)
          text_content = last_message.content[-1]  # ResponseOutputText
          if text_content.type == "output_text":
              # Get the last annotation (most recent file)
              if text_content.annotations:
                  file_citation = text_content.annotations[-1]  # AnnotationContainerFileCitation
                  if file_citation.type == "container_file_citation":
                      file_id = file_citation.file_id
                      filename = file_citation.filename
                      container_id = file_citation.container_id
                      print(f"Found generated file: {filename} (ID: {file_id})")

      # Download the generated file if available
      if file_id and filename:
          safe_filename = os.path.basename(filename)
          file_content = openai_client.containers.files.content.retrieve(file_id=file_id, container_id=container_id)
          with open(safe_filename, "wb") as f:
              f.write(file_content.read())
              print(f"File {safe_filename} downloaded successfully.")
          print(f"File ready for download: {safe_filename}")
      else:
          print("No file generated in response")
      #uncomment these lines if you want to delete your agent
      #print("\nCleaning up...")
      #project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
      #print("Agent deleted")
  ```

  ### Expected output

  The sample code produces output similar to the following example:

  ```console
  File uploaded (id: file-xxxxxxxxxxxxxxxxxxxx)
  Agent created (id: agent-xxxxxxxxxxxxxxxxxxxx, name: MyAgent, version: 1)
  Created conversation (id: conv-xxxxxxxxxxxxxxxxxxxx)
  Response completed (id: resp-xxxxxxxxxxxxxxxxxxxx)
  Found generated file: transportation_operating_profit_bar_chart.png (ID: file-xxxxxxxxxxxxxxxxxxxx)
  File transportation_operating_profit_bar_chart.png downloaded successfully.
  File ready for download: transportation_operating_profit_bar_chart.png
  ```

  The agent uploads your CSV file to Azure storage, creates a sandboxed Python environment, analyzes the data to filter transportation sector records, generates a PNG bar chart showing operating profit by quarter, and downloads the chart to your local directory. The file annotations in the response provide the file ID and container information needed to retrieve the generated chart.
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample of using agent with code interpreter and file attachment in C# SDK

  The following C# sample shows how to create an agent with the code interpreter tool and ask it to solve a mathematical equation. Replace the environment variable values (`FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`) with your actual resource details. The agent executes Python code in a sandboxed container to compute the solution. The code uses synchronous calls for simplicity. For asynchronous usage, refer to the [code sample](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample7_CodeInterpreter.md) in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Create project client and read the environment variables, which will be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Create Agent, capable to use Code Interpreter to answer questions.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful agent that can help fetch data from files you know about.",
      Tools = {
          ResponseTool.CreateCodeInterpreterTool(
              new CodeInterpreterToolContainer(
                  CodeInterpreterToolContainerConfiguration.CreateAutomaticContainerConfiguration(
                      fileIds: []
                  )
              )
          ),
      }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Ask the agent a question, which requires running python code in the container.
  AgentReference agentReference = new(name: agentVersion.Name, version: agentVersion.Version);
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentReference);

  ResponseResult response = responseClient.CreateResponse("I need to solve the equation sin(x) + x^2 = 42");

  // Write out the output of a response, raise the exception if the request was not successful.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine(response.GetOutputText());

  // Clean up resources by deleting conversations and the Agent.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected output

  The sample code produces output similar to the following example:

  ```console
  Response completed (id: resp-xxxxxxxxxxxxxxxxxxxx)
  The solution to the equation sin(x) + x^2 = 42 is approximately x = 6.324555320336759
  ```

  The agent creates a Code Interpreter session, writes Python code to solve the equation numerically, executes the code in a sandboxed environment, and returns the computed result. The agent iteratively refines its approach if the initial code doesn't produce a valid solution.
</ZonePivot>

## Check regional and model availability

Tool availability varies by region and model.

For the current list of supported regions and models for Code Interpreter, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

### Supported file types

| File format | MIME type                                                                   |
| ----------- | --------------------------------------------------------------------------- |
| `.c`        | `text/x-c`                                                                  |
| `.cpp`      | `text/x-c++`                                                                |
| `.csv`      | `application/csv`                                                           |
| `.docx`     | `application/vnd.openxmlformats-officedocument.wordprocessingml.document`   |
| `.html`     | `text/html`                                                                 |
| `.java`     | `text/x-java`                                                               |
| `.json`     | `application/json`                                                          |
| `.md`       | `text/markdown`                                                             |
| `.pdf`      | `application/pdf`                                                           |
| `.php`      | `text/x-php`                                                                |
| `.pptx`     | `application/vnd.openxmlformats-officedocument.presentationml.presentation` |
| `.py`       | `text/x-python`                                                             |
| `.py`       | `text/x-script.python`                                                      |
| `.rb`       | `text/x-ruby`                                                               |
| `.tex`      | `text/x-tex`                                                                |
| `.txt`      | `text/plain`                                                                |
| `.css`      | `text/css`                                                                  |
| `.jpeg`     | `image/jpeg`                                                                |
| `.jpg`      | `image/jpeg`                                                                |
| `.js`       | `text/javascript`                                                           |
| `.gif`      | `image/gif`                                                                 |
| `.png`      | `image/png`                                                                 |
| `.tar`      | `application/x-tar`                                                         |
| `.ts`       | `application/typescript`                                                    |
| `.xlsx`     | `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet`         |
| `.xml`      | `application/xml` or `text/xml`                                             |
| `.zip`      | `application/zip`                                                           |

## Troubleshooting

| Issue                               | Likely cause                                                 | Resolution                                                                                                                                                                                            |
| ----------------------------------- | ------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Code Interpreter doesn't run.       | Tool not enabled or model doesn't support it in your region. | Confirm Code Interpreter is enabled on the agent. Verify your model deployment supports the tool in your region. See [Check regional and model availability](#check-regional-and-model-availability). |
| No file is generated.               | Agent returned text-only response without file annotation.   | Check response annotations for `container_file_citation`. If none exist, the agent didn't generate a file. Rephrase the prompt to explicitly request file output.                                     |
| File upload fails.                  | Unsupported file type or wrong purpose.                      | Confirm the file type is in the [supported file types](#supported-file-types) list. Upload with `purpose="assistants"`.                                                                               |
| Generated file is corrupt or empty. | Code execution error or incomplete processing.               | Check the agent's response for error messages. Verify the input data is valid. Try a simpler request first.                                                                                           |
| Session timeout or high latency.    | Code Interpreter sessions have time limits.                  | Sessions have a 1-hour active timeout and 30-minute idle timeout. Reduce the complexity of operations or split into smaller tasks.                                                                    |
| Unexpected billing charges.         | Multiple concurrent sessions created.                        | Each conversation creates a separate session. Monitor session usage and consolidate operations where possible.                                                                                        |
| Python package not available.       | Code Interpreter has a fixed set of packages.                | Code Interpreter includes common data science packages. For custom packages, use [Custom code interpreter](custom-code-interpreter).                                                                  |
| File download fails.                | Container ID or file ID incorrect.                           | Verify you're using the correct `container_id` and `file_id` from the response annotations.                                                                                                           |

## Clean up resources

Delete resources you created in this sample when you no longer need them to avoid ongoing costs:

* Delete the agent version.
* Delete the conversation.
* Delete uploaded files.

For examples of conversation and file cleanup patterns, see [Web search tool (preview)](web-search) and [File search tool for agents](file-search).

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Custom code interpreter tool for agents (preview)](custom-code-interpreter)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

A custom code interpreter gives you full control over the runtime environment for agent-generated Python code. You can configure custom Python packages, compute resources, and [Azure Container Apps environment](https://learn.microsoft.com/en-us/azure/container-apps/environment) settings. The code interpreter container exposes a Model Context Protocol (MCP) server.

Use a custom code interpreter when the built-in [Code Interpreter tool for agents](code-interpreter) doesn't meet your requirements—for example, when you need specific Python packages, custom container images, or dedicated compute resources.

For more information about MCP and how agents connect to MCP tools, see [Connect to Model Context Protocol servers (preview)](model-context-protocol).

## Usage support

This article uses the Azure CLI and a runnable sample project.

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | -      | -              | -        | ✔️       | -                 | ✔️                   |

For the latest SDK and API support for agents tools, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

## Prerequisites

* [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) version 2.60.0 or later.

* (Optional) [uv](https://docs.astral.sh/uv/getting-started/installation/) for faster Python package management.

* An Azure subscription and resource group with the following role assignments:

  * [Azure AI Owner](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/ai-machine-learning#azure-ai-owner)
  * [Container Apps ManagedEnvironment Contributor](https://learn.microsoft.com/en-us/azure/role-based-access-control/built-in-roles/containers#container-apps-managedenvironments-contributor)

* Azure AI Projects SDK (prerelease). See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true) for installation.

### Environment variables

Set these environment variables after provisioning the infrastructure:

| Variable                        | Description                                                        |
| ------------------------------- | ------------------------------------------------------------------ |
| `FOUNDRY_PROJECT_ENDPOINT`      | Your Foundry project endpoint URL.                                 |
| `FOUNDRY_MODEL_DEPLOYMENT_NAME` | Your model deployment name (for example, `gpt-4o`).                |
| `MCP_SERVER_URL`                | The MCP server endpoint from your Azure Container Apps deployment. |
| `MCP_PROJECT_CONNECTION_ID`     | Your project connection ID for the custom code interpreter.        |

## Before you begin

This procedure provisions Azure infrastructure, including Azure Container Apps resources. Review your organization's Azure cost and governance requirements before deploying.

## Create an agent with custom code interpreter

The following steps show how to create an agent that uses a custom code interpreter MCP server.

### Register the preview feature

Register the MCP server feature for Azure Container Apps Dynamic Sessions:

```console
az feature register --namespace Microsoft.App --name SessionPoolsSupportMCP
az provider register -n Microsoft.App
```

### Get the sample code

Clone the [sample code in the GitHub repo](https://github.com/azure-ai-foundry/foundry-samples) and navigate to the `samples/python/hosted-agents/code-interpreter-custom` folder in your terminal.

### Provision the infrastructure

To provision the infrastructure, run the following command by using the Azure CLI (`az`):

```console
az deployment group create \
    --name custom-code-interpreter \
    --subscription <your_subscription> \
    --resource-group <your_resource_group> \
    --template-file ./infra.bicep
```

<Callout type="note">
  Deployment can take up to one hour, depending on the number of standby instances you request. The dynamic session pool allocation is the longest step.
</Callout>

### Configure and run the agent

Copy the `.env.sample` file from the repository to `.env` and populate the values from your deployment output. You can find these values in the Azure portal under the resource group.

Install the Python dependencies by using `uv sync` or `pip install`. Finally, run `./main.py`.

### Quick verification

Before running the full sample, verify your authentication and project connection:

```python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from dotenv import load_dotenv

load_dotenv()

with (
    DefaultAzureCredential() as credential,
    AIProjectClient(endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"], credential=credential) as project_client,
):
    print("Connected to project.")
    # List connections to verify MCP connection exists
    connections = project_client.connections.list()
    for conn in connections:
        print(f"  Connection: {conn.name} (type: {conn.type})")
```

If this code runs without errors, your credentials and project endpoint are configured correctly.

### Code example

The following Python sample shows how to create an agent with a custom code interpreter MCP tool:

```python
import os
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from azure.ai.projects.models import PromptAgentDefinition, MCPTool

load_dotenv()

project_client = AIProjectClient(
    endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
    credential=DefaultAzureCredential(),
)

with project_client:
    openai_client = project_client.get_openai_client()

    # Configure the custom code interpreter MCP tool
    custom_code_interpreter = MCPTool(
        server_label="custom-code-interpreter",
        server_url=os.environ["MCP_SERVER_URL"],
        project_connection_id=os.environ.get("MCP_PROJECT_CONNECTION_ID"),
    )

    agent = project_client.agents.create_version(
        agent_name="CustomCodeInterpreterAgent",
        definition=PromptAgentDefinition(
            model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
            instructions="You are a helpful assistant that can run Python code to analyze data and solve problems.",
            tools=[custom_code_interpreter],
        ),
        description="Agent with custom code interpreter for data analysis.",
    )
    print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

    # Test the agent with a simple calculation
    response = openai_client.responses.create(
        input="Calculate the factorial of 10 using Python.",
        extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
    )
    print(f"Response: {response.output_text}")

    # Clean up
    project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
    print("Agent deleted")
```

### Expected output

When you run the sample, you see output similar to:

```console
Agent created (id: agent-xxxxxxxxxxxx, name: CustomCodeInterpreterAgent, version: 1)
Response: The factorial of 10 is 3,628,800. I calculated this using Python's math.factorial() function.
Agent deleted
```

## Verify your setup

After you've provisioned the infrastructure and run the sample:

1. Confirm the Azure deployment completed successfully.
2. Confirm the sample connects using the values in your `.env` file.
3. In Microsoft Foundry, verify your agent calls the tool using tracing. For more information, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

## Troubleshooting

| Issue                                         | Likely cause                                                                                    | Resolution                                                                                                                                                                                                       |
| --------------------------------------------- | ----------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Feature registration is still pending         | The `az feature register` command returns `Registering` state.                                  | Wait for registration to complete (can take 15-30 minutes). Check status with `az feature show --namespace Microsoft.App --name SessionPoolsSupportMCP`. Then run `az provider register -n Microsoft.App` again. |
| Deployment fails with permission error        | Missing required role assignments.                                                              | Confirm you have **Azure AI Owner** and **Container Apps ManagedEnvironment Contributor** roles on the subscription or resource group.                                                                           |
| Deployment fails with region error            | The selected region doesn't support Azure Container Apps Dynamic Sessions.                      | Try a different region. See [Azure Container Apps regions](https://learn.microsoft.com/en-us/azure/container-apps/overview#regions) for supported regions.                                                       |
| Agent doesn't call the tool                   | The MCP connection isn't configured correctly, or the agent instructions don't prompt tool use. | Use tracing in Microsoft Foundry to confirm tool invocation. Verify the `MCP_SERVER_URL` matches your deployed Container Apps endpoint. See [Best practices](../../concepts/tool-best-practice).                 |
| MCP server connection timeout                 | The Container Apps session pool isn't running or has no standby instances.                      | Check the session pool status in the Azure portal. Increase `standbyInstanceCount` in your Bicep template if needed.                                                                                             |
| Code execution fails in container             | Missing Python packages in the custom container.                                                | Update your container image to include required packages. Rebuild and redeploy the container.                                                                                                                    |
| Authentication error connecting to MCP server | The project connection credentials are invalid or expired.                                      | Regenerate the connection credentials and update the `.env` file. Verify the `MCP_PROJECT_CONNECTION_ID` format.                                                                                                 |

## Limitations

The APIs don't directly support file input or output, or the use of file stores. To get data in and out, you must use URLs, such as data URLs for small files and Azure Blob Service shared access signature (SAS) URLs for large files.

## Security

If you use SAS URLs to pass data in or out of the runtime:

* Use short-lived SAS tokens.
* Don't log SAS URLs or store them in source control.
* Scope permissions to the minimum required (for example, read-only or write-only).

## Clean up

To stop billing for provisioned resources, delete the resources created by the sample deployment. If you used a dedicated resource group for this article, delete the resource group.

## Related content

* [Connect to Model Context Protocol servers (preview)](model-context-protocol)
* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Azure Container Apps Dynamic Sessions](https://learn.microsoft.com/en-us/azure/container-apps/sessions)
* [Session pools with custom containers](https://learn.microsoft.com/en-us/azure/container-apps/session-pool#custom-container-pool)
* [Azure Container Apps environment](https://learn.microsoft.com/en-us/azure/container-apps/environment)
* [Install the Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli)
* [Code Interpreter tool for agents](code-interpreter)


<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

This article explains how to configure and use the Browser Automation tool with Foundry agents to automate web browsing workflows.

<Callout type="warning">
  The Browser Automation tool comes with significant security risks. Both errors in judgment by the AI and the presence of malicious or confusing instructions on web pages that the AI encounters can cause it to execute commands you or others don't intend. These actions can compromise the security of your or other users' browsers, computers, and any accounts to which the browser or AI has access, including personal, financial, or enterprise systems. By using the Browser Automation tool, you acknowledge that you bear responsibility and liability for any use of it and of any resulting agents you create with it. This responsibility extends to any other users to whom you make Browser Automation tool functionality available, including through resulting agents. Use the Browser Automation tool on low-privilege virtual machines with no access to sensitive data or critical resources.
</Callout>

For guidance on optimizing tool usage, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

In Microsoft Foundry, the Browser Automation tool enables you to perform real-world browser tasks through natural language prompts. When you use it with Foundry Agent Service, it creates isolated browser sessions in your provisioned Playwright workspace.

By using [Microsoft Playwright Workspaces](https://aka.ms/pww/docs/manage-workspaces), you can automate browser-based workflows such as searching, navigating, filling forms, and booking.

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## How it works

The interaction starts when the user sends a query to an agent connected to the Browser Automation tool. For example, *"Show me all available yoga classes this week from the following URL \<url>."* When the agent receives the request, Foundry Agent Service creates an isolated browser session using your provisioned Playwright workspace. Each session is sandboxed for privacy and security.

The browser performs Playwright-driven actions, such as navigating to relevant pages and applying filters or parameters based on user preferences (such as time, location, and instructor). By combining the model with Playwright, the model can parse HTML or XML into DOM documents, make decisions, and perform actions like selecting UI elements, typing, and navigating websites. Exercise caution when using this tool.

An example flow is:

1. A user sends a request to the model that includes a call to the Browser Automation tool with the URL you want to go to.

2. The Browser Automation tool receives a response from the model. If the response has action items, those items contain suggested actions to make progress toward the specified goal. For example, an action might be a screenshot so the model can assess the current state with an updated screenshot or click with X/Y coordinates indicating where the mouse should be moved.

3. The Browser Automation tool executes the action in a sandboxed environment.

4. After executing the action, the Browser Automation tool captures the updated state of the environment as a screenshot.

5. The tool sends a new request with the updated state, and repeats this loop until the model stops requesting actions or the user decides to stop.

   The Browser Automation tool supports multi-turn conversations, allowing the user to refine their request and complete a booking.

## Prerequisites

Before you begin, make sure you have:

* An Azure subscription. [Create one for free](https://azure.microsoft.com/free/).
* Contributor or Owner role on a resource group.
* A Foundry project with a configured endpoint.
* An AI model deployed in your project.
* A Playwright workspace resource.
* A project connection set up for your Playwright workspace.

For the SDK examples, set these environment variables:

* `FOUNDRY_PROJECT_ENDPOINT`: Your Foundry project endpoint URL.
* `FOUNDRY_MODEL_DEPLOYMENT_NAME`: Your deployed model name.
* `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`: The connection resource ID for the Playwright workspace connection.

Use the following format for your connection ID: `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`.

## Set up Browser Automation

1. Create a [Playwright Workspace](https://aka.ms/pww/docs/manage-workspaces) resource.

2. [Generate an access token](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/how-to-manage-access-tokens) for the Playwright workspace resource.

3. Copy the workspace region endpoint from the **Workspace Details** page.

4. Assign the Contributor role to your project's managed identity on the Playwright workspace resource. Alternatively, [configure a custom role](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/how-to-manage-workspace-access#create-a-custom-role-for-restricted-tenants).

5. Create a connection in your Foundry project using the Playwright workspace region endpoint and Playwright workspace access token.

   1. Go to the [Foundry portal](https://ai.azure.com/nextgen) and select your project.
   2. Select **Operate** in the upper-right navigation, then select **Admin** in the left pane.
   3. Select your project name in the **All projects** list.
   4. Select **Add connection**, then select **Serverless** connection.
   5. Enter a name for the connection (for example, `playwright-workspace`).
   6. Set **Endpoint** to the Playwright workspace region endpoint. It starts with `wss://`.
      * For more information, see the Playwright documentation for [configuring the service endpoint](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/quickstart-run-end-to-end-tests?tabs=playwrightcli\&pivots=playwright-test-runner#configure-the-browser-endpoint).
   7. Set **Key** to your Playwright access token.
   8. Select **Add connection**.

## Code example

After you run a sample, verify the tool was called by using tracing in Microsoft Foundry. For guidance on validating tool invocation, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice). If you use streaming, you can also look for `browser_automation_preview_call` events.

<Callout type="note">
  * You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).
  * This article assumes you already created the Playwright workspace connection. See the prerequisites section.
</Callout>

<ZonePivot pivot="python">
  ## Use BrowserAutomationAgentTool with agents example

  The following Python example demonstrates how to create an AI agent with browser automation capabilities by using the `BrowserAutomationAgentTool` and synchronous Azure AI Projects client. The agent can navigate to websites, interact with web elements, and perform tasks such as searching for stock prices. For a complete working example, ensure you have the necessary environment variables set up as indicated in the code comments.

  ```python
  import os
  import json
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      BrowserAutomationAgentTool,
      BrowserAutomationToolParameters,
      BrowserAutomationToolConnectionParameters,
  )

  load_dotenv()

  project_client = AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  openai_client = project_client.get_openai_client()

  connection_id = os.environ["BROWSER_AUTOMATION_PROJECT_CONNECTION_ID"]

  tool = BrowserAutomationAgentTool(
      browser_automation_preview=BrowserAutomationToolParameters(
          connection=BrowserAutomationToolConnectionParameters(
              project_connection_id=connection_id,
          )
      )
  )

  with project_client:
      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are an Agent helping with browser automation tasks.
              You can answer questions, provide information, and assist with various tasks
              related to web browsing using the Browser Automation tool available to you.""",
              tools=[tool],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input="""
              Your goal is to report the percent of Microsoft year-to-date stock price change.
              To do that, go to the website finance.yahoo.com.
              At the top of the page, you will find a search bar.
              Enter the value 'MSFT', to get information about the Microsoft stock price.
              At the top of the resulting page you will see a default chart of Microsoft stock price.
              Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.""",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              item = event.item
              if item.type == "browser_automation_preview_call":  # TODO: support browser_automation_preview_call schema
                  arguments_str = getattr(item, "arguments", "{}")

                  # Parse the arguments string into a dictionary
                  arguments = json.loads(arguments_str)
                  query = arguments.get("query")

                  print(f"Call ID: {getattr(item, 'call_id')}")
                  print(f"Query arguments: {query}")
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, then sends a prompt that requires the agent to use the tool. It also processes streaming events so you can observe progress and tool calls.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.

  ### Expected output

  When you create the agent, you see output similar to:

  ```console
  Agent created (id: ..., name: ..., version: ...)
  ```

  During streaming, you might also see deltas and tool-call details. Output varies based on the website content and model behavior.
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Use BrowserAutomationAgentTool with agents example

  Before running this sample, complete the setup steps in [Set up Browser Automation](#set-up-browser-automation).

  The following C# example demonstrates how to create an AI agent with Browser Automation capabilities by using the `BrowserAutomationAgentTool` and synchronous Azure AI Projects client. The agent can navigate to websites, interact with web elements, and perform tasks such as searching for stock prices. The example uses synchronous programming model for simplicity. For an asynchronous version, see the [Sample for use of BrowserAutomationAgentTool and Agents](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample23_BrowserAutomationTool.md) sample in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Create the Agent client and read the required environment variables.
  // Note that Browser automation operations can take longer than usual
  // and require the request timeout to be at least 5 minutes.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var playwrightConnectionId = System.Environment.GetEnvironmentVariable("BROWSER_AUTOMATION_PROJECT_CONNECTION_ID");
  AIProjectClientOptions options = new()
  {
      NetworkTimeout = TimeSpan.FromMinutes(5)
  };
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential(), options: options);

  // Create the Browser Automation tool using the Playwright connection.
  BrowserAutomationPreviewTool playwrightTool = new(
      new BrowserAutomationToolParameters(
      new BrowserAutomationToolConnectionParameters(playwrightConnectionId)
      ));

  // Create the Agent version with the Browser Automation tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are an Agent helping with browser automation tasks.\n" +
      "You can answer questions, provide information, and assist with various tasks\n" +
      "related to web browsing using the Browser Automation tool available to you.",
      Tools = { playwrightTool }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create the response stream. Also set ToolChoice = ResponseToolChoice.CreateRequiredChoice()
  // on the ResponseCreationOptions to ensure the agent uses the Browser Automation tool.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  CreateResponseOptions responseOptions = new()
  {
      ToolChoice = ResponseToolChoice.CreateRequiredChoice(),
      StreamingEnabled = true,
      InputItems =
      {
          ResponseItem.CreateUserMessageItem("Your goal is to report the percent of Microsoft year-to-date stock price change.\n" +
              "To do that, go to the website finance.yahoo.com.\n" +
              "At the top of the page, you will find a search bar.\n" +
              "Enter the value 'MSFT', to get information about the Microsoft stock price.\n" +
              "At the top of the resulting page you will see a default chart of Microsoft stock price.\n" +
              "Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.")
      }
  };
  foreach (StreamingResponseUpdate update in responseClient.CreateResponseStreaming(options: responseOptions))
  {
      if (update is StreamingResponseCreatedUpdate createUpdate)
      {
          Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
      }
      else if (update is StreamingResponseOutputTextDeltaUpdate textDelta)
      {
          Console.WriteLine($"Delta: {textDelta.Delta}");
      }
      else if (update is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
      {
          Console.WriteLine($"Response done with full message: {textDoneUpdate.Text}");
      }
      else if (update is StreamingResponseErrorUpdate errorUpdate)
      {
          throw new InvalidOperationException($"The stream has failed with the error: {errorUpdate.Message}");
      }
  }

  // Delete the Agent version to clean up resources.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, sends a prompt that requires tool usage, and prints streaming updates as the agent works through the browser steps.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.
  * A Playwright connection created in your Foundry project.

  ### Expected output

  You see streaming progress messages, such as text deltas, and a completed response. The output varies based on the website content and model behavior.
</ZonePivot>

<ZonePivot pivot="rest">
  The following cURL sample demonstrates how to create an agent with Browser Automation tool and perform web browsing tasks using REST API.

  ```bash
  curl --request POST \
    --url "${FOUNDRY_PROJECT_ENDPOINT}/openai/responses?api-version=${API_VERSION}" \
    --header "Authorization: Bearer ${AGENT_TOKEN}" \
    --header "Content-Type: application/json" \
    --header "User-Agent: insomnia/11.6.1" \
    --data @- <<JSON
  {
    "model": "${FOUNDRY_MODEL_DEPLOYMENT_NAME}",
    "input": [
      {
        "role": "user",
        "content": [
          {
            "type": "input_text",
            "text": "Your goal is to report the percent of Microsoft year-to-date stock price change."
          },
          {
            "type": "input_text",
            "text": "Go to finance.yahoo.com, search for MSFT, select YTD on the chart, and report the percent value shown."
          }
        ]
      }
    ],
    "tools": [
      {
        "type": "browser_automation_preview",
        "browser_automation_preview": {
          "connection": {
            "project_connection_id": "${BROWSER_AUTOMATION_PROJECT_CONNECTION_ID}"
          }
        }
      }
    ]
  }
  JSON
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Use Browser Automation tool with agents example

  The following TypeScript sample demonstrates how to create an agent with Browser Automation tool, perform web browsing tasks, and process streaming responses with browser automation events. For a JavaScript version of this sample, see the [JavaScript sample for Browser Automation tool](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentBrowserAutomation.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const browserAutomationProjectConnectionId =
    process.env["BROWSER_AUTOMATION_PROJECT_CONNECTION_ID"] ||
    "<browser automation project connection id>";

  const handleBrowserCall = (item: any) => {
    // TODO: support browser_automation_preview_call schema
    const callId = item.call_id;
    const argumentsStr = item.arguments;

    // Parse the arguments string into a dictionary
    let query = null;
    if (argumentsStr && typeof argumentsStr === "string") {
      try {
        const argumentsObj = JSON.parse(argumentsStr);
        query = argumentsObj.query;
      } catch (e) {
        console.error("Failed to parse arguments:", e);
      }
    }

    console.log(`Call ID: ${callId ?? "None"}`);
    console.log(`Query arguments: ${query ?? "None"}`);
  };

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with Browser Automation tool...");

    const agent = await project.agents.createVersion("MyAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions: `You are an Agent helping with browser automation tasks.
              You can answer questions, provide information, and assist with various tasks
              related to web browsing using the Browser Automation tool available to you.`,
      // Define Browser Automation tool
      tools: [
        {
          type: "browser_automation_preview",
          browser_automation_preview: {
            connection: {
              project_connection_id: browserAutomationProjectConnectionId,
            },
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    console.log("\nSending browser automation request with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: `Your goal is to report the percent of Microsoft year-to-date stock price change.
              To do that, go to the website finance.yahoo.com.
              At the top of the page, you will find a search bar.
              Enter the value 'MSFT', to get information about the Microsoft stock price.
              At the top of the resulting page you will see a default chart of Microsoft stock price.
              Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.`,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (
        event.type === "response.output_item.done" ||
        event.type === "response.output_item.added"
      ) {
        const item = event.item as any;
        if (item.type === "browser_automation_preview_call") {
          handleBrowserCall(item);
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nBrowser Automation sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, sends a prompt that requires tool usage, and processes streaming events, including browser automation call events, as they arrive.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.

  ### Expected output

  You see an "Agent created ..." message, streaming text output, and optionally, browser call details when the tool is invoked. The output varies based on the website content and model behavior.
</ZonePivot>

## Limitations

* **Trusted sites only**: Use this tool only with sites you trust. Avoid pages that prompt for credentials, payments, or other sensitive actions.
* **Page volatility**: Web pages can change at any time. Your agent might fail if the page layout, labels, or navigation flows change. Build error handling into your workflows.
* **Complex single-page applications**: JavaScript-heavy SPAs with dynamic content might not render correctly.

## Cost considerations

This tool uses a Playwright workspace resource to run browser sessions. Review the Playwright workspace documentation for pricing and usage details.

## Troubleshooting

### The agent doesn't use the tool

* Confirm you created the agent with the Browser Automation tool enabled.
* In your request, require tool usage (for example, `tool_choice="required"`).
* Use tracing in Microsoft Foundry to confirm whether a tool call occurred. For guidance, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

### Connection or authorization errors

* Confirm `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID` matches the Playwright workspace connection resource ID in your project.
* Confirm the project identity has access to the Playwright workspace resource.
* If you recently rotated the Playwright access token, update the Foundry project connection key.

### Requests time out

Browser automation can take longer than typical requests.

* Increase the client timeout (the C# sample sets a 5-minute timeout).
* Reduce the scope of your prompt (for example, fewer pages and fewer interactions).

## Clean up

* Delete the agent version you created for testing.
* Revoke or rotate the Playwright access token if you no longer need it.
* Remove the project connection if it’s no longer required. For more information, see [Add a connection in Microsoft Foundry](../../../how-to/connections-add).

## Example scenarios

* Booking and reservations: Automate form filling and schedule confirmation across booking portals.

* Product discovery: Navigate ecommerce or review sites, search by criteria, and extract summaries.

## Transparency note

Review the [transparency note](../../../responsible-ai/agents/transparency-note#enabling-autonomous-actions-with-or-without-human-input-through-action-tools) when using this tool. The Browser Automation tool is a tool that can perform real-world browser tasks through natural language prompts, enabling automated browsing activities without human intervention.

Review the [responsible AI considerations](../../../responsible-ai/agents/transparency-note#considerations-when-choosing-a-use-case) when using this tool.

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Computer use tool for agents](computer-use)
* [Add a connection in Microsoft Foundry](../../../how-to/connections-add)
* [Quickstart: Create your first agent](../../../quickstarts/get-started-code)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

This article explains how to configure and use the Browser Automation tool with Foundry agents to automate web browsing workflows.

<Callout type="warning">
  The Browser Automation tool comes with significant security risks. Both errors in judgment by the AI and the presence of malicious or confusing instructions on web pages that the AI encounters can cause it to execute commands you or others don't intend. These actions can compromise the security of your or other users' browsers, computers, and any accounts to which the browser or AI has access, including personal, financial, or enterprise systems. By using the Browser Automation tool, you acknowledge that you bear responsibility and liability for any use of it and of any resulting agents you create with it. This responsibility extends to any other users to whom you make Browser Automation tool functionality available, including through resulting agents. Use the Browser Automation tool on low-privilege virtual machines with no access to sensitive data or critical resources.
</Callout>

For guidance on optimizing tool usage, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

In Microsoft Foundry, the Browser Automation tool enables you to perform real-world browser tasks through natural language prompts. When you use it with Foundry Agent Service, it creates isolated browser sessions in your provisioned Playwright workspace.

By using [Microsoft Playwright Workspaces](https://aka.ms/pww/docs/manage-workspaces), you can automate browser-based workflows such as searching, navigating, filling forms, and booking.

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## How it works

The interaction starts when the user sends a query to an agent connected to the Browser Automation tool. For example, *"Show me all available yoga classes this week from the following URL \<url>."* When the agent receives the request, Foundry Agent Service creates an isolated browser session using your provisioned Playwright workspace. Each session is sandboxed for privacy and security.

The browser performs Playwright-driven actions, such as navigating to relevant pages and applying filters or parameters based on user preferences (such as time, location, and instructor). By combining the model with Playwright, the model can parse HTML or XML into DOM documents, make decisions, and perform actions like selecting UI elements, typing, and navigating websites. Exercise caution when using this tool.

An example flow is:

1. A user sends a request to the model that includes a call to the Browser Automation tool with the URL you want to go to.

2. The Browser Automation tool receives a response from the model. If the response has action items, those items contain suggested actions to make progress toward the specified goal. For example, an action might be a screenshot so the model can assess the current state with an updated screenshot or click with X/Y coordinates indicating where the mouse should be moved.

3. The Browser Automation tool executes the action in a sandboxed environment.

4. After executing the action, the Browser Automation tool captures the updated state of the environment as a screenshot.

5. The tool sends a new request with the updated state, and repeats this loop until the model stops requesting actions or the user decides to stop.

   The Browser Automation tool supports multi-turn conversations, allowing the user to refine their request and complete a booking.

## Prerequisites

Before you begin, make sure you have:

* An Azure subscription. [Create one for free](https://azure.microsoft.com/free/).
* Contributor or Owner role on a resource group.
* A Foundry project with a configured endpoint.
* An AI model deployed in your project.
* A Playwright workspace resource.
* A project connection set up for your Playwright workspace.

For the SDK examples, set these environment variables:

* `FOUNDRY_PROJECT_ENDPOINT`: Your Foundry project endpoint URL.
* `FOUNDRY_MODEL_DEPLOYMENT_NAME`: Your deployed model name.
* `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`: The connection resource ID for the Playwright workspace connection.

Use the following format for your connection ID: `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`.

## Set up Browser Automation

1. Create a [Playwright Workspace](https://aka.ms/pww/docs/manage-workspaces) resource.

2. [Generate an access token](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/how-to-manage-access-tokens) for the Playwright workspace resource.

3. Copy the workspace region endpoint from the **Workspace Details** page.

4. Assign the Contributor role to your project's managed identity on the Playwright workspace resource. Alternatively, [configure a custom role](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/how-to-manage-workspace-access#create-a-custom-role-for-restricted-tenants).

5. Create a connection in your Foundry project using the Playwright workspace region endpoint and Playwright workspace access token.

   1. Go to the [Foundry portal](https://ai.azure.com/nextgen) and select your project.
   2. Select **Operate** in the upper-right navigation, then select **Admin** in the left pane.
   3. Select your project name in the **All projects** list.
   4. Select **Add connection**, then select **Serverless** connection.
   5. Enter a name for the connection (for example, `playwright-workspace`).
   6. Set **Endpoint** to the Playwright workspace region endpoint. It starts with `wss://`.
      * For more information, see the Playwright documentation for [configuring the service endpoint](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/quickstart-run-end-to-end-tests?tabs=playwrightcli\&pivots=playwright-test-runner#configure-the-browser-endpoint).
   7. Set **Key** to your Playwright access token.
   8. Select **Add connection**.

## Code example

After you run a sample, verify the tool was called by using tracing in Microsoft Foundry. For guidance on validating tool invocation, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice). If you use streaming, you can also look for `browser_automation_preview_call` events.

<Callout type="note">
  * You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).
  * This article assumes you already created the Playwright workspace connection. See the prerequisites section.
</Callout>

<ZonePivot pivot="python">
  ## Use BrowserAutomationAgentTool with agents example

  The following Python example demonstrates how to create an AI agent with browser automation capabilities by using the `BrowserAutomationAgentTool` and synchronous Azure AI Projects client. The agent can navigate to websites, interact with web elements, and perform tasks such as searching for stock prices. For a complete working example, ensure you have the necessary environment variables set up as indicated in the code comments.

  ```python
  import os
  import json
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      BrowserAutomationAgentTool,
      BrowserAutomationToolParameters,
      BrowserAutomationToolConnectionParameters,
  )

  load_dotenv()

  project_client = AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  openai_client = project_client.get_openai_client()

  connection_id = os.environ["BROWSER_AUTOMATION_PROJECT_CONNECTION_ID"]

  tool = BrowserAutomationAgentTool(
      browser_automation_preview=BrowserAutomationToolParameters(
          connection=BrowserAutomationToolConnectionParameters(
              project_connection_id=connection_id,
          )
      )
  )

  with project_client:
      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are an Agent helping with browser automation tasks.
              You can answer questions, provide information, and assist with various tasks
              related to web browsing using the Browser Automation tool available to you.""",
              tools=[tool],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input="""
              Your goal is to report the percent of Microsoft year-to-date stock price change.
              To do that, go to the website finance.yahoo.com.
              At the top of the page, you will find a search bar.
              Enter the value 'MSFT', to get information about the Microsoft stock price.
              At the top of the resulting page you will see a default chart of Microsoft stock price.
              Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.""",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              item = event.item
              if item.type == "browser_automation_preview_call":  # TODO: support browser_automation_preview_call schema
                  arguments_str = getattr(item, "arguments", "{}")

                  # Parse the arguments string into a dictionary
                  arguments = json.loads(arguments_str)
                  query = arguments.get("query")

                  print(f"Call ID: {getattr(item, 'call_id')}")
                  print(f"Query arguments: {query}")
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, then sends a prompt that requires the agent to use the tool. It also processes streaming events so you can observe progress and tool calls.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.

  ### Expected output

  When you create the agent, you see output similar to:

  ```console
  Agent created (id: ..., name: ..., version: ...)
  ```

  During streaming, you might also see deltas and tool-call details. Output varies based on the website content and model behavior.
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Use BrowserAutomationAgentTool with agents example

  Before running this sample, complete the setup steps in [Set up Browser Automation](#set-up-browser-automation).

  The following C# example demonstrates how to create an AI agent with Browser Automation capabilities by using the `BrowserAutomationAgentTool` and synchronous Azure AI Projects client. The agent can navigate to websites, interact with web elements, and perform tasks such as searching for stock prices. The example uses synchronous programming model for simplicity. For an asynchronous version, see the [Sample for use of BrowserAutomationAgentTool and Agents](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample23_BrowserAutomationTool.md) sample in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Create the Agent client and read the required environment variables.
  // Note that Browser automation operations can take longer than usual
  // and require the request timeout to be at least 5 minutes.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var playwrightConnectionId = System.Environment.GetEnvironmentVariable("BROWSER_AUTOMATION_PROJECT_CONNECTION_ID");
  AIProjectClientOptions options = new()
  {
      NetworkTimeout = TimeSpan.FromMinutes(5)
  };
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential(), options: options);

  // Create the Browser Automation tool using the Playwright connection.
  BrowserAutomationPreviewTool playwrightTool = new(
      new BrowserAutomationToolParameters(
      new BrowserAutomationToolConnectionParameters(playwrightConnectionId)
      ));

  // Create the Agent version with the Browser Automation tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are an Agent helping with browser automation tasks.\n" +
      "You can answer questions, provide information, and assist with various tasks\n" +
      "related to web browsing using the Browser Automation tool available to you.",
      Tools = { playwrightTool }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create the response stream. Also set ToolChoice = ResponseToolChoice.CreateRequiredChoice()
  // on the ResponseCreationOptions to ensure the agent uses the Browser Automation tool.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  CreateResponseOptions responseOptions = new()
  {
      ToolChoice = ResponseToolChoice.CreateRequiredChoice(),
      StreamingEnabled = true,
      InputItems =
      {
          ResponseItem.CreateUserMessageItem("Your goal is to report the percent of Microsoft year-to-date stock price change.\n" +
              "To do that, go to the website finance.yahoo.com.\n" +
              "At the top of the page, you will find a search bar.\n" +
              "Enter the value 'MSFT', to get information about the Microsoft stock price.\n" +
              "At the top of the resulting page you will see a default chart of Microsoft stock price.\n" +
              "Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.")
      }
  };
  foreach (StreamingResponseUpdate update in responseClient.CreateResponseStreaming(options: responseOptions))
  {
      if (update is StreamingResponseCreatedUpdate createUpdate)
      {
          Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
      }
      else if (update is StreamingResponseOutputTextDeltaUpdate textDelta)
      {
          Console.WriteLine($"Delta: {textDelta.Delta}");
      }
      else if (update is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
      {
          Console.WriteLine($"Response done with full message: {textDoneUpdate.Text}");
      }
      else if (update is StreamingResponseErrorUpdate errorUpdate)
      {
          throw new InvalidOperationException($"The stream has failed with the error: {errorUpdate.Message}");
      }
  }

  // Delete the Agent version to clean up resources.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, sends a prompt that requires tool usage, and prints streaming updates as the agent works through the browser steps.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.
  * A Playwright connection created in your Foundry project.

  ### Expected output

  You see streaming progress messages, such as text deltas, and a completed response. The output varies based on the website content and model behavior.
</ZonePivot>

<ZonePivot pivot="rest">
  The following cURL sample demonstrates how to create an agent with Browser Automation tool and perform web browsing tasks using REST API.

  ```bash
  curl --request POST \
    --url "${FOUNDRY_PROJECT_ENDPOINT}/openai/responses?api-version=${API_VERSION}" \
    --header "Authorization: Bearer ${AGENT_TOKEN}" \
    --header "Content-Type: application/json" \
    --header "User-Agent: insomnia/11.6.1" \
    --data @- <<JSON
  {
    "model": "${FOUNDRY_MODEL_DEPLOYMENT_NAME}",
    "input": [
      {
        "role": "user",
        "content": [
          {
            "type": "input_text",
            "text": "Your goal is to report the percent of Microsoft year-to-date stock price change."
          },
          {
            "type": "input_text",
            "text": "Go to finance.yahoo.com, search for MSFT, select YTD on the chart, and report the percent value shown."
          }
        ]
      }
    ],
    "tools": [
      {
        "type": "browser_automation_preview",
        "browser_automation_preview": {
          "connection": {
            "project_connection_id": "${BROWSER_AUTOMATION_PROJECT_CONNECTION_ID}"
          }
        }
      }
    ]
  }
  JSON
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Use Browser Automation tool with agents example

  The following TypeScript sample demonstrates how to create an agent with Browser Automation tool, perform web browsing tasks, and process streaming responses with browser automation events. For a JavaScript version of this sample, see the [JavaScript sample for Browser Automation tool](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentBrowserAutomation.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const browserAutomationProjectConnectionId =
    process.env["BROWSER_AUTOMATION_PROJECT_CONNECTION_ID"] ||
    "<browser automation project connection id>";

  const handleBrowserCall = (item: any) => {
    // TODO: support browser_automation_preview_call schema
    const callId = item.call_id;
    const argumentsStr = item.arguments;

    // Parse the arguments string into a dictionary
    let query = null;
    if (argumentsStr && typeof argumentsStr === "string") {
      try {
        const argumentsObj = JSON.parse(argumentsStr);
        query = argumentsObj.query;
      } catch (e) {
        console.error("Failed to parse arguments:", e);
      }
    }

    console.log(`Call ID: ${callId ?? "None"}`);
    console.log(`Query arguments: ${query ?? "None"}`);
  };

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with Browser Automation tool...");

    const agent = await project.agents.createVersion("MyAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions: `You are an Agent helping with browser automation tasks.
              You can answer questions, provide information, and assist with various tasks
              related to web browsing using the Browser Automation tool available to you.`,
      // Define Browser Automation tool
      tools: [
        {
          type: "browser_automation_preview",
          browser_automation_preview: {
            connection: {
              project_connection_id: browserAutomationProjectConnectionId,
            },
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    console.log("\nSending browser automation request with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: `Your goal is to report the percent of Microsoft year-to-date stock price change.
              To do that, go to the website finance.yahoo.com.
              At the top of the page, you will find a search bar.
              Enter the value 'MSFT', to get information about the Microsoft stock price.
              At the top of the resulting page you will see a default chart of Microsoft stock price.
              Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.`,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (
        event.type === "response.output_item.done" ||
        event.type === "response.output_item.added"
      ) {
        const item = event.item as any;
        if (item.type === "browser_automation_preview_call") {
          handleBrowserCall(item);
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nBrowser Automation sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, sends a prompt that requires tool usage, and processes streaming events, including browser automation call events, as they arrive.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.

  ### Expected output

  You see an "Agent created ..." message, streaming text output, and optionally, browser call details when the tool is invoked. The output varies based on the website content and model behavior.
</ZonePivot>

## Limitations

* **Trusted sites only**: Use this tool only with sites you trust. Avoid pages that prompt for credentials, payments, or other sensitive actions.
* **Page volatility**: Web pages can change at any time. Your agent might fail if the page layout, labels, or navigation flows change. Build error handling into your workflows.
* **Complex single-page applications**: JavaScript-heavy SPAs with dynamic content might not render correctly.

## Cost considerations

This tool uses a Playwright workspace resource to run browser sessions. Review the Playwright workspace documentation for pricing and usage details.

## Troubleshooting

### The agent doesn't use the tool

* Confirm you created the agent with the Browser Automation tool enabled.
* In your request, require tool usage (for example, `tool_choice="required"`).
* Use tracing in Microsoft Foundry to confirm whether a tool call occurred. For guidance, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

### Connection or authorization errors

* Confirm `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID` matches the Playwright workspace connection resource ID in your project.
* Confirm the project identity has access to the Playwright workspace resource.
* If you recently rotated the Playwright access token, update the Foundry project connection key.

### Requests time out

Browser automation can take longer than typical requests.

* Increase the client timeout (the C# sample sets a 5-minute timeout).
* Reduce the scope of your prompt (for example, fewer pages and fewer interactions).

## Clean up

* Delete the agent version you created for testing.
* Revoke or rotate the Playwright access token if you no longer need it.
* Remove the project connection if it’s no longer required. For more information, see [Add a connection in Microsoft Foundry](../../../how-to/connections-add).

## Example scenarios

* Booking and reservations: Automate form filling and schedule confirmation across booking portals.

* Product discovery: Navigate ecommerce or review sites, search by criteria, and extract summaries.

## Transparency note

Review the [transparency note](../../../responsible-ai/agents/transparency-note#enabling-autonomous-actions-with-or-without-human-input-through-action-tools) when using this tool. The Browser Automation tool is a tool that can perform real-world browser tasks through natural language prompts, enabling automated browsing activities without human intervention.

Review the [responsible AI considerations](../../../responsible-ai/agents/transparency-note#considerations-when-choosing-a-use-case) when using this tool.

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Computer use tool for agents](computer-use)
* [Add a connection in Microsoft Foundry](../../../how-to/connections-add)
* [Quickstart: Create your first agent](../../../quickstarts/get-started-code)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

This article explains how to configure and use the Browser Automation tool with Foundry agents to automate web browsing workflows.

<Callout type="warning">
  The Browser Automation tool comes with significant security risks. Both errors in judgment by the AI and the presence of malicious or confusing instructions on web pages that the AI encounters can cause it to execute commands you or others don't intend. These actions can compromise the security of your or other users' browsers, computers, and any accounts to which the browser or AI has access, including personal, financial, or enterprise systems. By using the Browser Automation tool, you acknowledge that you bear responsibility and liability for any use of it and of any resulting agents you create with it. This responsibility extends to any other users to whom you make Browser Automation tool functionality available, including through resulting agents. Use the Browser Automation tool on low-privilege virtual machines with no access to sensitive data or critical resources.
</Callout>

For guidance on optimizing tool usage, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

In Microsoft Foundry, the Browser Automation tool enables you to perform real-world browser tasks through natural language prompts. When you use it with Foundry Agent Service, it creates isolated browser sessions in your provisioned Playwright workspace.

By using [Microsoft Playwright Workspaces](https://aka.ms/pww/docs/manage-workspaces), you can automate browser-based workflows such as searching, navigating, filling forms, and booking.

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## How it works

The interaction starts when the user sends a query to an agent connected to the Browser Automation tool. For example, *"Show me all available yoga classes this week from the following URL \<url>."* When the agent receives the request, Foundry Agent Service creates an isolated browser session using your provisioned Playwright workspace. Each session is sandboxed for privacy and security.

The browser performs Playwright-driven actions, such as navigating to relevant pages and applying filters or parameters based on user preferences (such as time, location, and instructor). By combining the model with Playwright, the model can parse HTML or XML into DOM documents, make decisions, and perform actions like selecting UI elements, typing, and navigating websites. Exercise caution when using this tool.

An example flow is:

1. A user sends a request to the model that includes a call to the Browser Automation tool with the URL you want to go to.

2. The Browser Automation tool receives a response from the model. If the response has action items, those items contain suggested actions to make progress toward the specified goal. For example, an action might be a screenshot so the model can assess the current state with an updated screenshot or click with X/Y coordinates indicating where the mouse should be moved.

3. The Browser Automation tool executes the action in a sandboxed environment.

4. After executing the action, the Browser Automation tool captures the updated state of the environment as a screenshot.

5. The tool sends a new request with the updated state, and repeats this loop until the model stops requesting actions or the user decides to stop.

   The Browser Automation tool supports multi-turn conversations, allowing the user to refine their request and complete a booking.

## Prerequisites

Before you begin, make sure you have:

* An Azure subscription. [Create one for free](https://azure.microsoft.com/free/).
* Contributor or Owner role on a resource group.
* A Foundry project with a configured endpoint.
* An AI model deployed in your project.
* A Playwright workspace resource.
* A project connection set up for your Playwright workspace.

For the SDK examples, set these environment variables:

* `FOUNDRY_PROJECT_ENDPOINT`: Your Foundry project endpoint URL.
* `FOUNDRY_MODEL_DEPLOYMENT_NAME`: Your deployed model name.
* `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`: The connection resource ID for the Playwright workspace connection.

Use the following format for your connection ID: `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`.

## Set up Browser Automation

1. Create a [Playwright Workspace](https://aka.ms/pww/docs/manage-workspaces) resource.

2. [Generate an access token](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/how-to-manage-access-tokens) for the Playwright workspace resource.

3. Copy the workspace region endpoint from the **Workspace Details** page.

4. Assign the Contributor role to your project's managed identity on the Playwright workspace resource. Alternatively, [configure a custom role](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/how-to-manage-workspace-access#create-a-custom-role-for-restricted-tenants).

5. Create a connection in your Foundry project using the Playwright workspace region endpoint and Playwright workspace access token.

   1. Go to the [Foundry portal](https://ai.azure.com/nextgen) and select your project.
   2. Select **Operate** in the upper-right navigation, then select **Admin** in the left pane.
   3. Select your project name in the **All projects** list.
   4. Select **Add connection**, then select **Serverless** connection.
   5. Enter a name for the connection (for example, `playwright-workspace`).
   6. Set **Endpoint** to the Playwright workspace region endpoint. It starts with `wss://`.
      * For more information, see the Playwright documentation for [configuring the service endpoint](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/quickstart-run-end-to-end-tests?tabs=playwrightcli\&pivots=playwright-test-runner#configure-the-browser-endpoint).
   7. Set **Key** to your Playwright access token.
   8. Select **Add connection**.

## Code example

After you run a sample, verify the tool was called by using tracing in Microsoft Foundry. For guidance on validating tool invocation, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice). If you use streaming, you can also look for `browser_automation_preview_call` events.

<Callout type="note">
  * You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).
  * This article assumes you already created the Playwright workspace connection. See the prerequisites section.
</Callout>

<ZonePivot pivot="python">
  ## Use BrowserAutomationAgentTool with agents example

  The following Python example demonstrates how to create an AI agent with browser automation capabilities by using the `BrowserAutomationAgentTool` and synchronous Azure AI Projects client. The agent can navigate to websites, interact with web elements, and perform tasks such as searching for stock prices. For a complete working example, ensure you have the necessary environment variables set up as indicated in the code comments.

  ```python
  import os
  import json
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      BrowserAutomationAgentTool,
      BrowserAutomationToolParameters,
      BrowserAutomationToolConnectionParameters,
  )

  load_dotenv()

  project_client = AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  openai_client = project_client.get_openai_client()

  connection_id = os.environ["BROWSER_AUTOMATION_PROJECT_CONNECTION_ID"]

  tool = BrowserAutomationAgentTool(
      browser_automation_preview=BrowserAutomationToolParameters(
          connection=BrowserAutomationToolConnectionParameters(
              project_connection_id=connection_id,
          )
      )
  )

  with project_client:
      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are an Agent helping with browser automation tasks.
              You can answer questions, provide information, and assist with various tasks
              related to web browsing using the Browser Automation tool available to you.""",
              tools=[tool],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input="""
              Your goal is to report the percent of Microsoft year-to-date stock price change.
              To do that, go to the website finance.yahoo.com.
              At the top of the page, you will find a search bar.
              Enter the value 'MSFT', to get information about the Microsoft stock price.
              At the top of the resulting page you will see a default chart of Microsoft stock price.
              Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.""",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              item = event.item
              if item.type == "browser_automation_preview_call":  # TODO: support browser_automation_preview_call schema
                  arguments_str = getattr(item, "arguments", "{}")

                  # Parse the arguments string into a dictionary
                  arguments = json.loads(arguments_str)
                  query = arguments.get("query")

                  print(f"Call ID: {getattr(item, 'call_id')}")
                  print(f"Query arguments: {query}")
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, then sends a prompt that requires the agent to use the tool. It also processes streaming events so you can observe progress and tool calls.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.

  ### Expected output

  When you create the agent, you see output similar to:

  ```console
  Agent created (id: ..., name: ..., version: ...)
  ```

  During streaming, you might also see deltas and tool-call details. Output varies based on the website content and model behavior.
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Use BrowserAutomationAgentTool with agents example

  Before running this sample, complete the setup steps in [Set up Browser Automation](#set-up-browser-automation).

  The following C# example demonstrates how to create an AI agent with Browser Automation capabilities by using the `BrowserAutomationAgentTool` and synchronous Azure AI Projects client. The agent can navigate to websites, interact with web elements, and perform tasks such as searching for stock prices. The example uses synchronous programming model for simplicity. For an asynchronous version, see the [Sample for use of BrowserAutomationAgentTool and Agents](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample23_BrowserAutomationTool.md) sample in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Create the Agent client and read the required environment variables.
  // Note that Browser automation operations can take longer than usual
  // and require the request timeout to be at least 5 minutes.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var playwrightConnectionId = System.Environment.GetEnvironmentVariable("BROWSER_AUTOMATION_PROJECT_CONNECTION_ID");
  AIProjectClientOptions options = new()
  {
      NetworkTimeout = TimeSpan.FromMinutes(5)
  };
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential(), options: options);

  // Create the Browser Automation tool using the Playwright connection.
  BrowserAutomationPreviewTool playwrightTool = new(
      new BrowserAutomationToolParameters(
      new BrowserAutomationToolConnectionParameters(playwrightConnectionId)
      ));

  // Create the Agent version with the Browser Automation tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are an Agent helping with browser automation tasks.\n" +
      "You can answer questions, provide information, and assist with various tasks\n" +
      "related to web browsing using the Browser Automation tool available to you.",
      Tools = { playwrightTool }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create the response stream. Also set ToolChoice = ResponseToolChoice.CreateRequiredChoice()
  // on the ResponseCreationOptions to ensure the agent uses the Browser Automation tool.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  CreateResponseOptions responseOptions = new()
  {
      ToolChoice = ResponseToolChoice.CreateRequiredChoice(),
      StreamingEnabled = true,
      InputItems =
      {
          ResponseItem.CreateUserMessageItem("Your goal is to report the percent of Microsoft year-to-date stock price change.\n" +
              "To do that, go to the website finance.yahoo.com.\n" +
              "At the top of the page, you will find a search bar.\n" +
              "Enter the value 'MSFT', to get information about the Microsoft stock price.\n" +
              "At the top of the resulting page you will see a default chart of Microsoft stock price.\n" +
              "Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.")
      }
  };
  foreach (StreamingResponseUpdate update in responseClient.CreateResponseStreaming(options: responseOptions))
  {
      if (update is StreamingResponseCreatedUpdate createUpdate)
      {
          Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
      }
      else if (update is StreamingResponseOutputTextDeltaUpdate textDelta)
      {
          Console.WriteLine($"Delta: {textDelta.Delta}");
      }
      else if (update is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
      {
          Console.WriteLine($"Response done with full message: {textDoneUpdate.Text}");
      }
      else if (update is StreamingResponseErrorUpdate errorUpdate)
      {
          throw new InvalidOperationException($"The stream has failed with the error: {errorUpdate.Message}");
      }
  }

  // Delete the Agent version to clean up resources.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, sends a prompt that requires tool usage, and prints streaming updates as the agent works through the browser steps.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.
  * A Playwright connection created in your Foundry project.

  ### Expected output

  You see streaming progress messages, such as text deltas, and a completed response. The output varies based on the website content and model behavior.
</ZonePivot>

<ZonePivot pivot="rest">
  The following cURL sample demonstrates how to create an agent with Browser Automation tool and perform web browsing tasks using REST API.

  ```bash
  curl --request POST \
    --url "${FOUNDRY_PROJECT_ENDPOINT}/openai/responses?api-version=${API_VERSION}" \
    --header "Authorization: Bearer ${AGENT_TOKEN}" \
    --header "Content-Type: application/json" \
    --header "User-Agent: insomnia/11.6.1" \
    --data @- <<JSON
  {
    "model": "${FOUNDRY_MODEL_DEPLOYMENT_NAME}",
    "input": [
      {
        "role": "user",
        "content": [
          {
            "type": "input_text",
            "text": "Your goal is to report the percent of Microsoft year-to-date stock price change."
          },
          {
            "type": "input_text",
            "text": "Go to finance.yahoo.com, search for MSFT, select YTD on the chart, and report the percent value shown."
          }
        ]
      }
    ],
    "tools": [
      {
        "type": "browser_automation_preview",
        "browser_automation_preview": {
          "connection": {
            "project_connection_id": "${BROWSER_AUTOMATION_PROJECT_CONNECTION_ID}"
          }
        }
      }
    ]
  }
  JSON
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Use Browser Automation tool with agents example

  The following TypeScript sample demonstrates how to create an agent with Browser Automation tool, perform web browsing tasks, and process streaming responses with browser automation events. For a JavaScript version of this sample, see the [JavaScript sample for Browser Automation tool](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentBrowserAutomation.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const browserAutomationProjectConnectionId =
    process.env["BROWSER_AUTOMATION_PROJECT_CONNECTION_ID"] ||
    "<browser automation project connection id>";

  const handleBrowserCall = (item: any) => {
    // TODO: support browser_automation_preview_call schema
    const callId = item.call_id;
    const argumentsStr = item.arguments;

    // Parse the arguments string into a dictionary
    let query = null;
    if (argumentsStr && typeof argumentsStr === "string") {
      try {
        const argumentsObj = JSON.parse(argumentsStr);
        query = argumentsObj.query;
      } catch (e) {
        console.error("Failed to parse arguments:", e);
      }
    }

    console.log(`Call ID: ${callId ?? "None"}`);
    console.log(`Query arguments: ${query ?? "None"}`);
  };

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with Browser Automation tool...");

    const agent = await project.agents.createVersion("MyAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions: `You are an Agent helping with browser automation tasks.
              You can answer questions, provide information, and assist with various tasks
              related to web browsing using the Browser Automation tool available to you.`,
      // Define Browser Automation tool
      tools: [
        {
          type: "browser_automation_preview",
          browser_automation_preview: {
            connection: {
              project_connection_id: browserAutomationProjectConnectionId,
            },
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    console.log("\nSending browser automation request with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: `Your goal is to report the percent of Microsoft year-to-date stock price change.
              To do that, go to the website finance.yahoo.com.
              At the top of the page, you will find a search bar.
              Enter the value 'MSFT', to get information about the Microsoft stock price.
              At the top of the resulting page you will see a default chart of Microsoft stock price.
              Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.`,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (
        event.type === "response.output_item.done" ||
        event.type === "response.output_item.added"
      ) {
        const item = event.item as any;
        if (item.type === "browser_automation_preview_call") {
          handleBrowserCall(item);
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nBrowser Automation sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, sends a prompt that requires tool usage, and processes streaming events, including browser automation call events, as they arrive.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.

  ### Expected output

  You see an "Agent created ..." message, streaming text output, and optionally, browser call details when the tool is invoked. The output varies based on the website content and model behavior.
</ZonePivot>

## Limitations

* **Trusted sites only**: Use this tool only with sites you trust. Avoid pages that prompt for credentials, payments, or other sensitive actions.
* **Page volatility**: Web pages can change at any time. Your agent might fail if the page layout, labels, or navigation flows change. Build error handling into your workflows.
* **Complex single-page applications**: JavaScript-heavy SPAs with dynamic content might not render correctly.

## Cost considerations

This tool uses a Playwright workspace resource to run browser sessions. Review the Playwright workspace documentation for pricing and usage details.

## Troubleshooting

### The agent doesn't use the tool

* Confirm you created the agent with the Browser Automation tool enabled.
* In your request, require tool usage (for example, `tool_choice="required"`).
* Use tracing in Microsoft Foundry to confirm whether a tool call occurred. For guidance, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

### Connection or authorization errors

* Confirm `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID` matches the Playwright workspace connection resource ID in your project.
* Confirm the project identity has access to the Playwright workspace resource.
* If you recently rotated the Playwright access token, update the Foundry project connection key.

### Requests time out

Browser automation can take longer than typical requests.

* Increase the client timeout (the C# sample sets a 5-minute timeout).
* Reduce the scope of your prompt (for example, fewer pages and fewer interactions).

## Clean up

* Delete the agent version you created for testing.
* Revoke or rotate the Playwright access token if you no longer need it.
* Remove the project connection if it’s no longer required. For more information, see [Add a connection in Microsoft Foundry](../../../how-to/connections-add).

## Example scenarios

* Booking and reservations: Automate form filling and schedule confirmation across booking portals.

* Product discovery: Navigate ecommerce or review sites, search by criteria, and extract summaries.

## Transparency note

Review the [transparency note](../../../responsible-ai/agents/transparency-note#enabling-autonomous-actions-with-or-without-human-input-through-action-tools) when using this tool. The Browser Automation tool is a tool that can perform real-world browser tasks through natural language prompts, enabling automated browsing activities without human intervention.

Review the [responsible AI considerations](../../../responsible-ai/agents/transparency-note#considerations-when-choosing-a-use-case) when using this tool.

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Computer use tool for agents](computer-use)
* [Add a connection in Microsoft Foundry](../../../how-to/connections-add)
* [Quickstart: Create your first agent](../../../quickstarts/get-started-code)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

This article explains how to configure and use the Browser Automation tool with Foundry agents to automate web browsing workflows.

<Callout type="warning">
  The Browser Automation tool comes with significant security risks. Both errors in judgment by the AI and the presence of malicious or confusing instructions on web pages that the AI encounters can cause it to execute commands you or others don't intend. These actions can compromise the security of your or other users' browsers, computers, and any accounts to which the browser or AI has access, including personal, financial, or enterprise systems. By using the Browser Automation tool, you acknowledge that you bear responsibility and liability for any use of it and of any resulting agents you create with it. This responsibility extends to any other users to whom you make Browser Automation tool functionality available, including through resulting agents. Use the Browser Automation tool on low-privilege virtual machines with no access to sensitive data or critical resources.
</Callout>

For guidance on optimizing tool usage, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

In Microsoft Foundry, the Browser Automation tool enables you to perform real-world browser tasks through natural language prompts. When you use it with Foundry Agent Service, it creates isolated browser sessions in your provisioned Playwright workspace.

By using [Microsoft Playwright Workspaces](https://aka.ms/pww/docs/manage-workspaces), you can automate browser-based workflows such as searching, navigating, filling forms, and booking.

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## How it works

The interaction starts when the user sends a query to an agent connected to the Browser Automation tool. For example, *"Show me all available yoga classes this week from the following URL \<url>."* When the agent receives the request, Foundry Agent Service creates an isolated browser session using your provisioned Playwright workspace. Each session is sandboxed for privacy and security.

The browser performs Playwright-driven actions, such as navigating to relevant pages and applying filters or parameters based on user preferences (such as time, location, and instructor). By combining the model with Playwright, the model can parse HTML or XML into DOM documents, make decisions, and perform actions like selecting UI elements, typing, and navigating websites. Exercise caution when using this tool.

An example flow is:

1. A user sends a request to the model that includes a call to the Browser Automation tool with the URL you want to go to.

2. The Browser Automation tool receives a response from the model. If the response has action items, those items contain suggested actions to make progress toward the specified goal. For example, an action might be a screenshot so the model can assess the current state with an updated screenshot or click with X/Y coordinates indicating where the mouse should be moved.

3. The Browser Automation tool executes the action in a sandboxed environment.

4. After executing the action, the Browser Automation tool captures the updated state of the environment as a screenshot.

5. The tool sends a new request with the updated state, and repeats this loop until the model stops requesting actions or the user decides to stop.

   The Browser Automation tool supports multi-turn conversations, allowing the user to refine their request and complete a booking.

## Prerequisites

Before you begin, make sure you have:

* An Azure subscription. [Create one for free](https://azure.microsoft.com/free/).
* Contributor or Owner role on a resource group.
* A Foundry project with a configured endpoint.
* An AI model deployed in your project.
* A Playwright workspace resource.
* A project connection set up for your Playwright workspace.

For the SDK examples, set these environment variables:

* `FOUNDRY_PROJECT_ENDPOINT`: Your Foundry project endpoint URL.
* `FOUNDRY_MODEL_DEPLOYMENT_NAME`: Your deployed model name.
* `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`: The connection resource ID for the Playwright workspace connection.

Use the following format for your connection ID: `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`.

## Set up Browser Automation

1. Create a [Playwright Workspace](https://aka.ms/pww/docs/manage-workspaces) resource.

2. [Generate an access token](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/how-to-manage-access-tokens) for the Playwright workspace resource.

3. Copy the workspace region endpoint from the **Workspace Details** page.

4. Assign the Contributor role to your project's managed identity on the Playwright workspace resource. Alternatively, [configure a custom role](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/how-to-manage-workspace-access#create-a-custom-role-for-restricted-tenants).

5. Create a connection in your Foundry project using the Playwright workspace region endpoint and Playwright workspace access token.

   1. Go to the [Foundry portal](https://ai.azure.com/nextgen) and select your project.
   2. Select **Operate** in the upper-right navigation, then select **Admin** in the left pane.
   3. Select your project name in the **All projects** list.
   4. Select **Add connection**, then select **Serverless** connection.
   5. Enter a name for the connection (for example, `playwright-workspace`).
   6. Set **Endpoint** to the Playwright workspace region endpoint. It starts with `wss://`.
      * For more information, see the Playwright documentation for [configuring the service endpoint](https://learn.microsoft.com/en-us/azure/app-testing/playwright-workspaces/quickstart-run-end-to-end-tests?tabs=playwrightcli\&pivots=playwright-test-runner#configure-the-browser-endpoint).
   7. Set **Key** to your Playwright access token.
   8. Select **Add connection**.

## Code example

After you run a sample, verify the tool was called by using tracing in Microsoft Foundry. For guidance on validating tool invocation, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice). If you use streaming, you can also look for `browser_automation_preview_call` events.

<Callout type="note">
  * You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).
  * This article assumes you already created the Playwright workspace connection. See the prerequisites section.
</Callout>

<ZonePivot pivot="python">
  ## Use BrowserAutomationAgentTool with agents example

  The following Python example demonstrates how to create an AI agent with browser automation capabilities by using the `BrowserAutomationAgentTool` and synchronous Azure AI Projects client. The agent can navigate to websites, interact with web elements, and perform tasks such as searching for stock prices. For a complete working example, ensure you have the necessary environment variables set up as indicated in the code comments.

  ```python
  import os
  import json
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      BrowserAutomationAgentTool,
      BrowserAutomationToolParameters,
      BrowserAutomationToolConnectionParameters,
  )

  load_dotenv()

  project_client = AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  openai_client = project_client.get_openai_client()

  connection_id = os.environ["BROWSER_AUTOMATION_PROJECT_CONNECTION_ID"]

  tool = BrowserAutomationAgentTool(
      browser_automation_preview=BrowserAutomationToolParameters(
          connection=BrowserAutomationToolConnectionParameters(
              project_connection_id=connection_id,
          )
      )
  )

  with project_client:
      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are an Agent helping with browser automation tasks.
              You can answer questions, provide information, and assist with various tasks
              related to web browsing using the Browser Automation tool available to you.""",
              tools=[tool],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input="""
              Your goal is to report the percent of Microsoft year-to-date stock price change.
              To do that, go to the website finance.yahoo.com.
              At the top of the page, you will find a search bar.
              Enter the value 'MSFT', to get information about the Microsoft stock price.
              At the top of the resulting page you will see a default chart of Microsoft stock price.
              Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.""",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              item = event.item
              if item.type == "browser_automation_preview_call":  # TODO: support browser_automation_preview_call schema
                  arguments_str = getattr(item, "arguments", "{}")

                  # Parse the arguments string into a dictionary
                  arguments = json.loads(arguments_str)
                  query = arguments.get("query")

                  print(f"Call ID: {getattr(item, 'call_id')}")
                  print(f"Query arguments: {query}")
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, then sends a prompt that requires the agent to use the tool. It also processes streaming events so you can observe progress and tool calls.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.

  ### Expected output

  When you create the agent, you see output similar to:

  ```console
  Agent created (id: ..., name: ..., version: ...)
  ```

  During streaming, you might also see deltas and tool-call details. Output varies based on the website content and model behavior.
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Use BrowserAutomationAgentTool with agents example

  Before running this sample, complete the setup steps in [Set up Browser Automation](#set-up-browser-automation).

  The following C# example demonstrates how to create an AI agent with Browser Automation capabilities by using the `BrowserAutomationAgentTool` and synchronous Azure AI Projects client. The agent can navigate to websites, interact with web elements, and perform tasks such as searching for stock prices. The example uses synchronous programming model for simplicity. For an asynchronous version, see the [Sample for use of BrowserAutomationAgentTool and Agents](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample23_BrowserAutomationTool.md) sample in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Create the Agent client and read the required environment variables.
  // Note that Browser automation operations can take longer than usual
  // and require the request timeout to be at least 5 minutes.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var playwrightConnectionId = System.Environment.GetEnvironmentVariable("BROWSER_AUTOMATION_PROJECT_CONNECTION_ID");
  AIProjectClientOptions options = new()
  {
      NetworkTimeout = TimeSpan.FromMinutes(5)
  };
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential(), options: options);

  // Create the Browser Automation tool using the Playwright connection.
  BrowserAutomationPreviewTool playwrightTool = new(
      new BrowserAutomationToolParameters(
      new BrowserAutomationToolConnectionParameters(playwrightConnectionId)
      ));

  // Create the Agent version with the Browser Automation tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are an Agent helping with browser automation tasks.\n" +
      "You can answer questions, provide information, and assist with various tasks\n" +
      "related to web browsing using the Browser Automation tool available to you.",
      Tools = { playwrightTool }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create the response stream. Also set ToolChoice = ResponseToolChoice.CreateRequiredChoice()
  // on the ResponseCreationOptions to ensure the agent uses the Browser Automation tool.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  CreateResponseOptions responseOptions = new()
  {
      ToolChoice = ResponseToolChoice.CreateRequiredChoice(),
      StreamingEnabled = true,
      InputItems =
      {
          ResponseItem.CreateUserMessageItem("Your goal is to report the percent of Microsoft year-to-date stock price change.\n" +
              "To do that, go to the website finance.yahoo.com.\n" +
              "At the top of the page, you will find a search bar.\n" +
              "Enter the value 'MSFT', to get information about the Microsoft stock price.\n" +
              "At the top of the resulting page you will see a default chart of Microsoft stock price.\n" +
              "Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.")
      }
  };
  foreach (StreamingResponseUpdate update in responseClient.CreateResponseStreaming(options: responseOptions))
  {
      if (update is StreamingResponseCreatedUpdate createUpdate)
      {
          Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
      }
      else if (update is StreamingResponseOutputTextDeltaUpdate textDelta)
      {
          Console.WriteLine($"Delta: {textDelta.Delta}");
      }
      else if (update is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
      {
          Console.WriteLine($"Response done with full message: {textDoneUpdate.Text}");
      }
      else if (update is StreamingResponseErrorUpdate errorUpdate)
      {
          throw new InvalidOperationException($"The stream has failed with the error: {errorUpdate.Message}");
      }
  }

  // Delete the Agent version to clean up resources.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, sends a prompt that requires tool usage, and prints streaming updates as the agent works through the browser steps.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.
  * A Playwright connection created in your Foundry project.

  ### Expected output

  You see streaming progress messages, such as text deltas, and a completed response. The output varies based on the website content and model behavior.
</ZonePivot>

<ZonePivot pivot="rest">
  The following cURL sample demonstrates how to create an agent with Browser Automation tool and perform web browsing tasks using REST API.

  ```bash
  curl --request POST \
    --url "${FOUNDRY_PROJECT_ENDPOINT}/openai/responses?api-version=${API_VERSION}" \
    --header "Authorization: Bearer ${AGENT_TOKEN}" \
    --header "Content-Type: application/json" \
    --header "User-Agent: insomnia/11.6.1" \
    --data @- <<JSON
  {
    "model": "${FOUNDRY_MODEL_DEPLOYMENT_NAME}",
    "input": [
      {
        "role": "user",
        "content": [
          {
            "type": "input_text",
            "text": "Your goal is to report the percent of Microsoft year-to-date stock price change."
          },
          {
            "type": "input_text",
            "text": "Go to finance.yahoo.com, search for MSFT, select YTD on the chart, and report the percent value shown."
          }
        ]
      }
    ],
    "tools": [
      {
        "type": "browser_automation_preview",
        "browser_automation_preview": {
          "connection": {
            "project_connection_id": "${BROWSER_AUTOMATION_PROJECT_CONNECTION_ID}"
          }
        }
      }
    ]
  }
  JSON
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Use Browser Automation tool with agents example

  The following TypeScript sample demonstrates how to create an agent with Browser Automation tool, perform web browsing tasks, and process streaming responses with browser automation events. For a JavaScript version of this sample, see the [JavaScript sample for Browser Automation tool](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentBrowserAutomation.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const browserAutomationProjectConnectionId =
    process.env["BROWSER_AUTOMATION_PROJECT_CONNECTION_ID"] ||
    "<browser automation project connection id>";

  const handleBrowserCall = (item: any) => {
    // TODO: support browser_automation_preview_call schema
    const callId = item.call_id;
    const argumentsStr = item.arguments;

    // Parse the arguments string into a dictionary
    let query = null;
    if (argumentsStr && typeof argumentsStr === "string") {
      try {
        const argumentsObj = JSON.parse(argumentsStr);
        query = argumentsObj.query;
      } catch (e) {
        console.error("Failed to parse arguments:", e);
      }
    }

    console.log(`Call ID: ${callId ?? "None"}`);
    console.log(`Query arguments: ${query ?? "None"}`);
  };

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with Browser Automation tool...");

    const agent = await project.agents.createVersion("MyAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions: `You are an Agent helping with browser automation tasks.
              You can answer questions, provide information, and assist with various tasks
              related to web browsing using the Browser Automation tool available to you.`,
      // Define Browser Automation tool
      tools: [
        {
          type: "browser_automation_preview",
          browser_automation_preview: {
            connection: {
              project_connection_id: browserAutomationProjectConnectionId,
            },
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    console.log("\nSending browser automation request with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: `Your goal is to report the percent of Microsoft year-to-date stock price change.
              To do that, go to the website finance.yahoo.com.
              At the top of the page, you will find a search bar.
              Enter the value 'MSFT', to get information about the Microsoft stock price.
              At the top of the resulting page you will see a default chart of Microsoft stock price.
              Click on 'YTD' at the top of that chart, and report the percent value that shows up just below it.`,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (
        event.type === "response.output_item.done" ||
        event.type === "response.output_item.added"
      ) {
        const item = event.item as any;
        if (item.type === "browser_automation_preview_call") {
          handleBrowserCall(item);
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nBrowser Automation sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### What this code does

  This example creates an agent version with the Browser Automation tool enabled, sends a prompt that requires tool usage, and processes streaming events, including browser automation call events, as they arrive.

  ### Required inputs

  * Environment variables: `FOUNDRY_PROJECT_ENDPOINT`, `FOUNDRY_MODEL_DEPLOYMENT_NAME`, `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID`.

  ### Expected output

  You see an "Agent created ..." message, streaming text output, and optionally, browser call details when the tool is invoked. The output varies based on the website content and model behavior.
</ZonePivot>

## Limitations

* **Trusted sites only**: Use this tool only with sites you trust. Avoid pages that prompt for credentials, payments, or other sensitive actions.
* **Page volatility**: Web pages can change at any time. Your agent might fail if the page layout, labels, or navigation flows change. Build error handling into your workflows.
* **Complex single-page applications**: JavaScript-heavy SPAs with dynamic content might not render correctly.

## Cost considerations

This tool uses a Playwright workspace resource to run browser sessions. Review the Playwright workspace documentation for pricing and usage details.

## Troubleshooting

### The agent doesn't use the tool

* Confirm you created the agent with the Browser Automation tool enabled.
* In your request, require tool usage (for example, `tool_choice="required"`).
* Use tracing in Microsoft Foundry to confirm whether a tool call occurred. For guidance, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

### Connection or authorization errors

* Confirm `BROWSER_AUTOMATION_PROJECT_CONNECTION_ID` matches the Playwright workspace connection resource ID in your project.
* Confirm the project identity has access to the Playwright workspace resource.
* If you recently rotated the Playwright access token, update the Foundry project connection key.

### Requests time out

Browser automation can take longer than typical requests.

* Increase the client timeout (the C# sample sets a 5-minute timeout).
* Reduce the scope of your prompt (for example, fewer pages and fewer interactions).

## Clean up

* Delete the agent version you created for testing.
* Revoke or rotate the Playwright access token if you no longer need it.
* Remove the project connection if it’s no longer required. For more information, see [Add a connection in Microsoft Foundry](../../../how-to/connections-add).

## Example scenarios

* Booking and reservations: Automate form filling and schedule confirmation across booking portals.

* Product discovery: Navigate ecommerce or review sites, search by criteria, and extract summaries.

## Transparency note

Review the [transparency note](../../../responsible-ai/agents/transparency-note#enabling-autonomous-actions-with-or-without-human-input-through-action-tools) when using this tool. The Browser Automation tool is a tool that can perform real-world browser tasks through natural language prompts, enabling automated browsing activities without human intervention.

Review the [responsible AI considerations](../../../responsible-ai/agents/transparency-note#considerations-when-choosing-a-use-case) when using this tool.

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Computer use tool for agents](computer-use)
* [Add a connection in Microsoft Foundry](../../../how-to/connections-add)
* [Quickstart: Create your first agent](../../../quickstarts/get-started-code)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="warning">
  The computer use tool comes with significant security and privacy risks, including prompt injection attacks. For more information about intended uses, capabilities, limitations, risks, and considerations when choosing a use case, see the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview).
</Callout>

Create agents that interpret screenshots and automate UI interactions like clicking, typing, and scrolling. The computer use tool uses the `computer-use-preview` model to propose actions based on visual content, enabling agents to interact with desktop and browser applications through their user interfaces.

This guide shows how to integrate the computer use tool into an application loop (screenshot → action → screenshot) by using the latest SDKs.

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | -        | ✔️                | ✔️                   |

## Prerequisites

* An Azure subscription. [Create one for free](https://azure.microsoft.com/free/).

* A [basic or standard agent environment](../../environment-setup).

* The latest prerelease SDK package:

  * **Python**: `azure-ai-projects>=2.0.0b1`, `azure-identity`, `python-dotenv`
  * **C#/.NET**: `Azure.AI.Agents.Persistent` (prerelease)
  * **TypeScript**: `@azure/ai-projects` v2-beta, `@azure/identity`

* Access to the `computer-use-preview` model. See [Request access](#request-access) below.

* A virtual machine or sandboxed environment for safe testing. Don't run on machines with access to sensitive data.

### Environment variables

Set these environment variables before running the samples:

| Variable                        | Description                                        |
| ------------------------------- | -------------------------------------------------- |
| `FOUNDRY_PROJECT_ENDPOINT`      | Your Foundry project endpoint URL.                 |
| `FOUNDRY_MODEL_DEPLOYMENT_NAME` | Your `computer-use-preview` model deployment name. |

### Quick verification

Verify your authentication and project connection before running the full samples:

```python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from dotenv import load_dotenv

load_dotenv()

with (
    DefaultAzureCredential() as credential,
    AIProjectClient(endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"], credential=credential) as project_client,
):
    print("Connected to project.")
    # Verify you can access the OpenAI client
    openai_client = project_client.get_openai_client()
    print("OpenAI client ready.")
```

If this code runs without errors, your credentials and project endpoint are configured correctly.

## Run the maintained SDK samples (recommended)

The code snippets in this article focus on the agent and Responses API integration. For an end-to-end runnable sample that includes helper code and sample screenshots, use the SDK samples on GitHub.

* Python: [https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects/samples/agents/tools](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects/samples/agents/tools)
* TypeScript: [https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentComputerUse.js](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentComputerUse.js)
* .NET (computer use tool sample): [https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Agents.Persistent/samples/Sample33\_Computer\_Use.md](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Agents.Persistent/samples/Sample33_Computer_Use.md)

<Callout type="tip">
  The SDK samples include helper utilities for screenshot capture, action execution, and image encoding. Clone the repository or copy these files to your project before running the samples.
</Callout>

## Request access

To access the `computer-use-preview` model, you need to register. Microsoft grants access based on eligibility criteria. If you have access to other limited access models, you still need to request access for this model.

To request access, see the [application form](https://aka.ms/oai/cuaaccess).

After Microsoft grants access, you need to create a deployment for the model.

## Code samples

<Callout type="warning">
  Use the computer use tool on virtual machines with no access to sensitive data or critical resources. For more information about the intended uses, capabilities, limitations, risks, and considerations when choosing a use case, see the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview).
</Callout>

To run this code, you need the latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#install-and-authenticate) for details.

<ZonePivot pivot="python">
  ### Screenshot initialization for computer use tool execution

  The following code sample demonstrates how to create an agent version with the computer use tool, send an initial request with a screenshot, and perform multiple iterations to complete a task.

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import AgentReference, PromptAgentDefinition, ComputerUsePreviewTool

  # Import shared helper functions
  from computer_use_util import (
      SearchState,
      load_screenshot_assets,
      handle_computer_action_and_take_screenshot,
      print_final_output,
  )

  load_dotenv()

  """Main function to demonstrate Computer Use Agent functionality."""
  # Initialize state machine
  current_state = SearchState.INITIAL

  # Load screenshot assets
  try:
      screenshots = load_screenshot_assets()
      print("Successfully loaded screenshot assets")
  except FileNotFoundError:
      print("Failed to load required screenshot assets. Use the maintained SDK sample on GitHub to get the helper file and images.")
      exit(1)
  ```

  ### Create an agent version with the tool

  ```python
  project_client = AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  computer_use_tool = ComputerUsePreviewTool(display_width=1026, display_height=769, environment="windows")

  with project_client:
      agent = project_client.agents.create_version(
          agent_name="ComputerUseAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""
              You are a computer automation assistant.

              Be direct and efficient. When you reach the search results page, read and describe the actual search result titles and descriptions you can see.
              """,
              tools=[computer_use_tool],
          ),
          description="Computer automation agent with screen interaction capabilities.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")
  ```

  ### One iteration for the tool to process the screenshot and take the next step

  ```python
      openai_client = project_client.get_openai_client()

      # Initial request with screenshot - start with Bing search page
      print("Starting computer automation session (initial screenshot: cua_browser_search.png)...")
      response = openai_client.responses.create(
          input=[
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "input_text",
                          "text": "I need you to help me search for 'OpenAI news'. Please type 'OpenAI news' and submit the search. Once you see search results, the task is complete.",
                      },
                      {
                          "type": "input_image",
                          "image_url": screenshots["browser_search"]["url"],
                          "detail": "high",
                      },  # Start with Bing search page
                  ],
              }
          ],
          extra_body={"agent": AgentReference(name=agent.name).as_dict()},
          truncation="auto",
      )

      print(f"Initial response received (ID: {response.id})")
  ```

  ### Perform multiple iterations

  Make sure you review each iteration and action. The following code sample shows a basic API request. After you send the initial API request, perform a loop where your application code carries out the specified action. Send a screenshot with each turn so the model can evaluate the updated state of the environment. For an example integration for a similar API, see the [Azure OpenAI documentation](../../../openai/how-to/computer-use#playwright-integration).

  ```python

  max_iterations = 10  # Allow enough iterations for completion
  iteration = 0

  while True:
      if iteration >= max_iterations:
          print(f"\nReached maximum iterations ({max_iterations}). Stopping.")
          break

      iteration += 1
      print(f"\n--- Iteration {iteration} ---")

      # Check for computer calls in the response
      computer_calls = [item for item in response.output if item.type == "computer_call"]

      if not computer_calls:
          print_final_output(response)
          break

      # Process the first computer call
      computer_call = computer_calls[0]
      action = computer_call.action
      call_id = computer_call.call_id

      print(f"Processing computer call (ID: {call_id})")

      # Handle the action and get the screenshot info
      screenshot_info, current_state = handle_computer_action_and_take_screenshot(action, current_state, screenshots)

      print(f"Sending action result back to agent (using {screenshot_info['filename']})...")

      # Regular response with just the screenshot
      response = openai_client.responses.create(
          previous_response_id=response.id,
          input=[
              {
                  "call_id": call_id,
                  "type": "computer_call_output",
                  "output": {
                      "type": "computer_screenshot",
                      "image_url": screenshot_info["url"],
                  },
              }
          ],
          extra_body={"agent": AgentReference(name=agent.name).as_dict()},
          truncation="auto",
      )

      print(f"Follow-up response received (ID: {response.id})")
  ```

  ### Clean up

  ```python
  project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
  print("Agent deleted")
  ```

  ### Expected output

  The following example shows the expected output when running the previous code sample:

  ```console
  Successfully loaded screenshot assets
  Agent created (id: ..., name: ComputerUseAgent, version: 1)
  Starting computer automation session (initial screenshot: cua_browser_search.png)...
  Initial response received (ID: ...)
  --- Iteration 1 ---
  Processing computer call (ID: ...)
    Typing text "OpenAI news" - Simulating keyboard input
    -> Action processed: type
  Sending action result back to agent (using cua_search_typed.png)...
  Follow-up response received (ID: ...)
  --- Iteration 2 ---
  Processing computer call (ID: ...)
      Click at (512, 384) - Simulating click on UI element
      -> Assuming click on Search button when search field was populated, displaying results.
      -> Action processed: click
  Sending action result back to agent (using cua_search_results.png)...
  Follow-up response received (ID: ...)
  OpenAI news - Latest Updates
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample for use of an Agent with Computer Use tool

  The following C# code sample demonstrates how to create an agent version with the computer use tool, send an initial request with a screenshot, and perform multiple iterations to complete a task. To enable your agent to use the computer use tool, use `ResponseTool.CreateComputerTool()` when configuring the agent's tools. This example uses synchronous code. For asynchronous usage, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample10_ComputerUse.md) example in the Azure SDK for .NET repository on GitHub.

  ```csharp
  class ComputerUseDemo
  {


      // Read image files using `ReadImageFile` method.
      private static BinaryData ReadImageFile(string name, [CallerFilePath] string pth = "")
      {
          var dirName = Path.GetDirectoryName(pth) ?? "";
          return new BinaryData(File.ReadAllBytes(Path.Combine(dirName, name)));
      }

      // Create a helper method to parse the ComputerTool outputs and to respond
      // to Agents queries with new screenshots. Note that throughout
      // this sample the media type for image is set. Agents support `image/jpeg`,
      // `image/png`, `image/gif` and `image/webp` media types.
      private static string ProcessComputerUseCall(ComputerCallResponseItem item, string oldScreenshot)
      {
          string currentScreenshot = "browser_search";
          switch (item.Action.Kind)
          {
              case ComputerCallActionKind.Type:
                  Console.WriteLine($"  Typing text \"{item.Action.TypeText}\" - Simulating keyboard input");
                  currentScreenshot = "search_typed";
                  break;
              case ComputerCallActionKind.KeyPress:
                  HashSet<string> codes = new(item.Action.KeyPressKeyCodes);
                  if (codes.Contains("Return") || codes.Contains("ENTER"))
                  {
                      // If we have typed the value to the search field, go to search results.
                      if (string.Equals(oldScreenshot, "search_typed"))
                      {
                          Console.WriteLine("  -> Detected ENTER key press, when search field was populated, displaying results.");
                          currentScreenshot = "search_results";
                      }
                      else
                      {
                          Console.WriteLine("  -> Detected ENTER key press, on results or unpopulated search, do nothing.");
                          currentScreenshot = oldScreenshot;
                      }
                  }
                  else
                  {
                      Console.WriteLine($"  Key press: {item.Action.KeyPressKeyCodes.Aggregate("", (agg, next) => agg + "+" + next)} - Simulating key combination");
                  }
                  break;
              case ComputerCallActionKind.Click:
                  Console.WriteLine($"  Click at ({item.Action.ClickCoordinates.Value.X}, {item.Action.ClickCoordinates.Value.Y}) - Simulating click on UI element");
                  if (string.Equals(oldScreenshot, "search_typed"))
                  {
                      Console.WriteLine("  -> Assuming click on Search button when search field was populated, displaying results.");
                      currentScreenshot = "search_results";
                  }
                  else
                  {
                      Console.WriteLine("  -> Assuming click on Search on results or when search was not populated, do nothing.");
                      currentScreenshot = oldScreenshot;
                  }
                  break;
              case ComputerCallActionKind.Drag:
                  string pathStr = item.Action.DragPath.ToArray().Select(p => $"{p.X}, {p.Y}").Aggregate("", (agg, next) => $"{agg} -> {next}");
                  Console.WriteLine($"  Drag path: {pathStr} - Simulating drag operation");
                  break;
              case ComputerCallActionKind.Scroll:
                  Console.WriteLine($"  Scroll at ({item.Action.ScrollCoordinates.Value.X}, {item.Action.ScrollCoordinates.Value.Y}) - Simulating scroll action");
                  break;
              case ComputerCallActionKind.Screenshot:
                  Console.WriteLine("  Taking screenshot - Capturing current screen state");
                  break;
              default:
                  break;
          }
          Console.WriteLine($"  -> Action processed: {item.Action.Kind}");

          return currentScreenshot;
      }

      public static void Main()
      {
          // Create project client and read the environment variables, which will be used in the next steps.
          var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
          var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
          AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

          // Read in three example screenshots and place them into a dictionary.
          Dictionary<string, BinaryData> screenshots = new() {
              { "browser_search", ReadImageFile("Assets/cua_browser_search.png")},
              { "search_typed", ReadImageFile("Assets/cua_search_typed.png")},
              { "search_results", ReadImageFile("Assets/cua_search_results.png")},
          };

          // Create a PromptAgentDefinition with ComputerTool.
          PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
          {
              Instructions = "You are a computer automation assistant.\n\n" +
                              "Be direct and efficient. When you reach the search results page, read and describe the actual search result titles and descriptions you can see.",
              Tools = {
                  ResponseTool.CreateComputerTool(
                      environment: new ComputerToolEnvironment("windows"),
                      displayWidth: 1026,
                      displayHeight: 769
                  ),
              }
          };
          AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
              agentName: "myAgent",
              options: new(agentDefinition)
          );
          // Create an `ResponseResult` using `ResponseItem`, containing two `ResponseContentPart`:
          // one with the image and another with the text. In the loop, request Agent
          // while it is continuing to browse web. Finally, print the tool output message.
          ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
          CreateResponseOptions responseOptions = new()
          {
              TruncationMode = ResponseTruncationMode.Auto,
              InputItems =
              {
                  ResponseItem.CreateUserMessageItem(
                  [
                      ResponseContentPart.CreateInputTextPart("I need you to help me search for 'OpenAI news'. Please type 'OpenAI news' and submit the search. Once you see search results, the task is complete."),
                      ResponseContentPart.CreateInputImagePart(imageBytes: screenshots["browser_search"], imageBytesMediaType: "image/png", imageDetailLevel: ResponseImageDetailLevel.High)
                  ]),
              },
          };
          bool computerUseCalled = false;
          string currentScreenshot = "browser_search";
          int limitIteration = 10;
          ResponseResult response;
          do
          {
              response = responseClient.CreateResponse(responseOptions);
              computerUseCalled = false;
              responseOptions.InputItems.Clear();
              responseOptions.PreviousResponseId = response.Id;
              foreach (ResponseItem responseItem in response.OutputItems)
              {
                  responseOptions.InputItems.Add(responseItem);
                  if (responseItem is ComputerCallResponseItem computerCall)
                  {
                      currentScreenshot = ProcessComputerUseCall(computerCall, currentScreenshot);
                      responseOptions.InputItems.Add(ResponseItem.CreateComputerCallOutputItem(callId: computerCall.CallId, output: ComputerCallOutput.CreateScreenshotOutput(screenshotImageBytes: screenshots[currentScreenshot], screenshotImageBytesMediaType: "image/png")));
                      computerUseCalled = true;
                  }
              }
              limitIteration--;
          } while (computerUseCalled && limitIteration > 0);
          Console.WriteLine(response.GetOutputText());

          // Clean up resources by deleting Agent.
          projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
      }
  }
  ```

  ### Expected output

  The following example shows the expected output when running the previous code sample:

  ```console
  Agent created (id: ..., name: myAgent, version: 1)
  Starting computer automation session (initial screenshot: cua_browser_search.png)...
  Initial response received (ID: ...)
  --- Iteration 1 ---
  Processing computer call (ID: ...)
    Typing text "OpenAI news" - Simulating keyboard input
    -> Action processed: Type
  Sending action result back to agent (using cua_search_typed.png)...
  Follow-up response received (ID: ...)
  --- Iteration 2 ---
  Processing computer call (ID: ...)
    Click at (512, 384) - Simulating click on UI element
    -> Assuming click on Search button when search field was populated, displaying results.
    -> Action processed: Click
  Sending action result back to agent (using cua_search_results.png)...
  Follow-up response received (ID: ...)
  OpenAI news - Latest Updates
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Sample for use of an Agent with Computer Use tool

  The following TypeScript code sample demonstrates how to create an agent version with the computer use tool, send an initial request with a screenshot, and perform multiple iterations to complete a task. For a JavaScript example, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentComputerUse.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";
  import {
    SearchState,
    loadScreenshotAssets,
    handleComputerActionAndTakeScreenshot,
    printFinalOutput,
    type ComputerAction,
  } from "./computerUseUtil.js";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
      process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

  export async function main(): Promise<void> {
    // Initialize state machine
    let currentState = SearchState.INITIAL;

    // Load screenshot assets
    const screenshots = loadScreenshotAssets();
    console.log("Successfully loaded screenshot assets");

    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating Computer Use Agent...");
    const agent = await project.agents.createVersion("ComputerUseAgent", {
      kind: "prompt" as const,
      model: deploymentName,
      instructions: `
  You are a computer automation assistant.

  Be direct and efficient. When you reach the search results page, read and describe the actual search result titles and descriptions you can see.
      `.trim(),
      tools: [
        {
          type: "computer_use_preview",
          display_width: 1026,
          display_height: 769,
          environment: "windows" as const,
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Initial request with screenshot - start with Bing search page
    console.log(
      "Starting computer automation session (initial screenshot: cua_browser_search.png)...",
    );
    let response = await openAIClient.responses.create(
      {
        input: [
          {
            role: "user" as const,
            content: [
              {
                type: "input_text",
                text: "I need you to help me search for 'OpenAI news'. Please type 'OpenAI news' and submit the search. Once you see search results, the task is complete.",
              },
              {
                type: "input_image",
                image_url: screenshots.browser_search.url,
                detail: "high",
              },
            ],
          },
        ],
        truncation: "auto",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
      },
    );

    console.log(`Initial response received (ID: ${response.id})`);

    // Main interaction loop with deterministic completion
    const maxIterations = 10; // Allow enough iterations for completion
    let iteration = 0;

    while (iteration < maxIterations) {
      iteration++;
      console.log(`\n--- Iteration ${iteration} ---`);

      // Check for computer calls in the response
      const computerCalls = response.output.filter((item) => item.type === "computer_call");

      if (computerCalls.length === 0) {
        printFinalOutput({
          output: response.output,
          status: response.status ?? "",
        });
        break;
      }

      // Process the first computer call
      const computerCall = computerCalls[0];
      const action: ComputerAction = computerCall.action;
      const callId: string = computerCall.call_id;

      console.log(`Processing computer call (ID: ${callId})`);

      // Handle the action and get the screenshot info
      const [screenshotInfo, updatedState] = handleComputerActionAndTakeScreenshot(
        action,
        currentState,
        screenshots,
      );
      currentState = updatedState;

      console.log(`Sending action result back to agent (using ${screenshotInfo.filename})...`);
      // Regular response with just the screenshot
      response = await openAIClient.responses.create(
        {
          previous_response_id: response.id,
          input: [
            {
              call_id: callId,
              type: "computer_call_output",
              output: {
                type: "computer_screenshot",
                image_url: screenshotInfo.url,
              },
            },
          ],
          truncation: "auto",
        },
        {
          body: { agent: { name: agent.name, type: "agent_reference" } },
        },
      );

      console.log(`Follow-up response received (ID: ${response.id})`);
    }

    if (iteration >= maxIterations) {
      console.log(`\nReached maximum iterations (${maxIterations}). Stopping.`);
    }

    // Clean up resources
    console.log("\nCleaning up...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nComputer Use Agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  The following example shows the expected output when running the previous code sample:

  ```console
  Successfully loaded screenshot assets
  Creating Computer Use Agent...
  Agent created (id: ..., name: ComputerUseAgent, version: 1)
  Starting computer automation session (initial screenshot: cua_browser_search.png)...
  Initial response received (ID: ...)
  --- Iteration 1 ---
  Processing computer call (ID: ...)
    Typing text "OpenAI news" - Simulating keyboard input
    -> Action processed: type
  Sending action result back to agent (using cua_search_typed.png)...
  Follow-up response received (ID: ...)
  --- Iteration 2 ---
  Processing computer call (ID: ...)
      Click at (512, 384) - Simulating click on UI element
      -> Assuming click on Search button when search field was populated, displaying results.
      -> Action processed: click
  Sending action result back to agent (using cua_search_results.png)...
  Follow-up response received (ID: ...)
  OpenAI news - Latest Updates
  Cleaning up...
  Agent deleted
  Computer Use Agent sample completed!
  ```
</ZonePivot>

## What you can do with the computer use tool

After you integrate the request-and-response loop (screenshot -> action -> screenshot), the computer use tool can help an agent:

* Propose UI actions such as clicking, typing, scrolling, and requesting a new screenshot.
* Adapt to UI changes by re-evaluating the latest screenshot after each action.
* Work across browser and desktop UI, depending on how you host your sandboxed environment.

The tool doesn't directly control a device. Your application executes each requested action and returns an updated screenshot.

## Differences between browser automation and computer use

The following table lists some of the differences between the computer use tool and [browser automation](browser-automation) tool.

| Feature                             | Browser Automation                                                 | Computer use tool                                                                                     |
| ----------------------------------- | ------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------- |
| Model support                       | All GPT models                                                     | `Computer-use-preview` model only                                                                     |
| Can I visualize what's happening?   | No                                                                 | Yes                                                                                                   |
| How it understands the screen       | Parses the HTML or XML pages into DOM documents                    | Raw pixel data from screenshots                                                                       |
| How it acts                         | A list of actions provided by the model                            | Virtual keyboard and mouse                                                                            |
| Is it multistep?                    | Yes                                                                | Yes                                                                                                   |
| Interfaces                          | Browser                                                            | Computer and browser                                                                                  |
| Do I need to bring my own resource? | Your own Playwright resource with the keys stored as a connection. | No additional resource required but we highly recommend running this tool in a sandboxed environment. |

### When to use each tool

**Choose computer use when you need to**:

* Interact with desktop applications beyond the browser
* Visualize what the agent sees through screenshots
* Work in environments where DOM parsing isn't available

**Choose browser automation when you need to**:

* Perform web-only interactions without limited access requirements
* Use any GPT model (not limited to `computer-use-preview`)
* Avoid managing screenshot capture and action execution loops

## Regional support

To use the computer use tool, you need a [computer use model](../../../foundry-models/concepts/models-sold-directly-by-azure#computer-use-preview) deployment. The computer use model is available in the following regions:

| Region          | Status    |
| --------------- | --------- |
| `eastus2`       | Available |
| `swedencentral` | Available |
| `southindia`    | Available |

## Understanding the computer use integration

When working with the computer use tool, integrate it into your application by performing the following steps:

1. Send a request to the model that includes a call to the computer use tool, the display size, and the environment. You can also include a screenshot of the initial state of the environment in the first API request.

2. Receive a response from the model. If the response has action items, those items contain suggested actions to make progress toward the specified goal. For example, an action might be `screenshot` so the model can assess the current state with an updated screenshot, or `click` with X/Y coordinates indicating where the mouse should be moved.

3. Execute the action by using your application code on your computer or browser environment.

4. After executing the action, capture the updated state of the environment as a screenshot.

5. Send a new request with the updated state as a `tool_call_output`, and repeat this loop until the model stops requesting actions or you decide to stop.

   <Callout type="note">
     Before using the tool, set up an environment that can capture screenshots and execute the recommended actions by the agent. For safety reasons, use a sandboxed environment, such as Playwright.
   </Callout>

## Manage conversation history

Use the `previous_response_id` parameter to link the current request to the previous response. Use this parameter when you don't want to send the full conversation history with each call.

If you don't use this parameter, make sure to include all the items returned in the response output of the previous request in your inputs array. This requirement includes reasoning items if present.

## Safety checks and security considerations

<Callout type="warning">
  Computer use carries substantial security and privacy risks and user responsibility. Both errors in judgment by the AI and the presence of malicious or confusing instructions on web pages, desktops, or other operating environments that the AI encounters might cause it to execute commands you or others don't intend. These risks could compromise the security of your or other users’ browsers, computers, and any accounts to which AI has access, including personal, financial, or enterprise systems.

  Use the computer use tool on virtual machines with no access to sensitive data or critical resources. For more information about the intended uses, capabilities, limitations, risks, and considerations when choosing a use case, see the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview).
</Callout>

The API has safety checks to help protect against prompt injection and model mistakes. These checks include:

**Malicious instruction detection**: The system evaluates the screenshot image and checks if it contains adversarial content that might change the model's behavior.

**Irrelevant domain detection**: The system evaluates the `current_url` parameter (if provided) and checks if the current domain is relevant given the conversation history.

**Sensitive domain detection**: The system checks the `current_url` parameter (if provided) and raises a warning when it detects the user is on a sensitive domain.

If one or more of the preceding checks are triggered, the model raises a safety check when it returns the next `computer_call` by using the `pending_safety_checks` parameter.

```json
"output": [
    {
        "type": "reasoning",
        "id": "rs_67cb...",
        "summary": [
            {
                "type": "summary_text",
                "text": "Exploring 'File' menu option."
            }
        ]
    },
    {
        "type": "computer_call",
        "id": "cu_67cb...",
        "call_id": "call_nEJ...",
        "action": {
            "type": "click",
            "button": "left",
            "x": 135,
            "y": 193
        },
        "pending_safety_checks": [
            {
                "id": "cu_sc_67cb...",
                "code": "malicious_instructions",
                "message": "We've detected instructions that may cause your application to perform malicious or unauthorized actions. Please acknowledge this warning if you'd like to proceed."
            }
        ],
        "status": "completed"
    }
]
```

You need to pass the safety checks back as `acknowledged_safety_checks` in the next request to proceed.

```json
"input":[
        {
            "type": "computer_call_output",
            "call_id": "<call_id>",
            "acknowledged_safety_checks": [
                {
                    "id": "<safety_check_id>",
                    "code": "malicious_instructions",
                    "message": "We've detected instructions that may cause your application to perform malicious or unauthorized actions. Please acknowledge this warning if you'd like to proceed."
                }
            ],
            "output": {
                "type": "computer_screenshot",
                "image_url": "<image_url>"
            }
        }
    ]
```

## Safety check handling

In all cases where `pending_safety_checks` are returned, hand over actions to the end user to confirm proper model behavior and accuracy.

`malicious_instructions` and `irrelevant_domain`: End users should review model actions and confirm that the model behaves as intended.

`sensitive_domain`: Ensure an end user actively monitors the model actions on these sites. The exact implementation of this "watch mode" can vary by application, but a potential example could be collecting user impression data on the site to make sure there's active end user engagement with the application.

## Troubleshooting

| Issue                                                           | Cause                                                                                                                                           | Resolution                                                                                                                                                                   |
| --------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You don't see a `computer_call` in the response.                | The agent isn't configured with the computer use tool, the deployment isn't a computer use model, or the prompt doesn't require UI interaction. | Confirm the agent has a `computer_use_preview` tool, your deployment is the `computer-use-preview` model, and your prompt requires a UI action (type, click, or screenshot). |
| The sample code fails with missing helper files or screenshots. | The snippets reference helper utilities and sample images that aren't part of this documentation repo.                                          | Run the maintained SDK samples in the "Run the maintained SDK samples" section, or copy the helper file and sample images from the SDK repo into your project.               |
| The loop stops at the iteration limit.                          | The task needs more turns, or the app isn't applying the actions the model requests.                                                            | Increase the iteration limit, and verify that your code executes the requested action and sends a new screenshot after each turn.                                            |
| You receive `pending_safety_checks`.                            | The service detected a potential security risk (for example, prompt injection or a sensitive domain).                                           | Pause automation, require an end user to review the request, and only continue after you send `acknowledged_safety_checks` with the next `computer_call_output`.             |
| The model repeats "take a screenshot" without making progress.  | The screenshot isn't updating, is low quality, or doesn't show the relevant UI state.                                                           | Send a fresh screenshot after each action and use a higher-detail image when needed. Ensure the screenshot includes the relevant UI.                                         |
| Access denied when requesting `computer-use-preview` model.     | You haven't registered for access or access hasn't been granted.                                                                                | Submit the [application form](https://aka.ms/oai/cuaaccess) and wait for approval. Check your email for confirmation.                                                        |
| Screenshot encoding errors.                                     | Image format not supported or base64 encoding issue.                                                                                            | Use PNG or JPEG format. Ensure proper base64 encoding without corruption. Check image dimensions match `display_width` and `display_height`.                                 |
| Actions execute on wrong coordinates.                           | Screen resolution mismatch between screenshot and actual display.                                                                               | Ensure `display_width` and `display_height` in `ComputerUsePreviewTool` match your actual screen resolution.                                                                 |
| Model hallucinates UI elements.                                 | Screenshot quality too low or UI changed between turns.                                                                                         | Use higher resolution screenshots. Send fresh screenshots immediately after each action. Reduce delay between action and screenshot.                                         |

## Related content

* [Tool best practices for agents](../../concepts/tool-best-practice)
* [Browser automation tool](browser-automation)
* [Set up your agent environment](../../environment-setup)
* [Get started with Foundry agents](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true)
* [Computer use risks and limitations](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="warning">
  The computer use tool comes with significant security and privacy risks, including prompt injection attacks. For more information about intended uses, capabilities, limitations, risks, and considerations when choosing a use case, see the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview).
</Callout>

Create agents that interpret screenshots and automate UI interactions like clicking, typing, and scrolling. The computer use tool uses the `computer-use-preview` model to propose actions based on visual content, enabling agents to interact with desktop and browser applications through their user interfaces.

This guide shows how to integrate the computer use tool into an application loop (screenshot → action → screenshot) by using the latest SDKs.

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | -        | ✔️                | ✔️                   |

## Prerequisites

* An Azure subscription. [Create one for free](https://azure.microsoft.com/free/).

* A [basic or standard agent environment](../../environment-setup).

* The latest prerelease SDK package:

  * **Python**: `azure-ai-projects>=2.0.0b1`, `azure-identity`, `python-dotenv`
  * **C#/.NET**: `Azure.AI.Agents.Persistent` (prerelease)
  * **TypeScript**: `@azure/ai-projects` v2-beta, `@azure/identity`

* Access to the `computer-use-preview` model. See [Request access](#request-access) below.

* A virtual machine or sandboxed environment for safe testing. Don't run on machines with access to sensitive data.

### Environment variables

Set these environment variables before running the samples:

| Variable                        | Description                                        |
| ------------------------------- | -------------------------------------------------- |
| `FOUNDRY_PROJECT_ENDPOINT`      | Your Foundry project endpoint URL.                 |
| `FOUNDRY_MODEL_DEPLOYMENT_NAME` | Your `computer-use-preview` model deployment name. |

### Quick verification

Verify your authentication and project connection before running the full samples:

```python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from dotenv import load_dotenv

load_dotenv()

with (
    DefaultAzureCredential() as credential,
    AIProjectClient(endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"], credential=credential) as project_client,
):
    print("Connected to project.")
    # Verify you can access the OpenAI client
    openai_client = project_client.get_openai_client()
    print("OpenAI client ready.")
```

If this code runs without errors, your credentials and project endpoint are configured correctly.

## Run the maintained SDK samples (recommended)

The code snippets in this article focus on the agent and Responses API integration. For an end-to-end runnable sample that includes helper code and sample screenshots, use the SDK samples on GitHub.

* Python: [https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects/samples/agents/tools](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects/samples/agents/tools)
* TypeScript: [https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentComputerUse.js](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentComputerUse.js)
* .NET (computer use tool sample): [https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Agents.Persistent/samples/Sample33\_Computer\_Use.md](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Agents.Persistent/samples/Sample33_Computer_Use.md)

<Callout type="tip">
  The SDK samples include helper utilities for screenshot capture, action execution, and image encoding. Clone the repository or copy these files to your project before running the samples.
</Callout>

## Request access

To access the `computer-use-preview` model, you need to register. Microsoft grants access based on eligibility criteria. If you have access to other limited access models, you still need to request access for this model.

To request access, see the [application form](https://aka.ms/oai/cuaaccess).

After Microsoft grants access, you need to create a deployment for the model.

## Code samples

<Callout type="warning">
  Use the computer use tool on virtual machines with no access to sensitive data or critical resources. For more information about the intended uses, capabilities, limitations, risks, and considerations when choosing a use case, see the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview).
</Callout>

To run this code, you need the latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#install-and-authenticate) for details.

<ZonePivot pivot="python">
  ### Screenshot initialization for computer use tool execution

  The following code sample demonstrates how to create an agent version with the computer use tool, send an initial request with a screenshot, and perform multiple iterations to complete a task.

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import AgentReference, PromptAgentDefinition, ComputerUsePreviewTool

  # Import shared helper functions
  from computer_use_util import (
      SearchState,
      load_screenshot_assets,
      handle_computer_action_and_take_screenshot,
      print_final_output,
  )

  load_dotenv()

  """Main function to demonstrate Computer Use Agent functionality."""
  # Initialize state machine
  current_state = SearchState.INITIAL

  # Load screenshot assets
  try:
      screenshots = load_screenshot_assets()
      print("Successfully loaded screenshot assets")
  except FileNotFoundError:
      print("Failed to load required screenshot assets. Use the maintained SDK sample on GitHub to get the helper file and images.")
      exit(1)
  ```

  ### Create an agent version with the tool

  ```python
  project_client = AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  computer_use_tool = ComputerUsePreviewTool(display_width=1026, display_height=769, environment="windows")

  with project_client:
      agent = project_client.agents.create_version(
          agent_name="ComputerUseAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""
              You are a computer automation assistant.

              Be direct and efficient. When you reach the search results page, read and describe the actual search result titles and descriptions you can see.
              """,
              tools=[computer_use_tool],
          ),
          description="Computer automation agent with screen interaction capabilities.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")
  ```

  ### One iteration for the tool to process the screenshot and take the next step

  ```python
      openai_client = project_client.get_openai_client()

      # Initial request with screenshot - start with Bing search page
      print("Starting computer automation session (initial screenshot: cua_browser_search.png)...")
      response = openai_client.responses.create(
          input=[
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "input_text",
                          "text": "I need you to help me search for 'OpenAI news'. Please type 'OpenAI news' and submit the search. Once you see search results, the task is complete.",
                      },
                      {
                          "type": "input_image",
                          "image_url": screenshots["browser_search"]["url"],
                          "detail": "high",
                      },  # Start with Bing search page
                  ],
              }
          ],
          extra_body={"agent": AgentReference(name=agent.name).as_dict()},
          truncation="auto",
      )

      print(f"Initial response received (ID: {response.id})")
  ```

  ### Perform multiple iterations

  Make sure you review each iteration and action. The following code sample shows a basic API request. After you send the initial API request, perform a loop where your application code carries out the specified action. Send a screenshot with each turn so the model can evaluate the updated state of the environment. For an example integration for a similar API, see the [Azure OpenAI documentation](../../../openai/how-to/computer-use#playwright-integration).

  ```python

  max_iterations = 10  # Allow enough iterations for completion
  iteration = 0

  while True:
      if iteration >= max_iterations:
          print(f"\nReached maximum iterations ({max_iterations}). Stopping.")
          break

      iteration += 1
      print(f"\n--- Iteration {iteration} ---")

      # Check for computer calls in the response
      computer_calls = [item for item in response.output if item.type == "computer_call"]

      if not computer_calls:
          print_final_output(response)
          break

      # Process the first computer call
      computer_call = computer_calls[0]
      action = computer_call.action
      call_id = computer_call.call_id

      print(f"Processing computer call (ID: {call_id})")

      # Handle the action and get the screenshot info
      screenshot_info, current_state = handle_computer_action_and_take_screenshot(action, current_state, screenshots)

      print(f"Sending action result back to agent (using {screenshot_info['filename']})...")

      # Regular response with just the screenshot
      response = openai_client.responses.create(
          previous_response_id=response.id,
          input=[
              {
                  "call_id": call_id,
                  "type": "computer_call_output",
                  "output": {
                      "type": "computer_screenshot",
                      "image_url": screenshot_info["url"],
                  },
              }
          ],
          extra_body={"agent": AgentReference(name=agent.name).as_dict()},
          truncation="auto",
      )

      print(f"Follow-up response received (ID: {response.id})")
  ```

  ### Clean up

  ```python
  project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
  print("Agent deleted")
  ```

  ### Expected output

  The following example shows the expected output when running the previous code sample:

  ```console
  Successfully loaded screenshot assets
  Agent created (id: ..., name: ComputerUseAgent, version: 1)
  Starting computer automation session (initial screenshot: cua_browser_search.png)...
  Initial response received (ID: ...)
  --- Iteration 1 ---
  Processing computer call (ID: ...)
    Typing text "OpenAI news" - Simulating keyboard input
    -> Action processed: type
  Sending action result back to agent (using cua_search_typed.png)...
  Follow-up response received (ID: ...)
  --- Iteration 2 ---
  Processing computer call (ID: ...)
      Click at (512, 384) - Simulating click on UI element
      -> Assuming click on Search button when search field was populated, displaying results.
      -> Action processed: click
  Sending action result back to agent (using cua_search_results.png)...
  Follow-up response received (ID: ...)
  OpenAI news - Latest Updates
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample for use of an Agent with Computer Use tool

  The following C# code sample demonstrates how to create an agent version with the computer use tool, send an initial request with a screenshot, and perform multiple iterations to complete a task. To enable your agent to use the computer use tool, use `ResponseTool.CreateComputerTool()` when configuring the agent's tools. This example uses synchronous code. For asynchronous usage, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample10_ComputerUse.md) example in the Azure SDK for .NET repository on GitHub.

  ```csharp
  class ComputerUseDemo
  {


      // Read image files using `ReadImageFile` method.
      private static BinaryData ReadImageFile(string name, [CallerFilePath] string pth = "")
      {
          var dirName = Path.GetDirectoryName(pth) ?? "";
          return new BinaryData(File.ReadAllBytes(Path.Combine(dirName, name)));
      }

      // Create a helper method to parse the ComputerTool outputs and to respond
      // to Agents queries with new screenshots. Note that throughout
      // this sample the media type for image is set. Agents support `image/jpeg`,
      // `image/png`, `image/gif` and `image/webp` media types.
      private static string ProcessComputerUseCall(ComputerCallResponseItem item, string oldScreenshot)
      {
          string currentScreenshot = "browser_search";
          switch (item.Action.Kind)
          {
              case ComputerCallActionKind.Type:
                  Console.WriteLine($"  Typing text \"{item.Action.TypeText}\" - Simulating keyboard input");
                  currentScreenshot = "search_typed";
                  break;
              case ComputerCallActionKind.KeyPress:
                  HashSet<string> codes = new(item.Action.KeyPressKeyCodes);
                  if (codes.Contains("Return") || codes.Contains("ENTER"))
                  {
                      // If we have typed the value to the search field, go to search results.
                      if (string.Equals(oldScreenshot, "search_typed"))
                      {
                          Console.WriteLine("  -> Detected ENTER key press, when search field was populated, displaying results.");
                          currentScreenshot = "search_results";
                      }
                      else
                      {
                          Console.WriteLine("  -> Detected ENTER key press, on results or unpopulated search, do nothing.");
                          currentScreenshot = oldScreenshot;
                      }
                  }
                  else
                  {
                      Console.WriteLine($"  Key press: {item.Action.KeyPressKeyCodes.Aggregate("", (agg, next) => agg + "+" + next)} - Simulating key combination");
                  }
                  break;
              case ComputerCallActionKind.Click:
                  Console.WriteLine($"  Click at ({item.Action.ClickCoordinates.Value.X}, {item.Action.ClickCoordinates.Value.Y}) - Simulating click on UI element");
                  if (string.Equals(oldScreenshot, "search_typed"))
                  {
                      Console.WriteLine("  -> Assuming click on Search button when search field was populated, displaying results.");
                      currentScreenshot = "search_results";
                  }
                  else
                  {
                      Console.WriteLine("  -> Assuming click on Search on results or when search was not populated, do nothing.");
                      currentScreenshot = oldScreenshot;
                  }
                  break;
              case ComputerCallActionKind.Drag:
                  string pathStr = item.Action.DragPath.ToArray().Select(p => $"{p.X}, {p.Y}").Aggregate("", (agg, next) => $"{agg} -> {next}");
                  Console.WriteLine($"  Drag path: {pathStr} - Simulating drag operation");
                  break;
              case ComputerCallActionKind.Scroll:
                  Console.WriteLine($"  Scroll at ({item.Action.ScrollCoordinates.Value.X}, {item.Action.ScrollCoordinates.Value.Y}) - Simulating scroll action");
                  break;
              case ComputerCallActionKind.Screenshot:
                  Console.WriteLine("  Taking screenshot - Capturing current screen state");
                  break;
              default:
                  break;
          }
          Console.WriteLine($"  -> Action processed: {item.Action.Kind}");

          return currentScreenshot;
      }

      public static void Main()
      {
          // Create project client and read the environment variables, which will be used in the next steps.
          var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
          var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
          AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

          // Read in three example screenshots and place them into a dictionary.
          Dictionary<string, BinaryData> screenshots = new() {
              { "browser_search", ReadImageFile("Assets/cua_browser_search.png")},
              { "search_typed", ReadImageFile("Assets/cua_search_typed.png")},
              { "search_results", ReadImageFile("Assets/cua_search_results.png")},
          };

          // Create a PromptAgentDefinition with ComputerTool.
          PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
          {
              Instructions = "You are a computer automation assistant.\n\n" +
                              "Be direct and efficient. When you reach the search results page, read and describe the actual search result titles and descriptions you can see.",
              Tools = {
                  ResponseTool.CreateComputerTool(
                      environment: new ComputerToolEnvironment("windows"),
                      displayWidth: 1026,
                      displayHeight: 769
                  ),
              }
          };
          AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
              agentName: "myAgent",
              options: new(agentDefinition)
          );
          // Create an `ResponseResult` using `ResponseItem`, containing two `ResponseContentPart`:
          // one with the image and another with the text. In the loop, request Agent
          // while it is continuing to browse web. Finally, print the tool output message.
          ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
          CreateResponseOptions responseOptions = new()
          {
              TruncationMode = ResponseTruncationMode.Auto,
              InputItems =
              {
                  ResponseItem.CreateUserMessageItem(
                  [
                      ResponseContentPart.CreateInputTextPart("I need you to help me search for 'OpenAI news'. Please type 'OpenAI news' and submit the search. Once you see search results, the task is complete."),
                      ResponseContentPart.CreateInputImagePart(imageBytes: screenshots["browser_search"], imageBytesMediaType: "image/png", imageDetailLevel: ResponseImageDetailLevel.High)
                  ]),
              },
          };
          bool computerUseCalled = false;
          string currentScreenshot = "browser_search";
          int limitIteration = 10;
          ResponseResult response;
          do
          {
              response = responseClient.CreateResponse(responseOptions);
              computerUseCalled = false;
              responseOptions.InputItems.Clear();
              responseOptions.PreviousResponseId = response.Id;
              foreach (ResponseItem responseItem in response.OutputItems)
              {
                  responseOptions.InputItems.Add(responseItem);
                  if (responseItem is ComputerCallResponseItem computerCall)
                  {
                      currentScreenshot = ProcessComputerUseCall(computerCall, currentScreenshot);
                      responseOptions.InputItems.Add(ResponseItem.CreateComputerCallOutputItem(callId: computerCall.CallId, output: ComputerCallOutput.CreateScreenshotOutput(screenshotImageBytes: screenshots[currentScreenshot], screenshotImageBytesMediaType: "image/png")));
                      computerUseCalled = true;
                  }
              }
              limitIteration--;
          } while (computerUseCalled && limitIteration > 0);
          Console.WriteLine(response.GetOutputText());

          // Clean up resources by deleting Agent.
          projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
      }
  }
  ```

  ### Expected output

  The following example shows the expected output when running the previous code sample:

  ```console
  Agent created (id: ..., name: myAgent, version: 1)
  Starting computer automation session (initial screenshot: cua_browser_search.png)...
  Initial response received (ID: ...)
  --- Iteration 1 ---
  Processing computer call (ID: ...)
    Typing text "OpenAI news" - Simulating keyboard input
    -> Action processed: Type
  Sending action result back to agent (using cua_search_typed.png)...
  Follow-up response received (ID: ...)
  --- Iteration 2 ---
  Processing computer call (ID: ...)
    Click at (512, 384) - Simulating click on UI element
    -> Assuming click on Search button when search field was populated, displaying results.
    -> Action processed: Click
  Sending action result back to agent (using cua_search_results.png)...
  Follow-up response received (ID: ...)
  OpenAI news - Latest Updates
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Sample for use of an Agent with Computer Use tool

  The following TypeScript code sample demonstrates how to create an agent version with the computer use tool, send an initial request with a screenshot, and perform multiple iterations to complete a task. For a JavaScript example, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentComputerUse.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";
  import {
    SearchState,
    loadScreenshotAssets,
    handleComputerActionAndTakeScreenshot,
    printFinalOutput,
    type ComputerAction,
  } from "./computerUseUtil.js";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
      process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

  export async function main(): Promise<void> {
    // Initialize state machine
    let currentState = SearchState.INITIAL;

    // Load screenshot assets
    const screenshots = loadScreenshotAssets();
    console.log("Successfully loaded screenshot assets");

    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating Computer Use Agent...");
    const agent = await project.agents.createVersion("ComputerUseAgent", {
      kind: "prompt" as const,
      model: deploymentName,
      instructions: `
  You are a computer automation assistant.

  Be direct and efficient. When you reach the search results page, read and describe the actual search result titles and descriptions you can see.
      `.trim(),
      tools: [
        {
          type: "computer_use_preview",
          display_width: 1026,
          display_height: 769,
          environment: "windows" as const,
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Initial request with screenshot - start with Bing search page
    console.log(
      "Starting computer automation session (initial screenshot: cua_browser_search.png)...",
    );
    let response = await openAIClient.responses.create(
      {
        input: [
          {
            role: "user" as const,
            content: [
              {
                type: "input_text",
                text: "I need you to help me search for 'OpenAI news'. Please type 'OpenAI news' and submit the search. Once you see search results, the task is complete.",
              },
              {
                type: "input_image",
                image_url: screenshots.browser_search.url,
                detail: "high",
              },
            ],
          },
        ],
        truncation: "auto",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
      },
    );

    console.log(`Initial response received (ID: ${response.id})`);

    // Main interaction loop with deterministic completion
    const maxIterations = 10; // Allow enough iterations for completion
    let iteration = 0;

    while (iteration < maxIterations) {
      iteration++;
      console.log(`\n--- Iteration ${iteration} ---`);

      // Check for computer calls in the response
      const computerCalls = response.output.filter((item) => item.type === "computer_call");

      if (computerCalls.length === 0) {
        printFinalOutput({
          output: response.output,
          status: response.status ?? "",
        });
        break;
      }

      // Process the first computer call
      const computerCall = computerCalls[0];
      const action: ComputerAction = computerCall.action;
      const callId: string = computerCall.call_id;

      console.log(`Processing computer call (ID: ${callId})`);

      // Handle the action and get the screenshot info
      const [screenshotInfo, updatedState] = handleComputerActionAndTakeScreenshot(
        action,
        currentState,
        screenshots,
      );
      currentState = updatedState;

      console.log(`Sending action result back to agent (using ${screenshotInfo.filename})...`);
      // Regular response with just the screenshot
      response = await openAIClient.responses.create(
        {
          previous_response_id: response.id,
          input: [
            {
              call_id: callId,
              type: "computer_call_output",
              output: {
                type: "computer_screenshot",
                image_url: screenshotInfo.url,
              },
            },
          ],
          truncation: "auto",
        },
        {
          body: { agent: { name: agent.name, type: "agent_reference" } },
        },
      );

      console.log(`Follow-up response received (ID: ${response.id})`);
    }

    if (iteration >= maxIterations) {
      console.log(`\nReached maximum iterations (${maxIterations}). Stopping.`);
    }

    // Clean up resources
    console.log("\nCleaning up...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nComputer Use Agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  The following example shows the expected output when running the previous code sample:

  ```console
  Successfully loaded screenshot assets
  Creating Computer Use Agent...
  Agent created (id: ..., name: ComputerUseAgent, version: 1)
  Starting computer automation session (initial screenshot: cua_browser_search.png)...
  Initial response received (ID: ...)
  --- Iteration 1 ---
  Processing computer call (ID: ...)
    Typing text "OpenAI news" - Simulating keyboard input
    -> Action processed: type
  Sending action result back to agent (using cua_search_typed.png)...
  Follow-up response received (ID: ...)
  --- Iteration 2 ---
  Processing computer call (ID: ...)
      Click at (512, 384) - Simulating click on UI element
      -> Assuming click on Search button when search field was populated, displaying results.
      -> Action processed: click
  Sending action result back to agent (using cua_search_results.png)...
  Follow-up response received (ID: ...)
  OpenAI news - Latest Updates
  Cleaning up...
  Agent deleted
  Computer Use Agent sample completed!
  ```
</ZonePivot>

## What you can do with the computer use tool

After you integrate the request-and-response loop (screenshot -> action -> screenshot), the computer use tool can help an agent:

* Propose UI actions such as clicking, typing, scrolling, and requesting a new screenshot.
* Adapt to UI changes by re-evaluating the latest screenshot after each action.
* Work across browser and desktop UI, depending on how you host your sandboxed environment.

The tool doesn't directly control a device. Your application executes each requested action and returns an updated screenshot.

## Differences between browser automation and computer use

The following table lists some of the differences between the computer use tool and [browser automation](browser-automation) tool.

| Feature                             | Browser Automation                                                 | Computer use tool                                                                                     |
| ----------------------------------- | ------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------- |
| Model support                       | All GPT models                                                     | `Computer-use-preview` model only                                                                     |
| Can I visualize what's happening?   | No                                                                 | Yes                                                                                                   |
| How it understands the screen       | Parses the HTML or XML pages into DOM documents                    | Raw pixel data from screenshots                                                                       |
| How it acts                         | A list of actions provided by the model                            | Virtual keyboard and mouse                                                                            |
| Is it multistep?                    | Yes                                                                | Yes                                                                                                   |
| Interfaces                          | Browser                                                            | Computer and browser                                                                                  |
| Do I need to bring my own resource? | Your own Playwright resource with the keys stored as a connection. | No additional resource required but we highly recommend running this tool in a sandboxed environment. |

### When to use each tool

**Choose computer use when you need to**:

* Interact with desktop applications beyond the browser
* Visualize what the agent sees through screenshots
* Work in environments where DOM parsing isn't available

**Choose browser automation when you need to**:

* Perform web-only interactions without limited access requirements
* Use any GPT model (not limited to `computer-use-preview`)
* Avoid managing screenshot capture and action execution loops

## Regional support

To use the computer use tool, you need a [computer use model](../../../foundry-models/concepts/models-sold-directly-by-azure#computer-use-preview) deployment. The computer use model is available in the following regions:

| Region          | Status    |
| --------------- | --------- |
| `eastus2`       | Available |
| `swedencentral` | Available |
| `southindia`    | Available |

## Understanding the computer use integration

When working with the computer use tool, integrate it into your application by performing the following steps:

1. Send a request to the model that includes a call to the computer use tool, the display size, and the environment. You can also include a screenshot of the initial state of the environment in the first API request.

2. Receive a response from the model. If the response has action items, those items contain suggested actions to make progress toward the specified goal. For example, an action might be `screenshot` so the model can assess the current state with an updated screenshot, or `click` with X/Y coordinates indicating where the mouse should be moved.

3. Execute the action by using your application code on your computer or browser environment.

4. After executing the action, capture the updated state of the environment as a screenshot.

5. Send a new request with the updated state as a `tool_call_output`, and repeat this loop until the model stops requesting actions or you decide to stop.

   <Callout type="note">
     Before using the tool, set up an environment that can capture screenshots and execute the recommended actions by the agent. For safety reasons, use a sandboxed environment, such as Playwright.
   </Callout>

## Manage conversation history

Use the `previous_response_id` parameter to link the current request to the previous response. Use this parameter when you don't want to send the full conversation history with each call.

If you don't use this parameter, make sure to include all the items returned in the response output of the previous request in your inputs array. This requirement includes reasoning items if present.

## Safety checks and security considerations

<Callout type="warning">
  Computer use carries substantial security and privacy risks and user responsibility. Both errors in judgment by the AI and the presence of malicious or confusing instructions on web pages, desktops, or other operating environments that the AI encounters might cause it to execute commands you or others don't intend. These risks could compromise the security of your or other users’ browsers, computers, and any accounts to which AI has access, including personal, financial, or enterprise systems.

  Use the computer use tool on virtual machines with no access to sensitive data or critical resources. For more information about the intended uses, capabilities, limitations, risks, and considerations when choosing a use case, see the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview).
</Callout>

The API has safety checks to help protect against prompt injection and model mistakes. These checks include:

**Malicious instruction detection**: The system evaluates the screenshot image and checks if it contains adversarial content that might change the model's behavior.

**Irrelevant domain detection**: The system evaluates the `current_url` parameter (if provided) and checks if the current domain is relevant given the conversation history.

**Sensitive domain detection**: The system checks the `current_url` parameter (if provided) and raises a warning when it detects the user is on a sensitive domain.

If one or more of the preceding checks are triggered, the model raises a safety check when it returns the next `computer_call` by using the `pending_safety_checks` parameter.

```json
"output": [
    {
        "type": "reasoning",
        "id": "rs_67cb...",
        "summary": [
            {
                "type": "summary_text",
                "text": "Exploring 'File' menu option."
            }
        ]
    },
    {
        "type": "computer_call",
        "id": "cu_67cb...",
        "call_id": "call_nEJ...",
        "action": {
            "type": "click",
            "button": "left",
            "x": 135,
            "y": 193
        },
        "pending_safety_checks": [
            {
                "id": "cu_sc_67cb...",
                "code": "malicious_instructions",
                "message": "We've detected instructions that may cause your application to perform malicious or unauthorized actions. Please acknowledge this warning if you'd like to proceed."
            }
        ],
        "status": "completed"
    }
]
```

You need to pass the safety checks back as `acknowledged_safety_checks` in the next request to proceed.

```json
"input":[
        {
            "type": "computer_call_output",
            "call_id": "<call_id>",
            "acknowledged_safety_checks": [
                {
                    "id": "<safety_check_id>",
                    "code": "malicious_instructions",
                    "message": "We've detected instructions that may cause your application to perform malicious or unauthorized actions. Please acknowledge this warning if you'd like to proceed."
                }
            ],
            "output": {
                "type": "computer_screenshot",
                "image_url": "<image_url>"
            }
        }
    ]
```

## Safety check handling

In all cases where `pending_safety_checks` are returned, hand over actions to the end user to confirm proper model behavior and accuracy.

`malicious_instructions` and `irrelevant_domain`: End users should review model actions and confirm that the model behaves as intended.

`sensitive_domain`: Ensure an end user actively monitors the model actions on these sites. The exact implementation of this "watch mode" can vary by application, but a potential example could be collecting user impression data on the site to make sure there's active end user engagement with the application.

## Troubleshooting

| Issue                                                           | Cause                                                                                                                                           | Resolution                                                                                                                                                                   |
| --------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You don't see a `computer_call` in the response.                | The agent isn't configured with the computer use tool, the deployment isn't a computer use model, or the prompt doesn't require UI interaction. | Confirm the agent has a `computer_use_preview` tool, your deployment is the `computer-use-preview` model, and your prompt requires a UI action (type, click, or screenshot). |
| The sample code fails with missing helper files or screenshots. | The snippets reference helper utilities and sample images that aren't part of this documentation repo.                                          | Run the maintained SDK samples in the "Run the maintained SDK samples" section, or copy the helper file and sample images from the SDK repo into your project.               |
| The loop stops at the iteration limit.                          | The task needs more turns, or the app isn't applying the actions the model requests.                                                            | Increase the iteration limit, and verify that your code executes the requested action and sends a new screenshot after each turn.                                            |
| You receive `pending_safety_checks`.                            | The service detected a potential security risk (for example, prompt injection or a sensitive domain).                                           | Pause automation, require an end user to review the request, and only continue after you send `acknowledged_safety_checks` with the next `computer_call_output`.             |
| The model repeats "take a screenshot" without making progress.  | The screenshot isn't updating, is low quality, or doesn't show the relevant UI state.                                                           | Send a fresh screenshot after each action and use a higher-detail image when needed. Ensure the screenshot includes the relevant UI.                                         |
| Access denied when requesting `computer-use-preview` model.     | You haven't registered for access or access hasn't been granted.                                                                                | Submit the [application form](https://aka.ms/oai/cuaaccess) and wait for approval. Check your email for confirmation.                                                        |
| Screenshot encoding errors.                                     | Image format not supported or base64 encoding issue.                                                                                            | Use PNG or JPEG format. Ensure proper base64 encoding without corruption. Check image dimensions match `display_width` and `display_height`.                                 |
| Actions execute on wrong coordinates.                           | Screen resolution mismatch between screenshot and actual display.                                                                               | Ensure `display_width` and `display_height` in `ComputerUsePreviewTool` match your actual screen resolution.                                                                 |
| Model hallucinates UI elements.                                 | Screenshot quality too low or UI changed between turns.                                                                                         | Use higher resolution screenshots. Send fresh screenshots immediately after each action. Reduce delay between action and screenshot.                                         |

## Related content

* [Tool best practices for agents](../../concepts/tool-best-practice)
* [Browser automation tool](browser-automation)
* [Set up your agent environment](../../environment-setup)
* [Get started with Foundry agents](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true)
* [Computer use risks and limitations](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="warning">
  The computer use tool comes with significant security and privacy risks, including prompt injection attacks. For more information about intended uses, capabilities, limitations, risks, and considerations when choosing a use case, see the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview).
</Callout>

Create agents that interpret screenshots and automate UI interactions like clicking, typing, and scrolling. The computer use tool uses the `computer-use-preview` model to propose actions based on visual content, enabling agents to interact with desktop and browser applications through their user interfaces.

This guide shows how to integrate the computer use tool into an application loop (screenshot → action → screenshot) by using the latest SDKs.

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | -        | ✔️                | ✔️                   |

## Prerequisites

* An Azure subscription. [Create one for free](https://azure.microsoft.com/free/).

* A [basic or standard agent environment](../../environment-setup).

* The latest prerelease SDK package:

  * **Python**: `azure-ai-projects>=2.0.0b1`, `azure-identity`, `python-dotenv`
  * **C#/.NET**: `Azure.AI.Agents.Persistent` (prerelease)
  * **TypeScript**: `@azure/ai-projects` v2-beta, `@azure/identity`

* Access to the `computer-use-preview` model. See [Request access](#request-access) below.

* A virtual machine or sandboxed environment for safe testing. Don't run on machines with access to sensitive data.

### Environment variables

Set these environment variables before running the samples:

| Variable                        | Description                                        |
| ------------------------------- | -------------------------------------------------- |
| `FOUNDRY_PROJECT_ENDPOINT`      | Your Foundry project endpoint URL.                 |
| `FOUNDRY_MODEL_DEPLOYMENT_NAME` | Your `computer-use-preview` model deployment name. |

### Quick verification

Verify your authentication and project connection before running the full samples:

```python
import os
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient
from dotenv import load_dotenv

load_dotenv()

with (
    DefaultAzureCredential() as credential,
    AIProjectClient(endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"], credential=credential) as project_client,
):
    print("Connected to project.")
    # Verify you can access the OpenAI client
    openai_client = project_client.get_openai_client()
    print("OpenAI client ready.")
```

If this code runs without errors, your credentials and project endpoint are configured correctly.

## Run the maintained SDK samples (recommended)

The code snippets in this article focus on the agent and Responses API integration. For an end-to-end runnable sample that includes helper code and sample screenshots, use the SDK samples on GitHub.

* Python: [https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects/samples/agents/tools](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects/samples/agents/tools)
* TypeScript: [https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentComputerUse.js](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentComputerUse.js)
* .NET (computer use tool sample): [https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Agents.Persistent/samples/Sample33\_Computer\_Use.md](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Agents.Persistent/samples/Sample33_Computer_Use.md)

<Callout type="tip">
  The SDK samples include helper utilities for screenshot capture, action execution, and image encoding. Clone the repository or copy these files to your project before running the samples.
</Callout>

## Request access

To access the `computer-use-preview` model, you need to register. Microsoft grants access based on eligibility criteria. If you have access to other limited access models, you still need to request access for this model.

To request access, see the [application form](https://aka.ms/oai/cuaaccess).

After Microsoft grants access, you need to create a deployment for the model.

## Code samples

<Callout type="warning">
  Use the computer use tool on virtual machines with no access to sensitive data or critical resources. For more information about the intended uses, capabilities, limitations, risks, and considerations when choosing a use case, see the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview).
</Callout>

To run this code, you need the latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#install-and-authenticate) for details.

<ZonePivot pivot="python">
  ### Screenshot initialization for computer use tool execution

  The following code sample demonstrates how to create an agent version with the computer use tool, send an initial request with a screenshot, and perform multiple iterations to complete a task.

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import AgentReference, PromptAgentDefinition, ComputerUsePreviewTool

  # Import shared helper functions
  from computer_use_util import (
      SearchState,
      load_screenshot_assets,
      handle_computer_action_and_take_screenshot,
      print_final_output,
  )

  load_dotenv()

  """Main function to demonstrate Computer Use Agent functionality."""
  # Initialize state machine
  current_state = SearchState.INITIAL

  # Load screenshot assets
  try:
      screenshots = load_screenshot_assets()
      print("Successfully loaded screenshot assets")
  except FileNotFoundError:
      print("Failed to load required screenshot assets. Use the maintained SDK sample on GitHub to get the helper file and images.")
      exit(1)
  ```

  ### Create an agent version with the tool

  ```python
  project_client = AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  computer_use_tool = ComputerUsePreviewTool(display_width=1026, display_height=769, environment="windows")

  with project_client:
      agent = project_client.agents.create_version(
          agent_name="ComputerUseAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""
              You are a computer automation assistant.

              Be direct and efficient. When you reach the search results page, read and describe the actual search result titles and descriptions you can see.
              """,
              tools=[computer_use_tool],
          ),
          description="Computer automation agent with screen interaction capabilities.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")
  ```

  ### One iteration for the tool to process the screenshot and take the next step

  ```python
      openai_client = project_client.get_openai_client()

      # Initial request with screenshot - start with Bing search page
      print("Starting computer automation session (initial screenshot: cua_browser_search.png)...")
      response = openai_client.responses.create(
          input=[
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "input_text",
                          "text": "I need you to help me search for 'OpenAI news'. Please type 'OpenAI news' and submit the search. Once you see search results, the task is complete.",
                      },
                      {
                          "type": "input_image",
                          "image_url": screenshots["browser_search"]["url"],
                          "detail": "high",
                      },  # Start with Bing search page
                  ],
              }
          ],
          extra_body={"agent": AgentReference(name=agent.name).as_dict()},
          truncation="auto",
      )

      print(f"Initial response received (ID: {response.id})")
  ```

  ### Perform multiple iterations

  Make sure you review each iteration and action. The following code sample shows a basic API request. After you send the initial API request, perform a loop where your application code carries out the specified action. Send a screenshot with each turn so the model can evaluate the updated state of the environment. For an example integration for a similar API, see the [Azure OpenAI documentation](../../../openai/how-to/computer-use#playwright-integration).

  ```python

  max_iterations = 10  # Allow enough iterations for completion
  iteration = 0

  while True:
      if iteration >= max_iterations:
          print(f"\nReached maximum iterations ({max_iterations}). Stopping.")
          break

      iteration += 1
      print(f"\n--- Iteration {iteration} ---")

      # Check for computer calls in the response
      computer_calls = [item for item in response.output if item.type == "computer_call"]

      if not computer_calls:
          print_final_output(response)
          break

      # Process the first computer call
      computer_call = computer_calls[0]
      action = computer_call.action
      call_id = computer_call.call_id

      print(f"Processing computer call (ID: {call_id})")

      # Handle the action and get the screenshot info
      screenshot_info, current_state = handle_computer_action_and_take_screenshot(action, current_state, screenshots)

      print(f"Sending action result back to agent (using {screenshot_info['filename']})...")

      # Regular response with just the screenshot
      response = openai_client.responses.create(
          previous_response_id=response.id,
          input=[
              {
                  "call_id": call_id,
                  "type": "computer_call_output",
                  "output": {
                      "type": "computer_screenshot",
                      "image_url": screenshot_info["url"],
                  },
              }
          ],
          extra_body={"agent": AgentReference(name=agent.name).as_dict()},
          truncation="auto",
      )

      print(f"Follow-up response received (ID: {response.id})")
  ```

  ### Clean up

  ```python
  project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
  print("Agent deleted")
  ```

  ### Expected output

  The following example shows the expected output when running the previous code sample:

  ```console
  Successfully loaded screenshot assets
  Agent created (id: ..., name: ComputerUseAgent, version: 1)
  Starting computer automation session (initial screenshot: cua_browser_search.png)...
  Initial response received (ID: ...)
  --- Iteration 1 ---
  Processing computer call (ID: ...)
    Typing text "OpenAI news" - Simulating keyboard input
    -> Action processed: type
  Sending action result back to agent (using cua_search_typed.png)...
  Follow-up response received (ID: ...)
  --- Iteration 2 ---
  Processing computer call (ID: ...)
      Click at (512, 384) - Simulating click on UI element
      -> Assuming click on Search button when search field was populated, displaying results.
      -> Action processed: click
  Sending action result back to agent (using cua_search_results.png)...
  Follow-up response received (ID: ...)
  OpenAI news - Latest Updates
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample for use of an Agent with Computer Use tool

  The following C# code sample demonstrates how to create an agent version with the computer use tool, send an initial request with a screenshot, and perform multiple iterations to complete a task. To enable your agent to use the computer use tool, use `ResponseTool.CreateComputerTool()` when configuring the agent's tools. This example uses synchronous code. For asynchronous usage, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample10_ComputerUse.md) example in the Azure SDK for .NET repository on GitHub.

  ```csharp
  class ComputerUseDemo
  {


      // Read image files using `ReadImageFile` method.
      private static BinaryData ReadImageFile(string name, [CallerFilePath] string pth = "")
      {
          var dirName = Path.GetDirectoryName(pth) ?? "";
          return new BinaryData(File.ReadAllBytes(Path.Combine(dirName, name)));
      }

      // Create a helper method to parse the ComputerTool outputs and to respond
      // to Agents queries with new screenshots. Note that throughout
      // this sample the media type for image is set. Agents support `image/jpeg`,
      // `image/png`, `image/gif` and `image/webp` media types.
      private static string ProcessComputerUseCall(ComputerCallResponseItem item, string oldScreenshot)
      {
          string currentScreenshot = "browser_search";
          switch (item.Action.Kind)
          {
              case ComputerCallActionKind.Type:
                  Console.WriteLine($"  Typing text \"{item.Action.TypeText}\" - Simulating keyboard input");
                  currentScreenshot = "search_typed";
                  break;
              case ComputerCallActionKind.KeyPress:
                  HashSet<string> codes = new(item.Action.KeyPressKeyCodes);
                  if (codes.Contains("Return") || codes.Contains("ENTER"))
                  {
                      // If we have typed the value to the search field, go to search results.
                      if (string.Equals(oldScreenshot, "search_typed"))
                      {
                          Console.WriteLine("  -> Detected ENTER key press, when search field was populated, displaying results.");
                          currentScreenshot = "search_results";
                      }
                      else
                      {
                          Console.WriteLine("  -> Detected ENTER key press, on results or unpopulated search, do nothing.");
                          currentScreenshot = oldScreenshot;
                      }
                  }
                  else
                  {
                      Console.WriteLine($"  Key press: {item.Action.KeyPressKeyCodes.Aggregate("", (agg, next) => agg + "+" + next)} - Simulating key combination");
                  }
                  break;
              case ComputerCallActionKind.Click:
                  Console.WriteLine($"  Click at ({item.Action.ClickCoordinates.Value.X}, {item.Action.ClickCoordinates.Value.Y}) - Simulating click on UI element");
                  if (string.Equals(oldScreenshot, "search_typed"))
                  {
                      Console.WriteLine("  -> Assuming click on Search button when search field was populated, displaying results.");
                      currentScreenshot = "search_results";
                  }
                  else
                  {
                      Console.WriteLine("  -> Assuming click on Search on results or when search was not populated, do nothing.");
                      currentScreenshot = oldScreenshot;
                  }
                  break;
              case ComputerCallActionKind.Drag:
                  string pathStr = item.Action.DragPath.ToArray().Select(p => $"{p.X}, {p.Y}").Aggregate("", (agg, next) => $"{agg} -> {next}");
                  Console.WriteLine($"  Drag path: {pathStr} - Simulating drag operation");
                  break;
              case ComputerCallActionKind.Scroll:
                  Console.WriteLine($"  Scroll at ({item.Action.ScrollCoordinates.Value.X}, {item.Action.ScrollCoordinates.Value.Y}) - Simulating scroll action");
                  break;
              case ComputerCallActionKind.Screenshot:
                  Console.WriteLine("  Taking screenshot - Capturing current screen state");
                  break;
              default:
                  break;
          }
          Console.WriteLine($"  -> Action processed: {item.Action.Kind}");

          return currentScreenshot;
      }

      public static void Main()
      {
          // Create project client and read the environment variables, which will be used in the next steps.
          var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
          var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
          AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

          // Read in three example screenshots and place them into a dictionary.
          Dictionary<string, BinaryData> screenshots = new() {
              { "browser_search", ReadImageFile("Assets/cua_browser_search.png")},
              { "search_typed", ReadImageFile("Assets/cua_search_typed.png")},
              { "search_results", ReadImageFile("Assets/cua_search_results.png")},
          };

          // Create a PromptAgentDefinition with ComputerTool.
          PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
          {
              Instructions = "You are a computer automation assistant.\n\n" +
                              "Be direct and efficient. When you reach the search results page, read and describe the actual search result titles and descriptions you can see.",
              Tools = {
                  ResponseTool.CreateComputerTool(
                      environment: new ComputerToolEnvironment("windows"),
                      displayWidth: 1026,
                      displayHeight: 769
                  ),
              }
          };
          AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
              agentName: "myAgent",
              options: new(agentDefinition)
          );
          // Create an `ResponseResult` using `ResponseItem`, containing two `ResponseContentPart`:
          // one with the image and another with the text. In the loop, request Agent
          // while it is continuing to browse web. Finally, print the tool output message.
          ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
          CreateResponseOptions responseOptions = new()
          {
              TruncationMode = ResponseTruncationMode.Auto,
              InputItems =
              {
                  ResponseItem.CreateUserMessageItem(
                  [
                      ResponseContentPart.CreateInputTextPart("I need you to help me search for 'OpenAI news'. Please type 'OpenAI news' and submit the search. Once you see search results, the task is complete."),
                      ResponseContentPart.CreateInputImagePart(imageBytes: screenshots["browser_search"], imageBytesMediaType: "image/png", imageDetailLevel: ResponseImageDetailLevel.High)
                  ]),
              },
          };
          bool computerUseCalled = false;
          string currentScreenshot = "browser_search";
          int limitIteration = 10;
          ResponseResult response;
          do
          {
              response = responseClient.CreateResponse(responseOptions);
              computerUseCalled = false;
              responseOptions.InputItems.Clear();
              responseOptions.PreviousResponseId = response.Id;
              foreach (ResponseItem responseItem in response.OutputItems)
              {
                  responseOptions.InputItems.Add(responseItem);
                  if (responseItem is ComputerCallResponseItem computerCall)
                  {
                      currentScreenshot = ProcessComputerUseCall(computerCall, currentScreenshot);
                      responseOptions.InputItems.Add(ResponseItem.CreateComputerCallOutputItem(callId: computerCall.CallId, output: ComputerCallOutput.CreateScreenshotOutput(screenshotImageBytes: screenshots[currentScreenshot], screenshotImageBytesMediaType: "image/png")));
                      computerUseCalled = true;
                  }
              }
              limitIteration--;
          } while (computerUseCalled && limitIteration > 0);
          Console.WriteLine(response.GetOutputText());

          // Clean up resources by deleting Agent.
          projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
      }
  }
  ```

  ### Expected output

  The following example shows the expected output when running the previous code sample:

  ```console
  Agent created (id: ..., name: myAgent, version: 1)
  Starting computer automation session (initial screenshot: cua_browser_search.png)...
  Initial response received (ID: ...)
  --- Iteration 1 ---
  Processing computer call (ID: ...)
    Typing text "OpenAI news" - Simulating keyboard input
    -> Action processed: Type
  Sending action result back to agent (using cua_search_typed.png)...
  Follow-up response received (ID: ...)
  --- Iteration 2 ---
  Processing computer call (ID: ...)
    Click at (512, 384) - Simulating click on UI element
    -> Assuming click on Search button when search field was populated, displaying results.
    -> Action processed: Click
  Sending action result back to agent (using cua_search_results.png)...
  Follow-up response received (ID: ...)
  OpenAI news - Latest Updates
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Sample for use of an Agent with Computer Use tool

  The following TypeScript code sample demonstrates how to create an agent version with the computer use tool, send an initial request with a screenshot, and perform multiple iterations to complete a task. For a JavaScript example, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentComputerUse.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";
  import {
    SearchState,
    loadScreenshotAssets,
    handleComputerActionAndTakeScreenshot,
    printFinalOutput,
    type ComputerAction,
  } from "./computerUseUtil.js";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
      process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

  export async function main(): Promise<void> {
    // Initialize state machine
    let currentState = SearchState.INITIAL;

    // Load screenshot assets
    const screenshots = loadScreenshotAssets();
    console.log("Successfully loaded screenshot assets");

    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating Computer Use Agent...");
    const agent = await project.agents.createVersion("ComputerUseAgent", {
      kind: "prompt" as const,
      model: deploymentName,
      instructions: `
  You are a computer automation assistant.

  Be direct and efficient. When you reach the search results page, read and describe the actual search result titles and descriptions you can see.
      `.trim(),
      tools: [
        {
          type: "computer_use_preview",
          display_width: 1026,
          display_height: 769,
          environment: "windows" as const,
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Initial request with screenshot - start with Bing search page
    console.log(
      "Starting computer automation session (initial screenshot: cua_browser_search.png)...",
    );
    let response = await openAIClient.responses.create(
      {
        input: [
          {
            role: "user" as const,
            content: [
              {
                type: "input_text",
                text: "I need you to help me search for 'OpenAI news'. Please type 'OpenAI news' and submit the search. Once you see search results, the task is complete.",
              },
              {
                type: "input_image",
                image_url: screenshots.browser_search.url,
                detail: "high",
              },
            ],
          },
        ],
        truncation: "auto",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
      },
    );

    console.log(`Initial response received (ID: ${response.id})`);

    // Main interaction loop with deterministic completion
    const maxIterations = 10; // Allow enough iterations for completion
    let iteration = 0;

    while (iteration < maxIterations) {
      iteration++;
      console.log(`\n--- Iteration ${iteration} ---`);

      // Check for computer calls in the response
      const computerCalls = response.output.filter((item) => item.type === "computer_call");

      if (computerCalls.length === 0) {
        printFinalOutput({
          output: response.output,
          status: response.status ?? "",
        });
        break;
      }

      // Process the first computer call
      const computerCall = computerCalls[0];
      const action: ComputerAction = computerCall.action;
      const callId: string = computerCall.call_id;

      console.log(`Processing computer call (ID: ${callId})`);

      // Handle the action and get the screenshot info
      const [screenshotInfo, updatedState] = handleComputerActionAndTakeScreenshot(
        action,
        currentState,
        screenshots,
      );
      currentState = updatedState;

      console.log(`Sending action result back to agent (using ${screenshotInfo.filename})...`);
      // Regular response with just the screenshot
      response = await openAIClient.responses.create(
        {
          previous_response_id: response.id,
          input: [
            {
              call_id: callId,
              type: "computer_call_output",
              output: {
                type: "computer_screenshot",
                image_url: screenshotInfo.url,
              },
            },
          ],
          truncation: "auto",
        },
        {
          body: { agent: { name: agent.name, type: "agent_reference" } },
        },
      );

      console.log(`Follow-up response received (ID: ${response.id})`);
    }

    if (iteration >= maxIterations) {
      console.log(`\nReached maximum iterations (${maxIterations}). Stopping.`);
    }

    // Clean up resources
    console.log("\nCleaning up...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nComputer Use Agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  The following example shows the expected output when running the previous code sample:

  ```console
  Successfully loaded screenshot assets
  Creating Computer Use Agent...
  Agent created (id: ..., name: ComputerUseAgent, version: 1)
  Starting computer automation session (initial screenshot: cua_browser_search.png)...
  Initial response received (ID: ...)
  --- Iteration 1 ---
  Processing computer call (ID: ...)
    Typing text "OpenAI news" - Simulating keyboard input
    -> Action processed: type
  Sending action result back to agent (using cua_search_typed.png)...
  Follow-up response received (ID: ...)
  --- Iteration 2 ---
  Processing computer call (ID: ...)
      Click at (512, 384) - Simulating click on UI element
      -> Assuming click on Search button when search field was populated, displaying results.
      -> Action processed: click
  Sending action result back to agent (using cua_search_results.png)...
  Follow-up response received (ID: ...)
  OpenAI news - Latest Updates
  Cleaning up...
  Agent deleted
  Computer Use Agent sample completed!
  ```
</ZonePivot>

## What you can do with the computer use tool

After you integrate the request-and-response loop (screenshot -> action -> screenshot), the computer use tool can help an agent:

* Propose UI actions such as clicking, typing, scrolling, and requesting a new screenshot.
* Adapt to UI changes by re-evaluating the latest screenshot after each action.
* Work across browser and desktop UI, depending on how you host your sandboxed environment.

The tool doesn't directly control a device. Your application executes each requested action and returns an updated screenshot.

## Differences between browser automation and computer use

The following table lists some of the differences between the computer use tool and [browser automation](browser-automation) tool.

| Feature                             | Browser Automation                                                 | Computer use tool                                                                                     |
| ----------------------------------- | ------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------- |
| Model support                       | All GPT models                                                     | `Computer-use-preview` model only                                                                     |
| Can I visualize what's happening?   | No                                                                 | Yes                                                                                                   |
| How it understands the screen       | Parses the HTML or XML pages into DOM documents                    | Raw pixel data from screenshots                                                                       |
| How it acts                         | A list of actions provided by the model                            | Virtual keyboard and mouse                                                                            |
| Is it multistep?                    | Yes                                                                | Yes                                                                                                   |
| Interfaces                          | Browser                                                            | Computer and browser                                                                                  |
| Do I need to bring my own resource? | Your own Playwright resource with the keys stored as a connection. | No additional resource required but we highly recommend running this tool in a sandboxed environment. |

### When to use each tool

**Choose computer use when you need to**:

* Interact with desktop applications beyond the browser
* Visualize what the agent sees through screenshots
* Work in environments where DOM parsing isn't available

**Choose browser automation when you need to**:

* Perform web-only interactions without limited access requirements
* Use any GPT model (not limited to `computer-use-preview`)
* Avoid managing screenshot capture and action execution loops

## Regional support

To use the computer use tool, you need a [computer use model](../../../foundry-models/concepts/models-sold-directly-by-azure#computer-use-preview) deployment. The computer use model is available in the following regions:

| Region          | Status    |
| --------------- | --------- |
| `eastus2`       | Available |
| `swedencentral` | Available |
| `southindia`    | Available |

## Understanding the computer use integration

When working with the computer use tool, integrate it into your application by performing the following steps:

1. Send a request to the model that includes a call to the computer use tool, the display size, and the environment. You can also include a screenshot of the initial state of the environment in the first API request.

2. Receive a response from the model. If the response has action items, those items contain suggested actions to make progress toward the specified goal. For example, an action might be `screenshot` so the model can assess the current state with an updated screenshot, or `click` with X/Y coordinates indicating where the mouse should be moved.

3. Execute the action by using your application code on your computer or browser environment.

4. After executing the action, capture the updated state of the environment as a screenshot.

5. Send a new request with the updated state as a `tool_call_output`, and repeat this loop until the model stops requesting actions or you decide to stop.

   <Callout type="note">
     Before using the tool, set up an environment that can capture screenshots and execute the recommended actions by the agent. For safety reasons, use a sandboxed environment, such as Playwright.
   </Callout>

## Manage conversation history

Use the `previous_response_id` parameter to link the current request to the previous response. Use this parameter when you don't want to send the full conversation history with each call.

If you don't use this parameter, make sure to include all the items returned in the response output of the previous request in your inputs array. This requirement includes reasoning items if present.

## Safety checks and security considerations

<Callout type="warning">
  Computer use carries substantial security and privacy risks and user responsibility. Both errors in judgment by the AI and the presence of malicious or confusing instructions on web pages, desktops, or other operating environments that the AI encounters might cause it to execute commands you or others don't intend. These risks could compromise the security of your or other users’ browsers, computers, and any accounts to which AI has access, including personal, financial, or enterprise systems.

  Use the computer use tool on virtual machines with no access to sensitive data or critical resources. For more information about the intended uses, capabilities, limitations, risks, and considerations when choosing a use case, see the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview).
</Callout>

The API has safety checks to help protect against prompt injection and model mistakes. These checks include:

**Malicious instruction detection**: The system evaluates the screenshot image and checks if it contains adversarial content that might change the model's behavior.

**Irrelevant domain detection**: The system evaluates the `current_url` parameter (if provided) and checks if the current domain is relevant given the conversation history.

**Sensitive domain detection**: The system checks the `current_url` parameter (if provided) and raises a warning when it detects the user is on a sensitive domain.

If one or more of the preceding checks are triggered, the model raises a safety check when it returns the next `computer_call` by using the `pending_safety_checks` parameter.

```json
"output": [
    {
        "type": "reasoning",
        "id": "rs_67cb...",
        "summary": [
            {
                "type": "summary_text",
                "text": "Exploring 'File' menu option."
            }
        ]
    },
    {
        "type": "computer_call",
        "id": "cu_67cb...",
        "call_id": "call_nEJ...",
        "action": {
            "type": "click",
            "button": "left",
            "x": 135,
            "y": 193
        },
        "pending_safety_checks": [
            {
                "id": "cu_sc_67cb...",
                "code": "malicious_instructions",
                "message": "We've detected instructions that may cause your application to perform malicious or unauthorized actions. Please acknowledge this warning if you'd like to proceed."
            }
        ],
        "status": "completed"
    }
]
```

You need to pass the safety checks back as `acknowledged_safety_checks` in the next request to proceed.

```json
"input":[
        {
            "type": "computer_call_output",
            "call_id": "<call_id>",
            "acknowledged_safety_checks": [
                {
                    "id": "<safety_check_id>",
                    "code": "malicious_instructions",
                    "message": "We've detected instructions that may cause your application to perform malicious or unauthorized actions. Please acknowledge this warning if you'd like to proceed."
                }
            ],
            "output": {
                "type": "computer_screenshot",
                "image_url": "<image_url>"
            }
        }
    ]
```

## Safety check handling

In all cases where `pending_safety_checks` are returned, hand over actions to the end user to confirm proper model behavior and accuracy.

`malicious_instructions` and `irrelevant_domain`: End users should review model actions and confirm that the model behaves as intended.

`sensitive_domain`: Ensure an end user actively monitors the model actions on these sites. The exact implementation of this "watch mode" can vary by application, but a potential example could be collecting user impression data on the site to make sure there's active end user engagement with the application.

## Troubleshooting

| Issue                                                           | Cause                                                                                                                                           | Resolution                                                                                                                                                                   |
| --------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| You don't see a `computer_call` in the response.                | The agent isn't configured with the computer use tool, the deployment isn't a computer use model, or the prompt doesn't require UI interaction. | Confirm the agent has a `computer_use_preview` tool, your deployment is the `computer-use-preview` model, and your prompt requires a UI action (type, click, or screenshot). |
| The sample code fails with missing helper files or screenshots. | The snippets reference helper utilities and sample images that aren't part of this documentation repo.                                          | Run the maintained SDK samples in the "Run the maintained SDK samples" section, or copy the helper file and sample images from the SDK repo into your project.               |
| The loop stops at the iteration limit.                          | The task needs more turns, or the app isn't applying the actions the model requests.                                                            | Increase the iteration limit, and verify that your code executes the requested action and sends a new screenshot after each turn.                                            |
| You receive `pending_safety_checks`.                            | The service detected a potential security risk (for example, prompt injection or a sensitive domain).                                           | Pause automation, require an end user to review the request, and only continue after you send `acknowledged_safety_checks` with the next `computer_call_output`.             |
| The model repeats "take a screenshot" without making progress.  | The screenshot isn't updating, is low quality, or doesn't show the relevant UI state.                                                           | Send a fresh screenshot after each action and use a higher-detail image when needed. Ensure the screenshot includes the relevant UI.                                         |
| Access denied when requesting `computer-use-preview` model.     | You haven't registered for access or access hasn't been granted.                                                                                | Submit the [application form](https://aka.ms/oai/cuaaccess) and wait for approval. Check your email for confirmation.                                                        |
| Screenshot encoding errors.                                     | Image format not supported or base64 encoding issue.                                                                                            | Use PNG or JPEG format. Ensure proper base64 encoding without corruption. Check image dimensions match `display_width` and `display_height`.                                 |
| Actions execute on wrong coordinates.                           | Screen resolution mismatch between screenshot and actual display.                                                                               | Ensure `display_width` and `display_height` in `ComputerUsePreviewTool` match your actual screen resolution.                                                                 |
| Model hallucinates UI elements.                                 | Screenshot quality too low or UI changed between turns.                                                                                         | Use higher resolution screenshots. Send fresh screenshots immediately after each action. Reduce delay between action and screenshot.                                         |

## Related content

* [Tool best practices for agents](../../concepts/tool-best-practice)
* [Browser automation tool](browser-automation)
* [Set up your agent environment](../../environment-setup)
* [Get started with Foundry agents](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true)
* [Computer use risks and limitations](../../../responsible-ai/openai/transparency-note#risk-and-limitations-of-computer-use-preview)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="important">
  * The image generation tool requires the `gpt-image-1` model. See the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note?tabs=image) for limitations and responsible AI considerations.
  * You also need a compatible orchestrator model (`gpt-4o`, `gpt-4o-mini`, `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`, `o3`, or `gpt-5` series) deployed in the same Foundry project.
</Callout>

The **image generation tool** in Microsoft Foundry Agent Service generates images from text prompts in conversations and multistep workflows. Use it to create AI-generated visuals and return base64-encoded output that you can save to a file.

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* An Azure account with an active subscription.

* A Foundry project.

* A basic or standard agent environment. See [agent environment setup](../../environment-setup).

* Permissions to create and manage agent versions in the project.

* Two model deployments in the same Foundry project:

  * A compatible Azure OpenAI model deployment for the agent (for example, `gpt-4o`).
  * An image generation model deployment (`gpt-image-1`).

Set these environment variables for the samples:

* `FOUNDRY_PROJECT_ENDPOINT`
* `FOUNDRY_MODEL_DEPLOYMENT_NAME`
* `IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME`

## Configure the image generation tool

1. Deploy your orchestrator model (for example, `gpt-4o`) to your Foundry project.
2. Deploy `gpt-image-1` to the same Foundry project.
3. Confirm your region and model support for image generation. See [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).
4. Configure the environment variables listed in the prerequisites.

## Code examples

Before you start, install the `azure-ai-projects` package (version 2.0.0b1 or later). For package installation instructions, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).

<ZonePivot pivot="python">
  ## Create an agent with the image generation tool

  This sample creates an agent with the image generation tool, generates an image, and saves it to a file.

  ```python
  import base64
  import os

  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition, ImageGenTool

  project_client = AIProjectClient(
    endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
    credential=DefaultAzureCredential(),
  )

  with project_client:
    openai_client = project_client.get_openai_client()

    agent = project_client.agents.create_version(
      agent_name="agent-image-generation",
      definition=PromptAgentDefinition(
        model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
        instructions="Generate images based on user prompts.",
        tools=[ImageGenTool(quality="low", size="1024x1024")],
      ),
      description="Agent for image generation.",
    )
    print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

    response = openai_client.responses.create(
      input="Generate an image of the Microsoft logo.",
      extra_headers={
        "x-ms-oai-image-generation-deployment": os.environ["IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME"],
      },
      extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
    )
    print(f"Response created: {response.id}")

    image_items = [item for item in (response.output or []) if item.type == "image_generation_call"]
    if image_items and getattr(image_items[0], "result", None):
      print("Downloading generated image...")
      file_path = os.path.abspath("microsoft.png")
      with open(file_path, "wb") as f:
        f.write(base64.b64decode(image_items[0].result))
      print(f"Image downloaded and saved to: {file_path}")
    else:
      print("No image data found in the response.")

    project_client.agents.delete_version(agent.name, agent.version)
    print("Agent deleted")
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample for image generation in Azure.AI.Projects.OpenAI.

  In this example, you generate an image based on a simple prompt. The code in this example is synchronous. For an asynchronous example, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample2_Image_Generation.md) example in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Read the environment variables
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var imageGenerationDeploymentName = System.Environment.GetEnvironmentVariable("IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME");

  // Create the AI Project client with custom header policy
  AIProjectClientOptions projectOptions = new();
  projectOptions.AddPolicy(new HeaderPolicy(imageGenerationDeploymentName), PipelinePosition.PerCall);

  // Create the AI Project client
  AIProjectClient projectClient = new(
      endpoint: new Uri(projectEndpoint),
      tokenProvider: new DefaultAzureCredential(),
      options: projectOptions
  );

  // Use the client to create the versioned agent object.
  // To generate images, we need to provide agent with the ImageGenerationTool
  // when creating this tool. The ImageGenerationTool parameters include
  // the image generation model, image quality and resolution.
  // Supported image generation models include gpt-image-1.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
  Instructions = "Generate images based on user prompts.",
  Tools = {
          ResponseTool.CreateImageGenerationTool(
              model: imageGenerationDeploymentName,
              quality: ImageGenerationToolQuality.Low,
              size:ImageGenerationToolSize.W1024xH1024
          )
      }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  ProjectOpenAIClient openAIClient = projectClient.GetProjectOpenAIClient();
  ProjectResponsesClient responseClient = openAIClient.GetProjectResponsesClientForAgent(new AgentReference(name: agentVersion.Name));

  ResponseResult response = responseClient.CreateResponse("Generate parody of Newton with apple.");

  // Parse the ResponseResult object and save the generated image.
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is ImageGenerationCallResponseItem imageItem)
      {
          File.WriteAllBytes("newton.png", imageItem.ImageResultBytes.ToArray());
          Console.WriteLine($"Image downloaded and saved to: {Path.GetFullPath("newton.png")}");
      }
  }

  // Clean up resources by deleting the Agent.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);

  // To use image generation, provide the custom header to web requests,
  // which contain the model deployment name, for example:
  // `x-ms-oai-image-generation-deployment: gpt-image-1`.
  // To implement it, create a custom header policy.
  internal class HeaderPolicy(string image_deployment) : PipelinePolicy
  {
      private const string image_deployment_header = "x-ms-oai-image-generation-deployment";

      public override void Process(PipelineMessage message, IReadOnlyList<PipelinePolicy> pipeline, int currentIndex)
      {
          message.Request.Headers.Add(image_deployment_header, image_deployment);
          ProcessNext(message, pipeline, currentIndex);
      }

      public override async ValueTask ProcessAsync(PipelineMessage message, IReadOnlyList<PipelinePolicy> pipeline, int currentIndex)
      {
          // Add your desired header name and value
          message.Request.Headers.Add(image_deployment_header, image_deployment);
          await ProcessNextAsync(message, pipeline, currentIndex);
      }
  }
  ```

  ### Expected output

  When you run the sample, you see the following output:

  ```console
  Agent created (id: <agent-id>, name: myAgent, version: 1)
  Image downloaded and saved to: /path/to/newton.png
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="rest-api">
  ## Create an agent with the image generation tool

  The following example creates an agent that uses the image generation tool.

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/agents/$AGENTVERSION_NAME/versions?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -d '{
    "description": "Test agent for image generation capabilities",
    "definition": {
    "kind": "prompt",
    "model": "{{model}}",
    "tools": [
      {
        "type": "image_generation"
      }
    ],
      "instructions": "You are a creative assistant that generates images when requested. Please respond to image generation requests clearly and concisely."
    }
  }'
  ```

  ## Create a response

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -H "x-ms-oai-image-generation-deployment: $IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME" \
    -d '{
    "agent": {
      "type": "agent_reference",
      "name": "{{agentVersion.name}}",
      "version": "{{agentVersion.version}}"
    },
    "metadata": {
      "test_response": "image_generation_enabled",
      "test_scenario": "basic_imagegen"
    },
    "input": [{
      "type": "message",
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "Please generate small image of a sunset over a mountain lake."
        }
      ]
    }],
    "background": true,
    "stream": false
  }'
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Create an agent with image generation tool

  This sample demonstrates how to create an AI agent with image generation capabilities by using the Azure AI Projects client. The agent generates images based on text prompts and saves them to files. For a JavaScript example, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentImageGeneration.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as fs from "fs";
  import * as path from "path";
  import { fileURLToPath } from "url";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
    process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const imageDeploymentName =
    process.env["IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME"] || "<image generation deployment name>";

  export async function main(): Promise<void> {
    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with image generation tool...");

    // Create Agent with image generation tool
    const agent = await project.agents.createVersion("agent-image-generation", {
      kind: "prompt",
      model: deploymentName,
      instructions: "Generate images based on user prompts",
      tools: [
        {
          type: "image_generation",
          quality: "low",
          size: "1024x1024",
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Generate image using the agent
    console.log("\nGenerating image...");
    const response = await openAIClient.responses.create(
      {
        input: "Generate an image of Microsoft logo.",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
        headers: { "x-ms-oai-image-generation-deployment": imageDeploymentName },
      },
    );
    console.log(`Response created: ${response.id}`);

    // Extract and save the generated image
    const imageData = response.output?.filter((output) => output.type === "image_generation_call");

    if (imageData && imageData.length > 0 && imageData[0].result) {
      console.log("Downloading generated image...");

      const __filename = fileURLToPath(import.meta.url);
      const __dirname = path.dirname(__filename);
      const filename = "microsoft.png";
      const filePath = path.join(__dirname, filename);

      // Decode base64 and save to file
      const imageBuffer = Buffer.from(imageData[0].result, "base64");
      fs.writeFileSync(filePath, imageBuffer);

      console.log(`Image downloaded and saved to: ${path.resolve(filePath)}`);
    } else {
      console.log("No image data found in the response.");
    }

    // Clean up resources
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nImage generation sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  When you run the sample, you see the following output:

  ```console
  Creating agent with image generation tool...
  Agent created (id: <agent-id>, name: agent-image-generation, version: 1)
  Generating image...
  Response created: <response-id>
  Downloading generated image...
  Image downloaded and saved to: /path/to/microsoft.png
  Cleaning up resources...
  Agent deleted
  ```
</ZonePivot>

## When to use the image generation tool

The image generation tool in Agent Service offers advantages over the Azure OpenAI Image API:

| Advantage       | Description                                                                   |
| --------------- | ----------------------------------------------------------------------------- |
| Streaming       | Display partial image outputs during generation to improve perceived latency. |
| Flexible inputs | Accept image file IDs as inputs, in addition to raw image bytes.              |

## Optional parameters

Customize image generation by specifying these optional parameters when you create the tool:

| Parameter            | Description                                                              |
| -------------------- | ------------------------------------------------------------------------ |
| `size`               | Image size. One of `1024x1024`, `1024x1536`, `1536x1024`, or `auto`.     |
| `quality`            | Image quality. One of `low`, `medium`, `high`, or `auto`.                |
| `background`         | Background type. One of `transparent`, `opaque`, or `auto`.              |
| `output_format`      | Output format. One of `png`, `webp`, or `jpeg`.                          |
| `output_compression` | Compression level for `webp` and `jpeg` output (0-100).                  |
| `moderation`         | Moderation level for the generated image. One of `auto` or `low`.        |
| `partial_images`     | Number of partial images to generate in streaming mode (0-3).            |
| `input_image_mask`   | Optional mask for inpainting. Provide `image_url` (base64) or `file_id`. |

<Callout type="note">
  Image generation time varies based on the `quality` setting and prompt complexity. For time-sensitive applications, consider using `quality: "low"` or enabling `partial_images` for streaming.
</Callout>

Use the Responses API if you want to:

* Build conversational image experiences with GPT Image.
* Stream partial image results during generation for a smoother user experience.

## Write effective text-to-image prompts

Effective prompts produce better images. Describe the subject, visual style, and composition you want. Use action words like "draw," "create," or "edit" to guide the model's output.

Content filtering can block image generation if the service detects unsafe content in your prompt. For more information, see [Content filter](../../../openai/concepts/content-filter).

<Callout type="tip">
  For a thorough look at how you can tweak your text prompts to generate different kinds of images, see [Image prompt engineering techniques](../../../openai/concepts/gpt-4-v-prompt-engineering).
</Callout>

## Verify tool execution

Use either of these approaches to confirm that image generation ran successfully:

* In the response payload, look for an output item with `type` set to `image_generation_call`.
* In the Foundry portal, open tracing/debug for your run to confirm the tool call and inspect inputs and outputs.

When image generation succeeds, the response includes an `image_generation_call` output item with a `result` field containing base64-encoded image data.

If you see only text output and no `image_generation_call` item, the request might not be routed to image generation. Review the troubleshooting section.

## Troubleshooting

| Issue                           | Cause                                 | Resolution                                                                                                                                                               |
| ------------------------------- | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Image generation fails          | Missing deployment                    | Verify both the orchestrator model (for example, `gpt-4o`) and `gpt-image-1` deployments exist in the same Foundry project.                                              |
| Image generation fails          | Missing or incorrect header           | Verify the header `x-ms-oai-image-generation-deployment` is present on the Responses request and matches your image generation deployment name.                          |
| Agent uses wrong deployment     | Environment variable misconfiguration | Confirm `FOUNDRY_MODEL_DEPLOYMENT_NAME` is set to your orchestrator deployment name, not the image generation deployment.                                                |
| Prompt doesn't produce an image | Content filtering blocked the request | Check content filtering logs. See [Content filter](../../../openai/concepts/content-filter) for guidelines on acceptable prompts.                                        |
| Tool not available              | Regional or model limitation          | Confirm the image generation tool is available in your region and with your orchestrator model. See [Best practices for using tools](../../concepts/tool-best-practice). |
| Generated image has low quality | Prompt lacks detail                   | Provide more specific and detailed prompts describing the desired image style, composition, and elements.                                                                |
| Image generation times out      | Large or complex image request        | Simplify the prompt or increase timeout settings. Consider breaking complex requests into multiple simpler ones.                                                         |
| Unexpected image content        | Ambiguous prompt                      | Refine your prompt to be more specific. Include negative prompts to exclude unwanted elements.                                                                           |

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Image generation in Azure OpenAI](../../../openai/how-to/dall-e)
* [Responses API in Azure OpenAI](../../../openai/how-to/responses)
* [Content filter](../../../openai/concepts/content-filter)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="important">
  * The image generation tool requires the `gpt-image-1` model. See the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note?tabs=image) for limitations and responsible AI considerations.
  * You also need a compatible orchestrator model (`gpt-4o`, `gpt-4o-mini`, `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`, `o3`, or `gpt-5` series) deployed in the same Foundry project.
</Callout>

The **image generation tool** in Microsoft Foundry Agent Service generates images from text prompts in conversations and multistep workflows. Use it to create AI-generated visuals and return base64-encoded output that you can save to a file.

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* An Azure account with an active subscription.

* A Foundry project.

* A basic or standard agent environment. See [agent environment setup](../../environment-setup).

* Permissions to create and manage agent versions in the project.

* Two model deployments in the same Foundry project:

  * A compatible Azure OpenAI model deployment for the agent (for example, `gpt-4o`).
  * An image generation model deployment (`gpt-image-1`).

Set these environment variables for the samples:

* `FOUNDRY_PROJECT_ENDPOINT`
* `FOUNDRY_MODEL_DEPLOYMENT_NAME`
* `IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME`

## Configure the image generation tool

1. Deploy your orchestrator model (for example, `gpt-4o`) to your Foundry project.
2. Deploy `gpt-image-1` to the same Foundry project.
3. Confirm your region and model support for image generation. See [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).
4. Configure the environment variables listed in the prerequisites.

## Code examples

Before you start, install the `azure-ai-projects` package (version 2.0.0b1 or later). For package installation instructions, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).

<ZonePivot pivot="python">
  ## Create an agent with the image generation tool

  This sample creates an agent with the image generation tool, generates an image, and saves it to a file.

  ```python
  import base64
  import os

  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition, ImageGenTool

  project_client = AIProjectClient(
    endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
    credential=DefaultAzureCredential(),
  )

  with project_client:
    openai_client = project_client.get_openai_client()

    agent = project_client.agents.create_version(
      agent_name="agent-image-generation",
      definition=PromptAgentDefinition(
        model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
        instructions="Generate images based on user prompts.",
        tools=[ImageGenTool(quality="low", size="1024x1024")],
      ),
      description="Agent for image generation.",
    )
    print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

    response = openai_client.responses.create(
      input="Generate an image of the Microsoft logo.",
      extra_headers={
        "x-ms-oai-image-generation-deployment": os.environ["IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME"],
      },
      extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
    )
    print(f"Response created: {response.id}")

    image_items = [item for item in (response.output or []) if item.type == "image_generation_call"]
    if image_items and getattr(image_items[0], "result", None):
      print("Downloading generated image...")
      file_path = os.path.abspath("microsoft.png")
      with open(file_path, "wb") as f:
        f.write(base64.b64decode(image_items[0].result))
      print(f"Image downloaded and saved to: {file_path}")
    else:
      print("No image data found in the response.")

    project_client.agents.delete_version(agent.name, agent.version)
    print("Agent deleted")
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample for image generation in Azure.AI.Projects.OpenAI.

  In this example, you generate an image based on a simple prompt. The code in this example is synchronous. For an asynchronous example, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample2_Image_Generation.md) example in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Read the environment variables
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var imageGenerationDeploymentName = System.Environment.GetEnvironmentVariable("IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME");

  // Create the AI Project client with custom header policy
  AIProjectClientOptions projectOptions = new();
  projectOptions.AddPolicy(new HeaderPolicy(imageGenerationDeploymentName), PipelinePosition.PerCall);

  // Create the AI Project client
  AIProjectClient projectClient = new(
      endpoint: new Uri(projectEndpoint),
      tokenProvider: new DefaultAzureCredential(),
      options: projectOptions
  );

  // Use the client to create the versioned agent object.
  // To generate images, we need to provide agent with the ImageGenerationTool
  // when creating this tool. The ImageGenerationTool parameters include
  // the image generation model, image quality and resolution.
  // Supported image generation models include gpt-image-1.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
  Instructions = "Generate images based on user prompts.",
  Tools = {
          ResponseTool.CreateImageGenerationTool(
              model: imageGenerationDeploymentName,
              quality: ImageGenerationToolQuality.Low,
              size:ImageGenerationToolSize.W1024xH1024
          )
      }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  ProjectOpenAIClient openAIClient = projectClient.GetProjectOpenAIClient();
  ProjectResponsesClient responseClient = openAIClient.GetProjectResponsesClientForAgent(new AgentReference(name: agentVersion.Name));

  ResponseResult response = responseClient.CreateResponse("Generate parody of Newton with apple.");

  // Parse the ResponseResult object and save the generated image.
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is ImageGenerationCallResponseItem imageItem)
      {
          File.WriteAllBytes("newton.png", imageItem.ImageResultBytes.ToArray());
          Console.WriteLine($"Image downloaded and saved to: {Path.GetFullPath("newton.png")}");
      }
  }

  // Clean up resources by deleting the Agent.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);

  // To use image generation, provide the custom header to web requests,
  // which contain the model deployment name, for example:
  // `x-ms-oai-image-generation-deployment: gpt-image-1`.
  // To implement it, create a custom header policy.
  internal class HeaderPolicy(string image_deployment) : PipelinePolicy
  {
      private const string image_deployment_header = "x-ms-oai-image-generation-deployment";

      public override void Process(PipelineMessage message, IReadOnlyList<PipelinePolicy> pipeline, int currentIndex)
      {
          message.Request.Headers.Add(image_deployment_header, image_deployment);
          ProcessNext(message, pipeline, currentIndex);
      }

      public override async ValueTask ProcessAsync(PipelineMessage message, IReadOnlyList<PipelinePolicy> pipeline, int currentIndex)
      {
          // Add your desired header name and value
          message.Request.Headers.Add(image_deployment_header, image_deployment);
          await ProcessNextAsync(message, pipeline, currentIndex);
      }
  }
  ```

  ### Expected output

  When you run the sample, you see the following output:

  ```console
  Agent created (id: <agent-id>, name: myAgent, version: 1)
  Image downloaded and saved to: /path/to/newton.png
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="rest-api">
  ## Create an agent with the image generation tool

  The following example creates an agent that uses the image generation tool.

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/agents/$AGENTVERSION_NAME/versions?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -d '{
    "description": "Test agent for image generation capabilities",
    "definition": {
    "kind": "prompt",
    "model": "{{model}}",
    "tools": [
      {
        "type": "image_generation"
      }
    ],
      "instructions": "You are a creative assistant that generates images when requested. Please respond to image generation requests clearly and concisely."
    }
  }'
  ```

  ## Create a response

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -H "x-ms-oai-image-generation-deployment: $IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME" \
    -d '{
    "agent": {
      "type": "agent_reference",
      "name": "{{agentVersion.name}}",
      "version": "{{agentVersion.version}}"
    },
    "metadata": {
      "test_response": "image_generation_enabled",
      "test_scenario": "basic_imagegen"
    },
    "input": [{
      "type": "message",
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "Please generate small image of a sunset over a mountain lake."
        }
      ]
    }],
    "background": true,
    "stream": false
  }'
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Create an agent with image generation tool

  This sample demonstrates how to create an AI agent with image generation capabilities by using the Azure AI Projects client. The agent generates images based on text prompts and saves them to files. For a JavaScript example, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentImageGeneration.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as fs from "fs";
  import * as path from "path";
  import { fileURLToPath } from "url";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
    process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const imageDeploymentName =
    process.env["IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME"] || "<image generation deployment name>";

  export async function main(): Promise<void> {
    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with image generation tool...");

    // Create Agent with image generation tool
    const agent = await project.agents.createVersion("agent-image-generation", {
      kind: "prompt",
      model: deploymentName,
      instructions: "Generate images based on user prompts",
      tools: [
        {
          type: "image_generation",
          quality: "low",
          size: "1024x1024",
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Generate image using the agent
    console.log("\nGenerating image...");
    const response = await openAIClient.responses.create(
      {
        input: "Generate an image of Microsoft logo.",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
        headers: { "x-ms-oai-image-generation-deployment": imageDeploymentName },
      },
    );
    console.log(`Response created: ${response.id}`);

    // Extract and save the generated image
    const imageData = response.output?.filter((output) => output.type === "image_generation_call");

    if (imageData && imageData.length > 0 && imageData[0].result) {
      console.log("Downloading generated image...");

      const __filename = fileURLToPath(import.meta.url);
      const __dirname = path.dirname(__filename);
      const filename = "microsoft.png";
      const filePath = path.join(__dirname, filename);

      // Decode base64 and save to file
      const imageBuffer = Buffer.from(imageData[0].result, "base64");
      fs.writeFileSync(filePath, imageBuffer);

      console.log(`Image downloaded and saved to: ${path.resolve(filePath)}`);
    } else {
      console.log("No image data found in the response.");
    }

    // Clean up resources
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nImage generation sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  When you run the sample, you see the following output:

  ```console
  Creating agent with image generation tool...
  Agent created (id: <agent-id>, name: agent-image-generation, version: 1)
  Generating image...
  Response created: <response-id>
  Downloading generated image...
  Image downloaded and saved to: /path/to/microsoft.png
  Cleaning up resources...
  Agent deleted
  ```
</ZonePivot>

## When to use the image generation tool

The image generation tool in Agent Service offers advantages over the Azure OpenAI Image API:

| Advantage       | Description                                                                   |
| --------------- | ----------------------------------------------------------------------------- |
| Streaming       | Display partial image outputs during generation to improve perceived latency. |
| Flexible inputs | Accept image file IDs as inputs, in addition to raw image bytes.              |

## Optional parameters

Customize image generation by specifying these optional parameters when you create the tool:

| Parameter            | Description                                                              |
| -------------------- | ------------------------------------------------------------------------ |
| `size`               | Image size. One of `1024x1024`, `1024x1536`, `1536x1024`, or `auto`.     |
| `quality`            | Image quality. One of `low`, `medium`, `high`, or `auto`.                |
| `background`         | Background type. One of `transparent`, `opaque`, or `auto`.              |
| `output_format`      | Output format. One of `png`, `webp`, or `jpeg`.                          |
| `output_compression` | Compression level for `webp` and `jpeg` output (0-100).                  |
| `moderation`         | Moderation level for the generated image. One of `auto` or `low`.        |
| `partial_images`     | Number of partial images to generate in streaming mode (0-3).            |
| `input_image_mask`   | Optional mask for inpainting. Provide `image_url` (base64) or `file_id`. |

<Callout type="note">
  Image generation time varies based on the `quality` setting and prompt complexity. For time-sensitive applications, consider using `quality: "low"` or enabling `partial_images` for streaming.
</Callout>

Use the Responses API if you want to:

* Build conversational image experiences with GPT Image.
* Stream partial image results during generation for a smoother user experience.

## Write effective text-to-image prompts

Effective prompts produce better images. Describe the subject, visual style, and composition you want. Use action words like "draw," "create," or "edit" to guide the model's output.

Content filtering can block image generation if the service detects unsafe content in your prompt. For more information, see [Content filter](../../../openai/concepts/content-filter).

<Callout type="tip">
  For a thorough look at how you can tweak your text prompts to generate different kinds of images, see [Image prompt engineering techniques](../../../openai/concepts/gpt-4-v-prompt-engineering).
</Callout>

## Verify tool execution

Use either of these approaches to confirm that image generation ran successfully:

* In the response payload, look for an output item with `type` set to `image_generation_call`.
* In the Foundry portal, open tracing/debug for your run to confirm the tool call and inspect inputs and outputs.

When image generation succeeds, the response includes an `image_generation_call` output item with a `result` field containing base64-encoded image data.

If you see only text output and no `image_generation_call` item, the request might not be routed to image generation. Review the troubleshooting section.

## Troubleshooting

| Issue                           | Cause                                 | Resolution                                                                                                                                                               |
| ------------------------------- | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Image generation fails          | Missing deployment                    | Verify both the orchestrator model (for example, `gpt-4o`) and `gpt-image-1` deployments exist in the same Foundry project.                                              |
| Image generation fails          | Missing or incorrect header           | Verify the header `x-ms-oai-image-generation-deployment` is present on the Responses request and matches your image generation deployment name.                          |
| Agent uses wrong deployment     | Environment variable misconfiguration | Confirm `FOUNDRY_MODEL_DEPLOYMENT_NAME` is set to your orchestrator deployment name, not the image generation deployment.                                                |
| Prompt doesn't produce an image | Content filtering blocked the request | Check content filtering logs. See [Content filter](../../../openai/concepts/content-filter) for guidelines on acceptable prompts.                                        |
| Tool not available              | Regional or model limitation          | Confirm the image generation tool is available in your region and with your orchestrator model. See [Best practices for using tools](../../concepts/tool-best-practice). |
| Generated image has low quality | Prompt lacks detail                   | Provide more specific and detailed prompts describing the desired image style, composition, and elements.                                                                |
| Image generation times out      | Large or complex image request        | Simplify the prompt or increase timeout settings. Consider breaking complex requests into multiple simpler ones.                                                         |
| Unexpected image content        | Ambiguous prompt                      | Refine your prompt to be more specific. Include negative prompts to exclude unwanted elements.                                                                           |

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Image generation in Azure OpenAI](../../../openai/how-to/dall-e)
* [Responses API in Azure OpenAI](../../../openai/how-to/responses)
* [Content filter](../../../openai/concepts/content-filter)


<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="important">
  * The image generation tool requires the `gpt-image-1` model. See the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note?tabs=image) for limitations and responsible AI considerations.
  * You also need a compatible orchestrator model (`gpt-4o`, `gpt-4o-mini`, `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`, `o3`, or `gpt-5` series) deployed in the same Foundry project.
</Callout>

The **image generation tool** in Microsoft Foundry Agent Service generates images from text prompts in conversations and multistep workflows. Use it to create AI-generated visuals and return base64-encoded output that you can save to a file.

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* An Azure account with an active subscription.

* A Foundry project.

* A basic or standard agent environment. See [agent environment setup](../../environment-setup).

* Permissions to create and manage agent versions in the project.

* Two model deployments in the same Foundry project:

  * A compatible Azure OpenAI model deployment for the agent (for example, `gpt-4o`).
  * An image generation model deployment (`gpt-image-1`).

Set these environment variables for the samples:

* `FOUNDRY_PROJECT_ENDPOINT`
* `FOUNDRY_MODEL_DEPLOYMENT_NAME`
* `IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME`

## Configure the image generation tool

1. Deploy your orchestrator model (for example, `gpt-4o`) to your Foundry project.
2. Deploy `gpt-image-1` to the same Foundry project.
3. Confirm your region and model support for image generation. See [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).
4. Configure the environment variables listed in the prerequisites.

## Code examples

Before you start, install the `azure-ai-projects` package (version 2.0.0b1 or later). For package installation instructions, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).

<ZonePivot pivot="python">
  ## Create an agent with the image generation tool

  This sample creates an agent with the image generation tool, generates an image, and saves it to a file.

  ```python
  import base64
  import os

  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition, ImageGenTool

  project_client = AIProjectClient(
    endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
    credential=DefaultAzureCredential(),
  )

  with project_client:
    openai_client = project_client.get_openai_client()

    agent = project_client.agents.create_version(
      agent_name="agent-image-generation",
      definition=PromptAgentDefinition(
        model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
        instructions="Generate images based on user prompts.",
        tools=[ImageGenTool(quality="low", size="1024x1024")],
      ),
      description="Agent for image generation.",
    )
    print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

    response = openai_client.responses.create(
      input="Generate an image of the Microsoft logo.",
      extra_headers={
        "x-ms-oai-image-generation-deployment": os.environ["IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME"],
      },
      extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
    )
    print(f"Response created: {response.id}")

    image_items = [item for item in (response.output or []) if item.type == "image_generation_call"]
    if image_items and getattr(image_items[0], "result", None):
      print("Downloading generated image...")
      file_path = os.path.abspath("microsoft.png")
      with open(file_path, "wb") as f:
        f.write(base64.b64decode(image_items[0].result))
      print(f"Image downloaded and saved to: {file_path}")
    else:
      print("No image data found in the response.")

    project_client.agents.delete_version(agent.name, agent.version)
    print("Agent deleted")
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample for image generation in Azure.AI.Projects.OpenAI.

  In this example, you generate an image based on a simple prompt. The code in this example is synchronous. For an asynchronous example, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample2_Image_Generation.md) example in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Read the environment variables
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var imageGenerationDeploymentName = System.Environment.GetEnvironmentVariable("IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME");

  // Create the AI Project client with custom header policy
  AIProjectClientOptions projectOptions = new();
  projectOptions.AddPolicy(new HeaderPolicy(imageGenerationDeploymentName), PipelinePosition.PerCall);

  // Create the AI Project client
  AIProjectClient projectClient = new(
      endpoint: new Uri(projectEndpoint),
      tokenProvider: new DefaultAzureCredential(),
      options: projectOptions
  );

  // Use the client to create the versioned agent object.
  // To generate images, we need to provide agent with the ImageGenerationTool
  // when creating this tool. The ImageGenerationTool parameters include
  // the image generation model, image quality and resolution.
  // Supported image generation models include gpt-image-1.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
  Instructions = "Generate images based on user prompts.",
  Tools = {
          ResponseTool.CreateImageGenerationTool(
              model: imageGenerationDeploymentName,
              quality: ImageGenerationToolQuality.Low,
              size:ImageGenerationToolSize.W1024xH1024
          )
      }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  ProjectOpenAIClient openAIClient = projectClient.GetProjectOpenAIClient();
  ProjectResponsesClient responseClient = openAIClient.GetProjectResponsesClientForAgent(new AgentReference(name: agentVersion.Name));

  ResponseResult response = responseClient.CreateResponse("Generate parody of Newton with apple.");

  // Parse the ResponseResult object and save the generated image.
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is ImageGenerationCallResponseItem imageItem)
      {
          File.WriteAllBytes("newton.png", imageItem.ImageResultBytes.ToArray());
          Console.WriteLine($"Image downloaded and saved to: {Path.GetFullPath("newton.png")}");
      }
  }

  // Clean up resources by deleting the Agent.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);

  // To use image generation, provide the custom header to web requests,
  // which contain the model deployment name, for example:
  // `x-ms-oai-image-generation-deployment: gpt-image-1`.
  // To implement it, create a custom header policy.
  internal class HeaderPolicy(string image_deployment) : PipelinePolicy
  {
      private const string image_deployment_header = "x-ms-oai-image-generation-deployment";

      public override void Process(PipelineMessage message, IReadOnlyList<PipelinePolicy> pipeline, int currentIndex)
      {
          message.Request.Headers.Add(image_deployment_header, image_deployment);
          ProcessNext(message, pipeline, currentIndex);
      }

      public override async ValueTask ProcessAsync(PipelineMessage message, IReadOnlyList<PipelinePolicy> pipeline, int currentIndex)
      {
          // Add your desired header name and value
          message.Request.Headers.Add(image_deployment_header, image_deployment);
          await ProcessNextAsync(message, pipeline, currentIndex);
      }
  }
  ```

  ### Expected output

  When you run the sample, you see the following output:

  ```console
  Agent created (id: <agent-id>, name: myAgent, version: 1)
  Image downloaded and saved to: /path/to/newton.png
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="rest-api">
  ## Create an agent with the image generation tool

  The following example creates an agent that uses the image generation tool.

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/agents/$AGENTVERSION_NAME/versions?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -d '{
    "description": "Test agent for image generation capabilities",
    "definition": {
    "kind": "prompt",
    "model": "{{model}}",
    "tools": [
      {
        "type": "image_generation"
      }
    ],
      "instructions": "You are a creative assistant that generates images when requested. Please respond to image generation requests clearly and concisely."
    }
  }'
  ```

  ## Create a response

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -H "x-ms-oai-image-generation-deployment: $IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME" \
    -d '{
    "agent": {
      "type": "agent_reference",
      "name": "{{agentVersion.name}}",
      "version": "{{agentVersion.version}}"
    },
    "metadata": {
      "test_response": "image_generation_enabled",
      "test_scenario": "basic_imagegen"
    },
    "input": [{
      "type": "message",
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "Please generate small image of a sunset over a mountain lake."
        }
      ]
    }],
    "background": true,
    "stream": false
  }'
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Create an agent with image generation tool

  This sample demonstrates how to create an AI agent with image generation capabilities by using the Azure AI Projects client. The agent generates images based on text prompts and saves them to files. For a JavaScript example, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentImageGeneration.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as fs from "fs";
  import * as path from "path";
  import { fileURLToPath } from "url";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
    process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const imageDeploymentName =
    process.env["IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME"] || "<image generation deployment name>";

  export async function main(): Promise<void> {
    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with image generation tool...");

    // Create Agent with image generation tool
    const agent = await project.agents.createVersion("agent-image-generation", {
      kind: "prompt",
      model: deploymentName,
      instructions: "Generate images based on user prompts",
      tools: [
        {
          type: "image_generation",
          quality: "low",
          size: "1024x1024",
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Generate image using the agent
    console.log("\nGenerating image...");
    const response = await openAIClient.responses.create(
      {
        input: "Generate an image of Microsoft logo.",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
        headers: { "x-ms-oai-image-generation-deployment": imageDeploymentName },
      },
    );
    console.log(`Response created: ${response.id}`);

    // Extract and save the generated image
    const imageData = response.output?.filter((output) => output.type === "image_generation_call");

    if (imageData && imageData.length > 0 && imageData[0].result) {
      console.log("Downloading generated image...");

      const __filename = fileURLToPath(import.meta.url);
      const __dirname = path.dirname(__filename);
      const filename = "microsoft.png";
      const filePath = path.join(__dirname, filename);

      // Decode base64 and save to file
      const imageBuffer = Buffer.from(imageData[0].result, "base64");
      fs.writeFileSync(filePath, imageBuffer);

      console.log(`Image downloaded and saved to: ${path.resolve(filePath)}`);
    } else {
      console.log("No image data found in the response.");
    }

    // Clean up resources
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nImage generation sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  When you run the sample, you see the following output:

  ```console
  Creating agent with image generation tool...
  Agent created (id: <agent-id>, name: agent-image-generation, version: 1)
  Generating image...
  Response created: <response-id>
  Downloading generated image...
  Image downloaded and saved to: /path/to/microsoft.png
  Cleaning up resources...
  Agent deleted
  ```
</ZonePivot>

## When to use the image generation tool

The image generation tool in Agent Service offers advantages over the Azure OpenAI Image API:

| Advantage       | Description                                                                   |
| --------------- | ----------------------------------------------------------------------------- |
| Streaming       | Display partial image outputs during generation to improve perceived latency. |
| Flexible inputs | Accept image file IDs as inputs, in addition to raw image bytes.              |

## Optional parameters

Customize image generation by specifying these optional parameters when you create the tool:

| Parameter            | Description                                                              |
| -------------------- | ------------------------------------------------------------------------ |
| `size`               | Image size. One of `1024x1024`, `1024x1536`, `1536x1024`, or `auto`.     |
| `quality`            | Image quality. One of `low`, `medium`, `high`, or `auto`.                |
| `background`         | Background type. One of `transparent`, `opaque`, or `auto`.              |
| `output_format`      | Output format. One of `png`, `webp`, or `jpeg`.                          |
| `output_compression` | Compression level for `webp` and `jpeg` output (0-100).                  |
| `moderation`         | Moderation level for the generated image. One of `auto` or `low`.        |
| `partial_images`     | Number of partial images to generate in streaming mode (0-3).            |
| `input_image_mask`   | Optional mask for inpainting. Provide `image_url` (base64) or `file_id`. |

<Callout type="note">
  Image generation time varies based on the `quality` setting and prompt complexity. For time-sensitive applications, consider using `quality: "low"` or enabling `partial_images` for streaming.
</Callout>

Use the Responses API if you want to:

* Build conversational image experiences with GPT Image.
* Stream partial image results during generation for a smoother user experience.

## Write effective text-to-image prompts

Effective prompts produce better images. Describe the subject, visual style, and composition you want. Use action words like "draw," "create," or "edit" to guide the model's output.

Content filtering can block image generation if the service detects unsafe content in your prompt. For more information, see [Content filter](../../../openai/concepts/content-filter).

<Callout type="tip">
  For a thorough look at how you can tweak your text prompts to generate different kinds of images, see [Image prompt engineering techniques](../../../openai/concepts/gpt-4-v-prompt-engineering).
</Callout>

## Verify tool execution

Use either of these approaches to confirm that image generation ran successfully:

* In the response payload, look for an output item with `type` set to `image_generation_call`.
* In the Foundry portal, open tracing/debug for your run to confirm the tool call and inspect inputs and outputs.

When image generation succeeds, the response includes an `image_generation_call` output item with a `result` field containing base64-encoded image data.

If you see only text output and no `image_generation_call` item, the request might not be routed to image generation. Review the troubleshooting section.

## Troubleshooting

| Issue                           | Cause                                 | Resolution                                                                                                                                                               |
| ------------------------------- | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Image generation fails          | Missing deployment                    | Verify both the orchestrator model (for example, `gpt-4o`) and `gpt-image-1` deployments exist in the same Foundry project.                                              |
| Image generation fails          | Missing or incorrect header           | Verify the header `x-ms-oai-image-generation-deployment` is present on the Responses request and matches your image generation deployment name.                          |
| Agent uses wrong deployment     | Environment variable misconfiguration | Confirm `FOUNDRY_MODEL_DEPLOYMENT_NAME` is set to your orchestrator deployment name, not the image generation deployment.                                                |
| Prompt doesn't produce an image | Content filtering blocked the request | Check content filtering logs. See [Content filter](../../../openai/concepts/content-filter) for guidelines on acceptable prompts.                                        |
| Tool not available              | Regional or model limitation          | Confirm the image generation tool is available in your region and with your orchestrator model. See [Best practices for using tools](../../concepts/tool-best-practice). |
| Generated image has low quality | Prompt lacks detail                   | Provide more specific and detailed prompts describing the desired image style, composition, and elements.                                                                |
| Image generation times out      | Large or complex image request        | Simplify the prompt or increase timeout settings. Consider breaking complex requests into multiple simpler ones.                                                         |
| Unexpected image content        | Ambiguous prompt                      | Refine your prompt to be more specific. Include negative prompts to exclude unwanted elements.                                                                           |

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Image generation in Azure OpenAI](../../../openai/how-to/dall-e)
* [Responses API in Azure OpenAI](../../../openai/how-to/responses)
* [Content filter](../../../openai/concepts/content-filter)


<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="important">
  * The image generation tool requires the `gpt-image-1` model. See the [Azure OpenAI transparency note](../../../responsible-ai/openai/transparency-note?tabs=image) for limitations and responsible AI considerations.
  * You also need a compatible orchestrator model (`gpt-4o`, `gpt-4o-mini`, `gpt-4.1`, `gpt-4.1-mini`, `gpt-4.1-nano`, `o3`, or `gpt-5` series) deployed in the same Foundry project.
</Callout>

The **image generation tool** in Microsoft Foundry Agent Service generates images from text prompts in conversations and multistep workflows. Use it to create AI-generated visuals and return base64-encoded output that you can save to a file.

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* An Azure account with an active subscription.

* A Foundry project.

* A basic or standard agent environment. See [agent environment setup](../../environment-setup).

* Permissions to create and manage agent versions in the project.

* Two model deployments in the same Foundry project:

  * A compatible Azure OpenAI model deployment for the agent (for example, `gpt-4o`).
  * An image generation model deployment (`gpt-image-1`).

Set these environment variables for the samples:

* `FOUNDRY_PROJECT_ENDPOINT`
* `FOUNDRY_MODEL_DEPLOYMENT_NAME`
* `IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME`

## Configure the image generation tool

1. Deploy your orchestrator model (for example, `gpt-4o`) to your Foundry project.
2. Deploy `gpt-image-1` to the same Foundry project.
3. Confirm your region and model support for image generation. See [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).
4. Configure the environment variables listed in the prerequisites.

## Code examples

Before you start, install the `azure-ai-projects` package (version 2.0.0b1 or later). For package installation instructions, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).

<ZonePivot pivot="python">
  ## Create an agent with the image generation tool

  This sample creates an agent with the image generation tool, generates an image, and saves it to a file.

  ```python
  import base64
  import os

  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition, ImageGenTool

  project_client = AIProjectClient(
    endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
    credential=DefaultAzureCredential(),
  )

  with project_client:
    openai_client = project_client.get_openai_client()

    agent = project_client.agents.create_version(
      agent_name="agent-image-generation",
      definition=PromptAgentDefinition(
        model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
        instructions="Generate images based on user prompts.",
        tools=[ImageGenTool(quality="low", size="1024x1024")],
      ),
      description="Agent for image generation.",
    )
    print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

    response = openai_client.responses.create(
      input="Generate an image of the Microsoft logo.",
      extra_headers={
        "x-ms-oai-image-generation-deployment": os.environ["IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME"],
      },
      extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
    )
    print(f"Response created: {response.id}")

    image_items = [item for item in (response.output or []) if item.type == "image_generation_call"]
    if image_items and getattr(image_items[0], "result", None):
      print("Downloading generated image...")
      file_path = os.path.abspath("microsoft.png")
      with open(file_path, "wb") as f:
        f.write(base64.b64decode(image_items[0].result))
      print(f"Image downloaded and saved to: {file_path}")
    else:
      print("No image data found in the response.")

    project_client.agents.delete_version(agent.name, agent.version)
    print("Agent deleted")
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample for image generation in Azure.AI.Projects.OpenAI.

  In this example, you generate an image based on a simple prompt. The code in this example is synchronous. For an asynchronous example, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample2_Image_Generation.md) example in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Read the environment variables
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var imageGenerationDeploymentName = System.Environment.GetEnvironmentVariable("IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME");

  // Create the AI Project client with custom header policy
  AIProjectClientOptions projectOptions = new();
  projectOptions.AddPolicy(new HeaderPolicy(imageGenerationDeploymentName), PipelinePosition.PerCall);

  // Create the AI Project client
  AIProjectClient projectClient = new(
      endpoint: new Uri(projectEndpoint),
      tokenProvider: new DefaultAzureCredential(),
      options: projectOptions
  );

  // Use the client to create the versioned agent object.
  // To generate images, we need to provide agent with the ImageGenerationTool
  // when creating this tool. The ImageGenerationTool parameters include
  // the image generation model, image quality and resolution.
  // Supported image generation models include gpt-image-1.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
  Instructions = "Generate images based on user prompts.",
  Tools = {
          ResponseTool.CreateImageGenerationTool(
              model: imageGenerationDeploymentName,
              quality: ImageGenerationToolQuality.Low,
              size:ImageGenerationToolSize.W1024xH1024
          )
      }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  ProjectOpenAIClient openAIClient = projectClient.GetProjectOpenAIClient();
  ProjectResponsesClient responseClient = openAIClient.GetProjectResponsesClientForAgent(new AgentReference(name: agentVersion.Name));

  ResponseResult response = responseClient.CreateResponse("Generate parody of Newton with apple.");

  // Parse the ResponseResult object and save the generated image.
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is ImageGenerationCallResponseItem imageItem)
      {
          File.WriteAllBytes("newton.png", imageItem.ImageResultBytes.ToArray());
          Console.WriteLine($"Image downloaded and saved to: {Path.GetFullPath("newton.png")}");
      }
  }

  // Clean up resources by deleting the Agent.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);

  // To use image generation, provide the custom header to web requests,
  // which contain the model deployment name, for example:
  // `x-ms-oai-image-generation-deployment: gpt-image-1`.
  // To implement it, create a custom header policy.
  internal class HeaderPolicy(string image_deployment) : PipelinePolicy
  {
      private const string image_deployment_header = "x-ms-oai-image-generation-deployment";

      public override void Process(PipelineMessage message, IReadOnlyList<PipelinePolicy> pipeline, int currentIndex)
      {
          message.Request.Headers.Add(image_deployment_header, image_deployment);
          ProcessNext(message, pipeline, currentIndex);
      }

      public override async ValueTask ProcessAsync(PipelineMessage message, IReadOnlyList<PipelinePolicy> pipeline, int currentIndex)
      {
          // Add your desired header name and value
          message.Request.Headers.Add(image_deployment_header, image_deployment);
          await ProcessNextAsync(message, pipeline, currentIndex);
      }
  }
  ```

  ### Expected output

  When you run the sample, you see the following output:

  ```console
  Agent created (id: <agent-id>, name: myAgent, version: 1)
  Image downloaded and saved to: /path/to/newton.png
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="rest-api">
  ## Create an agent with the image generation tool

  The following example creates an agent that uses the image generation tool.

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/agents/$AGENTVERSION_NAME/versions?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -d '{
    "description": "Test agent for image generation capabilities",
    "definition": {
    "kind": "prompt",
    "model": "{{model}}",
    "tools": [
      {
        "type": "image_generation"
      }
    ],
      "instructions": "You are a creative assistant that generates images when requested. Please respond to image generation requests clearly and concisely."
    }
  }'
  ```

  ## Create a response

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -H "x-ms-oai-image-generation-deployment: $IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME" \
    -d '{
    "agent": {
      "type": "agent_reference",
      "name": "{{agentVersion.name}}",
      "version": "{{agentVersion.version}}"
    },
    "metadata": {
      "test_response": "image_generation_enabled",
      "test_scenario": "basic_imagegen"
    },
    "input": [{
      "type": "message",
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "Please generate small image of a sunset over a mountain lake."
        }
      ]
    }],
    "background": true,
    "stream": false
  }'
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Create an agent with image generation tool

  This sample demonstrates how to create an AI agent with image generation capabilities by using the Azure AI Projects client. The agent generates images based on text prompts and saves them to files. For a JavaScript example, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentImageGeneration.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as fs from "fs";
  import * as path from "path";
  import { fileURLToPath } from "url";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
    process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const imageDeploymentName =
    process.env["IMAGE_GENERATION_MODEL_DEPLOYMENT_NAME"] || "<image generation deployment name>";

  export async function main(): Promise<void> {
    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with image generation tool...");

    // Create Agent with image generation tool
    const agent = await project.agents.createVersion("agent-image-generation", {
      kind: "prompt",
      model: deploymentName,
      instructions: "Generate images based on user prompts",
      tools: [
        {
          type: "image_generation",
          quality: "low",
          size: "1024x1024",
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Generate image using the agent
    console.log("\nGenerating image...");
    const response = await openAIClient.responses.create(
      {
        input: "Generate an image of Microsoft logo.",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
        headers: { "x-ms-oai-image-generation-deployment": imageDeploymentName },
      },
    );
    console.log(`Response created: ${response.id}`);

    // Extract and save the generated image
    const imageData = response.output?.filter((output) => output.type === "image_generation_call");

    if (imageData && imageData.length > 0 && imageData[0].result) {
      console.log("Downloading generated image...");

      const __filename = fileURLToPath(import.meta.url);
      const __dirname = path.dirname(__filename);
      const filename = "microsoft.png";
      const filePath = path.join(__dirname, filename);

      // Decode base64 and save to file
      const imageBuffer = Buffer.from(imageData[0].result, "base64");
      fs.writeFileSync(filePath, imageBuffer);

      console.log(`Image downloaded and saved to: ${path.resolve(filePath)}`);
    } else {
      console.log("No image data found in the response.");
    }

    // Clean up resources
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nImage generation sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  When you run the sample, you see the following output:

  ```console
  Creating agent with image generation tool...
  Agent created (id: <agent-id>, name: agent-image-generation, version: 1)
  Generating image...
  Response created: <response-id>
  Downloading generated image...
  Image downloaded and saved to: /path/to/microsoft.png
  Cleaning up resources...
  Agent deleted
  ```
</ZonePivot>

## When to use the image generation tool

The image generation tool in Agent Service offers advantages over the Azure OpenAI Image API:

| Advantage       | Description                                                                   |
| --------------- | ----------------------------------------------------------------------------- |
| Streaming       | Display partial image outputs during generation to improve perceived latency. |
| Flexible inputs | Accept image file IDs as inputs, in addition to raw image bytes.              |

## Optional parameters

Customize image generation by specifying these optional parameters when you create the tool:

| Parameter            | Description                                                              |
| -------------------- | ------------------------------------------------------------------------ |
| `size`               | Image size. One of `1024x1024`, `1024x1536`, `1536x1024`, or `auto`.     |
| `quality`            | Image quality. One of `low`, `medium`, `high`, or `auto`.                |
| `background`         | Background type. One of `transparent`, `opaque`, or `auto`.              |
| `output_format`      | Output format. One of `png`, `webp`, or `jpeg`.                          |
| `output_compression` | Compression level for `webp` and `jpeg` output (0-100).                  |
| `moderation`         | Moderation level for the generated image. One of `auto` or `low`.        |
| `partial_images`     | Number of partial images to generate in streaming mode (0-3).            |
| `input_image_mask`   | Optional mask for inpainting. Provide `image_url` (base64) or `file_id`. |

<Callout type="note">
  Image generation time varies based on the `quality` setting and prompt complexity. For time-sensitive applications, consider using `quality: "low"` or enabling `partial_images` for streaming.
</Callout>

Use the Responses API if you want to:

* Build conversational image experiences with GPT Image.
* Stream partial image results during generation for a smoother user experience.

## Write effective text-to-image prompts

Effective prompts produce better images. Describe the subject, visual style, and composition you want. Use action words like "draw," "create," or "edit" to guide the model's output.

Content filtering can block image generation if the service detects unsafe content in your prompt. For more information, see [Content filter](../../../openai/concepts/content-filter).

<Callout type="tip">
  For a thorough look at how you can tweak your text prompts to generate different kinds of images, see [Image prompt engineering techniques](../../../openai/concepts/gpt-4-v-prompt-engineering).
</Callout>

## Verify tool execution

Use either of these approaches to confirm that image generation ran successfully:

* In the response payload, look for an output item with `type` set to `image_generation_call`.
* In the Foundry portal, open tracing/debug for your run to confirm the tool call and inspect inputs and outputs.

When image generation succeeds, the response includes an `image_generation_call` output item with a `result` field containing base64-encoded image data.

If you see only text output and no `image_generation_call` item, the request might not be routed to image generation. Review the troubleshooting section.

## Troubleshooting

| Issue                           | Cause                                 | Resolution                                                                                                                                                               |
| ------------------------------- | ------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Image generation fails          | Missing deployment                    | Verify both the orchestrator model (for example, `gpt-4o`) and `gpt-image-1` deployments exist in the same Foundry project.                                              |
| Image generation fails          | Missing or incorrect header           | Verify the header `x-ms-oai-image-generation-deployment` is present on the Responses request and matches your image generation deployment name.                          |
| Agent uses wrong deployment     | Environment variable misconfiguration | Confirm `FOUNDRY_MODEL_DEPLOYMENT_NAME` is set to your orchestrator deployment name, not the image generation deployment.                                                |
| Prompt doesn't produce an image | Content filtering blocked the request | Check content filtering logs. See [Content filter](../../../openai/concepts/content-filter) for guidelines on acceptable prompts.                                        |
| Tool not available              | Regional or model limitation          | Confirm the image generation tool is available in your region and with your orchestrator model. See [Best practices for using tools](../../concepts/tool-best-practice). |
| Generated image has low quality | Prompt lacks detail                   | Provide more specific and detailed prompts describing the desired image style, composition, and elements.                                                                |
| Image generation times out      | Large or complex image request        | Simplify the prompt or increase timeout settings. Consider breaking complex requests into multiple simpler ones.                                                         |
| Unexpected image content        | Ambiguous prompt                      | Refine your prompt to be more specific. Include negative prompts to exclude unwanted elements.                                                                           |

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Image generation in Azure OpenAI](../../../openai/how-to/dall-e)
* [Responses API in Azure OpenAI](../../../openai/how-to/responses)
* [Content filter](../../../openai/concepts/content-filter)


<Callout type="important">
  Memory (preview) in Foundry Agent Service and the Memory Store API (preview) are licensed to you as part of your Azure subscription and are subject to terms applicable to "Previews" in the [Microsoft Product Terms](https://www.microsoft.com/licensing/terms/product/ForOnlineServices/all) and the [Microsoft Products and Services Data Protection Addendum](https://aka.ms/DPA), as well as the Microsoft Generative AI Services Previews terms in the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

Memory in Microsoft Foundry Agent Service is a managed, long-term memory solution. It enables agent continuity across sessions, devices, and workflows. By creating and managing memory stores, you can build agents that retain user preferences, maintain conversation history, and deliver personalized experiences.

This article provides an overview of agent memory, including its concepts, use cases, and limitations. For usage instructions, see [Create and use memory in Foundry Agent Service](../how-to/memory-usage).

## What is memory?

Memory is persistent knowledge retained by an agent across sessions. Generally, agent memory falls into two categories:

* **Short-term memory** tracks the current session's conversation and maintains immediate context for ongoing interactions. Agent orchestration frameworks typically manage this memory as part of the session context.

* **Long-term memory** retains distilled knowledge across sessions. The model can recall and build on previous user interactions over time. Long-term memory requires a persistent system that extracts, consolidates, and manages knowledge.

Memory in Foundry Agent Service is designed for long-term memory. It extracts meaningful information from conversations, consolidates it into durable knowledge, and makes it available across sessions.

## How memory works

Behind the scenes, memories are stored as items in a managed memory store. The system may apply consolidation and conflict‑resolution logic where applicable (for example, to merge duplicate or overlapping user profile information).

<Callout type="note">
  Consolidation behavior can vary by memory type and may change during preview. For the latest behavior, see [Create and use memory in Foundry Agent Service](../how-to/memory-usage).
</Callout>

Memory operates in the following phases:

1. **Extraction:** When a user interacts with an agent, the system actively extracts key information from the conversation, such as user preferences, facts, and relevant context. For example, preferences like "allergic to dairy" and summaries of recent activities are identified and stored.

2. **Consolidation:** Extracted memories are consolidated to keep the memory store efficient and relevant. The system uses LLMs to merge similar or duplicate topics so that the agent doesn't store redundant information. Conflicting facts, such as a new allergy, are resolved to maintain an accurate memory.

3. **Retrieval:** When the agent needs to recall information, it searches the memory store for the most relevant memories. This allows the agent to quickly surface the right context, making conversations feel natural and informed. For best results, retrieve stable user profile information early in the conversation so the agent can personalize responses.

Here's an example of how memory can improve and personalize interactions between a recipe agent and a user who previously expressed a food allergy:

![Diagram that shows memory extraction, storage, and retrieval for an agent across sessions.](https://learn.microsoft.com/azure/ai-foundry/agents/default/agents/media/memory/agent-memory-diagram.svg)

<Callout type="tip">
  Need help deciding when to use memory? Consider these guidelines:

  * Use memory for user-specific context that persists over time.
  * Use a [Foundry IQ](what-is-foundry-iq) knowledge base to ground your agent on curated organizational content.
  * Use the [file search tool](../how-to/tools/file-search) to search user-provided documents during an interaction.
</Callout>

## Memory types

Memory in Foundry Agent Service extracts and stores two types of long-term memory:

| Type                | Description                                                                                                                                                                                                                                                                                                                          | Configuration                                                                                           |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------- |
| User profile memory | Information and preferences about the user, such as preferred name, dietary restrictions, and language preference. These memories are considered "static" with respect to a conversation because they generally don't depend on the current chat context. Retrieve user profile memories once at the beginning of each conversation. | Specify `user_profile_details` in a [memory store](../how-to/memory-usage#customize-memory).            |
| Chat summary memory | A distilled summary of each topic or thread covered in a chat session. These memories allow users to continue conversations or reference prior sessions without repeating earlier context. Retrieve chat summary memories based on the current conversation to surface relevant threads.                                             | Set `chat_summary_enabled` to `true` in a [memory store](../how-to/memory-usage#create-a-memory-store). |

## Working with memory

There are two ways to use memory for agent interactions:

* **Memory search tool:** Attach the memory search tool to a prompt agent to enable reading from and writing to the memory store during conversations. This approach is ideal for most scenarios because it simplifies memory management. For more information, see [Use memories via an agent tool](../how-to/memory-usage#use-memories-via-an-agent-tool).

* **Memory store APIs:** Interact directly with the memory store using the low-level APIs. This approach provides more control and flexibility for advanced use cases. For more information, see [Use memories via APIs](../how-to/memory-usage#use-memories-via-apis).

## Use cases

The following examples illustrate how memory can enhance various types of agents.

<Tabs>
  <Tab title="Conversational agent">
    * A customer support agent that remembers your name, previous issues and resolutions, ticket numbers, and your preferred contact method (chat, email, or call back). This memory helps you avoid repeating information, so conversations are more efficient and satisfying.

    * A personal shopping assistant that remembers your size in specific brands, preferred colors, past returns, and recent purchases. The agent can suggest relevant items as soon as you start a session and avoid recommending products you already own.
  </Tab>

  <Tab title="Planning agent">
    * A travel agent that knows your flight preferences (window or aisle), seat selections, food choices, nonstop versus connecting flights, loyalty programs, and feedback from past trips. The agent uses this information to quickly build an optimized itinerary.

    * An architectural design agent that remembers local building codes, material costs from previous bids, and initial client feedback. The agent refines designs iteratively, ensuring the final plan is feasible and meets all requirements.
  </Tab>

  <Tab title="Research agent">
    * A medical research agent that remembers which compounds were previously tested and failed, key findings from different labs, and complex relationships between proteins. The agent uses this knowledge to suggest new, untested research hypotheses.
  </Tab>
</Tabs>

## Security risks

When you work with memory in Foundry Agent Service, the large language model (LLM) extracts and consolidates memories based on conversations. Protect memory against threats such as prompt injection and memory corruption. These risks arise when incorrect or harmful data is stored in the agent's memory, potentially influencing agent responses and actions.

To mitigate security risks, consider these actions:

* **Use [Azure AI Content Safety](https://ai.azure.com/explore/contentsafety) and its [prompt injection detection](../../../ai-services/content-safety/concepts/jailbreak-detection):** Validate all prompts entering or leaving the memory system to prevent malicious content.

* **Perform attack and adversarial testing:** Regularly stress-test your agent for injection vulnerabilities through controlled adversarial exercises.

## Limitations and quotas

* Memory currently requires compatible Azure OpenAI chat and embedding model deployments. For a list of supported models, see [Azure OpenAI models and regions for Foundry Agent Service](model-region-support).
* You must set the `scope` value explicitly. Automatic population from the user identity specified in the request isn't currently supported.

### Quotas

* Maximum scopes per memory store: 100
* Maximum memories per scope: 10,000
* Search memories: 1,000 requests per minute
* Update memories: 1,000 requests per minute

For broader Foundry Agent Service quotas and limits, see [Foundry Agent Service quotas and limits](../quotas-limits).

## Pricing

Memory is currently in public preview. Pricing and billing for memory and the Memory Store API can change during preview.

You're billed for usage of the underlying chat and embedding models you configure. For current pricing details, see [Foundry Agent Service pricing](https://azure.microsoft.com/pricing/details/foundry-agent-service/?msockid=053845effeba692426b55062faba6f36).

## Related content

* Follow the end-to-end setup: [Create and use memory in Foundry Agent Service](../how-to/memory-usage).
* Confirm model availability: [Azure OpenAI models and regions for Foundry Agent Service](model-region-support).
* Build a complete agent: [Microsoft Foundry Quickstart](../../quickstarts/get-started-code).

<Callout type="important">
  Memory (preview) in Foundry Agent Service and the Memory Store API (preview) are licensed to you as part of your Azure subscription and are subject to terms applicable to "Previews" in the [Microsoft Product Terms](https://www.microsoft.com/licensing/terms/product/ForOnlineServices/all) and the [Microsoft Products and Services Data Protection Addendum](https://aka.ms/DPA), as well as the Microsoft Generative AI Services Previews terms in the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

Memory in Foundry Agent Service is a managed, long-term memory solution. It enables agent continuity across sessions, devices, and workflows. By creating and managing memory stores, you can build agents that retain user preferences, maintain conversation history, and deliver personalized experiences.

Memory stores act as persistent storage, defining which types of information are relevant to each agent. You control access using the `scope` parameter, which segments memory across users to ensure secure and isolated experiences.

This article explains how to create, manage, and use memory stores. For conceptual information, see [Memory in Foundry Agent Service](../concepts/what-is-memory).

### Usage support

| Capability                                     | Python SDK | REST API |
| ---------------------------------------------- | ---------- | -------- |
| Create, update, list, and delete memory stores | ✔️         | ✔️       |
| Update and search memories                     | ✔️         | ✔️       |
| Attach memory to a prompt agent                | ✔️         | ✔️       |

## Prerequisites

* An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).

* A [Microsoft Foundry project](../../how-to/create-projects) with [authorization and permissions](#authorization-and-permissions) configured.

* [Chat model deployment](../../foundry-models/how-to/create-model-deployments) (for example, `gpt-5.2`) in your project.

* [Embedding model deployment](../../openai/tutorials/embeddings) (for example, `text-embedding-3-small`) in your project.

* For Python examples:

  * Python 3.8 or later with a [configured environment](../../quickstarts/get-started-code?tabs=python\&view=foundry\&preserve-view=true)
  * Required packages: `pip install azure-ai-projects azure-identity`

* For REST API examples, Azure CLI authenticated to your subscription.

### Authorization and permissions

We recommend [role-based access control](../../concepts/rbac-foundry) for production deployments. If roles aren't feasible, skip this section and use key-based authentication instead.

To configure role-based access:

1. Sign in to the [Azure portal](https://portal.azure.com/).

2. On your project:

   1. From the left pane, select **Resource Management** > **Identity**.
   2. Use the toggle to enable a system-assigned managed identity.

3. On the resource that contains your project:

   1. From the left pane, select **Access control (IAM)**.
   2. Select **Add** > **Add role assignment**.
   3. Assign **Azure AI User** to the managed identity of your project.

### Set project endpoint

For the Python examples in this article, set an environment variable for your project endpoint:

```bash
export FOUNDRY_PROJECT_ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
```

```powershell
$env:FOUNDRY_PROJECT_ENDPOINT = "https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
```

## Understand scope

The `scope` parameter controls how memory is partitioned. Each scope in the memory store keeps an isolated collection of memory items. For example, if you create a customer support agent with memory, each customer should have their own individual memory.

As a developer, you choose the key used to store and retrieve memory items. You can pass a static value, such as a universally unique identifier (UUID) or another stable identifier from your system.

Alternatively, when you specify `{{$userId}}` as the scope, the system automatically extracts the tenant ID (TID) and object ID (OID) from the request authentication header. This approach gives each authenticated user their own isolated memory partition, eliminating the need to manage identifiers manually.

## Create a memory store

Create a dedicated memory store for each agent to establish clear boundaries for memory access and optimization. When you create a memory store, specify the chat model and embedding model deployments that process your memory content.

<Tabs>
  <Tab title="Python">
    ```python
    import os
    from azure.ai.projects import AIProjectClient
    from azure.ai.projects.models import MemoryStoreDefaultDefinition, MemoryStoreDefaultOptions
    from azure.identity import DefaultAzureCredential

    project_client = AIProjectClient(
        endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
        credential=DefaultAzureCredential(),
    )

    memory_store_name = "my_memory_store"

    # Specify memory store options
    options = MemoryStoreDefaultOptions(
        chat_summary_enabled=True,
        user_profile_enabled=True,
        user_profile_details="Avoid irrelevant or sensitive data, such as age, financials, precise location, and credentials"
    )

    # Create memory store
    definition = MemoryStoreDefaultDefinition(
        chat_model="gpt-5.2",  # Your chat model deployment name
        embedding_model="text-embedding-3-small",  # Your embedding model deployment name
        options=options
    )

    memory_store = project_client.memory_stores.create(
        name=memory_store_name,
        definition=definition,
        description="Memory store for customer support agent",
    )

    print(f"Created memory store: {memory_store.name}")
    ```
  </Tab>

  <Tab title="REST API">
    ```bash
    # Configuration
    ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
    API_VERSION="2025-11-15-preview"

    # Get a short-lived access token using Azure CLI
    ACCESS_TOKEN="$(az account get-access-token --resource https://ai.azure.com/ --query accessToken -o tsv)"

    curl -X POST "${ENDPOINT}/memory_stores?api-version=${API_VERSION}" \
      -H "Authorization: Bearer ${ACCESS_TOKEN}" \
      -H "Content-Type: application/json" \
      -d '{
        "name": "my_memory_store",
        "description": "Memory store for customer support agent",
        "definition": {
          "kind": "default",
          "chat_model": "gpt-5.2",
          "embedding_model": "text-embedding-3-small",
          "options": {
            "chat_summary_enabled": true,
            "user_profile_enabled": true,
            "user_profile_details": "Avoid irrelevant or sensitive data, such as age, financials, precise location, and credentials"
          }
        }
      }'
    ```
  </Tab>
</Tabs>

### Customize memory

Customize what information the agent stores to keep memory efficient, relevant, and privacy-respecting. Use the `user_profile_details` parameter to specify the types of data that are critical to the agent's function.

For example, set `user_profile_details` to prioritize "flight carrier preference and dietary restrictions" for a travel agent. This focused approach helps the memory system know which details to extract, summarize, and commit to long-term memory.

You can also use this parameter to exclude certain types of data, keeping memory lean and compliant with privacy requirements. For example, set `user_profile_details` to "avoid irrelevant or sensitive data, such as age, financials, precise location, and credentials."

## Update a memory store

Update memory store properties, such as `description` or `metadata`, to better manage memory stores.

<Tabs>
  <Tab title="Python">
    ```python
    # Update memory store properties
    updated_store = project_client.memory_stores.update(
        name=memory_store_name,
        description="Updated description"
    )

    print(f"Updated: {updated_store.description}")
    ```
  </Tab>

  <Tab title="REST API">
    ```bash
    # Configuration
    ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
    API_VERSION="2025-11-15-preview"
    ACCESS_TOKEN="$(az account get-access-token --resource https://ai.azure.com/ --query accessToken -o tsv)"

    MEMORY_STORE_NAME="my_memory_store"

    curl -X POST "${ENDPOINT}/memory_stores/${MEMORY_STORE_NAME}?api-version=${API_VERSION}" \
      -H "Authorization: Bearer ${ACCESS_TOKEN}" \
      -H "Content-Type: application/json" \
      -d '{
        "description": "Updated description"
      }'
    ```
  </Tab>
</Tabs>

## List memory stores

Retrieve a list of memory stores in your project to manage and monitor your memory infrastructure.

<Tabs>
  <Tab title="Python">
    ```python
    # List all memory stores
    stores_list = project_client.memory_stores.list()

    print(f"Found {len(stores_list.data)} memory stores")
    for store in stores_list.data:
        print(f"- {store.name} ({store.description})")
    ```
  </Tab>

  <Tab title="REST API">
    ```bash
    # Configuration
    ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
    API_VERSION="2025-11-15-preview"
    ACCESS_TOKEN="$(az account get-access-token --resource https://ai.azure.com/ --query accessToken -o tsv)"

    curl -X GET "${ENDPOINT}/memory_stores?api-version=${API_VERSION}" \
      -H "Authorization: Bearer ${ACCESS_TOKEN}"
    ```
  </Tab>
</Tabs>

## Use memories via an agent tool

After you create a memory store, you can attach the memory search tool to a prompt agent. This tool enables the agent to read from and write to your memory store during conversations. Configure the tool with the appropriate `scope` and `update_delay` to control how and when memories are updated.

<Tabs>
  <Tab title="Python">
    ```python
    # Continue from the previous Python snippets.
    from azure.ai.projects.models import MemorySearchTool, PromptAgentDefinition

    # Set scope to associate the memories with
    # You can also use "{{$userId}}" to take the TID and OID of the request authentication header
    scope = "user_123"

    openai_client = project_client.get_openai_client()

    # Create memory search tool
    tool = MemorySearchTool(
        memory_store_name=memory_store_name,
        scope=scope,
        update_delay=1,  # Wait 1 second of inactivity before updating memories
        # In a real application, set this to a higher value like 300 (5 minutes, default)
    )

    # Create a prompt agent with memory search tool
    agent = project_client.agents.create_version(
        agent_name="MyAgent",
        definition=PromptAgentDefinition(
            model="gpt-5.2",
            instructions="You are a helpful assistant that answers general questions",
            tools=[tool],
        )
    )

    print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")
    ```
  </Tab>

  <Tab title="REST API">
    ```bash
    # Configuration
    ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
    API_VERSION="2025-11-15-preview"
    ACCESS_TOKEN="$(az account get-access-token --resource https://ai.azure.com/ --query accessToken -o tsv)"

    curl -X POST "${ENDPOINT}/agents/MyAgent/versions?api-version=${API_VERSION}" \
      -H "Authorization: Bearer ${ACCESS_TOKEN}" \
      -H "Content-Type: application/json" \
      -d '{
        "definition": {
            "kind": "prompt",
            "model": "gpt-5.2",
            "instructions": "You are a helpful assistant that answers general questions",
            "tools": [
                {
                  "type": "memory_search",
                  "memory_store_name": "my_memory_store",
                  "scope": "user_123",
                  "update_delay": 1
                }
            ]
        }
    }'
    ```
  </Tab>
</Tabs>

### Create a conversation

You can now create conversations and request agent responses. At the start of each conversation, static memories are injected so the agent has immediate, persistent context. Contextual memories are retrieved per turn based on the latest messages to inform each response.

After each agent response, the service internally calls `update_memories`. However, actual writes to long‑term memory are debounced by the `update_delay` setting. The update is scheduled and only completes after the configured period of inactivity.

<Tabs>
  <Tab title="Python">
    ```python
    import time

    # Create a conversation with the agent with memory tool enabled
    conversation = openai_client.conversations.create()
    print(f"Created conversation (id: {conversation.id})")

    # Create an agent response to initial user message
    response = openai_client.responses.create(
        input="I prefer dark roast coffee",
        conversation=conversation.id,
        extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
    )

    print(f"Response output: {response.output_text}")

    # After an inactivity in the conversation, memories will be extracted from the conversation and stored
    print("Waiting for memories to be stored...")
    time.sleep(65)

    # Create a new conversation
    new_conversation = openai_client.conversations.create()
    print(f"Created new conversation (id: {new_conversation.id})")

    # Create an agent response with stored memories
    new_response = openai_client.responses.create(
        input="Please order my usual coffee",
        conversation=new_conversation.id,
        extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
    )

    print(f"Response output: {new_response.output_text}")
    ```
  </Tab>

  <Tab title="REST API">
    ```bash
    # Configuration
    ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
    API_VERSION="2025-11-15-preview"
    ACCESS_TOKEN="$(az account get-access-token --resource https://ai.azure.com/ --query accessToken -o tsv)"

    curl -X POST "${ENDPOINT}/openai/conversations?api-version=${API_VERSION}" \
        -H "Authorization: Bearer ${ACCESS_TOKEN}" \
        -H "Content-Type: application/json" \
        -d '{}'

    # Copy the "id" field from the previous response.
    curl -X POST "${ENDPOINT}/openai/responses?api-version=${API_VERSION}" \
        -H "Authorization: Bearer ${ACCESS_TOKEN}" \
        -H "Content-Type: application/json" \
        -d '{
          "input": "I prefer dark roast coffee",
          "conversation": "{conversation-id}",
          "agent": {
            "name": "MyAgent",
            "type": "agent_reference"
          }
        }'
    ```
  </Tab>
</Tabs>

## Use memories via APIs

You can interact with a memory store directly using the memory store APIs. Start by adding memories from conversation content to the memory store, and then search for relevant memories to provide context for agent interactions.

### Add memories to a memory store

Add memories by providing conversation content to the memory store. The system preprocesses and postprocesses the data, including memory extraction and consolidation, to optimize the agent's memory. This long-running operation might take about one minute.

Decide how to segment memory across users by specifying the `scope` parameter. You can scope the memory to a specific end user, a team, or another identifier.

You can update a memory store with content from multiple conversation turns, or update after each turn and chain updates using the previous update operation ID.

<Tabs>
  <Tab title="Python">
    ```python
    # Continue from the previous Python snippets.
    from azure.ai.projects.models import ResponsesUserMessageItemParam

    # Set scope to associate the memories with
    scope = "user_123"

    user_message = ResponsesUserMessageItemParam(
        content="I prefer dark roast coffee and usually drink it in the morning"
    )

    update_poller = project_client.memory_stores.begin_update_memories(
        name=memory_store_name,
        scope=scope,
        items=[user_message],  # Pass conversation items that you want to add to memory
        update_delay=0,  # Trigger update immediately without waiting for inactivity
    )

    # Wait for the update operation to complete, but can also fire and forget
    update_result = update_poller.result()
    print(f"Updated with {len(update_result.memory_operations)} memory operations")
    for operation in update_result.memory_operations:
        print(
            f"  - Operation: {operation.kind}, Memory ID: {operation.memory_item.memory_id}, Content: {operation.memory_item.content}"
        )

    # Extend the previous update with another update and more messages
    new_message = ResponsesUserMessageItemParam(content="I also like cappuccinos in the afternoon")
    new_update_poller = project_client.memory_stores.begin_update_memories(
        name=memory_store_name,
        scope=scope,
        items=[new_message],
        previous_update_id=update_poller.update_id,  # Extend from previous update ID
        update_delay=0,  # Trigger update immediately without waiting for inactivity
    )
    new_update_result = new_update_poller.result()
    for operation in new_update_result.memory_operations:
        print(
            f"  - Operation: {operation.kind}, Memory ID: {operation.memory_item.memory_id}, Content: {operation.memory_item.content}"
        )
    ```
  </Tab>

  <Tab title="REST API">
    ```bash
    # Configuration
    ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
    API_VERSION="2025-11-15-preview"
    ACCESS_TOKEN="$(az account get-access-token --resource https://ai.azure.com/ --query accessToken -o tsv)"

    curl -X POST "${ENDPOINT}/memory_stores/my_memory_store:update_memories?api-version=${API_VERSION}" \
      -H "Authorization: Bearer ${ACCESS_TOKEN}" \
      -H "Content-Type: application/json" \
      -d '{
        "scope": "user_123",
        "items": [
          {
            "type": "message",
            "role": "user",
            "content": [
              {
                "type": "input_text",
                "text": "I prefer dark roast coffee and usually drink it in the morning"
              }
            ]
          }
        ],
        "update_delay": 0
      }'

    # Get add memory status by polling the update_id
    # Use the "update_id" from previous response
    UPDATE_ID=<your_update_id>
    curl -X GET "${ENDPOINT}/memory_stores/my_memory_store/updates/${UPDATE_ID}?api-version=${API_VERSION}" \
      -H "Authorization: Bearer ${ACCESS_TOKEN}"
    ```
  </Tab>
</Tabs>

### Search for memories in a memory store

Search memories to retrieve relevant context for agent interactions. Specify the memory store name and scope to narrow the search.

<Tabs>
  <Tab title="Python">
    ```python
    # Continue from the previous Python snippets.
    from azure.ai.projects.models import MemorySearchOptions, ResponsesUserMessageItemParam

    # Search memories by a query
    query_message = ResponsesUserMessageItemParam(content="What are my coffee preferences?")

    search_response = project_client.memory_stores.search_memories(
        name=memory_store_name,
        scope=scope,
        items=[query_message],
        options=MemorySearchOptions(max_memories=5)
    )
    print(f"Found {len(search_response.memories)} memories")
    for memory in search_response.memories:
        print(f"  - Memory ID: {memory.memory_item.memory_id}, Content: {memory.memory_item.content}")
    ```
  </Tab>

  <Tab title="REST API">
    ```bash
    # Configuration
    ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
    API_VERSION="2025-11-15-preview"
    ACCESS_TOKEN="$(az account get-access-token --resource https://ai.azure.com/ --query accessToken -o tsv)"

    curl -X POST "${ENDPOINT}/memory_stores/my_memory_store:search_memories?api-version=${API_VERSION}" \
      -H "Authorization: Bearer ${ACCESS_TOKEN}" \
      -H "Content-Type: application/json" \
      -d '{
        "scope": "user_123",
        "items": [
          {
            "type": "message",
            "role": "user",
            "content": [
              {
                "type": "input_text",
                "text": "What are my coffee preferences?"
              }
            ]
          }
        ],
        "options": {
          "max_memories": 5
        }
      }'
    ```
  </Tab>
</Tabs>

### Retrieve static or contextual memories

Often, user profile memories can't be retrieved based on semantic similarity to a user's message. We recommend that you inject static memories into the beginning of each conversation and use contextual memories to generate each agent response.

* To retrieve static memories, call `search_memories` with a `scope` but without `items` or `previous_search_id`. This returns user profile memories associated with the scope.

* To retrieve contextual memories, call `search_memories` with `items` set to the latest messages. This can return both user profile and chat summary memories most relevant to the given items.

For more information about user profile and chat summary memories, see [Memory types](../concepts/what-is-memory#memory-types).

## Delete memories

<Callout type="warning">
  Before you delete a memory store, consider the impact on dependent agents. Agents with attached memory stores might lose access to historical context.
</Callout>

Memories are organized by scope within a memory store. You can delete memories for a specific scope to remove user-specific data, or you can delete the entire memory store to remove all memories across all scopes.

### Delete memories by scope

Remove all memories associated with a particular user or group scope while preserving the memory store structure. Use this operation to handle user data deletion requests or reset memory for specific users.

<Tabs>
  <Tab title="Python">
    ```python
    # Delete memories for a specific scope
    delete_scope_response = project_client.memory_stores.delete_scope(
        name=memory_store_name,
        scope="user_123"
    )

    print(f"Deleted memories for scope: user_123")
    ```
  </Tab>

  <Tab title="REST API">
    ```bash
    # Configuration
    ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
    API_VERSION="2025-11-15-preview"
    ACCESS_TOKEN="$(az account get-access-token --resource https://ai.azure.com/ --query accessToken -o tsv)"

    curl -X POST "${ENDPOINT}/memory_stores/my_memory_store:delete_scope?api-version=${API_VERSION}" \
      -H "Authorization: Bearer ${ACCESS_TOKEN}" \
      -H "Content-Type: application/json" \
      -d '{
        "scope": "user_123"
      }'
    ```
  </Tab>
</Tabs>

### Delete a memory store

Remove the entire memory store and all associated memories across all scopes. This operation is irreversible.

<Tabs>
  <Tab title="Python">
    ```python
    # Delete the entire memory store
    delete_response = project_client.memory_stores.delete(memory_store_name)
    print(f"Deleted memory store: {delete_response.deleted}")
    ```
  </Tab>

  <Tab title="REST API">
    ```bash
    # Configuration
    ENDPOINT="https://{your-ai-services-account}.services.ai.azure.com/api/projects/{project-name}"
    API_VERSION="2025-11-15-preview"
    ACCESS_TOKEN="$(az account get-access-token --resource https://ai.azure.com/ --query accessToken -o tsv)"

    curl -X DELETE "${ENDPOINT}/memory_stores/my_memory_store?api-version=${API_VERSION}" \
      -H "Authorization: Bearer ${ACCESS_TOKEN}"
    ```
  </Tab>
</Tabs>

## Best practices

* **Implement per-user access controls:** Avoid giving agents access to memories shared across all users. Use the `scope` property to partition the memory store by user. When you share `scope` across users, use `user_profile_details` to instruct the memory system not to store personal information.

* **Map scope to an authenticated user:** When you specify scope in the [memory search tool](#use-memories-via-an-agent-tool), set `scope={{$userId}}` to map to the user from the authentication token (`{tid}_{oid}`). This ensures that memory searches automatically target the correct user.

* **Minimize and protect sensitive data:** Store only what's necessary for your use case. If you must store sensitive data, such as personal data, health data, or confidential business inputs, redact or remove other content that could be used to trace back to an individual.

* **Support privacy and compliance:** Provide users with transparency, including options to access and delete their data. Record all deletions in a tamper-evident audit trail. Ensure the system adheres to local compliance requirements and regulatory standards.

* **Segment data and isolate memory:** In multi-agent systems, segment memory logically and operationally. Allow customers to define, isolate, inspect, and delete their own memory footprint.

* **Monitor memory usage:** Track token usage and memory operations to understand costs and optimize performance.

## Troubleshooting

| Issue                                                        | Cause                                                                                          | Resolution                                                                                                                                    |
| ------------------------------------------------------------ | ---------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| Requests fail with an authentication or authorization error. | Your identity or the project managed identity doesn’t have the required roles.                 | Verify the roles in [Authorization and permissions](#authorization-and-permissions). For REST calls, generate a fresh access token and retry. |
| Memories don’t appear after a conversation.                  | Memory updates are debounced or still processing.                                              | Increase the wait time or call the update API with `update_delay` set to `0` to trigger processing immediately.                               |
| Memory search returns no results.                            | The `scope` value doesn’t match the scope used when memories were stored.                      | Use the same scope for update and search. If you map scope to users, use a stable user identifier.                                            |
| The agent response doesn’t use stored memory.                | The agent isn’t configured with the memory search tool, or the memory store name is incorrect. | Confirm the agent definition includes the `memory_search` tool and references the correct memory store name.                                  |

## Related content

* [Python code samples](https://github.com/Azure/azure-sdk-for-python/tree/main/sdk/ai/azure-ai-projects/samples/memories)
* [Memory store REST API reference](../../reference/foundry-project-rest-preview)
* [Memory in Foundry Agent Service](../concepts/what-is-memory)
* [Foundry Agent Service quotas and limits](../quotas-limits)
* [Build an agent with Microsoft Foundry](../quickstart)

Retrieval augmented generation (RAG) is a pattern that combines search with large language models (LLMs) so responses are grounded in your data. This article explains how RAG works in Microsoft Foundry, what role indexes play, and how agentic retrieval changes classic RAG patterns.

LLMs are trained on public data available at training time. If you need answers based on your private data, or on frequently changing information, RAG helps you:

* Retrieve relevant information from your data (often through an index).
* Provide that information to the model as grounding data.
* Generate a response that can include citations back to source content.

## What is RAG?

Large language models (LLMs) like ChatGPT are trained on public internet data that was available when the model was trained. The public data might not be sufficient for your needs. For example, you might want answers based on private documents, or you might need up-to-date information.

RAG addresses this by retrieving relevant content from your data and including it in the model input. The model can then generate responses grounded in the retrieved content.

Key concepts for RAG:

* **Grounding data**: Retrieved content you provide to the model to reduce guessing.
* **Index**: A data structure optimized for retrieval (keyword, semantic, vector, or hybrid search).
* **Embeddings**: Numeric representations of content used for vector similarity search. See [Understand embeddings](../openai/concepts/understand-embeddings).
* **System message and prompts**: Instructions that guide how the model uses retrieved content. See [Prompt engineering](../openai/concepts/prompt-engineering) and [Safety system messages](../openai/concepts/system-message).

## How does RAG work?

RAG follows a three-step flow:

1. **Retrieve**: When a user asks a question, your application queries an index or data store to find relevant content.
2. **Augment**: The app combines the user's question and the retrieved content (grounding data) into a prompt.
3. **Generate**: The model receives the augmented prompt and generates a response grounded in the retrieved content, reducing inaccuracies and enabling accurate citations.

![Diagram that shows a user query, retrieval from a data store, and a grounded model response.](https://learn.microsoft.com/azure/ai-foundry/media/index-retrieve/rag-pattern.png)

## What is an index and why do I need it?

RAG works best when you can retrieve relevant content quickly and consistently. An index helps by organizing your content for efficient retrieval.

Many RAG solutions use an index that supports one or more of these retrieval modes:

* **Keyword search**
* **Semantic search**
* **Vector search**
* **Hybrid search** (keyword + vector, sometimes with semantic ranking)

An index can also store fields that improve citation quality (for example, document titles, URLs, or file names).

![Diagram that shows retrieval from an index and how the retrieved passages are added to the model prompt.](https://learn.microsoft.com/azure/ai-foundry/media/index-retrieve/rag-pattern-with-index.png)

Foundry can connect your project to an Azure AI Search service and index for retrieval. Depending on the feature and API surface you're using, this connection information might be represented as a project connection or an *index asset ID*.

For example, the Foundry Project REST API preview includes an `index_asset_id` field for Azure AI Search index resources. See [Foundry Project REST API preview](../reference/foundry-project-rest-preview).

Azure AI Search is a recommended index store for RAG scenarios. Azure AI Search supports retrieval over vector and textual data stored in search indexes, and it can also query other targets if you use agentic retrieval. See [What is Azure AI Search?](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search).

## Agentic RAG: modern approach to retrieval

Traditional RAG patterns often use a single query to retrieve information from your data. *Agentic retrieval*, also known as agentic RAG, is an evolution in retrieval architecture that uses a model to break down complex inputs into multiple focused subqueries, run them in parallel, and return structured grounding data that works well with chat completion models.

Agentic retrieval provides several advantages over classic RAG:

* **Context-aware query planning** - Uses conversation history to understand context and intent. Follow-up questions retain the context of earlier exchanges, making multi-turn conversations more natural.
* **Parallel execution** - Runs multiple focused subqueries simultaneously for better coverage. Instead of retrieving from a single query sequentially, parallel execution reduces latency and retrieves more diverse relevant results.
* **Structured responses** - Returns grounding data, citations, and execution metadata along with results. This structured output makes it easier for your application to cite sources accurately and trace the reasoning behind answers.
* **Built-in semantic ranking** - Ensures optimal relevance of results. Semantic ranking filters noise and prioritizes truly relevant passages, which is especially important with large datasets.
* **Optional answer synthesis** - Can include LLM-formulated answers directly in the query response. Alternatively, you can choose to return raw, verbatim passages for your application to process.

If you're using Azure AI Search as your retrieval engine, see [Agentic retrieval](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-overview) and [Quickstart: Agentic retrieval](../../search/search-get-started-agentic-retrieval).

## Choose an approach in Foundry

Foundry supports multiple patterns for working with private data. Choose based on your use case complexity and how much control you need:

* **Use RAG** when you need answers grounded in private or frequently changing data.
* **Use fine-tuning** when you need to change model behavior, style, or task performance, rather than add fresh knowledge.
* **Use a managed “use your data” experience** if you want a more guided way to connect, ingest, and chat over your data. See [Azure OpenAI On Your Data](../openai/concepts/use-your-data) and [Quickstart: Chat with Azure OpenAI models using your own data](../openai/use-your-data-quickstart).

- **Use agent tools** when you're building an agent that needs retrieval as a tool. For example, see [File search tool for agents](../agents/how-to/tools/file-search).

## Getting started with RAG in Foundry

Implementing RAG in Foundry typically follows this workflow:

1. **Prepare your data**: Organize and chunk your private documents or knowledge base into searchable content
2. **Set up an index**: Create an Azure AI Search index or use another retrieval service to organize your content for efficient searching
3. **Connect to Foundry**: Create a connection from your Foundry project to your index or retrieval service
4. **Build your RAG application**: Integrate retrieval with your LLM calls using the Foundry SDK or REST APIs
5. **Test and evaluate**: Verify that retrieval quality is good and responses are accurate and properly cited

To get started, choose one of these paths based on your needs:

* **Guided experience**: Start with [Azure OpenAI On Your Data](../openai/concepts/use-your-data), which provides a managed setup for connecting data and chatting over it. See [Quickstart: Chat with Azure OpenAI models using your own data](../openai/use-your-data-quickstart).
* **Agent with retrieval**: If you're building an agent, use retrieval as a tool. See [File search tool for agents](../agents/how-to/tools/file-search).
* **Custom RAG application**: Build a full RAG app with the Foundry SDK for complete control.

## Security and privacy considerations

RAG systems can expose sensitive content if you don't design access and prompting carefully.

* **Apply access control at retrieval time**. If you're using Azure AI Search as a data source, you can use document-level access control with security filters. See the [document-level access control](../openai/concepts/use-your-data#document-level-access-control) section.
* **Prefer Microsoft Entra ID over API keys for production**. API keys are convenient for development but aren't recommended for production scenarios. For Azure AI Search RBAC guidance, see [Connect to Azure AI Search using roles](../../search/search-security-rbac).
* **Treat retrieved content as untrusted input**. Your system message and application logic should reduce the risk of prompt injection from documents and retrieved passages. See [Safety system messages](../openai/concepts/system-message).

## Cost and latency considerations

RAG adds extra work compared to a model-only request:

* **Retrieval costs and latency**: Querying an index adds round trips and compute.
* **Embedding costs and latency**: Vector search requires embeddings at indexing time, and often at query time.
* **Token usage**: Retrieved passages increase input tokens, which can increase cost.

If you're using Azure AI Search, confirm service tier and pricing before production rollout. If you're using semantic or hybrid retrieval, review Azure AI Search pricing and limits in the Azure AI Search documentation.

## Limitations and troubleshooting

### Known limitations

* RAG quality depends on content preparation, retrieval configuration, and prompt design. Poor data preparation or indexing strategy directly impacts response quality.
* If retrieval returns irrelevant or incomplete passages, the model can still produce incomplete or inaccurate answers despite grounding.
* If you don't control access to source content, grounded responses can leak sensitive information from your index.

### Common challenges and mitigation

* **Poor retrieval quality**: If your index isn't returning relevant passages, review your data chunking strategy, embedding model quality, and search configuration (keyword vs. semantic vs. hybrid).
* **Hallucination despite grounding**: Even with retrieved content, models can still generate inaccurate responses. Enable citations and use clear system messages and prompts to instruct the model to stick to retrieved content.
* **Latency issues**: Large indexes can slow retrieval. Consider indexing strategy, filtering, and re-ranking to reduce the volume of passages processed.
* **Token budget exceeded**: Retrieved passages can quickly consume token limits. Implement passage filtering, ranking, or summarization to stay within budget.

For guidance on evaluating RAG effectiveness, see the tutorials and quickstarts in the related content section below.

## Related content

* [Azure OpenAI On Your Data](../openai/concepts/use-your-data)
* [File search tool for agents](../agents/how-to/tools/file-search)
* [Quickstart: Agentic retrieval](../../search/search-get-started-agentic-retrieval)
* [File search tool for agents](../agents/how-to/tools/file-search)


<Callout type="tip">
  For a managed knowledge base experience, see [Foundry IQ](../foundry-iq-connect). For tool optimization, see [best practices](../../concepts/tool-best-practice).
</Callout>

Ground your Foundry agent's responses in your proprietary content by connecting it to an Azure AI Search index. The [Azure AI Search](../../../../search/search-what-is-azure-search) tool retrieves indexed documents and generates answers with inline citations, enabling accurate, source-backed responses.

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

Java SDK samples are coming soon.

## Prerequisites

*Estimated setup time: 15-30 minutes if you have an existing search index*

* A [basic or standard agent environment](../../environment-setup).

* The latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code) for details.

  * **Python**: `pip install azure-ai-projects --pre`
  * **C#**: Install the `Azure.AI.Projects` NuGet package (prerelease)
  * **JavaScript/TypeScript**: `npm install @azure/ai-projects`

* An Azure subscription and Microsoft Foundry project with:

  * Project endpoint
  * Model deployment name
  * Authentication configured (for example, `DefaultAzureCredential`)

* An [Azure AI Search index configured for vector search](../../../../search/search-get-started-portal-import-vectors) with:

  * One or more `Edm.String` fields that are searchable and retrievable
  * One or more `Collection(Edm.Single)` vector fields that are searchable
  * At least one retrievable text field that contains the content you want the agent to cite
  * A retrievable field that contains a source URL (and optionally a title) so citations can include a link

* A connection between your Foundry project and your Azure AI Search service (see [Setup](#setup)).

* For keyless authentication, assign the following Azure role-based access control (RBAC) roles to your project's managed identity:

  * **Search Index Data Contributor**
  * **Search Service Contributor**

### Set environment variables

| Variable                          | Description                                                                                                    |
| --------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| `FOUNDRY_PROJECT_ENDPOINT`        | Your Foundry project endpoint.                                                                                 |
| `FOUNDRY_MODEL_DEPLOYMENT_NAME`   | Your model deployment name.                                                                                    |
| `AZURE_AI_SEARCH_CONNECTION_NAME` | The name of your project connection to Azure AI Search (used by the SDK samples to look up the connection ID). |
| `AZURE_AI_SEARCH_CONNECTION_ID`   | The resource ID of your project connection to Azure AI Search (used by the TypeScript and REST samples).       |
| `AI_SEARCH_INDEX_NAME`            | Your Azure AI Search index name.                                                                               |

## Configure tool parameters

| Azure AI Search tool parameter | Required | Notes                                                                                                                                     |
| ------------------------------ | -------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| `project_connection_id`        | Yes      | The resource ID of the project connection to Azure AI Search.                                                                             |
| `index_name`                   | Yes      | The name of the index in your Azure AI Search resource.                                                                                   |
| `top_k`                        | No       | Defaults to 5.                                                                                                                            |
| `query_type`                   | No       | Defaults to `vector_semantic_hybrid`. Supported values: `simple`, `vector`, `semantic`, `vector_simple_hybrid`, `vector_semantic_hybrid`. |
| `filter`                       | No       | Applies to all queries the agent makes to the index.                                                                                      |

## Code example

<Callout type="note">
  * You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code).
  * If you're using the REST or TypeScript sample, the connection ID is in the format `/subscriptions/{{subscriptionId}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{connectionName}}`.
  * If you're using the Python or C# sample, you can provide the connection name and retrieve the connection ID with the SDK.
</Callout>

<ZonePivot pivot="python">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```python
  import os

  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential
  from dotenv import load_dotenv

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"], credential=credential) as project_client,
  ):
      print("Connected to project.")

      # Verify Azure AI Search connection exists
      connection_name = os.environ.get("AZURE_AI_SEARCH_CONNECTION_NAME")
      if connection_name:
          try:
              conn = project_client.connections.get(connection_name)
              print(f"Azure AI Search connection verified: {conn.name}")
              print(f"Connection ID: {conn.id}")
          except Exception as e:
              print(f"Azure AI Search connection '{connection_name}' not found: {e}")
      else:
          # List available connections to help find the right one
          print("AZURE_AI_SEARCH_CONNECTION_NAME not set. Available connections:")
          for conn in project_client.connections.list():
              print(f"  - {conn.name}")
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      AzureAISearchAgentTool,
      PromptAgentDefinition,
      AzureAISearchToolResource,
      AISearchIndexResource,
      AzureAISearchQueryType,
  )

  load_dotenv()

  project_client = AIProjectClient(
    endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  openai_client = project_client.get_openai_client()

  with project_client:

      azs_connection = project_client.connections.get(os.environ["AZURE_AI_SEARCH_CONNECTION_NAME"])
      connection_id = azs_connection.id
      print(f"Azure AI Search connection ID: {connection_id}")

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
            model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are a helpful assistant. You must always provide citations for
              answers using the tool and render them as: `[message_idx:search_idx†source]`.""",
              tools=[
                  AzureAISearchAgentTool(
                      azure_ai_search=AzureAISearchToolResource(
                          indexes=[
                              AISearchIndexResource(
                                  project_connection_id=connection_id,
                                  index_name=os.environ["AI_SEARCH_INDEX_NAME"],
                                  query_type=AzureAISearchQueryType.SIMPLE,
                              ),
                          ]
                      )
                  )
              ],
          ),
          description="You are a helpful agent.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      user_input = input(
          """Enter your question for the AI Search agent available in the index
          (e.g., 'Tell me about the mental health services available from Premera'): \n"""
      )

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input=user_input,
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              if event.item.type == "message":
                  item = event.item
                  if item.content[-1].type == "output_text":
                      text_content = item.content[-1]
                      for annotation in text_content.annotations:
                          if annotation.type == "url_citation":
                              print(
                                  f"URL Citation: {annotation.url}, "
                                  f"Start index: {annotation.start_index}, "
                                  f"End index: {annotation.end_index}"
                              )
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")

      print("\nCleaning up...")
      project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
      print("Agent deleted")
  ```

  ### Expected outcome

  The agent queries the search index and returns a response with inline citations. Console output shows the agent ID, streaming delta updates as the response generates, URL citations with start and end indices, and the final complete response text. The agent is then successfully deleted.
</ZonePivot>

<ZonePivot pivot="csharp">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```csharp
  using Azure.AI.Projects;
  using Azure.Identity;

  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Verify Azure AI Search connection exists
  try
  {
      AIProjectConnection conn = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);
      Console.WriteLine($"Azure AI Search connection verified: {conn.Name}");
      Console.WriteLine($"Connection ID: {conn.Id}");
  }
  catch (Exception ex)
  {
      Console.WriteLine($"Azure AI Search connection '{aiSearchConnectionName}' not found: {ex.Message}");
      // List available connections
      Console.WriteLine("Available connections:");
      foreach (var conn in projectClient.Connections.GetConnections())
      {
          Console.WriteLine($"  - {conn.Name}");
      }
  }
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  The following sample code shows synchronous examples of how to use the Azure AI Search tool in [Azure.AI.Projects.OpenAI](https://github.com/Azure/azure-sdk-for-net/tree/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI) to query an index. For asynchronous C# examples, see the [GitHub repo](https://github.com/Azure/azure-sdk-for-net/tree/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI).

  This example shows how to use the Azure AI Search tool with agents to query an index.

  ```csharp
  // Read the environment variables to be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");
  var aiSearchIndexName = System.Environment.GetEnvironmentVariable("AI_SEARCH_INDEX_NAME");

  // Create an AIProjectClient object that will be used to create the agent and query the index.
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Resolve the project connection ID from the connection name.
  AIProjectConnection aiSearchConnection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);

  // Create an AzureAISearchToolIndex object that defines the index and the search parameters.
  AzureAISearchToolIndex index = new()
  {
      ProjectConnectionId = aiSearchConnection.Id,
      IndexName = aiSearchIndexName,
      TopK = 5,
      Filter = "category eq 'sleeping bag'",
      QueryType = AzureAISearchQueryType.Simple
  };

  // Create the agent definition with the Azure AI Search tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `\u3010message_idx:search_idx\u2020source\u3011`.",
      Tools = { new AzureAISearchTool(new AzureAISearchToolOptions(indexes: [index])) }
  };

  // Create the agent version with the agent definition.
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create an OpenAIResponse object with the ProjectResponsesClient object.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  ResponseResult response = responseClient.CreateResponse("What is the temperature rating of the cozynights sleeping bag?");

  // In the search, an index containing "embedding", "token", "category", "title", and "url" fields is used.
  // The last two fields are needed to get citation title and URL, which the agent retrieves.
  // To get the reference, you need to parse the output items.
  string result = "";
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is MessageResponseItem messageItem)
      {
          foreach (ResponseContentPart content in messageItem.Content)
          {
              foreach (ResponseMessageAnnotation annotation in content.OutputTextAnnotations)
              {
                  if (annotation is UriCitationMessageAnnotation uriAnnotation)
                  {
                      result = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                  }
              }
          }
      }
  }

  // Use the helper method to output the result.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine($"{response.GetOutputText()}{result}");

  // Finally, delete all the resources you created in this sample.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected outcome

  The agent queries the specified index for information about the sleeping bag. The response includes the temperature rating and a formatted citation with the document title and URL. The response status is `Completed`, and the agent version is successfully deleted.

  ## Use agents with Azure AI Search tool for streaming scenarios

  This example shows how to use the Azure AI Search tool with agents to query an index in a streaming scenario.

  ```csharp
  // Read the environment variables to be used in the next steps
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");
  var aiSearchIndexName = System.Environment.GetEnvironmentVariable("AI_SEARCH_INDEX_NAME");

  // Create an AIProjectClient object that will be used to create the agent and query the index.
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Resolve the project connection ID from the connection name.
  AIProjectConnection aiSearchConnection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);

  // Create an AzureAISearchToolIndex object that defines the index and the search parameters.
  AzureAISearchToolIndex index = new()
  {
      ProjectConnectionId = aiSearchConnection.Id,
      IndexName = aiSearchIndexName,
      TopK = 5,
      Filter = "category eq 'sleeping bag'",
      QueryType = AzureAISearchQueryType.Simple
  };

  // Create the agent definition with the Azure AI Search tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `\u3010message_idx:search_idx\u2020source\u3011`.",
      Tools = { new AzureAISearchTool(new AzureAISearchToolOptions(indexes: [index])) }
  };

  // Create the agent version with the agent definition.
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create an OpenAIResponse object with the ProjectResponsesClient object.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  string annotation = "";
  string text = "";

  // Stream the response from the agent and parse the output items for citations.
  foreach (StreamingResponseUpdate streamResponse in responseClient.CreateResponseStreaming("What is the temperature rating of the cozynights sleeping bag?"))
  {
      if (streamResponse is StreamingResponseCreatedUpdate createUpdate)
      {
          Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
      }
      else if (streamResponse is StreamingResponseOutputTextDeltaUpdate textDelta)
      {
          Console.WriteLine($"Delta: {textDelta.Delta}");
      }
      else if (streamResponse is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
      {
          text = textDoneUpdate.Text;
      }
      else if (streamResponse is StreamingResponseOutputItemDoneUpdate itemDoneUpdate)
      {
          if (annotation.Length == 0)
          {
              if (itemDoneUpdate.Item is MessageResponseItem messageItem)
              {
                  // Use an index containing "embedding", "token", "category", "title", and "url" fields.
                  // The last two fields are needed to get citation title and URL, retrieved by the agent.
                  foreach (ResponseContentPart content in messageItem.Content)
                  {
                      foreach (ResponseMessageAnnotation messageAnnotation in content.OutputTextAnnotations)
                      {
                          if (messageAnnotation is UriCitationMessageAnnotation uriAnnotation)
                          {
                              annotation = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                          }
                      }
                  }
              }
          }
      }
      else if (streamResponse is StreamingResponseErrorUpdate errorUpdate)
      {
          throw new InvalidOperationException($"The stream has failed: {errorUpdate.Message}");
      }
  }
  Console.WriteLine($"{text}{annotation}");

  // Finally, delete all the resources that were created in this sample.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected outcome

  The streaming response displays the agent's response creation, text deltas as they stream in real-time, and a formatted citation when complete. The final output includes the sleeping bag temperature rating with document reference. The agent version is deleted after the query completes.
</ZonePivot>

<ZonePivot pivot="rest">
  ## Use agents with Azure AI Search tool

  The following example shows how to use the Azure AI Search tool with the REST API to query an index. The example uses cURL, but you can use any HTTP client.

  Before running this sample, obtain a bearer token for authentication. Use the Azure CLI to get a token:

  ```bash
  az account get-access-token --resource https://cognitiveservices.azure.com
  ```

  Set `AGENT_TOKEN` to the token value and `API_VERSION` to the current API version (for example, `2025-01-01-preview`).

  ```bash
  curl --request POST \
    --url "$FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    --data '{
      "model": "$FOUNDRY_MODEL_DEPLOYMENT_NAME",
      "input": "Tell me about the mental health services available from Premera.",
      "tool_choice": "required",
      "tools": [
        {
          "type": "azure_ai_search",
          "azure_ai_search": {
            "indexes": [
              {
                "project_connection_id": "$AZURE_AI_SEARCH_CONNECTION_ID",
                "index_name": "$AI_SEARCH_INDEX_NAME",
                "query_type": "semantic",
                "top_k": 5
              }
            ]
          }
        }
      ]
    }'
  ```

  ### Expected outcome

  The API returns a JSON response containing the agent's answer about mental health services from the Premera index. The response includes citations and references to the indexed documents that generated the answer.
</ZonePivot>

<ZonePivot pivot="typescript">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const aiSearchConnectionName = process.env["AZURE_AI_SEARCH_CONNECTION_NAME"] || "<ai search connection name>";

  async function verifyConnection(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    console.log("Connected to project.");

    try {
      const conn = await project.connections.get(aiSearchConnectionName);
      console.log(`Azure AI Search connection verified: ${conn.name}`);
      console.log(`Connection ID: ${conn.id}`);
    } catch (error) {
      console.log(`Azure AI Search connection '${aiSearchConnectionName}' not found: ${error}`);
      // List available connections
      console.log("Available connections:");
      for await (const conn of project.connections.list()) {
        console.log(`  - ${conn.name}`);
      }
    }
  }

  verifyConnection().catch(console.error);
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  This sample demonstrates how to create an AI agent with Azure AI Search capabilities by using the `AzureAISearchAgentTool` and synchronous Azure AI Projects client. The agent can search indexed content and provide responses with citations from search results.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as readline from "readline";
  import "dotenv/config";

  // Load environment variables
  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const aiSearchConnectionName =
    process.env["AZURE_AI_SEARCH_CONNECTION_NAME"] || "<ai search connection name>";
  const aiSearchIndexName = process.env["AI_SEARCH_INDEX_NAME"] || "<ai search index name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Get connection ID from connection name
    const aiSearchConnection = await project.connections.get(aiSearchConnectionName);
    console.log(`Azure AI Search connection ID: ${aiSearchConnection.id}`);

    console.log("Creating agent with Azure AI Search tool...");

    // Define Azure AI Search tool that searches indexed content
    const agent = await project.agents.createVersion("MyAISearchAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `[message_idx:search_idx†source]`.",
      tools: [
        {
          type: "azure_ai_search",
          azure_ai_search: {
            indexes: [
              {
                project_connection_id: aiSearchConnection.id,
                index_name: aiSearchIndexName,
                query_type: "simple",
              },
            ],
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Prompt user for input
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    });

    const userInput = await new Promise<string>((resolve) => {
      rl.question(
        "Enter your question for the AI Search agent available in the index (e.g., 'Tell me about the mental health services available from Premera'): \n",
        (answer) => {
          rl.close();
          resolve(answer);
        },
      );
    });

    console.log("\nSending request to AI Search agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: userInput,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        if (event.item.type === "message") {
          const item = event.item;
          if (item.content && item.content.length > 0) {
            const lastContent = item.content[item.content.length - 1];
            if (lastContent.type === "output_text" && lastContent.annotations) {
              for (const annotation of lastContent.annotations) {
                if (annotation.type === "url_citation") {
                  console.log(
                    `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                  );
                }
              }
            }
          }
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nAzure AI Search agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected outcome

  The application creates an agent with Azure AI Search capabilities, prompts for user input, queries the search index, and streams the response with real-time delta updates. Console output includes the agent ID, streaming text deltas, URL citations with indices, and confirmation of successful agent deletion. The agent provides answers grounded in the indexed content with proper citations.
</ZonePivot>

## Limitations

Keep these constraints in mind when using the Azure AI Search tool:

* **Virtual network access**: Azure AI Search doesn't support virtual network (vNET) configurations with agents at this time.
* The Azure AI Search tool can only target one index.
* Your Azure AI Search resource and your Microsoft Foundry Agent must be in the same tenant.

## Verify results

After you run a sample, validate that the agent is grounding responses from your index.

1. Ask a question that you know is answered in a specific indexed document.
2. Confirm the response includes citations formatted as `[message_idx:search_idx†source]`.
3. If you're streaming, confirm you see `url_citation` annotations in the response with valid URLs.
4. Verify the cited content matches your source documents in the search index.

If citations are missing or incorrect, see the [Troubleshooting](#troubleshooting) section.

## Setup

In this section, you create a connection between the Microsoft Foundry project that contains your agent and the Azure AI Search service that contains your index.

If you already connected your project to your search service, skip this section.

To create the connection, you need your search service endpoint and authentication method. The following steps guide you through gathering these details.

### Gather connection details

Before creating a project connection, gather your Azure AI Search service endpoint and authentication credentials.

The project connection requires the endpoint of your search service and either key-based authentication or keyless authentication with Microsoft Entra ID.

For keyless authentication, you must enable role-based access control (RBAC) and assign roles to your project's managed identity. Although this method involves extra steps, it enhances security by eliminating the need for hard-coded API keys.

Select the tab for your desired authentication method.

<Tabs>
  <Tab title="Key-based authentication">
    1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.

    2. To get the endpoint:

       1. From the left pane, select **Overview**.

       2. Make a note of the URL, which should look like `https://my-service.search.windows.net`.

          ![A screenshot of an AI Search resource Overview tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/connection-endpoint.png)

    3. To get the API key:

       1. From the left pane, select **Settings** > **Keys**.
       2. Select **Both** to enable both key-based and keyless authentication, which is recommended for most scenarios.

       ![A screenshot of an AI Search resource Keys tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/azure-portal.png)

       1. Make a note of one of the keys under **Manage admin keys**.
  </Tab>

  <Tab title="Keyless authentication">
    1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.

    2. To get the endpoint:

       1. From the left pane, select **Overview**.

       2. Make a note of the URL, which should look like `https://my-service.search.windows.net`.

          ![A screenshot of an AI Search resource Overview tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/connection-endpoint.png)

    3. To enable RBAC:

       1. From the left pane, select **Settings** > **Keys**.
       2. Select **Both** to enable both key-based and keyless authentication, which is recommended for most scenarios.

       ![A screenshot of an AI Search resource Keys tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/azure-portal.png)

    4. To assign the necessary roles:

       1. From the left pane, select **Access control (IAM)**.
       2. Select **Add** > **Add role assignment**.
       3. Assign the **Search Index Data Contributor** role to the managed identity of your project.
       4. Repeat the role assignment for **Search Service Contributor**.
  </Tab>
</Tabs>

### Create a project connection

Create the project connection by using the search service details you gathered.

Use one of the following options.

<Tabs>
  <Tab title="Azure CLI">
    **Create the following connection.yml file:**

    You can use a YAML configuration file for both key-based and keyless authentication. Replace the `name`, `endpoint`, and `api_key` (optional) placeholders with your search service details. For more information, see the [Azure AI Search connection YAML schema](../../../../machine-learning/reference-yaml-connection-ai-search).

    Here's a key-based example:

    ```yml
    name: my_project_acs_connection_keys
    type: azure_ai_search
    endpoint: https://contoso.search.windows.net/
    api_key: XXXXXXXXXXXXXXX
    ```

    <Callout type="important">
      Don't put real keys in source control. Store secrets in a secure store (for example, Azure Key Vault) and inject them at deployment time.
    </Callout>

    Here's a keyless example:

    ```yml
    name: my_project_acs_connection_keyless
    type: azure_ai_search
    endpoint: https://contoso.search.windows.net/
    ```

    **Then, run the following command:**

    Replace the placeholders with the resource group and project name.

    ```azurecli
    az ml connection create --file connection.yml --resource-group <resource-group> --workspace-name <project-name>
    ```
  </Tab>

  <Tab title="Python">
    Replace the `my_connection_name`, `my_endpoint`, and `my_key` (optional) placeholders with your search service details, and then run the following code:

    ```python
    from azure.identity import DefaultAzureCredential
    from azure.ai.ml import MLClient
    from azure.ai.ml.entities import AzureAISearchConnection

    # Create an Azure AI Search project connection
    my_connection_name = "my-connection-name"
    my_endpoint = "my-endpoint" # This could also be called target
    my_api_keys = None # Leave blank for Authentication type = AAD

    my_connection = AzureAISearchConnection(name=my_connection_name,
                                        endpoint=my_endpoint,
                                        api_key= my_api_keys)

    # Create MLClient
    ml_client = MLClient(
      credential=DefaultAzureCredential(),
      subscription_id="<subscription-id>",
      resource_group_name="<resource-group>",
      workspace_name="<project-name>",
    )

    # Create the connection
    ml_client.connections.create_or_update(my_connection)
    ```
  </Tab>
</Tabs>

### Confirm the connection ID

If you use the REST or TypeScript sample, you need the project connection ID.

**Python**

```python
import os
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient

load_dotenv()

project_client = AIProjectClient(
  endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
  credential=DefaultAzureCredential(),
)

connection = project_client.connections.get(os.environ["AZURE_AI_SEARCH_CONNECTION_NAME"])
print(connection.id)
```

**C#**

```csharp
var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");

AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());
AIProjectConnection connection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);
Console.WriteLine(connection.Id);
```

## Troubleshooting

| Issue                                 | Cause                                      | Resolution                                                                                                                                                                                    |
| ------------------------------------- | ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Response has no citations             | Agent instructions don't request citations | Update your agent instructions to explicitly request citations in responses.                                                                                                                  |
| Response has no citations (streaming) | Annotations not captured                   | Confirm you receive `url_citation` annotations when streaming. Check your stream processing logic.                                                                                            |
| Tool can't access the index (401/403) | Missing RBAC roles (keyless auth)          | Assign the **Search Index Data Contributor** and **Search Service Contributor** roles to the Foundry project's managed identity. See [Azure RBAC in Foundry](../../../concepts/rbac-foundry). |
| Tool can't access the index (401/403) | Invalid or disabled API key                | Confirm the API key is correct and enabled in the Azure AI Search resource.                                                                                                                   |
| Tool returns "index not found"        | Index name mismatch                        | Confirm `AI_SEARCH_INDEX_NAME` matches the exact index name in your Azure AI Search resource (case-sensitive).                                                                                |
| Tool returns "index not found"        | Wrong connection endpoint                  | Confirm the project connection points to the Azure AI Search resource that contains the index.                                                                                                |
| Search returns no results             | Query doesn't match indexed content        | Verify the index contains the expected data. Use Azure AI Search's test query feature to validate.                                                                                            |
| Slow search performance               | Index not optimized                        | Review index configuration, consider adding semantic ranking, or optimize the index schema.                                                                                                   |

## Related content

* [Connect a Foundry IQ knowledge base to Foundry Agent Service](../foundry-iq-connect)
* [Tool best practices](../../concepts/tool-best-practice)
* [Create a vector search index in Azure AI Search](../../../../search/search-get-started-portal-import-vectors)
* [Quickstart: Build an agent with Foundry](../../../quickstarts/get-started-code)

<Callout type="tip">
  For a managed knowledge base experience, see [Foundry IQ](../foundry-iq-connect). For tool optimization, see [best practices](../../concepts/tool-best-practice).
</Callout>

Ground your Foundry agent's responses in your proprietary content by connecting it to an Azure AI Search index. The [Azure AI Search](../../../../search/search-what-is-azure-search) tool retrieves indexed documents and generates answers with inline citations, enabling accurate, source-backed responses.

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

Java SDK samples are coming soon.

## Prerequisites

*Estimated setup time: 15-30 minutes if you have an existing search index*

* A [basic or standard agent environment](../../environment-setup).

* The latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code) for details.

  * **Python**: `pip install azure-ai-projects --pre`
  * **C#**: Install the `Azure.AI.Projects` NuGet package (prerelease)
  * **JavaScript/TypeScript**: `npm install @azure/ai-projects`

* An Azure subscription and Microsoft Foundry project with:

  * Project endpoint
  * Model deployment name
  * Authentication configured (for example, `DefaultAzureCredential`)

* An [Azure AI Search index configured for vector search](../../../../search/search-get-started-portal-import-vectors) with:

  * One or more `Edm.String` fields that are searchable and retrievable
  * One or more `Collection(Edm.Single)` vector fields that are searchable
  * At least one retrievable text field that contains the content you want the agent to cite
  * A retrievable field that contains a source URL (and optionally a title) so citations can include a link

* A connection between your Foundry project and your Azure AI Search service (see [Setup](#setup)).

* For keyless authentication, assign the following Azure role-based access control (RBAC) roles to your project's managed identity:

  * **Search Index Data Contributor**
  * **Search Service Contributor**

### Set environment variables

| Variable                          | Description                                                                                                    |
| --------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| `FOUNDRY_PROJECT_ENDPOINT`        | Your Foundry project endpoint.                                                                                 |
| `FOUNDRY_MODEL_DEPLOYMENT_NAME`   | Your model deployment name.                                                                                    |
| `AZURE_AI_SEARCH_CONNECTION_NAME` | The name of your project connection to Azure AI Search (used by the SDK samples to look up the connection ID). |
| `AZURE_AI_SEARCH_CONNECTION_ID`   | The resource ID of your project connection to Azure AI Search (used by the TypeScript and REST samples).       |
| `AI_SEARCH_INDEX_NAME`            | Your Azure AI Search index name.                                                                               |

## Configure tool parameters

| Azure AI Search tool parameter | Required | Notes                                                                                                                                     |
| ------------------------------ | -------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| `project_connection_id`        | Yes      | The resource ID of the project connection to Azure AI Search.                                                                             |
| `index_name`                   | Yes      | The name of the index in your Azure AI Search resource.                                                                                   |
| `top_k`                        | No       | Defaults to 5.                                                                                                                            |
| `query_type`                   | No       | Defaults to `vector_semantic_hybrid`. Supported values: `simple`, `vector`, `semantic`, `vector_simple_hybrid`, `vector_semantic_hybrid`. |
| `filter`                       | No       | Applies to all queries the agent makes to the index.                                                                                      |

## Code example

<Callout type="note">
  * You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code).
  * If you're using the REST or TypeScript sample, the connection ID is in the format `/subscriptions/{{subscriptionId}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{connectionName}}`.
  * If you're using the Python or C# sample, you can provide the connection name and retrieve the connection ID with the SDK.
</Callout>

<ZonePivot pivot="python">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```python
  import os

  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential
  from dotenv import load_dotenv

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"], credential=credential) as project_client,
  ):
      print("Connected to project.")

      # Verify Azure AI Search connection exists
      connection_name = os.environ.get("AZURE_AI_SEARCH_CONNECTION_NAME")
      if connection_name:
          try:
              conn = project_client.connections.get(connection_name)
              print(f"Azure AI Search connection verified: {conn.name}")
              print(f"Connection ID: {conn.id}")
          except Exception as e:
              print(f"Azure AI Search connection '{connection_name}' not found: {e}")
      else:
          # List available connections to help find the right one
          print("AZURE_AI_SEARCH_CONNECTION_NAME not set. Available connections:")
          for conn in project_client.connections.list():
              print(f"  - {conn.name}")
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      AzureAISearchAgentTool,
      PromptAgentDefinition,
      AzureAISearchToolResource,
      AISearchIndexResource,
      AzureAISearchQueryType,
  )

  load_dotenv()

  project_client = AIProjectClient(
    endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  openai_client = project_client.get_openai_client()

  with project_client:

      azs_connection = project_client.connections.get(os.environ["AZURE_AI_SEARCH_CONNECTION_NAME"])
      connection_id = azs_connection.id
      print(f"Azure AI Search connection ID: {connection_id}")

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
            model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are a helpful assistant. You must always provide citations for
              answers using the tool and render them as: `[message_idx:search_idx†source]`.""",
              tools=[
                  AzureAISearchAgentTool(
                      azure_ai_search=AzureAISearchToolResource(
                          indexes=[
                              AISearchIndexResource(
                                  project_connection_id=connection_id,
                                  index_name=os.environ["AI_SEARCH_INDEX_NAME"],
                                  query_type=AzureAISearchQueryType.SIMPLE,
                              ),
                          ]
                      )
                  )
              ],
          ),
          description="You are a helpful agent.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      user_input = input(
          """Enter your question for the AI Search agent available in the index
          (e.g., 'Tell me about the mental health services available from Premera'): \n"""
      )

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input=user_input,
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              if event.item.type == "message":
                  item = event.item
                  if item.content[-1].type == "output_text":
                      text_content = item.content[-1]
                      for annotation in text_content.annotations:
                          if annotation.type == "url_citation":
                              print(
                                  f"URL Citation: {annotation.url}, "
                                  f"Start index: {annotation.start_index}, "
                                  f"End index: {annotation.end_index}"
                              )
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")

      print("\nCleaning up...")
      project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
      print("Agent deleted")
  ```

  ### Expected outcome

  The agent queries the search index and returns a response with inline citations. Console output shows the agent ID, streaming delta updates as the response generates, URL citations with start and end indices, and the final complete response text. The agent is then successfully deleted.
</ZonePivot>

<ZonePivot pivot="csharp">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```csharp
  using Azure.AI.Projects;
  using Azure.Identity;

  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Verify Azure AI Search connection exists
  try
  {
      AIProjectConnection conn = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);
      Console.WriteLine($"Azure AI Search connection verified: {conn.Name}");
      Console.WriteLine($"Connection ID: {conn.Id}");
  }
  catch (Exception ex)
  {
      Console.WriteLine($"Azure AI Search connection '{aiSearchConnectionName}' not found: {ex.Message}");
      // List available connections
      Console.WriteLine("Available connections:");
      foreach (var conn in projectClient.Connections.GetConnections())
      {
          Console.WriteLine($"  - {conn.Name}");
      }
  }
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  The following sample code shows synchronous examples of how to use the Azure AI Search tool in [Azure.AI.Projects.OpenAI](https://github.com/Azure/azure-sdk-for-net/tree/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI) to query an index. For asynchronous C# examples, see the [GitHub repo](https://github.com/Azure/azure-sdk-for-net/tree/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI).

  This example shows how to use the Azure AI Search tool with agents to query an index.

  ```csharp
  // Read the environment variables to be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");
  var aiSearchIndexName = System.Environment.GetEnvironmentVariable("AI_SEARCH_INDEX_NAME");

  // Create an AIProjectClient object that will be used to create the agent and query the index.
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Resolve the project connection ID from the connection name.
  AIProjectConnection aiSearchConnection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);

  // Create an AzureAISearchToolIndex object that defines the index and the search parameters.
  AzureAISearchToolIndex index = new()
  {
      ProjectConnectionId = aiSearchConnection.Id,
      IndexName = aiSearchIndexName,
      TopK = 5,
      Filter = "category eq 'sleeping bag'",
      QueryType = AzureAISearchQueryType.Simple
  };

  // Create the agent definition with the Azure AI Search tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `\u3010message_idx:search_idx\u2020source\u3011`.",
      Tools = { new AzureAISearchTool(new AzureAISearchToolOptions(indexes: [index])) }
  };

  // Create the agent version with the agent definition.
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create an OpenAIResponse object with the ProjectResponsesClient object.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  ResponseResult response = responseClient.CreateResponse("What is the temperature rating of the cozynights sleeping bag?");

  // In the search, an index containing "embedding", "token", "category", "title", and "url" fields is used.
  // The last two fields are needed to get citation title and URL, which the agent retrieves.
  // To get the reference, you need to parse the output items.
  string result = "";
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is MessageResponseItem messageItem)
      {
          foreach (ResponseContentPart content in messageItem.Content)
          {
              foreach (ResponseMessageAnnotation annotation in content.OutputTextAnnotations)
              {
                  if (annotation is UriCitationMessageAnnotation uriAnnotation)
                  {
                      result = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                  }
              }
          }
      }
  }

  // Use the helper method to output the result.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine($"{response.GetOutputText()}{result}");

  // Finally, delete all the resources you created in this sample.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected outcome

  The agent queries the specified index for information about the sleeping bag. The response includes the temperature rating and a formatted citation with the document title and URL. The response status is `Completed`, and the agent version is successfully deleted.

  ## Use agents with Azure AI Search tool for streaming scenarios

  This example shows how to use the Azure AI Search tool with agents to query an index in a streaming scenario.

  ```csharp
  // Read the environment variables to be used in the next steps
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");
  var aiSearchIndexName = System.Environment.GetEnvironmentVariable("AI_SEARCH_INDEX_NAME");

  // Create an AIProjectClient object that will be used to create the agent and query the index.
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Resolve the project connection ID from the connection name.
  AIProjectConnection aiSearchConnection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);

  // Create an AzureAISearchToolIndex object that defines the index and the search parameters.
  AzureAISearchToolIndex index = new()
  {
      ProjectConnectionId = aiSearchConnection.Id,
      IndexName = aiSearchIndexName,
      TopK = 5,
      Filter = "category eq 'sleeping bag'",
      QueryType = AzureAISearchQueryType.Simple
  };

  // Create the agent definition with the Azure AI Search tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `\u3010message_idx:search_idx\u2020source\u3011`.",
      Tools = { new AzureAISearchTool(new AzureAISearchToolOptions(indexes: [index])) }
  };

  // Create the agent version with the agent definition.
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create an OpenAIResponse object with the ProjectResponsesClient object.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  string annotation = "";
  string text = "";

  // Stream the response from the agent and parse the output items for citations.
  foreach (StreamingResponseUpdate streamResponse in responseClient.CreateResponseStreaming("What is the temperature rating of the cozynights sleeping bag?"))
  {
      if (streamResponse is StreamingResponseCreatedUpdate createUpdate)
      {
          Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
      }
      else if (streamResponse is StreamingResponseOutputTextDeltaUpdate textDelta)
      {
          Console.WriteLine($"Delta: {textDelta.Delta}");
      }
      else if (streamResponse is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
      {
          text = textDoneUpdate.Text;
      }
      else if (streamResponse is StreamingResponseOutputItemDoneUpdate itemDoneUpdate)
      {
          if (annotation.Length == 0)
          {
              if (itemDoneUpdate.Item is MessageResponseItem messageItem)
              {
                  // Use an index containing "embedding", "token", "category", "title", and "url" fields.
                  // The last two fields are needed to get citation title and URL, retrieved by the agent.
                  foreach (ResponseContentPart content in messageItem.Content)
                  {
                      foreach (ResponseMessageAnnotation messageAnnotation in content.OutputTextAnnotations)
                      {
                          if (messageAnnotation is UriCitationMessageAnnotation uriAnnotation)
                          {
                              annotation = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                          }
                      }
                  }
              }
          }
      }
      else if (streamResponse is StreamingResponseErrorUpdate errorUpdate)
      {
          throw new InvalidOperationException($"The stream has failed: {errorUpdate.Message}");
      }
  }
  Console.WriteLine($"{text}{annotation}");

  // Finally, delete all the resources that were created in this sample.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected outcome

  The streaming response displays the agent's response creation, text deltas as they stream in real-time, and a formatted citation when complete. The final output includes the sleeping bag temperature rating with document reference. The agent version is deleted after the query completes.
</ZonePivot>

<ZonePivot pivot="rest">
  ## Use agents with Azure AI Search tool

  The following example shows how to use the Azure AI Search tool with the REST API to query an index. The example uses cURL, but you can use any HTTP client.

  Before running this sample, obtain a bearer token for authentication. Use the Azure CLI to get a token:

  ```bash
  az account get-access-token --resource https://cognitiveservices.azure.com
  ```

  Set `AGENT_TOKEN` to the token value and `API_VERSION` to the current API version (for example, `2025-01-01-preview`).

  ```bash
  curl --request POST \
    --url "$FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    --data '{
      "model": "$FOUNDRY_MODEL_DEPLOYMENT_NAME",
      "input": "Tell me about the mental health services available from Premera.",
      "tool_choice": "required",
      "tools": [
        {
          "type": "azure_ai_search",
          "azure_ai_search": {
            "indexes": [
              {
                "project_connection_id": "$AZURE_AI_SEARCH_CONNECTION_ID",
                "index_name": "$AI_SEARCH_INDEX_NAME",
                "query_type": "semantic",
                "top_k": 5
              }
            ]
          }
        }
      ]
    }'
  ```

  ### Expected outcome

  The API returns a JSON response containing the agent's answer about mental health services from the Premera index. The response includes citations and references to the indexed documents that generated the answer.
</ZonePivot>

<ZonePivot pivot="typescript">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const aiSearchConnectionName = process.env["AZURE_AI_SEARCH_CONNECTION_NAME"] || "<ai search connection name>";

  async function verifyConnection(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    console.log("Connected to project.");

    try {
      const conn = await project.connections.get(aiSearchConnectionName);
      console.log(`Azure AI Search connection verified: ${conn.name}`);
      console.log(`Connection ID: ${conn.id}`);
    } catch (error) {
      console.log(`Azure AI Search connection '${aiSearchConnectionName}' not found: ${error}`);
      // List available connections
      console.log("Available connections:");
      for await (const conn of project.connections.list()) {
        console.log(`  - ${conn.name}`);
      }
    }
  }

  verifyConnection().catch(console.error);
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  This sample demonstrates how to create an AI agent with Azure AI Search capabilities by using the `AzureAISearchAgentTool` and synchronous Azure AI Projects client. The agent can search indexed content and provide responses with citations from search results.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as readline from "readline";
  import "dotenv/config";

  // Load environment variables
  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const aiSearchConnectionName =
    process.env["AZURE_AI_SEARCH_CONNECTION_NAME"] || "<ai search connection name>";
  const aiSearchIndexName = process.env["AI_SEARCH_INDEX_NAME"] || "<ai search index name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Get connection ID from connection name
    const aiSearchConnection = await project.connections.get(aiSearchConnectionName);
    console.log(`Azure AI Search connection ID: ${aiSearchConnection.id}`);

    console.log("Creating agent with Azure AI Search tool...");

    // Define Azure AI Search tool that searches indexed content
    const agent = await project.agents.createVersion("MyAISearchAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `[message_idx:search_idx†source]`.",
      tools: [
        {
          type: "azure_ai_search",
          azure_ai_search: {
            indexes: [
              {
                project_connection_id: aiSearchConnection.id,
                index_name: aiSearchIndexName,
                query_type: "simple",
              },
            ],
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Prompt user for input
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    });

    const userInput = await new Promise<string>((resolve) => {
      rl.question(
        "Enter your question for the AI Search agent available in the index (e.g., 'Tell me about the mental health services available from Premera'): \n",
        (answer) => {
          rl.close();
          resolve(answer);
        },
      );
    });

    console.log("\nSending request to AI Search agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: userInput,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        if (event.item.type === "message") {
          const item = event.item;
          if (item.content && item.content.length > 0) {
            const lastContent = item.content[item.content.length - 1];
            if (lastContent.type === "output_text" && lastContent.annotations) {
              for (const annotation of lastContent.annotations) {
                if (annotation.type === "url_citation") {
                  console.log(
                    `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                  );
                }
              }
            }
          }
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nAzure AI Search agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected outcome

  The application creates an agent with Azure AI Search capabilities, prompts for user input, queries the search index, and streams the response with real-time delta updates. Console output includes the agent ID, streaming text deltas, URL citations with indices, and confirmation of successful agent deletion. The agent provides answers grounded in the indexed content with proper citations.
</ZonePivot>

## Limitations

Keep these constraints in mind when using the Azure AI Search tool:

* **Virtual network access**: Azure AI Search doesn't support virtual network (vNET) configurations with agents at this time.
* The Azure AI Search tool can only target one index.
* Your Azure AI Search resource and your Microsoft Foundry Agent must be in the same tenant.

## Verify results

After you run a sample, validate that the agent is grounding responses from your index.

1. Ask a question that you know is answered in a specific indexed document.
2. Confirm the response includes citations formatted as `[message_idx:search_idx†source]`.
3. If you're streaming, confirm you see `url_citation` annotations in the response with valid URLs.
4. Verify the cited content matches your source documents in the search index.

If citations are missing or incorrect, see the [Troubleshooting](#troubleshooting) section.

## Setup

In this section, you create a connection between the Microsoft Foundry project that contains your agent and the Azure AI Search service that contains your index.

If you already connected your project to your search service, skip this section.

To create the connection, you need your search service endpoint and authentication method. The following steps guide you through gathering these details.

### Gather connection details

Before creating a project connection, gather your Azure AI Search service endpoint and authentication credentials.

The project connection requires the endpoint of your search service and either key-based authentication or keyless authentication with Microsoft Entra ID.

For keyless authentication, you must enable role-based access control (RBAC) and assign roles to your project's managed identity. Although this method involves extra steps, it enhances security by eliminating the need for hard-coded API keys.

Select the tab for your desired authentication method.

<Tabs>
  <Tab title="Key-based authentication">
    1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.

    2. To get the endpoint:

       1. From the left pane, select **Overview**.

       2. Make a note of the URL, which should look like `https://my-service.search.windows.net`.

          ![A screenshot of an AI Search resource Overview tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/connection-endpoint.png)

    3. To get the API key:

       1. From the left pane, select **Settings** > **Keys**.
       2. Select **Both** to enable both key-based and keyless authentication, which is recommended for most scenarios.

       ![A screenshot of an AI Search resource Keys tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/azure-portal.png)

       1. Make a note of one of the keys under **Manage admin keys**.
  </Tab>

  <Tab title="Keyless authentication">
    1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.

    2. To get the endpoint:

       1. From the left pane, select **Overview**.

       2. Make a note of the URL, which should look like `https://my-service.search.windows.net`.

          ![A screenshot of an AI Search resource Overview tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/connection-endpoint.png)

    3. To enable RBAC:

       1. From the left pane, select **Settings** > **Keys**.
       2. Select **Both** to enable both key-based and keyless authentication, which is recommended for most scenarios.

       ![A screenshot of an AI Search resource Keys tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/azure-portal.png)

    4. To assign the necessary roles:

       1. From the left pane, select **Access control (IAM)**.
       2. Select **Add** > **Add role assignment**.
       3. Assign the **Search Index Data Contributor** role to the managed identity of your project.
       4. Repeat the role assignment for **Search Service Contributor**.
  </Tab>
</Tabs>

### Create a project connection

Create the project connection by using the search service details you gathered.

Use one of the following options.

<Tabs>
  <Tab title="Azure CLI">
    **Create the following connection.yml file:**

    You can use a YAML configuration file for both key-based and keyless authentication. Replace the `name`, `endpoint`, and `api_key` (optional) placeholders with your search service details. For more information, see the [Azure AI Search connection YAML schema](../../../../machine-learning/reference-yaml-connection-ai-search).

    Here's a key-based example:

    ```yml
    name: my_project_acs_connection_keys
    type: azure_ai_search
    endpoint: https://contoso.search.windows.net/
    api_key: XXXXXXXXXXXXXXX
    ```

    <Callout type="important">
      Don't put real keys in source control. Store secrets in a secure store (for example, Azure Key Vault) and inject them at deployment time.
    </Callout>

    Here's a keyless example:

    ```yml
    name: my_project_acs_connection_keyless
    type: azure_ai_search
    endpoint: https://contoso.search.windows.net/
    ```

    **Then, run the following command:**

    Replace the placeholders with the resource group and project name.

    ```azurecli
    az ml connection create --file connection.yml --resource-group <resource-group> --workspace-name <project-name>
    ```
  </Tab>

  <Tab title="Python">
    Replace the `my_connection_name`, `my_endpoint`, and `my_key` (optional) placeholders with your search service details, and then run the following code:

    ```python
    from azure.identity import DefaultAzureCredential
    from azure.ai.ml import MLClient
    from azure.ai.ml.entities import AzureAISearchConnection

    # Create an Azure AI Search project connection
    my_connection_name = "my-connection-name"
    my_endpoint = "my-endpoint" # This could also be called target
    my_api_keys = None # Leave blank for Authentication type = AAD

    my_connection = AzureAISearchConnection(name=my_connection_name,
                                        endpoint=my_endpoint,
                                        api_key= my_api_keys)

    # Create MLClient
    ml_client = MLClient(
      credential=DefaultAzureCredential(),
      subscription_id="<subscription-id>",
      resource_group_name="<resource-group>",
      workspace_name="<project-name>",
    )

    # Create the connection
    ml_client.connections.create_or_update(my_connection)
    ```
  </Tab>
</Tabs>

### Confirm the connection ID

If you use the REST or TypeScript sample, you need the project connection ID.

**Python**

```python
import os
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient

load_dotenv()

project_client = AIProjectClient(
  endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
  credential=DefaultAzureCredential(),
)

connection = project_client.connections.get(os.environ["AZURE_AI_SEARCH_CONNECTION_NAME"])
print(connection.id)
```

**C#**

```csharp
var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");

AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());
AIProjectConnection connection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);
Console.WriteLine(connection.Id);
```

## Troubleshooting

| Issue                                 | Cause                                      | Resolution                                                                                                                                                                                    |
| ------------------------------------- | ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Response has no citations             | Agent instructions don't request citations | Update your agent instructions to explicitly request citations in responses.                                                                                                                  |
| Response has no citations (streaming) | Annotations not captured                   | Confirm you receive `url_citation` annotations when streaming. Check your stream processing logic.                                                                                            |
| Tool can't access the index (401/403) | Missing RBAC roles (keyless auth)          | Assign the **Search Index Data Contributor** and **Search Service Contributor** roles to the Foundry project's managed identity. See [Azure RBAC in Foundry](../../../concepts/rbac-foundry). |
| Tool can't access the index (401/403) | Invalid or disabled API key                | Confirm the API key is correct and enabled in the Azure AI Search resource.                                                                                                                   |
| Tool returns "index not found"        | Index name mismatch                        | Confirm `AI_SEARCH_INDEX_NAME` matches the exact index name in your Azure AI Search resource (case-sensitive).                                                                                |
| Tool returns "index not found"        | Wrong connection endpoint                  | Confirm the project connection points to the Azure AI Search resource that contains the index.                                                                                                |
| Search returns no results             | Query doesn't match indexed content        | Verify the index contains the expected data. Use Azure AI Search's test query feature to validate.                                                                                            |
| Slow search performance               | Index not optimized                        | Review index configuration, consider adding semantic ranking, or optimize the index schema.                                                                                                   |

## Related content

* [Connect a Foundry IQ knowledge base to Foundry Agent Service](../foundry-iq-connect)
* [Tool best practices](../../concepts/tool-best-practice)
* [Create a vector search index in Azure AI Search](../../../../search/search-get-started-portal-import-vectors)
* [Quickstart: Build an agent with Foundry](../../../quickstarts/get-started-code)

<Callout type="tip">
  For a managed knowledge base experience, see [Foundry IQ](../foundry-iq-connect). For tool optimization, see [best practices](../../concepts/tool-best-practice).
</Callout>

Ground your Foundry agent's responses in your proprietary content by connecting it to an Azure AI Search index. The [Azure AI Search](../../../../search/search-what-is-azure-search) tool retrieves indexed documents and generates answers with inline citations, enabling accurate, source-backed responses.

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

Java SDK samples are coming soon.

## Prerequisites

*Estimated setup time: 15-30 minutes if you have an existing search index*

* A [basic or standard agent environment](../../environment-setup).

* The latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code) for details.

  * **Python**: `pip install azure-ai-projects --pre`
  * **C#**: Install the `Azure.AI.Projects` NuGet package (prerelease)
  * **JavaScript/TypeScript**: `npm install @azure/ai-projects`

* An Azure subscription and Microsoft Foundry project with:

  * Project endpoint
  * Model deployment name
  * Authentication configured (for example, `DefaultAzureCredential`)

* An [Azure AI Search index configured for vector search](../../../../search/search-get-started-portal-import-vectors) with:

  * One or more `Edm.String` fields that are searchable and retrievable
  * One or more `Collection(Edm.Single)` vector fields that are searchable
  * At least one retrievable text field that contains the content you want the agent to cite
  * A retrievable field that contains a source URL (and optionally a title) so citations can include a link

* A connection between your Foundry project and your Azure AI Search service (see [Setup](#setup)).

* For keyless authentication, assign the following Azure role-based access control (RBAC) roles to your project's managed identity:

  * **Search Index Data Contributor**
  * **Search Service Contributor**

### Set environment variables

| Variable                          | Description                                                                                                    |
| --------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| `FOUNDRY_PROJECT_ENDPOINT`        | Your Foundry project endpoint.                                                                                 |
| `FOUNDRY_MODEL_DEPLOYMENT_NAME`   | Your model deployment name.                                                                                    |
| `AZURE_AI_SEARCH_CONNECTION_NAME` | The name of your project connection to Azure AI Search (used by the SDK samples to look up the connection ID). |
| `AZURE_AI_SEARCH_CONNECTION_ID`   | The resource ID of your project connection to Azure AI Search (used by the TypeScript and REST samples).       |
| `AI_SEARCH_INDEX_NAME`            | Your Azure AI Search index name.                                                                               |

## Configure tool parameters

| Azure AI Search tool parameter | Required | Notes                                                                                                                                     |
| ------------------------------ | -------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| `project_connection_id`        | Yes      | The resource ID of the project connection to Azure AI Search.                                                                             |
| `index_name`                   | Yes      | The name of the index in your Azure AI Search resource.                                                                                   |
| `top_k`                        | No       | Defaults to 5.                                                                                                                            |
| `query_type`                   | No       | Defaults to `vector_semantic_hybrid`. Supported values: `simple`, `vector`, `semantic`, `vector_simple_hybrid`, `vector_semantic_hybrid`. |
| `filter`                       | No       | Applies to all queries the agent makes to the index.                                                                                      |

## Code example

<Callout type="note">
  * You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code).
  * If you're using the REST or TypeScript sample, the connection ID is in the format `/subscriptions/{{subscriptionId}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{connectionName}}`.
  * If you're using the Python or C# sample, you can provide the connection name and retrieve the connection ID with the SDK.
</Callout>

<ZonePivot pivot="python">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```python
  import os

  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential
  from dotenv import load_dotenv

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"], credential=credential) as project_client,
  ):
      print("Connected to project.")

      # Verify Azure AI Search connection exists
      connection_name = os.environ.get("AZURE_AI_SEARCH_CONNECTION_NAME")
      if connection_name:
          try:
              conn = project_client.connections.get(connection_name)
              print(f"Azure AI Search connection verified: {conn.name}")
              print(f"Connection ID: {conn.id}")
          except Exception as e:
              print(f"Azure AI Search connection '{connection_name}' not found: {e}")
      else:
          # List available connections to help find the right one
          print("AZURE_AI_SEARCH_CONNECTION_NAME not set. Available connections:")
          for conn in project_client.connections.list():
              print(f"  - {conn.name}")
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      AzureAISearchAgentTool,
      PromptAgentDefinition,
      AzureAISearchToolResource,
      AISearchIndexResource,
      AzureAISearchQueryType,
  )

  load_dotenv()

  project_client = AIProjectClient(
    endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  openai_client = project_client.get_openai_client()

  with project_client:

      azs_connection = project_client.connections.get(os.environ["AZURE_AI_SEARCH_CONNECTION_NAME"])
      connection_id = azs_connection.id
      print(f"Azure AI Search connection ID: {connection_id}")

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
            model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are a helpful assistant. You must always provide citations for
              answers using the tool and render them as: `[message_idx:search_idx†source]`.""",
              tools=[
                  AzureAISearchAgentTool(
                      azure_ai_search=AzureAISearchToolResource(
                          indexes=[
                              AISearchIndexResource(
                                  project_connection_id=connection_id,
                                  index_name=os.environ["AI_SEARCH_INDEX_NAME"],
                                  query_type=AzureAISearchQueryType.SIMPLE,
                              ),
                          ]
                      )
                  )
              ],
          ),
          description="You are a helpful agent.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      user_input = input(
          """Enter your question for the AI Search agent available in the index
          (e.g., 'Tell me about the mental health services available from Premera'): \n"""
      )

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input=user_input,
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              if event.item.type == "message":
                  item = event.item
                  if item.content[-1].type == "output_text":
                      text_content = item.content[-1]
                      for annotation in text_content.annotations:
                          if annotation.type == "url_citation":
                              print(
                                  f"URL Citation: {annotation.url}, "
                                  f"Start index: {annotation.start_index}, "
                                  f"End index: {annotation.end_index}"
                              )
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")

      print("\nCleaning up...")
      project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
      print("Agent deleted")
  ```

  ### Expected outcome

  The agent queries the search index and returns a response with inline citations. Console output shows the agent ID, streaming delta updates as the response generates, URL citations with start and end indices, and the final complete response text. The agent is then successfully deleted.
</ZonePivot>

<ZonePivot pivot="csharp">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```csharp
  using Azure.AI.Projects;
  using Azure.Identity;

  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Verify Azure AI Search connection exists
  try
  {
      AIProjectConnection conn = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);
      Console.WriteLine($"Azure AI Search connection verified: {conn.Name}");
      Console.WriteLine($"Connection ID: {conn.Id}");
  }
  catch (Exception ex)
  {
      Console.WriteLine($"Azure AI Search connection '{aiSearchConnectionName}' not found: {ex.Message}");
      // List available connections
      Console.WriteLine("Available connections:");
      foreach (var conn in projectClient.Connections.GetConnections())
      {
          Console.WriteLine($"  - {conn.Name}");
      }
  }
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  The following sample code shows synchronous examples of how to use the Azure AI Search tool in [Azure.AI.Projects.OpenAI](https://github.com/Azure/azure-sdk-for-net/tree/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI) to query an index. For asynchronous C# examples, see the [GitHub repo](https://github.com/Azure/azure-sdk-for-net/tree/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI).

  This example shows how to use the Azure AI Search tool with agents to query an index.

  ```csharp
  // Read the environment variables to be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");
  var aiSearchIndexName = System.Environment.GetEnvironmentVariable("AI_SEARCH_INDEX_NAME");

  // Create an AIProjectClient object that will be used to create the agent and query the index.
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Resolve the project connection ID from the connection name.
  AIProjectConnection aiSearchConnection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);

  // Create an AzureAISearchToolIndex object that defines the index and the search parameters.
  AzureAISearchToolIndex index = new()
  {
      ProjectConnectionId = aiSearchConnection.Id,
      IndexName = aiSearchIndexName,
      TopK = 5,
      Filter = "category eq 'sleeping bag'",
      QueryType = AzureAISearchQueryType.Simple
  };

  // Create the agent definition with the Azure AI Search tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `\u3010message_idx:search_idx\u2020source\u3011`.",
      Tools = { new AzureAISearchTool(new AzureAISearchToolOptions(indexes: [index])) }
  };

  // Create the agent version with the agent definition.
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create an OpenAIResponse object with the ProjectResponsesClient object.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  ResponseResult response = responseClient.CreateResponse("What is the temperature rating of the cozynights sleeping bag?");

  // In the search, an index containing "embedding", "token", "category", "title", and "url" fields is used.
  // The last two fields are needed to get citation title and URL, which the agent retrieves.
  // To get the reference, you need to parse the output items.
  string result = "";
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is MessageResponseItem messageItem)
      {
          foreach (ResponseContentPart content in messageItem.Content)
          {
              foreach (ResponseMessageAnnotation annotation in content.OutputTextAnnotations)
              {
                  if (annotation is UriCitationMessageAnnotation uriAnnotation)
                  {
                      result = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                  }
              }
          }
      }
  }

  // Use the helper method to output the result.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine($"{response.GetOutputText()}{result}");

  // Finally, delete all the resources you created in this sample.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected outcome

  The agent queries the specified index for information about the sleeping bag. The response includes the temperature rating and a formatted citation with the document title and URL. The response status is `Completed`, and the agent version is successfully deleted.

  ## Use agents with Azure AI Search tool for streaming scenarios

  This example shows how to use the Azure AI Search tool with agents to query an index in a streaming scenario.

  ```csharp
  // Read the environment variables to be used in the next steps
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");
  var aiSearchIndexName = System.Environment.GetEnvironmentVariable("AI_SEARCH_INDEX_NAME");

  // Create an AIProjectClient object that will be used to create the agent and query the index.
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Resolve the project connection ID from the connection name.
  AIProjectConnection aiSearchConnection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);

  // Create an AzureAISearchToolIndex object that defines the index and the search parameters.
  AzureAISearchToolIndex index = new()
  {
      ProjectConnectionId = aiSearchConnection.Id,
      IndexName = aiSearchIndexName,
      TopK = 5,
      Filter = "category eq 'sleeping bag'",
      QueryType = AzureAISearchQueryType.Simple
  };

  // Create the agent definition with the Azure AI Search tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `\u3010message_idx:search_idx\u2020source\u3011`.",
      Tools = { new AzureAISearchTool(new AzureAISearchToolOptions(indexes: [index])) }
  };

  // Create the agent version with the agent definition.
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create an OpenAIResponse object with the ProjectResponsesClient object.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  string annotation = "";
  string text = "";

  // Stream the response from the agent and parse the output items for citations.
  foreach (StreamingResponseUpdate streamResponse in responseClient.CreateResponseStreaming("What is the temperature rating of the cozynights sleeping bag?"))
  {
      if (streamResponse is StreamingResponseCreatedUpdate createUpdate)
      {
          Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
      }
      else if (streamResponse is StreamingResponseOutputTextDeltaUpdate textDelta)
      {
          Console.WriteLine($"Delta: {textDelta.Delta}");
      }
      else if (streamResponse is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
      {
          text = textDoneUpdate.Text;
      }
      else if (streamResponse is StreamingResponseOutputItemDoneUpdate itemDoneUpdate)
      {
          if (annotation.Length == 0)
          {
              if (itemDoneUpdate.Item is MessageResponseItem messageItem)
              {
                  // Use an index containing "embedding", "token", "category", "title", and "url" fields.
                  // The last two fields are needed to get citation title and URL, retrieved by the agent.
                  foreach (ResponseContentPart content in messageItem.Content)
                  {
                      foreach (ResponseMessageAnnotation messageAnnotation in content.OutputTextAnnotations)
                      {
                          if (messageAnnotation is UriCitationMessageAnnotation uriAnnotation)
                          {
                              annotation = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                          }
                      }
                  }
              }
          }
      }
      else if (streamResponse is StreamingResponseErrorUpdate errorUpdate)
      {
          throw new InvalidOperationException($"The stream has failed: {errorUpdate.Message}");
      }
  }
  Console.WriteLine($"{text}{annotation}");

  // Finally, delete all the resources that were created in this sample.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected outcome

  The streaming response displays the agent's response creation, text deltas as they stream in real-time, and a formatted citation when complete. The final output includes the sleeping bag temperature rating with document reference. The agent version is deleted after the query completes.
</ZonePivot>

<ZonePivot pivot="rest">
  ## Use agents with Azure AI Search tool

  The following example shows how to use the Azure AI Search tool with the REST API to query an index. The example uses cURL, but you can use any HTTP client.

  Before running this sample, obtain a bearer token for authentication. Use the Azure CLI to get a token:

  ```bash
  az account get-access-token --resource https://cognitiveservices.azure.com
  ```

  Set `AGENT_TOKEN` to the token value and `API_VERSION` to the current API version (for example, `2025-01-01-preview`).

  ```bash
  curl --request POST \
    --url "$FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    --data '{
      "model": "$FOUNDRY_MODEL_DEPLOYMENT_NAME",
      "input": "Tell me about the mental health services available from Premera.",
      "tool_choice": "required",
      "tools": [
        {
          "type": "azure_ai_search",
          "azure_ai_search": {
            "indexes": [
              {
                "project_connection_id": "$AZURE_AI_SEARCH_CONNECTION_ID",
                "index_name": "$AI_SEARCH_INDEX_NAME",
                "query_type": "semantic",
                "top_k": 5
              }
            ]
          }
        }
      ]
    }'
  ```

  ### Expected outcome

  The API returns a JSON response containing the agent's answer about mental health services from the Premera index. The response includes citations and references to the indexed documents that generated the answer.
</ZonePivot>

<ZonePivot pivot="typescript">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const aiSearchConnectionName = process.env["AZURE_AI_SEARCH_CONNECTION_NAME"] || "<ai search connection name>";

  async function verifyConnection(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    console.log("Connected to project.");

    try {
      const conn = await project.connections.get(aiSearchConnectionName);
      console.log(`Azure AI Search connection verified: ${conn.name}`);
      console.log(`Connection ID: ${conn.id}`);
    } catch (error) {
      console.log(`Azure AI Search connection '${aiSearchConnectionName}' not found: ${error}`);
      // List available connections
      console.log("Available connections:");
      for await (const conn of project.connections.list()) {
        console.log(`  - ${conn.name}`);
      }
    }
  }

  verifyConnection().catch(console.error);
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  This sample demonstrates how to create an AI agent with Azure AI Search capabilities by using the `AzureAISearchAgentTool` and synchronous Azure AI Projects client. The agent can search indexed content and provide responses with citations from search results.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as readline from "readline";
  import "dotenv/config";

  // Load environment variables
  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const aiSearchConnectionName =
    process.env["AZURE_AI_SEARCH_CONNECTION_NAME"] || "<ai search connection name>";
  const aiSearchIndexName = process.env["AI_SEARCH_INDEX_NAME"] || "<ai search index name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Get connection ID from connection name
    const aiSearchConnection = await project.connections.get(aiSearchConnectionName);
    console.log(`Azure AI Search connection ID: ${aiSearchConnection.id}`);

    console.log("Creating agent with Azure AI Search tool...");

    // Define Azure AI Search tool that searches indexed content
    const agent = await project.agents.createVersion("MyAISearchAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `[message_idx:search_idx†source]`.",
      tools: [
        {
          type: "azure_ai_search",
          azure_ai_search: {
            indexes: [
              {
                project_connection_id: aiSearchConnection.id,
                index_name: aiSearchIndexName,
                query_type: "simple",
              },
            ],
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Prompt user for input
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    });

    const userInput = await new Promise<string>((resolve) => {
      rl.question(
        "Enter your question for the AI Search agent available in the index (e.g., 'Tell me about the mental health services available from Premera'): \n",
        (answer) => {
          rl.close();
          resolve(answer);
        },
      );
    });

    console.log("\nSending request to AI Search agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: userInput,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        if (event.item.type === "message") {
          const item = event.item;
          if (item.content && item.content.length > 0) {
            const lastContent = item.content[item.content.length - 1];
            if (lastContent.type === "output_text" && lastContent.annotations) {
              for (const annotation of lastContent.annotations) {
                if (annotation.type === "url_citation") {
                  console.log(
                    `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                  );
                }
              }
            }
          }
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nAzure AI Search agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected outcome

  The application creates an agent with Azure AI Search capabilities, prompts for user input, queries the search index, and streams the response with real-time delta updates. Console output includes the agent ID, streaming text deltas, URL citations with indices, and confirmation of successful agent deletion. The agent provides answers grounded in the indexed content with proper citations.
</ZonePivot>

## Limitations

Keep these constraints in mind when using the Azure AI Search tool:

* **Virtual network access**: Azure AI Search doesn't support virtual network (vNET) configurations with agents at this time.
* The Azure AI Search tool can only target one index.
* Your Azure AI Search resource and your Microsoft Foundry Agent must be in the same tenant.

## Verify results

After you run a sample, validate that the agent is grounding responses from your index.

1. Ask a question that you know is answered in a specific indexed document.
2. Confirm the response includes citations formatted as `[message_idx:search_idx†source]`.
3. If you're streaming, confirm you see `url_citation` annotations in the response with valid URLs.
4. Verify the cited content matches your source documents in the search index.

If citations are missing or incorrect, see the [Troubleshooting](#troubleshooting) section.

## Setup

In this section, you create a connection between the Microsoft Foundry project that contains your agent and the Azure AI Search service that contains your index.

If you already connected your project to your search service, skip this section.

To create the connection, you need your search service endpoint and authentication method. The following steps guide you through gathering these details.

### Gather connection details

Before creating a project connection, gather your Azure AI Search service endpoint and authentication credentials.

The project connection requires the endpoint of your search service and either key-based authentication or keyless authentication with Microsoft Entra ID.

For keyless authentication, you must enable role-based access control (RBAC) and assign roles to your project's managed identity. Although this method involves extra steps, it enhances security by eliminating the need for hard-coded API keys.

Select the tab for your desired authentication method.

<Tabs>
  <Tab title="Key-based authentication">
    1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.

    2. To get the endpoint:

       1. From the left pane, select **Overview**.

       2. Make a note of the URL, which should look like `https://my-service.search.windows.net`.

          ![A screenshot of an AI Search resource Overview tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/connection-endpoint.png)

    3. To get the API key:

       1. From the left pane, select **Settings** > **Keys**.
       2. Select **Both** to enable both key-based and keyless authentication, which is recommended for most scenarios.

       ![A screenshot of an AI Search resource Keys tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/azure-portal.png)

       1. Make a note of one of the keys under **Manage admin keys**.
  </Tab>

  <Tab title="Keyless authentication">
    1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.

    2. To get the endpoint:

       1. From the left pane, select **Overview**.

       2. Make a note of the URL, which should look like `https://my-service.search.windows.net`.

          ![A screenshot of an AI Search resource Overview tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/connection-endpoint.png)

    3. To enable RBAC:

       1. From the left pane, select **Settings** > **Keys**.
       2. Select **Both** to enable both key-based and keyless authentication, which is recommended for most scenarios.

       ![A screenshot of an AI Search resource Keys tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/azure-portal.png)

    4. To assign the necessary roles:

       1. From the left pane, select **Access control (IAM)**.
       2. Select **Add** > **Add role assignment**.
       3. Assign the **Search Index Data Contributor** role to the managed identity of your project.
       4. Repeat the role assignment for **Search Service Contributor**.
  </Tab>
</Tabs>

### Create a project connection

Create the project connection by using the search service details you gathered.

Use one of the following options.

<Tabs>
  <Tab title="Azure CLI">
    **Create the following connection.yml file:**

    You can use a YAML configuration file for both key-based and keyless authentication. Replace the `name`, `endpoint`, and `api_key` (optional) placeholders with your search service details. For more information, see the [Azure AI Search connection YAML schema](../../../../machine-learning/reference-yaml-connection-ai-search).

    Here's a key-based example:

    ```yml
    name: my_project_acs_connection_keys
    type: azure_ai_search
    endpoint: https://contoso.search.windows.net/
    api_key: XXXXXXXXXXXXXXX
    ```

    <Callout type="important">
      Don't put real keys in source control. Store secrets in a secure store (for example, Azure Key Vault) and inject them at deployment time.
    </Callout>

    Here's a keyless example:

    ```yml
    name: my_project_acs_connection_keyless
    type: azure_ai_search
    endpoint: https://contoso.search.windows.net/
    ```

    **Then, run the following command:**

    Replace the placeholders with the resource group and project name.

    ```azurecli
    az ml connection create --file connection.yml --resource-group <resource-group> --workspace-name <project-name>
    ```
  </Tab>

  <Tab title="Python">
    Replace the `my_connection_name`, `my_endpoint`, and `my_key` (optional) placeholders with your search service details, and then run the following code:

    ```python
    from azure.identity import DefaultAzureCredential
    from azure.ai.ml import MLClient
    from azure.ai.ml.entities import AzureAISearchConnection

    # Create an Azure AI Search project connection
    my_connection_name = "my-connection-name"
    my_endpoint = "my-endpoint" # This could also be called target
    my_api_keys = None # Leave blank for Authentication type = AAD

    my_connection = AzureAISearchConnection(name=my_connection_name,
                                        endpoint=my_endpoint,
                                        api_key= my_api_keys)

    # Create MLClient
    ml_client = MLClient(
      credential=DefaultAzureCredential(),
      subscription_id="<subscription-id>",
      resource_group_name="<resource-group>",
      workspace_name="<project-name>",
    )

    # Create the connection
    ml_client.connections.create_or_update(my_connection)
    ```
  </Tab>
</Tabs>

### Confirm the connection ID

If you use the REST or TypeScript sample, you need the project connection ID.

**Python**

```python
import os
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient

load_dotenv()

project_client = AIProjectClient(
  endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
  credential=DefaultAzureCredential(),
)

connection = project_client.connections.get(os.environ["AZURE_AI_SEARCH_CONNECTION_NAME"])
print(connection.id)
```

**C#**

```csharp
var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");

AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());
AIProjectConnection connection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);
Console.WriteLine(connection.Id);
```

## Troubleshooting

| Issue                                 | Cause                                      | Resolution                                                                                                                                                                                    |
| ------------------------------------- | ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Response has no citations             | Agent instructions don't request citations | Update your agent instructions to explicitly request citations in responses.                                                                                                                  |
| Response has no citations (streaming) | Annotations not captured                   | Confirm you receive `url_citation` annotations when streaming. Check your stream processing logic.                                                                                            |
| Tool can't access the index (401/403) | Missing RBAC roles (keyless auth)          | Assign the **Search Index Data Contributor** and **Search Service Contributor** roles to the Foundry project's managed identity. See [Azure RBAC in Foundry](../../../concepts/rbac-foundry). |
| Tool can't access the index (401/403) | Invalid or disabled API key                | Confirm the API key is correct and enabled in the Azure AI Search resource.                                                                                                                   |
| Tool returns "index not found"        | Index name mismatch                        | Confirm `AI_SEARCH_INDEX_NAME` matches the exact index name in your Azure AI Search resource (case-sensitive).                                                                                |
| Tool returns "index not found"        | Wrong connection endpoint                  | Confirm the project connection points to the Azure AI Search resource that contains the index.                                                                                                |
| Search returns no results             | Query doesn't match indexed content        | Verify the index contains the expected data. Use Azure AI Search's test query feature to validate.                                                                                            |
| Slow search performance               | Index not optimized                        | Review index configuration, consider adding semantic ranking, or optimize the index schema.                                                                                                   |

## Related content

* [Connect a Foundry IQ knowledge base to Foundry Agent Service](../foundry-iq-connect)
* [Tool best practices](../../concepts/tool-best-practice)
* [Create a vector search index in Azure AI Search](../../../../search/search-get-started-portal-import-vectors)
* [Quickstart: Build an agent with Foundry](../../../quickstarts/get-started-code)

<Callout type="tip">
  For a managed knowledge base experience, see [Foundry IQ](../foundry-iq-connect). For tool optimization, see [best practices](../../concepts/tool-best-practice).
</Callout>

Ground your Foundry agent's responses in your proprietary content by connecting it to an Azure AI Search index. The [Azure AI Search](../../../../search/search-what-is-azure-search) tool retrieves indexed documents and generates answers with inline citations, enabling accurate, source-backed responses.

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

Java SDK samples are coming soon.

## Prerequisites

*Estimated setup time: 15-30 minutes if you have an existing search index*

* A [basic or standard agent environment](../../environment-setup).

* The latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code) for details.

  * **Python**: `pip install azure-ai-projects --pre`
  * **C#**: Install the `Azure.AI.Projects` NuGet package (prerelease)
  * **JavaScript/TypeScript**: `npm install @azure/ai-projects`

* An Azure subscription and Microsoft Foundry project with:

  * Project endpoint
  * Model deployment name
  * Authentication configured (for example, `DefaultAzureCredential`)

* An [Azure AI Search index configured for vector search](../../../../search/search-get-started-portal-import-vectors) with:

  * One or more `Edm.String` fields that are searchable and retrievable
  * One or more `Collection(Edm.Single)` vector fields that are searchable
  * At least one retrievable text field that contains the content you want the agent to cite
  * A retrievable field that contains a source URL (and optionally a title) so citations can include a link

* A connection between your Foundry project and your Azure AI Search service (see [Setup](#setup)).

* For keyless authentication, assign the following Azure role-based access control (RBAC) roles to your project's managed identity:

  * **Search Index Data Contributor**
  * **Search Service Contributor**

### Set environment variables

| Variable                          | Description                                                                                                    |
| --------------------------------- | -------------------------------------------------------------------------------------------------------------- |
| `FOUNDRY_PROJECT_ENDPOINT`        | Your Foundry project endpoint.                                                                                 |
| `FOUNDRY_MODEL_DEPLOYMENT_NAME`   | Your model deployment name.                                                                                    |
| `AZURE_AI_SEARCH_CONNECTION_NAME` | The name of your project connection to Azure AI Search (used by the SDK samples to look up the connection ID). |
| `AZURE_AI_SEARCH_CONNECTION_ID`   | The resource ID of your project connection to Azure AI Search (used by the TypeScript and REST samples).       |
| `AI_SEARCH_INDEX_NAME`            | Your Azure AI Search index name.                                                                               |

## Configure tool parameters

| Azure AI Search tool parameter | Required | Notes                                                                                                                                     |
| ------------------------------ | -------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| `project_connection_id`        | Yes      | The resource ID of the project connection to Azure AI Search.                                                                             |
| `index_name`                   | Yes      | The name of the index in your Azure AI Search resource.                                                                                   |
| `top_k`                        | No       | Defaults to 5.                                                                                                                            |
| `query_type`                   | No       | Defaults to `vector_semantic_hybrid`. Supported values: `simple`, `vector`, `semantic`, `vector_simple_hybrid`, `vector_semantic_hybrid`. |
| `filter`                       | No       | Applies to all queries the agent makes to the index.                                                                                      |

## Code example

<Callout type="note">
  * You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code).
  * If you're using the REST or TypeScript sample, the connection ID is in the format `/subscriptions/{{subscriptionId}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{connectionName}}`.
  * If you're using the Python or C# sample, you can provide the connection name and retrieve the connection ID with the SDK.
</Callout>

<ZonePivot pivot="python">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```python
  import os

  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential
  from dotenv import load_dotenv

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"], credential=credential) as project_client,
  ):
      print("Connected to project.")

      # Verify Azure AI Search connection exists
      connection_name = os.environ.get("AZURE_AI_SEARCH_CONNECTION_NAME")
      if connection_name:
          try:
              conn = project_client.connections.get(connection_name)
              print(f"Azure AI Search connection verified: {conn.name}")
              print(f"Connection ID: {conn.id}")
          except Exception as e:
              print(f"Azure AI Search connection '{connection_name}' not found: {e}")
      else:
          # List available connections to help find the right one
          print("AZURE_AI_SEARCH_CONNECTION_NAME not set. Available connections:")
          for conn in project_client.connections.list():
              print(f"  - {conn.name}")
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      AzureAISearchAgentTool,
      PromptAgentDefinition,
      AzureAISearchToolResource,
      AISearchIndexResource,
      AzureAISearchQueryType,
  )

  load_dotenv()

  project_client = AIProjectClient(
    endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  openai_client = project_client.get_openai_client()

  with project_client:

      azs_connection = project_client.connections.get(os.environ["AZURE_AI_SEARCH_CONNECTION_NAME"])
      connection_id = azs_connection.id
      print(f"Azure AI Search connection ID: {connection_id}")

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
            model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are a helpful assistant. You must always provide citations for
              answers using the tool and render them as: `[message_idx:search_idx†source]`.""",
              tools=[
                  AzureAISearchAgentTool(
                      azure_ai_search=AzureAISearchToolResource(
                          indexes=[
                              AISearchIndexResource(
                                  project_connection_id=connection_id,
                                  index_name=os.environ["AI_SEARCH_INDEX_NAME"],
                                  query_type=AzureAISearchQueryType.SIMPLE,
                              ),
                          ]
                      )
                  )
              ],
          ),
          description="You are a helpful agent.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      user_input = input(
          """Enter your question for the AI Search agent available in the index
          (e.g., 'Tell me about the mental health services available from Premera'): \n"""
      )

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input=user_input,
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              if event.item.type == "message":
                  item = event.item
                  if item.content[-1].type == "output_text":
                      text_content = item.content[-1]
                      for annotation in text_content.annotations:
                          if annotation.type == "url_citation":
                              print(
                                  f"URL Citation: {annotation.url}, "
                                  f"Start index: {annotation.start_index}, "
                                  f"End index: {annotation.end_index}"
                              )
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")

      print("\nCleaning up...")
      project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
      print("Agent deleted")
  ```

  ### Expected outcome

  The agent queries the search index and returns a response with inline citations. Console output shows the agent ID, streaming delta updates as the response generates, URL citations with start and end indices, and the final complete response text. The agent is then successfully deleted.
</ZonePivot>

<ZonePivot pivot="csharp">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```csharp
  using Azure.AI.Projects;
  using Azure.Identity;

  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Verify Azure AI Search connection exists
  try
  {
      AIProjectConnection conn = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);
      Console.WriteLine($"Azure AI Search connection verified: {conn.Name}");
      Console.WriteLine($"Connection ID: {conn.Id}");
  }
  catch (Exception ex)
  {
      Console.WriteLine($"Azure AI Search connection '{aiSearchConnectionName}' not found: {ex.Message}");
      // List available connections
      Console.WriteLine("Available connections:");
      foreach (var conn in projectClient.Connections.GetConnections())
      {
          Console.WriteLine($"  - {conn.Name}");
      }
  }
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  The following sample code shows synchronous examples of how to use the Azure AI Search tool in [Azure.AI.Projects.OpenAI](https://github.com/Azure/azure-sdk-for-net/tree/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI) to query an index. For asynchronous C# examples, see the [GitHub repo](https://github.com/Azure/azure-sdk-for-net/tree/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI).

  This example shows how to use the Azure AI Search tool with agents to query an index.

  ```csharp
  // Read the environment variables to be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");
  var aiSearchIndexName = System.Environment.GetEnvironmentVariable("AI_SEARCH_INDEX_NAME");

  // Create an AIProjectClient object that will be used to create the agent and query the index.
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Resolve the project connection ID from the connection name.
  AIProjectConnection aiSearchConnection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);

  // Create an AzureAISearchToolIndex object that defines the index and the search parameters.
  AzureAISearchToolIndex index = new()
  {
      ProjectConnectionId = aiSearchConnection.Id,
      IndexName = aiSearchIndexName,
      TopK = 5,
      Filter = "category eq 'sleeping bag'",
      QueryType = AzureAISearchQueryType.Simple
  };

  // Create the agent definition with the Azure AI Search tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `\u3010message_idx:search_idx\u2020source\u3011`.",
      Tools = { new AzureAISearchTool(new AzureAISearchToolOptions(indexes: [index])) }
  };

  // Create the agent version with the agent definition.
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create an OpenAIResponse object with the ProjectResponsesClient object.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  ResponseResult response = responseClient.CreateResponse("What is the temperature rating of the cozynights sleeping bag?");

  // In the search, an index containing "embedding", "token", "category", "title", and "url" fields is used.
  // The last two fields are needed to get citation title and URL, which the agent retrieves.
  // To get the reference, you need to parse the output items.
  string result = "";
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is MessageResponseItem messageItem)
      {
          foreach (ResponseContentPart content in messageItem.Content)
          {
              foreach (ResponseMessageAnnotation annotation in content.OutputTextAnnotations)
              {
                  if (annotation is UriCitationMessageAnnotation uriAnnotation)
                  {
                      result = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                  }
              }
          }
      }
  }

  // Use the helper method to output the result.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine($"{response.GetOutputText()}{result}");

  // Finally, delete all the resources you created in this sample.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected outcome

  The agent queries the specified index for information about the sleeping bag. The response includes the temperature rating and a formatted citation with the document title and URL. The response status is `Completed`, and the agent version is successfully deleted.

  ## Use agents with Azure AI Search tool for streaming scenarios

  This example shows how to use the Azure AI Search tool with agents to query an index in a streaming scenario.

  ```csharp
  // Read the environment variables to be used in the next steps
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");
  var aiSearchIndexName = System.Environment.GetEnvironmentVariable("AI_SEARCH_INDEX_NAME");

  // Create an AIProjectClient object that will be used to create the agent and query the index.
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Resolve the project connection ID from the connection name.
  AIProjectConnection aiSearchConnection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);

  // Create an AzureAISearchToolIndex object that defines the index and the search parameters.
  AzureAISearchToolIndex index = new()
  {
      ProjectConnectionId = aiSearchConnection.Id,
      IndexName = aiSearchIndexName,
      TopK = 5,
      Filter = "category eq 'sleeping bag'",
      QueryType = AzureAISearchQueryType.Simple
  };

  // Create the agent definition with the Azure AI Search tool.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `\u3010message_idx:search_idx\u2020source\u3011`.",
      Tools = { new AzureAISearchTool(new AzureAISearchToolOptions(indexes: [index])) }
  };

  // Create the agent version with the agent definition.
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create an OpenAIResponse object with the ProjectResponsesClient object.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  string annotation = "";
  string text = "";

  // Stream the response from the agent and parse the output items for citations.
  foreach (StreamingResponseUpdate streamResponse in responseClient.CreateResponseStreaming("What is the temperature rating of the cozynights sleeping bag?"))
  {
      if (streamResponse is StreamingResponseCreatedUpdate createUpdate)
      {
          Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
      }
      else if (streamResponse is StreamingResponseOutputTextDeltaUpdate textDelta)
      {
          Console.WriteLine($"Delta: {textDelta.Delta}");
      }
      else if (streamResponse is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
      {
          text = textDoneUpdate.Text;
      }
      else if (streamResponse is StreamingResponseOutputItemDoneUpdate itemDoneUpdate)
      {
          if (annotation.Length == 0)
          {
              if (itemDoneUpdate.Item is MessageResponseItem messageItem)
              {
                  // Use an index containing "embedding", "token", "category", "title", and "url" fields.
                  // The last two fields are needed to get citation title and URL, retrieved by the agent.
                  foreach (ResponseContentPart content in messageItem.Content)
                  {
                      foreach (ResponseMessageAnnotation messageAnnotation in content.OutputTextAnnotations)
                      {
                          if (messageAnnotation is UriCitationMessageAnnotation uriAnnotation)
                          {
                              annotation = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                          }
                      }
                  }
              }
          }
      }
      else if (streamResponse is StreamingResponseErrorUpdate errorUpdate)
      {
          throw new InvalidOperationException($"The stream has failed: {errorUpdate.Message}");
      }
  }
  Console.WriteLine($"{text}{annotation}");

  // Finally, delete all the resources that were created in this sample.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected outcome

  The streaming response displays the agent's response creation, text deltas as they stream in real-time, and a formatted citation when complete. The final output includes the sleeping bag temperature rating with document reference. The agent version is deleted after the query completes.
</ZonePivot>

<ZonePivot pivot="rest">
  ## Use agents with Azure AI Search tool

  The following example shows how to use the Azure AI Search tool with the REST API to query an index. The example uses cURL, but you can use any HTTP client.

  Before running this sample, obtain a bearer token for authentication. Use the Azure CLI to get a token:

  ```bash
  az account get-access-token --resource https://cognitiveservices.azure.com
  ```

  Set `AGENT_TOKEN` to the token value and `API_VERSION` to the current API version (for example, `2025-01-01-preview`).

  ```bash
  curl --request POST \
    --url "$FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    --data '{
      "model": "$FOUNDRY_MODEL_DEPLOYMENT_NAME",
      "input": "Tell me about the mental health services available from Premera.",
      "tool_choice": "required",
      "tools": [
        {
          "type": "azure_ai_search",
          "azure_ai_search": {
            "indexes": [
              {
                "project_connection_id": "$AZURE_AI_SEARCH_CONNECTION_ID",
                "index_name": "$AI_SEARCH_INDEX_NAME",
                "query_type": "semantic",
                "top_k": 5
              }
            ]
          }
        }
      ]
    }'
  ```

  ### Expected outcome

  The API returns a JSON response containing the agent's answer about mental health services from the Premera index. The response includes citations and references to the indexed documents that generated the answer.
</ZonePivot>

<ZonePivot pivot="typescript">
  ### Quick verification

  Before running the full sample, verify your Azure AI Search connection exists:

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const aiSearchConnectionName = process.env["AZURE_AI_SEARCH_CONNECTION_NAME"] || "<ai search connection name>";

  async function verifyConnection(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    console.log("Connected to project.");

    try {
      const conn = await project.connections.get(aiSearchConnectionName);
      console.log(`Azure AI Search connection verified: ${conn.name}`);
      console.log(`Connection ID: ${conn.id}`);
    } catch (error) {
      console.log(`Azure AI Search connection '${aiSearchConnectionName}' not found: ${error}`);
      // List available connections
      console.log("Available connections:");
      for await (const conn of project.connections.list()) {
        console.log(`  - ${conn.name}`);
      }
    }
  }

  verifyConnection().catch(console.error);
  ```

  If this code runs without errors, your credentials and Azure AI Search connection are configured correctly.

  ### Full sample

  This sample demonstrates how to create an AI agent with Azure AI Search capabilities by using the `AzureAISearchAgentTool` and synchronous Azure AI Projects client. The agent can search indexed content and provide responses with citations from search results.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as readline from "readline";
  import "dotenv/config";

  // Load environment variables
  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["FOUNDRY_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const aiSearchConnectionName =
    process.env["AZURE_AI_SEARCH_CONNECTION_NAME"] || "<ai search connection name>";
  const aiSearchIndexName = process.env["AI_SEARCH_INDEX_NAME"] || "<ai search index name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Get connection ID from connection name
    const aiSearchConnection = await project.connections.get(aiSearchConnectionName);
    console.log(`Azure AI Search connection ID: ${aiSearchConnection.id}`);

    console.log("Creating agent with Azure AI Search tool...");

    // Define Azure AI Search tool that searches indexed content
    const agent = await project.agents.createVersion("MyAISearchAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a helpful assistant. You must always provide citations for answers using the tool and render them as: `[message_idx:search_idx†source]`.",
      tools: [
        {
          type: "azure_ai_search",
          azure_ai_search: {
            indexes: [
              {
                project_connection_id: aiSearchConnection.id,
                index_name: aiSearchIndexName,
                query_type: "simple",
              },
            ],
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Prompt user for input
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    });

    const userInput = await new Promise<string>((resolve) => {
      rl.question(
        "Enter your question for the AI Search agent available in the index (e.g., 'Tell me about the mental health services available from Premera'): \n",
        (answer) => {
          rl.close();
          resolve(answer);
        },
      );
    });

    console.log("\nSending request to AI Search agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: userInput,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        if (event.item.type === "message") {
          const item = event.item;
          if (item.content && item.content.length > 0) {
            const lastContent = item.content[item.content.length - 1];
            if (lastContent.type === "output_text" && lastContent.annotations) {
              for (const annotation of lastContent.annotations) {
                if (annotation.type === "url_citation") {
                  console.log(
                    `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                  );
                }
              }
            }
          }
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nAzure AI Search agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected outcome

  The application creates an agent with Azure AI Search capabilities, prompts for user input, queries the search index, and streams the response with real-time delta updates. Console output includes the agent ID, streaming text deltas, URL citations with indices, and confirmation of successful agent deletion. The agent provides answers grounded in the indexed content with proper citations.
</ZonePivot>

## Limitations

Keep these constraints in mind when using the Azure AI Search tool:

* **Virtual network access**: Azure AI Search doesn't support virtual network (vNET) configurations with agents at this time.
* The Azure AI Search tool can only target one index.
* Your Azure AI Search resource and your Microsoft Foundry Agent must be in the same tenant.

## Verify results

After you run a sample, validate that the agent is grounding responses from your index.

1. Ask a question that you know is answered in a specific indexed document.
2. Confirm the response includes citations formatted as `[message_idx:search_idx†source]`.
3. If you're streaming, confirm you see `url_citation` annotations in the response with valid URLs.
4. Verify the cited content matches your source documents in the search index.

If citations are missing or incorrect, see the [Troubleshooting](#troubleshooting) section.

## Setup

In this section, you create a connection between the Microsoft Foundry project that contains your agent and the Azure AI Search service that contains your index.

If you already connected your project to your search service, skip this section.

To create the connection, you need your search service endpoint and authentication method. The following steps guide you through gathering these details.

### Gather connection details

Before creating a project connection, gather your Azure AI Search service endpoint and authentication credentials.

The project connection requires the endpoint of your search service and either key-based authentication or keyless authentication with Microsoft Entra ID.

For keyless authentication, you must enable role-based access control (RBAC) and assign roles to your project's managed identity. Although this method involves extra steps, it enhances security by eliminating the need for hard-coded API keys.

Select the tab for your desired authentication method.

<Tabs>
  <Tab title="Key-based authentication">
    1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.

    2. To get the endpoint:

       1. From the left pane, select **Overview**.

       2. Make a note of the URL, which should look like `https://my-service.search.windows.net`.

          ![A screenshot of an AI Search resource Overview tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/connection-endpoint.png)

    3. To get the API key:

       1. From the left pane, select **Settings** > **Keys**.
       2. Select **Both** to enable both key-based and keyless authentication, which is recommended for most scenarios.

       ![A screenshot of an AI Search resource Keys tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/azure-portal.png)

       1. Make a note of one of the keys under **Manage admin keys**.
  </Tab>

  <Tab title="Keyless authentication">
    1. Sign in to the [Azure portal](https://portal.azure.com/) and select your search service.

    2. To get the endpoint:

       1. From the left pane, select **Overview**.

       2. Make a note of the URL, which should look like `https://my-service.search.windows.net`.

          ![A screenshot of an AI Search resource Overview tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/connection-endpoint.png)

    3. To enable RBAC:

       1. From the left pane, select **Settings** > **Keys**.
       2. Select **Both** to enable both key-based and keyless authentication, which is recommended for most scenarios.

       ![A screenshot of an AI Search resource Keys tab in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/ai-search/azure-portal.png)

    4. To assign the necessary roles:

       1. From the left pane, select **Access control (IAM)**.
       2. Select **Add** > **Add role assignment**.
       3. Assign the **Search Index Data Contributor** role to the managed identity of your project.
       4. Repeat the role assignment for **Search Service Contributor**.
  </Tab>
</Tabs>

### Create a project connection

Create the project connection by using the search service details you gathered.

Use one of the following options.

<Tabs>
  <Tab title="Azure CLI">
    **Create the following connection.yml file:**

    You can use a YAML configuration file for both key-based and keyless authentication. Replace the `name`, `endpoint`, and `api_key` (optional) placeholders with your search service details. For more information, see the [Azure AI Search connection YAML schema](../../../../machine-learning/reference-yaml-connection-ai-search).

    Here's a key-based example:

    ```yml
    name: my_project_acs_connection_keys
    type: azure_ai_search
    endpoint: https://contoso.search.windows.net/
    api_key: XXXXXXXXXXXXXXX
    ```

    <Callout type="important">
      Don't put real keys in source control. Store secrets in a secure store (for example, Azure Key Vault) and inject them at deployment time.
    </Callout>

    Here's a keyless example:

    ```yml
    name: my_project_acs_connection_keyless
    type: azure_ai_search
    endpoint: https://contoso.search.windows.net/
    ```

    **Then, run the following command:**

    Replace the placeholders with the resource group and project name.

    ```azurecli
    az ml connection create --file connection.yml --resource-group <resource-group> --workspace-name <project-name>
    ```
  </Tab>

  <Tab title="Python">
    Replace the `my_connection_name`, `my_endpoint`, and `my_key` (optional) placeholders with your search service details, and then run the following code:

    ```python
    from azure.identity import DefaultAzureCredential
    from azure.ai.ml import MLClient
    from azure.ai.ml.entities import AzureAISearchConnection

    # Create an Azure AI Search project connection
    my_connection_name = "my-connection-name"
    my_endpoint = "my-endpoint" # This could also be called target
    my_api_keys = None # Leave blank for Authentication type = AAD

    my_connection = AzureAISearchConnection(name=my_connection_name,
                                        endpoint=my_endpoint,
                                        api_key= my_api_keys)

    # Create MLClient
    ml_client = MLClient(
      credential=DefaultAzureCredential(),
      subscription_id="<subscription-id>",
      resource_group_name="<resource-group>",
      workspace_name="<project-name>",
    )

    # Create the connection
    ml_client.connections.create_or_update(my_connection)
    ```
  </Tab>
</Tabs>

### Confirm the connection ID

If you use the REST or TypeScript sample, you need the project connection ID.

**Python**

```python
import os
from dotenv import load_dotenv
from azure.identity import DefaultAzureCredential
from azure.ai.projects import AIProjectClient

load_dotenv()

project_client = AIProjectClient(
  endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
  credential=DefaultAzureCredential(),
)

connection = project_client.connections.get(os.environ["AZURE_AI_SEARCH_CONNECTION_NAME"])
print(connection.id)
```

**C#**

```csharp
var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
var aiSearchConnectionName = System.Environment.GetEnvironmentVariable("AZURE_AI_SEARCH_CONNECTION_NAME");

AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());
AIProjectConnection connection = projectClient.Connections.GetConnection(connectionName: aiSearchConnectionName);
Console.WriteLine(connection.Id);
```

## Troubleshooting

| Issue                                 | Cause                                      | Resolution                                                                                                                                                                                    |
| ------------------------------------- | ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Response has no citations             | Agent instructions don't request citations | Update your agent instructions to explicitly request citations in responses.                                                                                                                  |
| Response has no citations (streaming) | Annotations not captured                   | Confirm you receive `url_citation` annotations when streaming. Check your stream processing logic.                                                                                            |
| Tool can't access the index (401/403) | Missing RBAC roles (keyless auth)          | Assign the **Search Index Data Contributor** and **Search Service Contributor** roles to the Foundry project's managed identity. See [Azure RBAC in Foundry](../../../concepts/rbac-foundry). |
| Tool can't access the index (401/403) | Invalid or disabled API key                | Confirm the API key is correct and enabled in the Azure AI Search resource.                                                                                                                   |
| Tool returns "index not found"        | Index name mismatch                        | Confirm `AI_SEARCH_INDEX_NAME` matches the exact index name in your Azure AI Search resource (case-sensitive).                                                                                |
| Tool returns "index not found"        | Wrong connection endpoint                  | Confirm the project connection points to the Azure AI Search resource that contains the index.                                                                                                |
| Search returns no results             | Query doesn't match indexed content        | Verify the index contains the expected data. Use Azure AI Search's test query feature to validate.                                                                                            |
| Slow search performance               | Index not optimized                        | Review index configuration, consider adding semantic ranking, or optimize the index schema.                                                                                                   |

## Related content

* [Connect a Foundry IQ knowledge base to Foundry Agent Service](../foundry-iq-connect)
* [Tool best practices](../../concepts/tool-best-practice)
* [Create a vector search index in Azure AI Search](../../../../search/search-get-started-portal-import-vectors)
* [Quickstart: Build an agent with Foundry](../../../quickstarts/get-started-code)

Use the file search tool to enable Microsoft Foundry agents to search through your documents and retrieve relevant information. File search augments agents with knowledge from outside their model, such as proprietary product information or user-provided documents.

In this article, you learn how to:

* Upload files and create a vector store
* Configure an agent with file search enabled
* Query your documents through the agent

<Callout type="note">
  By using the standard agent setup, the improved file search tool ensures your files remain in your own storage. Your Azure AI Search resource ingests the files, so you maintain complete control over your data.
</Callout>

<Callout type="important">
  File search has [additional charges](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) beyond the token-based fees for model usage.
</Callout>

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* A [basic or standard agent environment](../../environment-setup)

* The latest prerelease SDK package:

  * **Python**: `pip install azure-ai-projects azure-identity python-dotenv --pre`
  * **C#**: `dotnet add package Azure.AI.Projects.OpenAI --prerelease`
  * **TypeScript**: `npm install @azure/ai-projects @azure/identity dotenv`

* **Storage Blob Data Contributor** role on your project's storage account (required for uploading files to your project's storage)

* **Azure AI Owner** role on your Foundry resource (required for creating agent resources)

* Environment variables configured: `FOUNDRY_PROJECT_ENDPOINT`, `MODEL_DEPLOYMENT_NAME`

## Create an agent with file search

<ZonePivot pivot="python">
  ## Create an agent with the file search tool

  The following code sample shows how to create an agent with the file search tool enabled. You need to upload files and create a vector store before running this code. See the sections below for details.

  ```python
  import os
  from pathlib import Path

  from dotenv import load_dotenv

  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import FileSearchTool, PromptAgentDefinition
  from azure.identity import DefaultAzureCredential

  load_dotenv()

  # Load the file to be indexed for search.
  asset_file_path = (Path(__file__).parent / "../assets/product_info.md").resolve()

  with (
    DefaultAzureCredential() as credential,
    AIProjectClient(
      endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"],
      credential=credential,
    ) as project_client,
    project_client.get_openai_client() as openai_client,
  ):
    print("Creating vector store...")
    vector_store = openai_client.vector_stores.create(name="ProductInfoStore")
    print(f"Vector store created (id: {vector_store.id})")

    print("Uploading file to vector store...")
    with asset_file_path.open("rb") as file_handle:
      vector_store_file = openai_client.vector_stores.files.upload_and_poll(
        vector_store_id=vector_store.id,
        file=file_handle,
      )
    print(f"File uploaded to vector store (id: {vector_store_file.id})")

    print("Creating agent with the file search tool...")
    agent = project_client.agents.create_version(
      agent_name="MyAgent",
      definition=PromptAgentDefinition(
        model=os.environ["MODEL_DEPLOYMENT_NAME"],
        instructions=(
          "You are a helpful agent that can search through product information. "
          "Use file search to answer questions from the uploaded files."
        ),
        tools=[FileSearchTool(vector_store_ids=[vector_store.id])],
      ),
      description="File search agent for product information queries.",
    )
    print(
      "Agent created "
      f"(id: {agent.id}, name: {agent.name}, version: {agent.version})"
    )

    print("Creating conversation...")
    conversation = openai_client.conversations.create()
    print(f"Created conversation (id: {conversation.id})")

    print("Creating response...")
    response = openai_client.responses.create(
      conversation=conversation.id,
      input="Tell me about Contoso products",
      extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
    )
    print(response.output_text)

    print("Cleaning up...")
    project_client.agents.delete_version(
      agent_name=agent.name,
      agent_version=agent.version,
    )
    openai_client.vector_stores.delete(vector_store.id)
  ```

  ### Expected output

  The following output comes from the preceding code sample:

  ```console
  Creating vector store...
  Vector store created (id: vs_abc123)
  Uploading file to vector store...
  File uploaded to vector store (id: file-xyz789)
  Creating agent with the file search tool...
  Agent created (id: agent_001, name: MyAgent, version: 1)
  Creating conversation...
  Created conversation (id: conv_456)
  Creating response...
  [Response text grounded in your uploaded document content]
  Cleaning up...
  ```

  ### References

  * Reference: [Azure SDK for Python sample: file search](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/ai/azure-ai-projects/samples/agents/tools/sample_agent_file_search_in_stream.py)
  * Reference: [Agents REST API (preview)](../../../reference/foundry-project-rest-preview)
</ZonePivot>

<ZonePivot pivot="csharp">
  ## File search sample with agent

  In this example, you create a local file, upload it to Azure, and use it in the newly created `VectorStore` for file search. The code in this example is synchronous and streaming. For asynchronous usage, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample8_FileSearch.md) in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Create project client and read the environment variables, which is used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME");
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Create a toy example file and upload it using OpenAI mechanism.
  string filePath = "sample_file_for_upload.txt";
  File.WriteAllText(
      path: filePath,
      contents: "The word 'apple' uses the code 442345, while the word 'banana' uses the code 673457.");
  OpenAIFileClient fileClient = projectClient.OpenAI.GetOpenAIFileClient();
  OpenAIFile uploadedFile = fileClient.UploadFile(filePath: filePath, purpose: FileUploadPurpose.Assistants);
  File.Delete(filePath);

  // Create the VectorStore and provide it with uploaded file ID.
  VectorStoreClient vctStoreClient = projectClient.OpenAI.GetVectorStoreClient();
  VectorStoreCreationOptions options = new()
  {
      Name = "MySampleStore",
      FileIds = { uploadedFile.Id }
  };
  VectorStore vectorStore = vctStoreClient.CreateVectorStore(options: options);

  // Create an Agent capable of using File search.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful agent that can help fetch data from files you know about.",
      Tools = { ResponseTool.CreateFileSearchTool(vectorStoreIds: new[] { vectorStore.Id }), }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Ask a question about the file's contents.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  ResponseResult response = responseClient.CreateResponse("Can you give me the documented codes for 'banana' and 'orange'?");

  // Create the response and throw an exception if the response contains the error.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine(response.GetOutputText());

  // Remove all the resources created in this sample.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  vctStoreClient.DeleteVectorStore(vectorStoreId: vectorStore.Id);
  fileClient.DeleteFile(uploadedFile.Id);
  ```

  ### Expected output

  The following output comes from the preceding code sample:

  ```console
  The code for 'banana' is 673457. I couldn't find any documented code for 'orange' in the files I have access to.
  ```

  ## File search sample with agent in streaming scenarios

  In this example, you create a local file, upload it to Azure, and use it in the newly created `VectorStore` for file search. The code in this example is synchronous and streaming. For asynchronous usage, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample11_FileSearch_Streaming.md) in the Azure SDK for .NET repository on GitHub.

  ```csharp
  class FileSearchStreamingDemo
  {
      // Create a helper method ParseResponse to format streaming response output.
      // If the stream ends up in error state, it will throw an error.
      private static void ParseResponse(StreamingResponseUpdate streamResponse)
      {
          if (streamResponse is StreamingResponseCreatedUpdate createUpdate)
          {
              Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
          }
          else if (streamResponse is StreamingResponseOutputTextDeltaUpdate textDelta)
          {
              Console.WriteLine($"Delta: {textDelta.Delta}");
          }
          else if (streamResponse is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
          {
              Console.WriteLine($"Response done with full message: {textDoneUpdate.Text}");
          }
          else if (streamResponse is StreamingResponseOutputItemDoneUpdate itemDoneUpdate)
          {
              if (itemDoneUpdate.Item is MessageResponseItem messageItem)
              {
                  foreach (ResponseContentPart part in messageItem.Content)
                  {
                      foreach (ResponseMessageAnnotation annotation in part.OutputTextAnnotations)
                      {
                          if (annotation is FileCitationMessageAnnotation fileAnnotation)
                          {
                              // Note fileAnnotation.Filename will be available in OpenAI package versions
                              // greater then 2.6.0.
                              Console.WriteLine($"File Citation - File ID: {fileAnnotation.FileId}");
                          }
                      }
                  }
              }
          }
          else if (streamResponse is StreamingResponseErrorUpdate errorUpdate)
          {
              throw new InvalidOperationException($"The stream has failed with the error: {errorUpdate.Message}");
          }
      }
      public static void Main()
      {
          // Create project client and read the environment variables, which will be used in the next steps.
          var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
          var modelDeploymentName = System.Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME");
          AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

          // Create a toy example file and upload it using OpenAI mechanism.
          string filePath = "sample_file_for_upload.txt";
          File.WriteAllText(
              path: filePath,
              contents: "The word 'apple' uses the code 442345, while the word 'banana' uses the code 673457.");
          OpenAIFile uploadedFile = projectClient.OpenAI.Files.UploadFile(filePath: filePath, purpose: FileUploadPurpose.Assistants);
          File.Delete(filePath);

          // Create the `VectorStore` and provide it with uploaded file ID.
          VectorStoreCreationOptions options = new()
          {
              Name = "MySampleStore",
              FileIds = { uploadedFile.Id }
          };
          VectorStore vectorStore = projectClient.OpenAI.VectorStores.CreateVectorStore(options);

          // Create an agent capable of using File search.
          PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
          {
              Instructions = "You are a helpful agent that can help fetch data from files you know about.",
              Tools = { ResponseTool.CreateFileSearchTool(vectorStoreIds: new[] { vectorStore.Id }), }
          };
          AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
              agentName: "myAgent",
              options: new(agentDefinition)
          );

          // Create the conversation to store responses.
          ProjectConversation conversation = projectClient.OpenAI.Conversations.CreateProjectConversation();
          CreateResponseOptions responseOptions = new()
          {
              Agent = agentVersion,
              AgentConversationId = conversation.Id,
              StreamingEnabled = true,
          };
          // Wait for the stream to complete.
          responseOptions.InputItems.Clear();
          responseOptions.InputItems.Add(ResponseItem.CreateUserMessageItem("Can you give me the documented codes for 'banana' and 'orange'?"));
          foreach (StreamingResponseUpdate streamResponse in projectClient.OpenAI.Responses.CreateResponseStreaming(responseOptions))
          {
              ParseResponse(streamResponse);
          }

          // Ask follow up question and start a new stream.
          Console.WriteLine("Demonstrating follow-up query with streaming...");
          responseOptions.InputItems.Clear();
          responseOptions.InputItems.Add(ResponseItem.CreateUserMessageItem("What was my previous question about?"));
          foreach (StreamingResponseUpdate streamResponse in projectClient.OpenAI.Responses.CreateResponseStreaming(responseOptions))
          {
              ParseResponse(streamResponse);
          }

          // Remove all the resources created in this sample.
          projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
          projectClient.OpenAI.VectorStores.DeleteVectorStore(vectorStoreId: vectorStore.Id);
          projectClient.OpenAI.Files.DeleteFile(uploadedFile.Id);
      }
  }
  ```

  ### Expected output

  The following output comes from the preceding code sample:

  ```console
  Stream response created with ID: <response-id>
  Delta: The code for 'banana' is 673457. I couldn't find any documented code for 'orange' in the files I have access to.
  Response done with full message: The code for 'banana' is 673457. I couldn't find any documented code for 'orange' in the files I have access to.
  File Citation - File ID: <file-id>
  Demonstrating follow-up query with streaming...
  Stream response created with ID: <response-id>
  Delta: Your previous question was about the documented codes for 'banana' and 'orange'.
  Response done with full message: Your previous question was about the documented codes for 'banana' and
  'orange'.
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Sample file search with agent

  The following TypeScript sample shows how to create an agent with the file search tool enabled. You need to upload files and create a vector store before running this code. See the [File search behavior by agent setup type](#file-search-behavior-by-agent-setup-type) section below for details. For a JavaScript example, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentFileSearch.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as fs from "fs";
  import * as path from "path";
  import { fileURLToPath } from "url";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

  export async function main(): Promise<void> {
    // Load the file to be indexed for search
    const __filename = fileURLToPath(import.meta.url);
    const __dirname = path.dirname(__filename);
    const assetFilePath = path.join(__dirname, "../assets/product_info.md");

    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Create vector store for file search
    console.log("Creating vector store...");
    const vectorStore = await openAIClient.vectorStores.create({
      name: "ProductInfoStore",
    });
    console.log(`Vector store created (id: ${vectorStore.id})`);

    // Upload file to vector store
    console.log("\nUploading file to vector store...");
    const fileStream = fs.createReadStream(assetFilePath);
    const file = await openAIClient.vectorStores.files.uploadAndPoll(vectorStore.id, fileStream);
    console.log(`File uploaded to vector store (id: ${file.id})`);

    // Create agent with file search tool
    console.log("\nCreating agent with file search tool...");
    const agent = await project.agents.createVersion("agent-file-search", {
      kind: "prompt",
      model: deploymentName,
      instructions: "You are a helpful assistant that can search through product information.",
      tools: [
        {
          type: "file_search",
          vector_store_ids: [vectorStore.id],
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Create a conversation for the agent interaction
    console.log("\nCreating conversation...");
    const conversation = await openAIClient.conversations.create();
    console.log(`Created conversation (id: ${conversation.id})`);

    // Send a query to search through the uploaded file
    console.log("\nGenerating response...");
    const response = await openAIClient.responses.create(
      {
        conversation: conversation.id,
        input: "Tell me about Contoso products",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
      },
    );
    console.log(`Response: ${response.output_text}`);

    // Clean up
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    await openAIClient.vectorStores.delete(vectorStore.id);
    console.log("Vector store deleted");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### References

  * Reference: [Azure SDK for JavaScript sample: file search](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentFileSearch.js)
  * Reference: [Agents REST API (preview)](../../../reference/foundry-project-rest-preview)
</ZonePivot>

<ZonePivot pivot="rest">
  ## Upload files and add them to a vector store

  To access your files, the file search tool uses the vector store object. Upload your files and create a vector store. Then poll the store's status until all files are out of the `in_progress` state to ensure that all content is fully processed. The SDK provides helpers for uploading and polling.

  ### Upload a file

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/openai/files?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -F purpose="assistants" \
    -F file="@c:\\path_to_file\\sample_file_for_upload.txt"
  ```

  ### Create a vector store

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/openai/vector_stores?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
      "name": "my_vector_store",
      "file_ids": ["{{filesUpload.id}}"]
    }'
  ```

  ## Create an agent version and enable file search

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/agents/$AGENTVERSION_NAME/versions?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -d '{
    "description": "Test agent version description",
    "definition": {
      "kind": "prompt",
      "model": "{{model}}",
      "tools": [
        {
          "type": "file_search",
          "vector_store_ids": ["{{vectorStore.id}}"],
          "max_num_results": 20
        }
      ],
      "instructions": "You are a customer support chatbot. Use file search results from the vector store to answer questions based on the uploaded files."
    }
  }'
  ```

  ## Create response with file search

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
    "agent": {
      "type": "agent_reference",
      "name": "{{agentVersion.name}}",
      "version": "{{agentVersion.version}}"
    },
    "metadata": {
      "test_response": "file_search_enabled",
      "vector_store_id": "{{vectorStore.id}}"
    },
    "input": [{
      "type": "message",
      "role": "user",
      "content": [
        {
          "type": "input_text",
          "text": "Can you search the uploaded file and tell me about Azure TV instructions?"
        }
      ]
    }],
    "stream": true
  }'
  ```

  The response returns streaming output containing the agent's answer based on information retrieved from the vector store. The agent searches through your uploaded file to answer the query about Azure TV instructions.

  ### Clean up

  Delete the agent version.

  ```bash
  curl --request DELETE \
    --url $FOUNDRY_PROJECT_ENDPOINT/agents/$AGENTVERSION_NAME/versions/$AGENTVERSION_VERSION?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{}'
  ```

  Delete the vector store.

  ```bash
  curl --request DELETE \
    --url $FOUNDRY_PROJECT_ENDPOINT/openai/vector_stores/$VECTORSTORE_ID?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{}'
  ```

  Delete the file.

  ```bash
  curl --request DELETE \
    --url $FOUNDRY_PROJECT_ENDPOINT/openai/files/$FILE_ID?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{}'
  ```

  ### References

  * Reference: [Agents REST API (preview)](../../../reference/foundry-project-rest-preview)
</ZonePivot>

## Verify file search results

After running a code sample in this article, verify that file search is working:

* Confirm that the vector store and file are created.
  * In the Python and TypeScript samples, the upload-and-poll helpers complete only after ingestion finishes.
* Ask a question that you can answer only from your uploaded content.
* Confirm that the response is grounded in your documents.

### File sources

* Upload local files (Basic and Standard agent setup)
* Azure Blob Storage (Standard setup only)

## File search behavior by agent setup type

### Basic agent setup

The file search tool has the same functionality as Azure OpenAI Responses API. The tool uses Microsoft managed search and storage resources.

* You store uploaded files in Microsoft managed storage.
* You create a vector store by using a Microsoft managed search resource.

### Standard agent setup

The file search tool uses the Azure AI Search and Azure Blob Storage resources you connect to during agent setup.

* You store uploaded files in your connected Azure Blob Storage account.
* You create vector stores by using your connected Azure AI Search resource.

For both agent setups, the service handles the entire ingestion process, which includes:

* Automatically parsing and chunking documents.
* Generating and storing embeddings.
* Utilizing both vector and keyword searches to retrieve relevant content for user queries.

The code is identical for both setups. The only variation is where your files and vector stores are stored.

## When to use file search

Choose file search when you need to:

* Search through documents you upload directly (PDFs, Word docs, code files)
* Enable agents to answer questions from proprietary or confidential content
* Process files up to 512 MB without managing external search infrastructure

Consider alternatives for these scenarios:

| Scenario                                      | Recommended tool                  |
| --------------------------------------------- | --------------------------------- |
| Search existing Azure AI Search indexes       | [Azure AI Search tool](ai-search) |
| Search the public web for current information | [Web search tool](web-search)     |
| Combine multiple data sources in one query    | Use multiple tools together       |

## How file search works

The file search tool uses retrieval best practices to extract relevant data from your files and improve model responses.

### Query processing

When you send a query, file search:

1. **Rewrites** your query to optimize it for search.
2. **Breaks down** complex queries into parallel searches.
3. **Runs hybrid search** combining keyword and semantic matching across vector stores.
4. **Reranks results** to select the most relevant content for the response.

### Default chunking settings

| Setting               | Default value                           |
| --------------------- | --------------------------------------- |
| Chunk size            | 800 tokens                              |
| Chunk overlap         | 400 tokens                              |
| Embedding model       | text-embedding-3-large (256 dimensions) |
| Max chunks in context | 20                                      |

## Vector stores

Vector store objects give the file search tool the ability to search your files. When you add a file to a vector store, the process automatically parses, chunks, embeds, and stores the file in a vector database that supports both keyword and semantic search. Each vector store can hold up to 10,000 files. You can attach vector stores to both agents and conversations. Currently, you can attach at most one vector store to an agent and at most one vector store to a conversation.

For background concepts and lifecycle guidance (readiness, deletion behavior, and expiration policies), see [Vector stores for file search](../../concepts/vector-stores).

Remove files from a vector store by:

* Deleting the vector store file object.
* Deleting the underlying file object. This action removes the file from all `vector_store` and `code_interpreter` configurations across all agents and conversations in your organization.

The maximum file size is 512 MB. Each file should contain no more than 5,000,000 tokens (computed automatically when you attach a file).

## Ensuring vector store readiness before creating runs

Ensure the system fully processes all files in a vector store before you create a run. This ensures all data in your vector store is searchable. Check for vector store readiness by using the polling helpers in the SDKs, or by manually polling the vector store object to ensure the status is **completed**.

As a fallback, the run object includes a 60-second maximum wait when the conversation's vector store contains files that are still processing. This wait ensures that any files your users upload in a conversation are fully searchable before the run proceeds. This fallback wait doesn't apply to the agent's vector store.

### Conversation vector stores have default expiration policies

Vector stores that you create by using conversation helpers (like `tool_resources.file_search.vector_stores` in conversations or `message.attachments` in Messages) have a default expiration policy of seven days after they were last active (defined as the last time the vector store was part of a run).

When a vector store expires, the runs on that conversation fail. To fix this problem, recreate a new vector store with the same files and reattach it to the conversation.

## Supported file types

<Callout type="note">
  For text MIME types, the encoding must be UTF-8, UTF-16, or ASCII.
</Callout>

| File format | MIME Type                                                                   |
| ----------- | --------------------------------------------------------------------------- |
| `.c`        | `text/x-c`                                                                  |
| `.cs`       | `text/x-csharp`                                                             |
| `.cpp`      | `text/x-c++`                                                                |
| `.doc`      | `application/msword`                                                        |
| `.docx`     | `application/vnd.openxmlformats-officedocument.wordprocessingml.document`   |
| `.html`     | `text/html`                                                                 |
| `.java`     | `text/x-java`                                                               |
| `.json`     | `application/json`                                                          |
| `.md`       | `text/markdown`                                                             |
| `.pdf`      | `application/pdf`                                                           |
| `.php`      | `text/x-php`                                                                |
| `.pptx`     | `application/vnd.openxmlformats-officedocument.presentationml.presentation` |
| `.py`       | `text/x-python`                                                             |
| `.py`       | `text/x-script.python`                                                      |
| `.rb`       | `text/x-ruby`                                                               |
| `.tex`      | `text/x-tex`                                                                |
| `.txt`      | `text/plain`                                                                |
| `.css`      | `text/css`                                                                  |
| `.js`       | `text/javascript`                                                           |
| `.sh`       | `application/x-sh`                                                          |
| `.ts`       | `application/typescript`                                                    |

## Limitations

Keep these limits in mind when you plan your file search integration:

* File search supports specific file formats and encodings. See [Supported file types](#supported-file-types).
* Each vector store can hold up to 10,000 files.
* You can attach at most one vector store to an agent and at most one vector store to a conversation.
* Features and availability vary by region. See [Azure AI Foundry region support](../../../reference/region-support).

## Troubleshooting

| Issue                             | Likely cause                                                                       | Resolution                                                                                                         |
| --------------------------------- | ---------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ |
| 401 Unauthorized                  | The access token is missing, expired, or scoped incorrectly.                       | Get a fresh token and retry the request. For REST calls, confirm you set `AGENT_TOKEN` correctly.                  |
| 403 Forbidden                     | The signed-in identity doesn't have the required roles.                            | Confirm the roles in [Prerequisites](#prerequisites) and retry after role assignment finishes propagating.         |
| 404 Not Found                     | The project endpoint or resource identifiers are incorrect.                        | Confirm `FOUNDRY_PROJECT_ENDPOINT` and IDs such as agent name, version, vector store ID, and file ID.              |
| Responses ignore your files       | The agent isn't configured with `file_search`, or the vector store isn't attached. | Confirm the agent definition includes `file_search` and the `vector_store_ids` list contains your vector store ID. |
| File upload times out             | Large file or slow network connection.                                             | Use `upload_and_poll` to handle large files. Consider chunking very large documents.                               |
| Vector store creation fails       | Quota exceeded or invalid file format.                                             | Check vector store limits (10,000 files per store). Verify file format is supported.                               |
| Search returns irrelevant results | File content not properly indexed or query too broad.                              | Wait for indexing to complete (check `vector_store.status`). Use more specific queries.                            |
| No citations in response          | Model didn't use file search or content not found.                                 | Use `tool_choice="required"` to force file search. Verify the file content matches your query topic.               |

## Related content

* [Azure AI Search tool](ai-search) - Search existing Azure AI Search indexes from your agents
* [Web search tool](web-search) - Enable agents to search the public web
* [Vector stores for file search](../../concepts/vector-stores) - Understand vector store lifecycle and expiration

Vector store objects give the [file search](../how-to/tools/file-search) tool the ability to search your files. When you add a file to a vector store, the service parses, chunks, embeds, and indexes it so the tool can run both keyword and semantic search.

Vector stores can be attached to both agents and conversations. Currently, you can attach at most one vector store to an agent and at most one vector store to a conversation. For a conceptual overview of conversations, see [Agent runtime components](runtime-components).

In the current agents developer experience, response generation uses **responses** and **conversations**. Some SDKs and older samples use the term *run*. If you see both terms, treat *run* as response generation. For migration guidance, see [How to migrate to the new agent service](../how-to/migrate).

For a list of limits for vector search (such as maximum allowable file sizes), see the [quotas and limits](../quotas-limits) article.

## Prerequisites

* A [Microsoft Foundry project](../../how-to/create-projects).
* An agent or conversation that uses the [file search](../how-to/tools/file-search) tool.
* If you use standard agent setup, connect Azure Blob Storage and Azure AI Search during setup so your files remain in your storage. See [Agent environment setup](../environment-setup).
* Roles and permissions vary by task (for example, creating projects, assigning roles for standard setup, or creating and editing agents). See the required permissions table in [Agent environment setup](../environment-setup).
* Feature availability can vary by region. For current coverage, see [Microsoft Foundry feature availability across cloud regions](../../reference/region-support).

## Key limits and defaults

Vector stores are often the first place retrieval workflows fail in production, so it helps to know the defaults and hard limits.

* **Files per vector store**: Each vector store can hold up to 10,000 files.

* **Attachments**: You can attach at most one vector store to an agent and at most one vector store to a conversation.

* **Default retrieval settings** (file search):

  * Chunk size: 800 tokens
  * Chunk overlap: 400 tokens
  * Embedding model: text-embedding-3-large at 256 dimensions
  * Maximum number of chunks added to context: 20

For file size and token limits, see [quotas and limits](../quotas-limits).

## Key concepts

| Term              | Meaning                                                                                       |
| ----------------- | --------------------------------------------------------------------------------------------- |
| Vector store      | A container for searchable file content (chunks and embeddings) used by the file search tool. |
| Ingestion         | The asynchronous process that parses, chunks, embeds, and indexes a file for search.          |
| Readiness         | Whether ingestion has completed and the vector store is searchable.                           |
| Expiration policy | A lifecycle policy that expires a vector store after a period of inactivity.                  |

## How vector stores work with file search

File search applies retrieval best practices to help your agent find the right content from your files. Depending on the query and your data, the tool can:

* Rewrite user queries to improve retrieval.
* Break down complex queries into multiple searches.
* Run both keyword and semantic searches across agent and conversation vector stores.
* Rerank results before adding them to the model context.

For current default retrieval settings (chunk size and overlap, embedding model, and the maximum number of chunks added to context), see [How it works](../how-to/tools/file-search#how-file-search-works).

## Where your data lives (basic vs standard agent setup)

Where files and search resources live depends on your agent setup:

* **Basic agent setup**: File search uses Microsoft-managed storage and search resources.
* **Standard agent setup**: File search uses the Azure Blob Storage and Azure AI Search resources you connect during setup, so your files remain in your storage.

To set up your environment, see [Agent environment setup](../environment-setup). For more detail, see [Dependency on agent setup](../how-to/tools/file-search#file-search-behavior-by-agent-setup-type).

## Ensure vector store readiness before creating responses

Ensure all files in a vector store are fully processed before you create a response. This step ensures that all the data in your vector store is searchable.

To check readiness, use the SDK polling helpers (for example, *create-and-poll* and *upload-and-poll*) or poll the vector store object until its status is **completed**. For code examples, see [File search tool for agents](../how-to/tools/file-search).

During ingestion, a vector store can be in **in\_progress** status. When ingestion completes, the status changes to **completed**.

As a fallback, response generation includes a 60-second maximum wait when the conversation's vector store contains files that are still being processed. This fallback wait doesn't apply to the agent's vector store.

## End-to-end workflow checklist

Use this checklist to validate a working vector-store workflow from ingestion to lifecycle management.

1. Decide whether you use basic agent setup or standard agent setup, based on where you want your files and search resources to live. See [Where your data lives (basic vs standard agent setup)](#where-your-data-lives-basic-vs-standard-agent-setup).
2. Upload your files and create a vector store. For a step-by-step example, see [Upload files and add them to a vector store](../how-to/tools/file-search#upload-files-and-add-them-to-a-vector-store).
3. Wait for ingestion to finish before you generate responses. Use SDK polling helpers or poll the vector store until its status is **completed** and no files remain in **in\_progress**. See [Ensuring vector store readiness before creating responses](../how-to/tools/file-search#ensuring-vector-store-readiness-before-creating-runs).
4. Attach the vector store to the agent or conversation that you use for file search. Keep the attachment limits in mind. See [Vector stores](../how-to/tools/file-search#vector-stores).
5. Create a response that uses file search and verify that the tool is retrieving from the expected sources. See [Create response with file search](../how-to/tools/file-search#create-response-with-file-search) and [Verify results](../how-to/tools/file-search#verify-file-search-results).
6. Manage lifecycle: remove files you no longer need, and plan for expiration policies (especially for vector stores created by conversation helpers). See [Vector stores](../how-to/tools/file-search#vector-stores) and [Conversation vector stores have default expiration policies](../how-to/tools/file-search#conversation-vector-stores-have-default-expiration-policies).

## Add files and manage vector stores

Adding files to vector stores is an asynchronous operation. To ensure ingestion completes, use the create-and-poll helpers in the official SDKs. If you aren't using an SDK, poll the vector store until its status is **completed** and no files remain in **in\_progress**.

Files can also be added to a vector store after it's created by creating vector store files. Alternatively, you can add several files to a vector store by creating batches of up to 500 files.

When you upload a file to create a vector store, the system automatically:

1. **Chunks your content** into manageable pieces.
2. **Converts each chunk** into high-dimensional vectors using embedding models.
3. **Stores these vectors** in an optimized search index.
4. **Creates associations** between the vectors and your original content.

## Remove files from vector stores

You can remove files from a vector store in two different ways:

* Delete the vector store file object.
* Delete the underlying file object. This removes the file from all vector store configurations across all agents and conversations in your organization.

### Manage lifecycle with expiration policies

Expiration policies help you manage vector store lifecycle. You can set these policies when creating or updating the vector store object.

### Conversation vector stores have default expiration policies

Vector stores created using conversation helpers have a default expiration policy of seven days after they were last active (defined as the last time the vector store was used during response generation).

When a vector store expires, response generation for that conversation fails. To fix the issue, recreate a new vector store with the same files and reattach it to the conversation. For more detail, see [Conversation vector stores have default expiration policies](../how-to/tools/file-search#conversation-vector-stores-have-default-expiration-policies).

## Supported file types and key limits

For the supported file types list and encoding requirements, see [Supported file types](../how-to/tools/file-search#supported-file-types).

Key limits to keep in mind:

* You can attach at most one vector store to an agent and at most one vector store to a conversation.
* File size and token limits vary by feature. See [Quotas and limits](../quotas-limits).

## Troubleshooting

* **Your vector store isn't searchable yet**: Wait for ingestion to finish. Use SDK polling helpers or poll the vector store until its status is **completed**.
* **Response generation fails after a few days**: Your conversation vector store might have expired. Recreate a new vector store with the same files and reattach it.
* **A file disappeared from multiple agents or conversations**: You might have deleted the underlying file object, which removes the file from all vector store configurations across your organization.
* **Uploads or ingestion fail**: Check file size and token limits in [Quotas and limits](../quotas-limits).

## Next steps

* Learn more about the [file search tool](../how-to/tools/file-search)
* Review [tool best practices](tool-best-practice) for guidance on reliability and security
* Learn about [agent runtime components](runtime-components)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="note">
  * This article describes the Microsoft SharePoint tool for Foundry Agent Service. For information on using and deploying SharePoint sites, see the [SharePoint documentation](https://learn.microsoft.com/en-us/sharepoint/).
  * See [best practices](../../concepts/tool-best-practice) for information on optimizing tool usage.
</Callout>

Use the SharePoint tool (preview) for SharePoint grounding in Microsoft Foundry Agent Service by retrieving content from a SharePoint site or folder (for example, `contoso.sharepoint.com/sites/policies`). When a user asks a question, the agent can invoke the SharePoint tool to retrieve relevant text from documents the user can access. The agent then generates a response based on that retrieved content.

<Callout type="note">
  SharePoint tool access requires either a Microsoft 365 Copilot license or an enabled pay-as-you-go model. See [Prerequisites](#prerequisites) for details.
</Callout>

This integration uses identity passthrough (On-Behalf-Of) so SharePoint permissions continue to apply to every request. For details on the underlying Microsoft 365 Copilot Retrieval API integration, see [How it works](#how-it-works).

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* Eligible license or pay-as-you-go model:

  * Developers and end users have a Microsoft 365 Copilot license, as required by the [Microsoft 365 Copilot Retrieval API](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api-reference/retrieval-api-overview).
  * If developers and end users don't have a Microsoft 365 Copilot license, you can enable the [pay-as-you-go model](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api/ai-services/retrieval/paygo-retrieval).

* Developers and end users have at least `Azure AI User` RBAC role assigned on the Foundry project. For more information about Azure role-based access control, see [Azure role-based access control in Foundry](../../../concepts/rbac-foundry?view=foundry\&preserve-view=true).

* Developers and end users have at least `READ` access to the SharePoint site.

* The latest prerelease package installed:

  * **Python**: `pip install azure-ai-projects --pre`
  * **C#**: Install the `Azure.AI.Projects` NuGet package (prerelease)
  * **TypeScript/JavaScript**: `npm install @azure/ai-projects`

* Environment variables configured:

  * `AZURE_AI_PROJECT_ENDPOINT`: Your Foundry project endpoint URL
  * `AZURE_AI_MODEL_DEPLOYMENT_NAME`: Your model deployment name (for example, `gpt-4`)
  * `SHAREPOINT_PROJECT_CONNECTION_ID`: Your SharePoint connection ID in the format `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`
  * For REST samples: `API_VERSION`, `AGENT_TOKEN`

* See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code) for additional authentication setup details.

## Parameters

The SharePoint tool uses your project connection to determine which SharePoint site or folder it can retrieve from.

| Parameter                                                                  | Required | Notes                                                |
| -------------------------------------------------------------------------- | -------- | ---------------------------------------------------- |
| `type`                                                                     | Yes      | Use `sharepoint_grounding_preview`.                  |
| `sharepoint_grounding_preview.project_connections[].project_connection_id` | Yes      | Use the value of `SHAREPOINT_PROJECT_CONNECTION_ID`. |

If you need to create a SharePoint connection for your project, see [Add a new connection to your project](../../../how-to/connections-add?view=foundry\&preserve-view=true).

## Code example

<ZonePivot pivot="python">
  ### Quick verification

  Before running the full sample, verify your SharePoint connection exists:

  ```python
  import os

  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential
  from dotenv import load_dotenv

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
  ):
      print("Connected to project.")

      # Verify SharePoint connection exists
      connection_name = os.environ.get("SHAREPOINT_PROJECT_CONNECTION_NAME")
      if connection_name:
          try:
              conn = project_client.connections.get(connection_name)
              print(f"SharePoint connection verified: {conn.name}")
              print(f"Connection ID: {conn.id}")
          except Exception as e:
              print(f"SharePoint connection '{connection_name}' not found: {e}")
      else:
          # List available connections to help find the right one
          print("SHAREPOINT_PROJECT_CONNECTION_NAME not set. Available connections:")
          for conn in project_client.connections.list():
              print(f"  - {conn.name}")
  ```

  If this code runs without errors, your credentials and SharePoint connection are configured correctly.

  ### Full sample

  The following sample demonstrates how to create an Agent that uses the SharePoint tool to ground responses with content from a SharePoint site.

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      SharepointAgentTool,
      SharepointGroundingToolParameters,
      ToolProjectConnection,
  )

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
      project_client.get_openai_client() as openai_client,
  ):
      # Get connection ID from connection name
      sharepoint_connection = project_client.connections.get(
          os.environ["SHAREPOINT_PROJECT_CONNECTION_NAME"],
      )
      print(f"SharePoint connection ID: {sharepoint_connection.id}")

      sharepoint_tool = SharepointAgentTool(
          sharepoint_grounding_preview=SharepointGroundingToolParameters(
              project_connections=[
                  ToolProjectConnection(project_connection_id=sharepoint_connection.id)
              ]
          )
      )

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are a helpful agent that can use SharePoint tools to assist users.
              Use the available SharePoint tools to answer questions and perform tasks.""",
              tools=[sharepoint_tool],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      # Send initial request that will trigger the SharePoint tool
      stream_response = openai_client.responses.create(
          stream=True,
        tool_choice="required",
          input="Please summarize the last meeting notes stored in SharePoint.",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              if event.item.type == "message":
                  item = event.item
                  if item.content[-1].type == "output_text":
                      text_content = item.content[-1]
                      for annotation in text_content.annotations:
                          if annotation.type == "url_citation":
                              print(
                                  f"URL Citation: {annotation.url}, "
                                  f"Start index: {annotation.start_index}, "
                                  f"End index: {annotation.end_index}"
                              )
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")
  ```

  ### What this code does

  This example creates an agent with SharePoint grounding capabilities and processes a streaming response:

  1. **Initialize the project client** by using your Foundry project endpoint and Azure credentials.
  2. **Configure the SharePoint tool** with your project connection to enable access to SharePoint content.
  3. **Create the agent** with instructions and the SharePoint tool attached.
  4. **Send a query** asking the agent to summarize meeting notes from SharePoint.
  5. **Process the streaming response** to display the agent's answer in real-time.
  6. **Extract URL citations** from the response annotations showing which SharePoint documents were referenced.

  ### Expected output

  When you run this code, you see output similar to:

  ```text
  Agent created (id: asst_abc123, name: MyAgent, version: 1)

  Sending request to SharePoint agent with streaming...
  Follow-up response created with ID: resp_xyz789
  Delta: Based
  Delta:  on
  Delta:  the
  Delta:  meeting
  Delta:  notes
  ...
  URL Citation: https://contoso.sharepoint.com/sites/policies/Documents/meeting-notes.docx, Start index: 0, End index: 245

  Follow-up response done!

  Follow-up completed!
  Full response: Based on the meeting notes from your SharePoint site, the last meeting covered the following topics: project timeline updates, budget review, and next quarter planning.
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ### Quick verification

  Before running the full sample, verify your SharePoint connection exists:

  ```csharp
  using Azure.AI.Projects;
  using Azure.Identity;

  var projectEndpoint = System.Environment.GetEnvironmentVariable("AZURE_AI_PROJECT_ENDPOINT");
  var sharepointConnectionName = System.Environment.GetEnvironmentVariable("SHAREPOINT_PROJECT_CONNECTION_NAME");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Verify SharePoint connection exists
  try
  {
      AIProjectConnection conn = projectClient.Connections.GetConnection(connectionName: sharepointConnectionName);
      Console.WriteLine($"SharePoint connection verified: {conn.Name}");
      Console.WriteLine($"Connection ID: {conn.Id}");
  }
  catch (Exception ex)
  {
      Console.WriteLine($"SharePoint connection '{sharepointConnectionName}' not found: {ex.Message}");
      // List available connections
      Console.WriteLine("Available connections:");
      foreach (var conn in projectClient.Connections.GetConnections())
      {
          Console.WriteLine($"  - {conn.Name}");
      }
  }
  ```

  If this code runs without errors, your credentials and SharePoint connection are configured correctly.

  ### Full sample

  The following sample demonstrates how to create an Agent that uses the SharePoint tool to ground responses with content from a SharePoint site. This example uses synchronous methods for simplicity. For an asynchronous version, refer to the [SharePoint agent sample documentation](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample24_Sharepoint.md) on the Azure SDK for .NET GitHub repository.

  To enable your Agent to access SharePoint, use `SharepointAgentTool`.

  ```csharp
  using System;
  using Azure.AI.Projects;
  using Azure.AI.Projects.OpenAI;
  using Azure.Identity;

  // Create an agent client and read the environment variables, which will be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("AZURE_AI_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("AZURE_AI_MODEL_DEPLOYMENT_NAME");
  var sharepointConnectionName = System.Environment.GetEnvironmentVariable("SHAREPOINT_PROJECT_CONNECTION_NAME");
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Get connection ID from connection name
  AIProjectConnection sharepointConnection = projectClient.Connections.GetConnection(connectionName: sharepointConnectionName);

  // Use the SharePoint connection ID to initialize the SharePointGroundingToolOptions,
  // which will be used to create SharepointAgentTool. Use this tool to create an Agent.
  SharePointGroundingToolOptions sharepointToolOption = new()
  {
      ProjectConnections = { new ToolProjectConnection(projectConnectionId: sharepointConnection.Id) }
  };
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant.",
      Tools = { new SharepointPreviewTool(sharepointToolOption), }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create the response and make sure we are always using tool.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  CreateResponseOptions responseOptions = new()
  {
      ToolChoice = ResponseToolChoice.CreateRequiredChoice(),
      InputItems = { ResponseItem.CreateUserMessageItem("What is Contoso whistleblower policy") }
  };
  ResponseResult response = responseClient.CreateResponse(options: responseOptions);

  // SharePoint tool can create the reference to the page, grounding the data.
  // Create the GetFormattedAnnotation method to get the URI annotation.
  string annotation = "";
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is MessageResponseItem messageItem)
      {
          foreach (ResponseContentPart content in messageItem.Content)
          {
              foreach (ResponseMessageAnnotation messageAnnotation in content.OutputTextAnnotations)
              {
                  if (messageAnnotation is UriCitationMessageAnnotation uriAnnotation)
                  {
                      annotation = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                  }
              }
          }
      }
  }

  // Print the Agent output and add the annotation at the end.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine($"{response.GetOutputText()}{annotation}");

  // After the sample is completed, delete the Agent we have created.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### What this code does

  This example demonstrates SharePoint grounding with synchronous methods:

  1. **Create a project client** using your Foundry project endpoint
  2. **Retrieve the SharePoint connection** by name from your project's connections
  3. **Configure the SharePoint tool** with the connection ID
  4. **Create an agent** with the SharePoint tool to enable document access
  5. **Create a response** asking about the Contoso whistleblower policy
  6. **Format and display the response** with a helper method that extracts URL citations from annotations
  7. **Clean up** by deleting the agent version

  ### Expected output

  When you run this code, you see output similar to:

  ```text
  The Contoso whistleblower policy outlines procedures for reporting unethical behavior confidentially. Employees can submit concerns through the ethics hotline or online portal. [Whistleblower Policy](https://contoso.sharepoint.com/sites/policies/Documents/whistleblower-policy.pdf)
  ```

  The output includes the agent's response grounded in SharePoint content, with a citation link to the source document.
</ZonePivot>

<ZonePivot pivot="rest">
  ## Sample for use of an Agent with SharePoint

  The following sample demonstrates how to create an Agent that uses the SharePoint tool to ground responses with content from a SharePoint site.

  ```bash
  curl --request POST \
    --url "$AZURE_AI_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
    "model": "'$AZURE_AI_MODEL_DEPLOYMENT_NAME'",
    "input": "Please summarize the last meeting notes stored in SharePoint.",
    "tool_choice": "required",
    "tools": [
      {
        "type": "sharepoint_grounding_preview",
        "sharepoint_grounding_preview": {
          "project_connections": [
            {
              "project_connection_id": "'$SHAREPOINT_PROJECT_CONNECTION_ID'"
            }
          ]
        }
      }
    ]
  }'
  ```

  ### What this code does

  This REST API call creates a response with SharePoint grounding:

  1. **Sends a POST request** to the `/openai/responses` endpoint
  2. **Authenticates** using a bearer token for your agent
  3. **Specifies the model** deployment to use for generating responses
  4. **Includes the user's query** asking for meeting notes from SharePoint
  5. **Configures the SharePoint tool** with your project connection ID to enable document retrieval
  6. **Returns a JSON response** with the agent's answer and citations to source documents

  ### Expected output

  The API returns a JSON response with the agent's answer and citation information:

  ```json
  {
    "id": "resp_abc123xyz",
    "object": "response",
    "created_at": 1702345678,
    "status": "completed",
    "output_text": "Based on the meeting notes from your SharePoint site, the last meeting covered project timeline updates, budget review, and next quarter planning.",
    "output_items": [
      {
        "type": "message",
        "content": [
          {
            "type": "output_text",
            "text": "Based on the meeting notes...",
            "annotations": [
              {
                "type": "url_citation",
                "url": "https://contoso.sharepoint.com/sites/policies/Documents/meeting-notes.docx",
                "start_index": 0,
                "end_index": 245
              }
            ]
          }
        ]
      }
    ]
  }
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ### Quick verification

  Before running the full sample, verify your SharePoint connection exists:

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const sharepointConnectionName = process.env["SHAREPOINT_PROJECT_CONNECTION_NAME"] || "<sharepoint connection name>";

  async function verifyConnection(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    console.log("Connected to project.");

    try {
      const conn = await project.connections.get(sharepointConnectionName);
      console.log(`SharePoint connection verified: ${conn.name}`);
      console.log(`Connection ID: ${conn.id}`);
    } catch (error) {
      console.log(`SharePoint connection '${sharepointConnectionName}' not found: ${error}`);
      // List available connections
      console.log("Available connections:");
      for await (const conn of project.connections.list()) {
        console.log(`  - ${conn.name}`);
      }
    }
  }

  verifyConnection().catch(console.error);
  ```

  If this code runs without errors, your credentials and SharePoint connection are configured correctly.

  ### Full sample

  This sample demonstrates how to create an AI agent with SharePoint capabilities using the `SharepointAgentTool` and synchronous Azure AI Projects client. The agent can search SharePoint content and provide responses with relevant information from SharePoint sites. For a JavaScript version of this sample, refer to the [SharePoint agent sample documentation](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentSharepoint.js) in the Azure SDK for JavaScript GitHub repository.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["AZURE_AI_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const sharepointConnectionName =
    process.env["SHAREPOINT_PROJECT_CONNECTION_NAME"] || "<sharepoint connection name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Get connection ID from connection name
    const sharepointConnection = await project.connections.get(sharepointConnectionName);
    console.log(`SharePoint connection ID: ${sharepointConnection.id}`);

    console.log("Creating agent with SharePoint tool...");

    const agent = await project.agents.createVersion("MyAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a helpful agent that can use SharePoint tools to assist users. Use the available SharePoint tools to answer questions and perform tasks.",
      // Define SharePoint tool that searches SharePoint content
      tools: [
        {
          type: "sharepoint_grounding_preview",
          sharepoint_grounding_preview: {
            project_connections: [
              {
                project_connection_id: sharepointConnection.id,
              },
            ],
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Send initial request that will trigger the SharePoint tool
    console.log("\nSending request to SharePoint agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: "Please summarize the last meeting notes stored in SharePoint.",
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        if (event.item.type === "message") {
          const item = event.item;
          if (item.content && item.content.length > 0) {
            const lastContent = item.content[item.content.length - 1];
            if (lastContent.type === "output_text" && lastContent.annotations) {
              for (const annotation of lastContent.annotations) {
                if (annotation.type === "url_citation") {
                  console.log(
                    `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                  );
                }
              }
            }
          }
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nSharePoint agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### What this code does

  This TypeScript example demonstrates the complete agent lifecycle with SharePoint:

  1. **Initialize the project client** with your Foundry endpoint and Azure credentials.
  2. **Get the OpenAI client** for creating responses.
  3. **Create an agent** with SharePoint tool configuration using your project connection ID.
  4. **Send a streaming request** asking the agent to summarize meeting notes.
  5. **Process streaming events** to display the response as it's generated.
  6. **Extract and display URL citations** from response annotations.
  7. **Clean up resources** by deleting the agent version after completion.

  ### Expected output

  When you run this code, you see output similar to:

  ```text
  Creating agent with SharePoint tool...
  Agent created (id: asst_abc123, name: MyAgent, version: 1)

  Sending request to SharePoint agent with streaming...
  Follow-up response created with ID: resp_xyz789
  Based on the meeting notes from your SharePoint site, the last meeting covered the following topics: project timeline updates, budget review, and next quarter planning.

  Follow-up response done!
  URL Citation: https://contoso.sharepoint.com/sites/policies/Documents/meeting-notes.docx, Start index: 0, End index: 245

  Follow-up completed!

  Cleaning up resources...
  Agent deleted

  SharePoint agent sample completed!
  ```
</ZonePivot>

## Limitations

* The SharePoint tool only supports user identity authentication. App-only (service principal) authentication isn't supported.
* Your SharePoint site and your Microsoft Foundry agent must be in the same tenant.
* You can add only one SharePoint tool per agent.
* The underlying Microsoft 365 Copilot Retrieval API returns text extracts. Retrieval from nontextual content, including images and charts, isn't supported.
* For semantic and hybrid retrieval, the Microsoft 365 Copilot Retrieval API supports `.doc`, `.docx`, `.pptx`, `.pdf`, `.aspx`, and `.one` file types. For details, see the [Microsoft 365 Copilot API](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api-reference/retrieval-api-overview).

## Setup

<Callout type="note">
  Start with SharePoint sites that have a simple folder structure and a small number of short documents.
</Callout>

1. Select **SharePoint** and follow the prompts to add the tool. You can only add one per agent.

2. Add a SharePoint connection.

   For step-by-step instructions, see [Add a new connection to your project](../../../how-to/connections-add?view=foundry\&preserve-view=true).

3. In the SharePoint connection configuration, enter the site URL or folder URL.

   * Site URL example: `https://<company>.sharepoint.com/sites/<site_name>`
   * Folder URL example: `https://<company>.sharepoint.com/sites/<site_name>/Shared%20documents/<folder_name>`

   <Callout type="note">
     Your `site_url` needs to follow the format above. If you copy the entire value from the address bar of your SharePoint, it doesn't work.
   </Callout>

4. Save the connection, and then copy its connection **ID**.

5. Set the connection ID as `SHAREPOINT_PROJECT_CONNECTION_ID`.

## How it works

The SharePoint tool makes it possible by enabling seamless integrations between AI agents and business documents stored in SharePoint. This capability is empowered by the [Microsoft 365 Copilot API](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api-reference/retrieval-api-overview). To ground your SharePoint documents, enter the sites or folders to connect with. The SharePoint tool leverages [built-in indexing capabilities](https://learn.microsoft.com/en-us/microsoftsearch/semantic-index-for-copilot) to enhance the search and retrieval experience, including intelligent indexing, query processing, and content chunking.

For more information about delegated access and identity passthrough in Foundry, see [Agent identity concepts in Microsoft Foundry](../../concepts/agent-identity).

Instead of requiring developers to export SharePoint content, build a custom semantic index, manage governance controls, and configure refresh logic, this capability automates the entire retrieval pipeline. It dynamically indexes documents, breaks content into meaningful chunks, and applies advanced query processing to surface the most relevant information. By using the same enterprise-grade retrieval stack that powers Microsoft 365 Copilot, this capability ensures AI agent responses are grounded in the most up-to-date and contextually relevant content.

Customers rely on data security in SharePoint to access, create, and share documents with flexible document-level access control. Enterprise features such as identity passthrough (On-Behalf-Of) authentication ensure proper access control. End users receive responses generated from SharePoint documents they have permission to access. By using OBO authentication, Foundry Agent Service uses the end user's identity to authorize and retrieve relevant SharePoint documents, generating responses tailored to specific end users.

## Troubleshooting

| Issue                                                                     | Cause                                                        | Resolution                                                                                                                                                                                                                |
| ------------------------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `AuthenticationError: AppOnly OBO tokens not supported by target service` | Using application identity instead of user identity          | The SharePoint tool requires user identity (identity passthrough). Don't use application-only authentication.                                                                                                             |
| `Forbidden: Authorization Failed - User does not have valid license`      | Missing Microsoft 365 Copilot license or pay-as-you-go model | Assign a Microsoft 365 Copilot license to the user or enable pay-as-you-go. See [Prerequisites](#prerequisites).                                                                                                          |
| 401 or authentication failures                                            | Cross-tenant access attempt                                  | Confirm the user in Foundry and Microsoft 365 is in the same tenant.                                                                                                                                                      |
| Tool returns no results                                                   | User lacks access to SharePoint content                      | Verify the user has read access to the SharePoint sites and documents being queried.                                                                                                                                      |
| Slow response times                                                       | Large document search scope                                  | Narrow the search scope by specifying specific sites or libraries. Consider using more specific search queries.                                                                                                           |
| Incomplete document retrieval                                             | Content not indexed                                          | Confirm the SharePoint content is indexed by Microsoft Search. Recently added content might need time to be indexed.                                                                                                      |
| `Resource not found` errors                                               | Invalid site or library path                                 | Verify the SharePoint site URL and library paths are correct and accessible to the user.                                                                                                                                  |
| Inconsistent search results                                               | Semantic index sync delay                                    | Wait for the semantic index to sync. Large content changes might take time to propagate. See [Semantic indexing for Microsoft 365 Copilot](https://learn.microsoft.com/en-us/microsoftsearch/semantic-index-for-copilot). |

## Next steps

* For reference, see articles about content retrieval used by the tool:

  * [Overview of the Microsoft 365 Copilot Retrieval API](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api-reference/retrieval-api-overview).
  * [Semantic indexing for Microsoft 365 Copilot](https://learn.microsoft.com/en-us/microsoftsearch/semantic-index-for-copilot)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="note">
  * This article describes the Microsoft SharePoint tool for Foundry Agent Service. For information on using and deploying SharePoint sites, see the [SharePoint documentation](https://learn.microsoft.com/en-us/sharepoint/).
  * See [best practices](../../concepts/tool-best-practice) for information on optimizing tool usage.
</Callout>

Use the SharePoint tool (preview) for SharePoint grounding in Microsoft Foundry Agent Service by retrieving content from a SharePoint site or folder (for example, `contoso.sharepoint.com/sites/policies`). When a user asks a question, the agent can invoke the SharePoint tool to retrieve relevant text from documents the user can access. The agent then generates a response based on that retrieved content.

<Callout type="note">
  SharePoint tool access requires either a Microsoft 365 Copilot license or an enabled pay-as-you-go model. See [Prerequisites](#prerequisites) for details.
</Callout>

This integration uses identity passthrough (On-Behalf-Of) so SharePoint permissions continue to apply to every request. For details on the underlying Microsoft 365 Copilot Retrieval API integration, see [How it works](#how-it-works).

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* Eligible license or pay-as-you-go model:

  * Developers and end users have a Microsoft 365 Copilot license, as required by the [Microsoft 365 Copilot Retrieval API](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api-reference/retrieval-api-overview).
  * If developers and end users don't have a Microsoft 365 Copilot license, you can enable the [pay-as-you-go model](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api/ai-services/retrieval/paygo-retrieval).

* Developers and end users have at least `Azure AI User` RBAC role assigned on the Foundry project. For more information about Azure role-based access control, see [Azure role-based access control in Foundry](../../../concepts/rbac-foundry?view=foundry\&preserve-view=true).

* Developers and end users have at least `READ` access to the SharePoint site.

* The latest prerelease package installed:

  * **Python**: `pip install azure-ai-projects --pre`
  * **C#**: Install the `Azure.AI.Projects` NuGet package (prerelease)
  * **TypeScript/JavaScript**: `npm install @azure/ai-projects`

* Environment variables configured:

  * `AZURE_AI_PROJECT_ENDPOINT`: Your Foundry project endpoint URL
  * `AZURE_AI_MODEL_DEPLOYMENT_NAME`: Your model deployment name (for example, `gpt-4`)
  * `SHAREPOINT_PROJECT_CONNECTION_ID`: Your SharePoint connection ID in the format `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`
  * For REST samples: `API_VERSION`, `AGENT_TOKEN`

* See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code) for additional authentication setup details.

## Parameters

The SharePoint tool uses your project connection to determine which SharePoint site or folder it can retrieve from.

| Parameter                                                                  | Required | Notes                                                |
| -------------------------------------------------------------------------- | -------- | ---------------------------------------------------- |
| `type`                                                                     | Yes      | Use `sharepoint_grounding_preview`.                  |
| `sharepoint_grounding_preview.project_connections[].project_connection_id` | Yes      | Use the value of `SHAREPOINT_PROJECT_CONNECTION_ID`. |

If you need to create a SharePoint connection for your project, see [Add a new connection to your project](../../../how-to/connections-add?view=foundry\&preserve-view=true).

## Code example

<ZonePivot pivot="python">
  ### Quick verification

  Before running the full sample, verify your SharePoint connection exists:

  ```python
  import os

  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential
  from dotenv import load_dotenv

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
  ):
      print("Connected to project.")

      # Verify SharePoint connection exists
      connection_name = os.environ.get("SHAREPOINT_PROJECT_CONNECTION_NAME")
      if connection_name:
          try:
              conn = project_client.connections.get(connection_name)
              print(f"SharePoint connection verified: {conn.name}")
              print(f"Connection ID: {conn.id}")
          except Exception as e:
              print(f"SharePoint connection '{connection_name}' not found: {e}")
      else:
          # List available connections to help find the right one
          print("SHAREPOINT_PROJECT_CONNECTION_NAME not set. Available connections:")
          for conn in project_client.connections.list():
              print(f"  - {conn.name}")
  ```

  If this code runs without errors, your credentials and SharePoint connection are configured correctly.

  ### Full sample

  The following sample demonstrates how to create an Agent that uses the SharePoint tool to ground responses with content from a SharePoint site.

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      SharepointAgentTool,
      SharepointGroundingToolParameters,
      ToolProjectConnection,
  )

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
      project_client.get_openai_client() as openai_client,
  ):
      # Get connection ID from connection name
      sharepoint_connection = project_client.connections.get(
          os.environ["SHAREPOINT_PROJECT_CONNECTION_NAME"],
      )
      print(f"SharePoint connection ID: {sharepoint_connection.id}")

      sharepoint_tool = SharepointAgentTool(
          sharepoint_grounding_preview=SharepointGroundingToolParameters(
              project_connections=[
                  ToolProjectConnection(project_connection_id=sharepoint_connection.id)
              ]
          )
      )

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are a helpful agent that can use SharePoint tools to assist users.
              Use the available SharePoint tools to answer questions and perform tasks.""",
              tools=[sharepoint_tool],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      # Send initial request that will trigger the SharePoint tool
      stream_response = openai_client.responses.create(
          stream=True,
        tool_choice="required",
          input="Please summarize the last meeting notes stored in SharePoint.",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              if event.item.type == "message":
                  item = event.item
                  if item.content[-1].type == "output_text":
                      text_content = item.content[-1]
                      for annotation in text_content.annotations:
                          if annotation.type == "url_citation":
                              print(
                                  f"URL Citation: {annotation.url}, "
                                  f"Start index: {annotation.start_index}, "
                                  f"End index: {annotation.end_index}"
                              )
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")
  ```

  ### What this code does

  This example creates an agent with SharePoint grounding capabilities and processes a streaming response:

  1. **Initialize the project client** by using your Foundry project endpoint and Azure credentials.
  2. **Configure the SharePoint tool** with your project connection to enable access to SharePoint content.
  3. **Create the agent** with instructions and the SharePoint tool attached.
  4. **Send a query** asking the agent to summarize meeting notes from SharePoint.
  5. **Process the streaming response** to display the agent's answer in real-time.
  6. **Extract URL citations** from the response annotations showing which SharePoint documents were referenced.

  ### Expected output

  When you run this code, you see output similar to:

  ```text
  Agent created (id: asst_abc123, name: MyAgent, version: 1)

  Sending request to SharePoint agent with streaming...
  Follow-up response created with ID: resp_xyz789
  Delta: Based
  Delta:  on
  Delta:  the
  Delta:  meeting
  Delta:  notes
  ...
  URL Citation: https://contoso.sharepoint.com/sites/policies/Documents/meeting-notes.docx, Start index: 0, End index: 245

  Follow-up response done!

  Follow-up completed!
  Full response: Based on the meeting notes from your SharePoint site, the last meeting covered the following topics: project timeline updates, budget review, and next quarter planning.
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ### Quick verification

  Before running the full sample, verify your SharePoint connection exists:

  ```csharp
  using Azure.AI.Projects;
  using Azure.Identity;

  var projectEndpoint = System.Environment.GetEnvironmentVariable("AZURE_AI_PROJECT_ENDPOINT");
  var sharepointConnectionName = System.Environment.GetEnvironmentVariable("SHAREPOINT_PROJECT_CONNECTION_NAME");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Verify SharePoint connection exists
  try
  {
      AIProjectConnection conn = projectClient.Connections.GetConnection(connectionName: sharepointConnectionName);
      Console.WriteLine($"SharePoint connection verified: {conn.Name}");
      Console.WriteLine($"Connection ID: {conn.Id}");
  }
  catch (Exception ex)
  {
      Console.WriteLine($"SharePoint connection '{sharepointConnectionName}' not found: {ex.Message}");
      // List available connections
      Console.WriteLine("Available connections:");
      foreach (var conn in projectClient.Connections.GetConnections())
      {
          Console.WriteLine($"  - {conn.Name}");
      }
  }
  ```

  If this code runs without errors, your credentials and SharePoint connection are configured correctly.

  ### Full sample

  The following sample demonstrates how to create an Agent that uses the SharePoint tool to ground responses with content from a SharePoint site. This example uses synchronous methods for simplicity. For an asynchronous version, refer to the [SharePoint agent sample documentation](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample24_Sharepoint.md) on the Azure SDK for .NET GitHub repository.

  To enable your Agent to access SharePoint, use `SharepointAgentTool`.

  ```csharp
  using System;
  using Azure.AI.Projects;
  using Azure.AI.Projects.OpenAI;
  using Azure.Identity;

  // Create an agent client and read the environment variables, which will be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("AZURE_AI_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("AZURE_AI_MODEL_DEPLOYMENT_NAME");
  var sharepointConnectionName = System.Environment.GetEnvironmentVariable("SHAREPOINT_PROJECT_CONNECTION_NAME");
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Get connection ID from connection name
  AIProjectConnection sharepointConnection = projectClient.Connections.GetConnection(connectionName: sharepointConnectionName);

  // Use the SharePoint connection ID to initialize the SharePointGroundingToolOptions,
  // which will be used to create SharepointAgentTool. Use this tool to create an Agent.
  SharePointGroundingToolOptions sharepointToolOption = new()
  {
      ProjectConnections = { new ToolProjectConnection(projectConnectionId: sharepointConnection.Id) }
  };
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant.",
      Tools = { new SharepointPreviewTool(sharepointToolOption), }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create the response and make sure we are always using tool.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  CreateResponseOptions responseOptions = new()
  {
      ToolChoice = ResponseToolChoice.CreateRequiredChoice(),
      InputItems = { ResponseItem.CreateUserMessageItem("What is Contoso whistleblower policy") }
  };
  ResponseResult response = responseClient.CreateResponse(options: responseOptions);

  // SharePoint tool can create the reference to the page, grounding the data.
  // Create the GetFormattedAnnotation method to get the URI annotation.
  string annotation = "";
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is MessageResponseItem messageItem)
      {
          foreach (ResponseContentPart content in messageItem.Content)
          {
              foreach (ResponseMessageAnnotation messageAnnotation in content.OutputTextAnnotations)
              {
                  if (messageAnnotation is UriCitationMessageAnnotation uriAnnotation)
                  {
                      annotation = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                  }
              }
          }
      }
  }

  // Print the Agent output and add the annotation at the end.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine($"{response.GetOutputText()}{annotation}");

  // After the sample is completed, delete the Agent we have created.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### What this code does

  This example demonstrates SharePoint grounding with synchronous methods:

  1. **Create a project client** using your Foundry project endpoint
  2. **Retrieve the SharePoint connection** by name from your project's connections
  3. **Configure the SharePoint tool** with the connection ID
  4. **Create an agent** with the SharePoint tool to enable document access
  5. **Create a response** asking about the Contoso whistleblower policy
  6. **Format and display the response** with a helper method that extracts URL citations from annotations
  7. **Clean up** by deleting the agent version

  ### Expected output

  When you run this code, you see output similar to:

  ```text
  The Contoso whistleblower policy outlines procedures for reporting unethical behavior confidentially. Employees can submit concerns through the ethics hotline or online portal. [Whistleblower Policy](https://contoso.sharepoint.com/sites/policies/Documents/whistleblower-policy.pdf)
  ```

  The output includes the agent's response grounded in SharePoint content, with a citation link to the source document.
</ZonePivot>

<ZonePivot pivot="rest">
  ## Sample for use of an Agent with SharePoint

  The following sample demonstrates how to create an Agent that uses the SharePoint tool to ground responses with content from a SharePoint site.

  ```bash
  curl --request POST \
    --url "$AZURE_AI_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
    "model": "'$AZURE_AI_MODEL_DEPLOYMENT_NAME'",
    "input": "Please summarize the last meeting notes stored in SharePoint.",
    "tool_choice": "required",
    "tools": [
      {
        "type": "sharepoint_grounding_preview",
        "sharepoint_grounding_preview": {
          "project_connections": [
            {
              "project_connection_id": "'$SHAREPOINT_PROJECT_CONNECTION_ID'"
            }
          ]
        }
      }
    ]
  }'
  ```

  ### What this code does

  This REST API call creates a response with SharePoint grounding:

  1. **Sends a POST request** to the `/openai/responses` endpoint
  2. **Authenticates** using a bearer token for your agent
  3. **Specifies the model** deployment to use for generating responses
  4. **Includes the user's query** asking for meeting notes from SharePoint
  5. **Configures the SharePoint tool** with your project connection ID to enable document retrieval
  6. **Returns a JSON response** with the agent's answer and citations to source documents

  ### Expected output

  The API returns a JSON response with the agent's answer and citation information:

  ```json
  {
    "id": "resp_abc123xyz",
    "object": "response",
    "created_at": 1702345678,
    "status": "completed",
    "output_text": "Based on the meeting notes from your SharePoint site, the last meeting covered project timeline updates, budget review, and next quarter planning.",
    "output_items": [
      {
        "type": "message",
        "content": [
          {
            "type": "output_text",
            "text": "Based on the meeting notes...",
            "annotations": [
              {
                "type": "url_citation",
                "url": "https://contoso.sharepoint.com/sites/policies/Documents/meeting-notes.docx",
                "start_index": 0,
                "end_index": 245
              }
            ]
          }
        ]
      }
    ]
  }
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ### Quick verification

  Before running the full sample, verify your SharePoint connection exists:

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const sharepointConnectionName = process.env["SHAREPOINT_PROJECT_CONNECTION_NAME"] || "<sharepoint connection name>";

  async function verifyConnection(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    console.log("Connected to project.");

    try {
      const conn = await project.connections.get(sharepointConnectionName);
      console.log(`SharePoint connection verified: ${conn.name}`);
      console.log(`Connection ID: ${conn.id}`);
    } catch (error) {
      console.log(`SharePoint connection '${sharepointConnectionName}' not found: ${error}`);
      // List available connections
      console.log("Available connections:");
      for await (const conn of project.connections.list()) {
        console.log(`  - ${conn.name}`);
      }
    }
  }

  verifyConnection().catch(console.error);
  ```

  If this code runs without errors, your credentials and SharePoint connection are configured correctly.

  ### Full sample

  This sample demonstrates how to create an AI agent with SharePoint capabilities using the `SharepointAgentTool` and synchronous Azure AI Projects client. The agent can search SharePoint content and provide responses with relevant information from SharePoint sites. For a JavaScript version of this sample, refer to the [SharePoint agent sample documentation](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentSharepoint.js) in the Azure SDK for JavaScript GitHub repository.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["AZURE_AI_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const sharepointConnectionName =
    process.env["SHAREPOINT_PROJECT_CONNECTION_NAME"] || "<sharepoint connection name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Get connection ID from connection name
    const sharepointConnection = await project.connections.get(sharepointConnectionName);
    console.log(`SharePoint connection ID: ${sharepointConnection.id}`);

    console.log("Creating agent with SharePoint tool...");

    const agent = await project.agents.createVersion("MyAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a helpful agent that can use SharePoint tools to assist users. Use the available SharePoint tools to answer questions and perform tasks.",
      // Define SharePoint tool that searches SharePoint content
      tools: [
        {
          type: "sharepoint_grounding_preview",
          sharepoint_grounding_preview: {
            project_connections: [
              {
                project_connection_id: sharepointConnection.id,
              },
            ],
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Send initial request that will trigger the SharePoint tool
    console.log("\nSending request to SharePoint agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: "Please summarize the last meeting notes stored in SharePoint.",
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        if (event.item.type === "message") {
          const item = event.item;
          if (item.content && item.content.length > 0) {
            const lastContent = item.content[item.content.length - 1];
            if (lastContent.type === "output_text" && lastContent.annotations) {
              for (const annotation of lastContent.annotations) {
                if (annotation.type === "url_citation") {
                  console.log(
                    `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                  );
                }
              }
            }
          }
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nSharePoint agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### What this code does

  This TypeScript example demonstrates the complete agent lifecycle with SharePoint:

  1. **Initialize the project client** with your Foundry endpoint and Azure credentials.
  2. **Get the OpenAI client** for creating responses.
  3. **Create an agent** with SharePoint tool configuration using your project connection ID.
  4. **Send a streaming request** asking the agent to summarize meeting notes.
  5. **Process streaming events** to display the response as it's generated.
  6. **Extract and display URL citations** from response annotations.
  7. **Clean up resources** by deleting the agent version after completion.

  ### Expected output

  When you run this code, you see output similar to:

  ```text
  Creating agent with SharePoint tool...
  Agent created (id: asst_abc123, name: MyAgent, version: 1)

  Sending request to SharePoint agent with streaming...
  Follow-up response created with ID: resp_xyz789
  Based on the meeting notes from your SharePoint site, the last meeting covered the following topics: project timeline updates, budget review, and next quarter planning.

  Follow-up response done!
  URL Citation: https://contoso.sharepoint.com/sites/policies/Documents/meeting-notes.docx, Start index: 0, End index: 245

  Follow-up completed!

  Cleaning up resources...
  Agent deleted

  SharePoint agent sample completed!
  ```
</ZonePivot>

## Limitations

* The SharePoint tool only supports user identity authentication. App-only (service principal) authentication isn't supported.
* Your SharePoint site and your Microsoft Foundry agent must be in the same tenant.
* You can add only one SharePoint tool per agent.
* The underlying Microsoft 365 Copilot Retrieval API returns text extracts. Retrieval from nontextual content, including images and charts, isn't supported.
* For semantic and hybrid retrieval, the Microsoft 365 Copilot Retrieval API supports `.doc`, `.docx`, `.pptx`, `.pdf`, `.aspx`, and `.one` file types. For details, see the [Microsoft 365 Copilot API](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api-reference/retrieval-api-overview).

## Setup

<Callout type="note">
  Start with SharePoint sites that have a simple folder structure and a small number of short documents.
</Callout>

1. Select **SharePoint** and follow the prompts to add the tool. You can only add one per agent.

2. Add a SharePoint connection.

   For step-by-step instructions, see [Add a new connection to your project](../../../how-to/connections-add?view=foundry\&preserve-view=true).

3. In the SharePoint connection configuration, enter the site URL or folder URL.

   * Site URL example: `https://<company>.sharepoint.com/sites/<site_name>`
   * Folder URL example: `https://<company>.sharepoint.com/sites/<site_name>/Shared%20documents/<folder_name>`

   <Callout type="note">
     Your `site_url` needs to follow the format above. If you copy the entire value from the address bar of your SharePoint, it doesn't work.
   </Callout>

4. Save the connection, and then copy its connection **ID**.

5. Set the connection ID as `SHAREPOINT_PROJECT_CONNECTION_ID`.

## How it works

The SharePoint tool makes it possible by enabling seamless integrations between AI agents and business documents stored in SharePoint. This capability is empowered by the [Microsoft 365 Copilot API](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api-reference/retrieval-api-overview). To ground your SharePoint documents, enter the sites or folders to connect with. The SharePoint tool leverages [built-in indexing capabilities](https://learn.microsoft.com/en-us/microsoftsearch/semantic-index-for-copilot) to enhance the search and retrieval experience, including intelligent indexing, query processing, and content chunking.

For more information about delegated access and identity passthrough in Foundry, see [Agent identity concepts in Microsoft Foundry](../../concepts/agent-identity).

Instead of requiring developers to export SharePoint content, build a custom semantic index, manage governance controls, and configure refresh logic, this capability automates the entire retrieval pipeline. It dynamically indexes documents, breaks content into meaningful chunks, and applies advanced query processing to surface the most relevant information. By using the same enterprise-grade retrieval stack that powers Microsoft 365 Copilot, this capability ensures AI agent responses are grounded in the most up-to-date and contextually relevant content.

Customers rely on data security in SharePoint to access, create, and share documents with flexible document-level access control. Enterprise features such as identity passthrough (On-Behalf-Of) authentication ensure proper access control. End users receive responses generated from SharePoint documents they have permission to access. By using OBO authentication, Foundry Agent Service uses the end user's identity to authorize and retrieve relevant SharePoint documents, generating responses tailored to specific end users.

## Troubleshooting

| Issue                                                                     | Cause                                                        | Resolution                                                                                                                                                                                                                |
| ------------------------------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `AuthenticationError: AppOnly OBO tokens not supported by target service` | Using application identity instead of user identity          | The SharePoint tool requires user identity (identity passthrough). Don't use application-only authentication.                                                                                                             |
| `Forbidden: Authorization Failed - User does not have valid license`      | Missing Microsoft 365 Copilot license or pay-as-you-go model | Assign a Microsoft 365 Copilot license to the user or enable pay-as-you-go. See [Prerequisites](#prerequisites).                                                                                                          |
| 401 or authentication failures                                            | Cross-tenant access attempt                                  | Confirm the user in Foundry and Microsoft 365 is in the same tenant.                                                                                                                                                      |
| Tool returns no results                                                   | User lacks access to SharePoint content                      | Verify the user has read access to the SharePoint sites and documents being queried.                                                                                                                                      |
| Slow response times                                                       | Large document search scope                                  | Narrow the search scope by specifying specific sites or libraries. Consider using more specific search queries.                                                                                                           |
| Incomplete document retrieval                                             | Content not indexed                                          | Confirm the SharePoint content is indexed by Microsoft Search. Recently added content might need time to be indexed.                                                                                                      |
| `Resource not found` errors                                               | Invalid site or library path                                 | Verify the SharePoint site URL and library paths are correct and accessible to the user.                                                                                                                                  |
| Inconsistent search results                                               | Semantic index sync delay                                    | Wait for the semantic index to sync. Large content changes might take time to propagate. See [Semantic indexing for Microsoft 365 Copilot](https://learn.microsoft.com/en-us/microsoftsearch/semantic-index-for-copilot). |

## Next steps

* For reference, see articles about content retrieval used by the tool:

  * [Overview of the Microsoft 365 Copilot Retrieval API](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/api-reference/retrieval-api-overview).
  * [Semantic indexing for Microsoft 365 Copilot](https://learn.microsoft.com/en-us/microsoftsearch/semantic-index-for-copilot)

Web grounding tools in Microsoft Foundry Agent Service connect your agents to real-time public web data, overcoming the knowledge cutoff that limits large language models. For example, you can ask questions such as "what is the top AI news today" and receive current, cited answers.

## How web grounding works

The grounding process involves several key steps:

1. **Query formulation**: The agent identifies information gaps and constructs search queries based on the user's input.
2. **Search execution**: The grounding tool submits queries to Bing and retrieves results.
3. **Information synthesis**: The agent processes search results and integrates findings into responses.
4. **Source attribution**: The agent provides transparency by citing search sources with URLs.

## Prerequisites

Before using any web grounding tool, ensure you have:

* A [basic or standard agent environment](../../environment-setup).
* The latest prerelease SDK package for your language (Python: `azure-ai-projects`, C#: `Azure.AI.Projects.OpenAI`, JavaScript: `@azure/ai-projects`). See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true) for installation steps.
* An Azure OpenAI model deployment in your Foundry project.

<Callout type="note">
  Web Search requires no extra roles beyond your Foundry project access. Grounding with Bing Search and Grounding with Bing Custom Search require **Contributor** or **Owner** role to create Bing resources, and **Azure AI Project Manager** role to create project connections. For details, see [agent environment setup](../../environment-setup).
</Callout>

<Callout type="important">
  * Web Search (preview) uses Grounding with Bing Search and Grounding with Bing Custom Search are [First Party Consumption Services](https://www.microsoft.com/licensing/terms/product/Glossary/EAEAS#:%7E:text=First-Party%20Consumption%20Services) with [terms for online services](https://www.microsoft.com/licensing/terms/product/ForOnlineServices/EAEAS). They're governed by the [Grounding with Bing terms of use](https://www.microsoft.com/bing/apis/grounding-legal-enterprise) and the [Microsoft Privacy Statement](https://go.microsoft.com/fwlink/?LinkId=521839\&clcid=0x409).
  * The Microsoft [Data Protection Addendum](https://aka.ms/dpa) doesn't apply to data sent to Grounding with Bing Search or Grounding with Bing Custom Search. When you use these services, your data flows outside the Azure compliance and Geo boundary. This also means use of these services waives all elevated Government Community Cloud security and compliance commitments, including data sovereignty and screened/citizenship-based support, as applicable.
  * Use of Grounding with Bing Search and Grounding with Bing Custom Search incurs costs. See pricing for [details](https://www.microsoft.com/bing/apis/grounding-pricing).
  * See the management section for information about how Azure admins can manage access to use of Grounding with Bing Search and Grounding with Bing Custom Search.
</Callout>

## Determine the best tool for your use cases

If you're just getting started, use [Web Search (preview)](web-search). It requires no extra Azure resources and is the simplest way to add web grounding to your agent.

The following use cases help you compare the available tools. Use case 1 covers general web search, where both Web Search and Grounding with Bing Search can retrieve results from the public web. Use case 2 covers domain-restricted search, which only Grounding with Bing Custom Search supports.

### Use case 1: Grounding from general web indexed by Bing

|                                  | [Web Search](web-search) (recommended)                                                                                                                                                                      | [Grounding with Bing Search](bing-tools)                                                                                                                                                                                                                                                                    |
| -------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Stage**                        | Preview                                                                                                                                                                                                     | GA                                                                                                                                                                                                                                                                                                          |
| **Grounding with Bing resource** | Managed by Microsoft                                                                                                                                                                                        | Managed by you — requires creating a Grounding with Bing Search resource first                                                                                                                                                                                                                              |
| **Supported parameters**         | - `user_location`: Provides geo‑relevant results - `search_context_size`: low/medium/high (default: medium) Learn more about [Web Search parameters](web-search#optional-parameters-for-general-web-search) | - `count`: the maximum of results returned by Bing - `freshness`: specifies the period for the search results - `market`: specifies the region for the search results - `set_lang`: specifies the language for the search results Learn more about [Bing Search parameters](bing-tools#optional-parameters) |
| **Data boundary**                | Data flows outside Azure compliance boundary                                                                                                                                                                | Data flows outside Azure compliance boundary                                                                                                                                                                                                                                                                |
| **Supported models**             | Azure OpenAI models                                                                                                                                                                                         | Azure OpenAI models and Azure direct models (non-OpenAI models deployed directly on Azure)                                                                                                                                                                                                                  |

### Use case 2: Grounding from specific domains you defined

|                         | [Grounding with Bing Custom Search](bing-tools)                                                                                                                                                                                                                                                                           |
| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Stage**               | Preview                                                                                                                                                                                                                                                                                                                   |
| **Pre-defined domains** | Supported — use `custom_search_configuration` to pre‑define allowed or blocked domains (requires creating a Bing Custom Search resource + instance)                                                                                                                                                                       |
| **Other parameters**    | - `count`: the maximum number of results returned by Bing - `freshness`: specifies the period for the search results - `market`: specifies the region for the search results - `set_lang`: specifies the language for the search results Learn more about [Bing Custom Search parameters](bing-tools#optional-parameters) |
| **Supported models**    | Azure OpenAI models and Azure direct models                                                                                                                                                                                                                                                                               |

## Common questions

### Which tool should I use if I'm just getting started?

Use [Web Search (preview)](web-search). It requires no additional Azure resources, handles Bing resource management automatically, and provides geo-relevant results with the `user_location` parameter.

### Can I use web grounding tools with network-secured Foundry projects?

Web grounding tools don't respect VPN or private endpoints. They act as public endpoints. Consider this security implication when using network-secured Foundry with these tools.

### How do I restrict search results to specific websites?

Use [Grounding with Bing Custom Search (preview)](bing-tools). This tool lets you define an allow-list or block-list of domains, so search results come only from sources you approve.

### Are there additional costs for web grounding?

Yes. Web Search (preview), Grounding with Bing Search and Grounding with Bing Custom Search (preview) incur costs beyond standard Azure OpenAI usage. See [pricing details](https://www.microsoft.com/bing/apis/grounding-pricing).

## Troubleshooting

| Issue                           | Likely cause                                                 | Resolution                                                                                                                                             |
| ------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Agent doesn't use web grounding | Tool not configured or model doesn't support the tool.       | Verify the tool is added to your agent definition. Use `tool_choice="required"` to force tool use. Check that your model deployment supports the tool. |
| No citations in response        | The model generated a response without using search results. | Add explicit instructions to always cite sources. Use `tool_choice="required"` to ensure tool invocation.                                              |
| Search results aren't relevant  | Query formulation didn't capture user intent.                | Improve agent instructions to guide query construction. For Bing tools, adjust `market` and `set_lang` parameters.                                     |
| Tool blocked by administrator   | Your organization disabled web grounding tools.              | Contact your Azure administrator to enable access. See [administrator control](web-search#administrator-control-for-the-web-search-tool).              |
| Unexpected costs                | Web grounding tools have usage-based pricing.                | Review [pricing details](https://www.microsoft.com/bing/apis/grounding-pricing) and implement rate limiting if needed.                                 |

## Related content

* [Use the Web Search tool (preview)](web-search)
* [Use Grounding with Bing Search and Grounding with Bing Custom Search](bing-tools)
* [Best practices for using tools in Foundry Agent Service](../../concepts/tool-best-practice)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

The web search tool in Foundry Agent Service enables models to retrieve and ground responses with real-time information from the public web before generating output. When enabled, the model can return up-to-date answers with inline citations, helping you build agents that provide current, factual information to users.

<Callout type="important">
  * Web Search(preview) uses Grounding with Bing Search and Grounding with Bing Custom Search, which are [First Party Consumption Services](https://www.microsoft.com/licensing/terms/product/Glossary/EAEAS#:%7E:text=First-Party%20Consumption%20Services) governed by these [Grounding with Bing terms of use](https://www.microsoft.com/bing/apis/grounding-legal-enterprise) and the [Microsoft Privacy Statement](https://go.microsoft.com/fwlink/?LinkId=521839\&clcid=0x409).
  * The Microsoft [Data Protection Addendum](https://aka.ms/dpa) doesn't apply to data sent to Grounding with Bing Search and Grounding with Bing Custom Search. When you use Grounding with Bing Search and Grounding with Bing Custom Search, data transfers occur outside compliance and geographic boundaries.
  * Use of Grounding with Bing Search and Grounding with Bing Custom Search incurs costs. See [pricing](https://www.microsoft.com/bing/apis/grounding-pricing) for details.
  * See the [management section](#administrator-control-for-the-web-search-tool) for information about how Azure admins can manage access to use of web search.
</Callout>

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* A [basic or standard agent environment](../../environment-setup)

* The latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#install-and-authenticate) for details.

* Azure credentials configured for authentication (such as `DefaultAzureCredential`).

* Environment variables configured:

  * `AZURE_AI_PROJECT_ENDPOINT` (or `PROJECT_ENDPOINT`): Your Foundry project endpoint URL.
  * `AZURE_AI_MODEL_DEPLOYMENT_NAME` (or `MODEL_DEPLOYMENT_NAME`): Your model deployment name.

## Code examples

<Callout type="note">
  * See [best practices](../../concepts/tool-best-practice) for information on optimizing tool usage.
  * You need the latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code) for details.
</Callout>

<ZonePivot pivot="python">
  ### General Web Search

  The following example shows how to set up the AI Project client by using the Azure Identity library for authentication.

  ```python
  import os
  from dotenv import load_dotenv

  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition, WebSearchPreviewTool, ApproximateLocation

  load_dotenv()

  project_client = AIProjectClient(
    endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )

  openai_client = project_client.get_openai_client()

  with project_client:
      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
          model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              instructions="You are a helpful assistant that can search the web",
              tools=[
            WebSearchPreviewTool(
              user_location=ApproximateLocation(country="GB", city="London", region="London")
            )
              ],
          ),
          description="Agent for web search.",
      )

      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input="What is today's date and weather in Seattle?",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              if event.item.type == "message":
                  item = event.item
                  if item.content[-1].type == "output_text":
                      text_content = item.content[-1]
                      for annotation in text_content.annotations:
                          if annotation.type == "url_citation":
                              print(f"URL Citation: {annotation.url}")
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ### General Web Search

  In this example, you use the agent to perform the web search in the given location. The example in this section uses synchronous calls. For an asynchronous example, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample13_WebSearch.md) in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Create project client and read the environment variables, which will be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME");
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Create an agent capable of using Web search and set the location to "London" in the WebSearchToolLocation.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant that can search the web",
      Tools = {
          ResponseTool.CreateWebSearchTool(userLocation: WebSearchToolLocation.CreateApproximateLocation(
              country: "GB",
              city: "London",
              region: "London"
              )
          ),
      }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Ask a question related to London.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  ResponseResult response = responseClient.CreateResponse("Show me the latest London Underground service updates");

  // Create the response and throw an exception if the response contains an error.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine(response.GetOutputText());

  // Delete the created agent version.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  **Expected output**

  The following is an example of the expected output when running the C# code:

  ```console
  Creating agent with web search tool...
  Agent created (id: 12345, name: myAgent, version: 1)
  Response: The agent returns a grounded response that includes citations.
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="rest-api">
  ### General Web Search

  The following example shows how to create a response by using an agent that has the web search tool enabled.

  ```bash
  curl --request POST \
    --url "$FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    --data '{
      "model": "$FOUNDRY_MODEL_DEPLOYMENT_NAME",
      "input": "Tell me about the latest news about AI",
      "tool_choice": "required",
      "tools": [
        {
          "type": "web_search_preview"
        }
      ]
    }'
  ```

  #### Expected output

  The following example shows the expected output when using the web search tool via the REST API:

  ```json
  {
    "id": "resp_abc123xyz",
    "object": "response",
    "created_at": 1702345678,
    "status": "completed",
    "output_text": "Here is a grounded response with citations.",
    "output_items": [
      {
        "type": "message",
        "content": [
          {
            "type": "output_text",
            "text": "Here is a grounded response with citations.",
            "annotations": [
              {
                "type": "url_citation",
                "url": "https://contoso.com/example-source",
                "start_index": 0,
                "end_index": 43
              }
            ]
          }
        ]
      }
    ]
  }
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Use the web search tool with TypeScript

  The following TypeScript example demonstrates how to create an agent with the web search tool. For an example that uses JavaScript, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentWebSearch.js) example in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

  export async function main(): Promise<void> {
    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with web search tool...");

    // Create Agent with web search tool
    const agent = await project.agents.createVersion("agent-web-search", {
      kind: "prompt",
      model: deploymentName,
      instructions: "You are a helpful assistant that can search the web",
      tools: [
        {
          type: "web_search_preview",
          user_location: {
            type: "approximate",
            country: "GB",
            city: "London",
            region: "London",
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Create a conversation for the agent interaction
    const conversation = await openAIClient.conversations.create();
    console.log(`Created conversation (id: ${conversation.id})`);

    // Send a query to search the web
    console.log("\nSending web search query...");
    const response = await openAIClient.responses.create(
      {
        conversation: conversation.id,
        input: "Show me the latest London Underground service updates",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
      },
    );
    console.log(`Response: ${response.output_text}`);

    // Clean up resources
    console.log("\nCleaning up resources...");
    await openAIClient.conversations.delete(conversation.id);
    console.log("Conversation deleted");

    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nWeb search sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  The following example shows the expected output when running the TypeScript code:

  ```console
  Creating agent with web search tool...
  Agent created (id: 12345, name: agent-web-search, version: 1)
  Created conversation (id: 67890)
  Sending web search query...
  Response: The agent returns a grounded response that includes citations.
  Cleaning up resources...
  Conversation deleted
  Agent deleted
  Web search sample completed!
  ```
</ZonePivot>

## Configure the web search tool

You can configure web search behavior when you create your agent.

### Optional parameters for general web search

* `user_location`: Helps web search return results relevant to a user’s geography. Use an approximate location when you want results localized to a country/region/city.
* `search_context_size`: Controls how much context window space to use for the search. Supported values are `low`, `medium`, and `high`. The default is `medium`.

## Security and privacy considerations

* Treat web search results as untrusted input. Validate and sanitize data before you use it in downstream systems.
* Avoid sending secrets or sensitive personal data in prompts that might be forwarded to external services.
* Review the terms, privacy, and data boundary notes in the preview section of this article before enabling web search in production.

## Troubleshooting

| Issue                                         | Cause                                                             | Resolution                                                                                                                                     |
| --------------------------------------------- | ----------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| Web search isn't used and no citations appear | Model didn't determine web search was needed                      | Update your instructions to explicitly allow web search for up-to-date questions, and ask a query that requires current information.           |
| Requests fail after enabling web search       | Web search is disabled at the subscription level                  | Ask an admin to enable web search. See [Administrator control for the web search tool](#administrator-control-for-the-web-search-tool).        |
| REST requests return authentication errors    | Bearer token is missing, expired, or has insufficient permissions | Refresh your token and confirm your access to the project and agent.                                                                           |
| Search returns outdated information           | Web content not recently indexed                                  | Refine your query to explicitly request the most recent information. Results depend on Bing's indexing schedule.                               |
| No results for specific topics                | Query too narrow or content not indexed                           | Broaden your search query. Some niche topics might have limited web coverage.                                                                  |
| Rate limiting errors (429)                    | Too many requests in a short time period                          | Implement exponential backoff and retry logic. Consider spacing out requests.                                                                  |
| Inconsistent citation formatting              | Response format varies by query type                              | Standardize citation handling in your application code. Parse both inline and reference-style citations.                                       |
| Tool not available for deployment             | Regional or model limitations                                     | Confirm web search is available in your region and with your model deployment. Check [tool best practices](../../concepts/tool-best-practice). |

## Administrator control for the web search tool

You can enable or disable the web search tool in Foundry Agent Service at the subscription level by using Azure CLI. This setting applies to all accounts within the specified subscription.

### Prerequisites

Before running the following commands, make sure that you:

1. Have [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) installed.
2. Are signed in to Azure by using `az login`.
3. Have Owner or Contributor access to the subscription.

### Disable Bing Web Search

To disable the web search tool for all accounts in a subscription, run the following command:

```azurecli
az feature register \
  --name OpenAI.BlockedTools.web_search \
  --namespace Microsoft.CognitiveServices \
  --subscription "<subscription-id>"
```

This command disables web search across all accounts in the specified subscription.

### Enable Bing Web Search

To enable the web search tool, run the following command:

```azurecli
az feature unregister \
  --name OpenAI.BlockedTools.web_search \
  --namespace Microsoft.CognitiveServices \
  --subscription "<subscription-id>"
```

This command enables web search functionality for all accounts in the subscription.

## Next steps

<Callout type="nextstepaction">
  [Review tool best practices](../../concepts/tool-best-practice)
</Callout>

<Callout type="nextstepaction">
  [Set up an agent environment](../../environment-setup)
</Callout>

Traditional language models work with a knowledge cutoff. They can't access new information beyond a fixed point in time. By using Grounding with Bing Search and Grounding with Bing Custom Search (preview), your agents can incorporate real-time public web data when generating responses. By using these tools, you can ask questions such as "what is the top AI news today".

The grounding process involves several key steps:

1. **Query formulation**: The agent identifies information gaps and constructs search queries.
2. **Search execution**: The grounding tool submits queries to search engines and retrieves results.
3. **Information synthesis**: The agent processes search results and integrates findings into responses.
4. **Source attribution**: The agent provides transparency by citing search sources.

<Callout type="important">
  * Grounding with Bing Search and Grounding with Bing Custom Search are [First Party Consumption Services](https://www.microsoft.com/licensing/terms/product/Glossary/EAEAS#:%7E:text=First-Party%20Consumption%20Services) with [terms for online services](https://www.microsoft.com/licensing/terms/product/ForOnlineServices/EAEAS). They're governed by the [Grounding with Bing terms of use](https://www.microsoft.com/bing/apis/grounding-legal-enterprise) and the [Microsoft Privacy Statement](https://go.microsoft.com/fwlink/?LinkId=521839\&clcid=0x409).
  * The Microsoft [Data Protection Addendum](https://aka.ms/dpa) doesn't apply to data sent to Grounding with Bing Search or Grounding with Bing Custom Search. When you use these services, your data flows outside the Azure compliance and Geo boundary. This also means use of these services waives all elevated Government Community Cloud security and compliance commitments, including data sovereignty and screened/citizenship-based support, as applicable.
  * Use of Grounding with Bing Search and Grounding with Bing Custom Search incurs costs. See pricing for [details](https://www.microsoft.com/bing/apis/grounding-pricing).
  * See the [manage section](#manage-grounding-with-bing-search-and-grounding-with-bing-custom-search) for information about how Azure admins can manage access to use of Grounding with Bing Search and Grounding with Bing Custom Search.
</Callout>

<Callout type="note">
  Start with the [web search tool (preview)](web-search). Learn more about the differences between web search and Grounding with Bing Search (or Grounding with Bing Custom Search) in the [web grounding overview](web-overview).
</Callout>

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

Before you begin, make sure you have:

* An Azure subscription with the right permissions.

* Azure RBAC roles:

  * **Contributor** or **Owner** role at the subscription or resource group level to create Bing resources and get resource keys.
  * **Azure AI Project Manager** role to create project connections in Foundry. For more information, see [Role-based access control for Microsoft Foundry](../../../concepts/rbac-foundry?view=foundry\&preserve-view=true).

* A Foundry project created with a configured endpoint.

* An AI model deployed in your project.

* SDK installed for your preferred language:

  * Python: `azure-ai-projects` (latest prerelease version)
  * C#: `Azure.AI.Projects.OpenAI`
  * TypeScript/JavaScript: `@azure/ai-projects`

* Environment variables set up:

  * `AZURE_AI_PROJECT_ENDPOINT`: Your Foundry project endpoint URL.

  * `AZURE_AI_MODEL_DEPLOYMENT_NAME`: Your deployed model name.

  * For SDK samples:

    * `BING_PROJECT_CONNECTION_NAME`: Your Grounding with Bing Search project connection name.
    * `BING_CUSTOM_SEARCH_PROJECT_CONNECTION_NAME`: Your Grounding with Bing Custom Search project connection name.

  * For REST samples:

    * `BING_PROJECT_CONNECTION_ID`: Your Grounding with Bing Search project connection ID.
    * `BING_CUSTOM_SEARCH_PROJECT_CONNECTION_ID`: Your Grounding with Bing Custom Search project connection ID.
    * `API_VERSION`, `AGENT_TOKEN`.

  * For Bing Custom Search: `BING_CUSTOM_SEARCH_INSTANCE_NAME`: Your custom search instance name.

* A Bing Grounding or Bing Custom Search resource created and connected to your Foundry project. A paid subscription is required to create a Grounding with Bing Search or Grounding with Bing Custom Search resource.

* The Grounding with Bing Search tool works in a network-secured Foundry project, but it behaves like a public endpoint. Consider this behavior when you use the tool in a network-secured environment.

## Setup

In this section, you add a project connection for the Bing resource and capture the values used in the samples. SDK samples use the project connection name and resolve the connection ID at runtime. REST samples use the project connection ID. You can use this [bicep template](https://github.com/microsoft-foundry/foundry-samples/tree/main/infrastructure/infrastructure-setup-bicep/45-basic-agent-bing) to create a basic agent with Grounding with Bing Search tool enabled.

If you already have a project connection ID for the Bing resource you want to use, skip this section.

1. Add the appropriate connection to your project.

   For step-by-step instructions, see [Add a new connection to your project](../../../how-to/connections-add?view=foundry\&preserve-view=true).

   <Callout type="important">
     * You need the **Contributor** or **Owner** role at the subscription or resource group level to create Bing resources and get resource keys.
     * To find the resource keys, go to your Grounding with Bing resource in the [Azure portal](https://portal.azure.com) > **Resource Management** > **Keys**.
   </Callout>

2. Get the project connection name and ID from the connection details, then set the values as environment variables.

* For SDK samples:

  * For Grounding with Bing Search: set `BING_PROJECT_CONNECTION_NAME`.
  * For Grounding with Bing Custom Search: set `BING_CUSTOM_SEARCH_PROJECT_CONNECTION_NAME`.

* For REST samples:

  * For Grounding with Bing Search: set `BING_PROJECT_CONNECTION_ID`.
  * For Grounding with Bing Custom Search: set `BING_CUSTOM_SEARCH_PROJECT_CONNECTION_ID`.

The project connection ID uses the format:

`/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`

## Available tools

| Tool                                        | Description                                                                                                                                                                                | Use case                                    |
| ------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------- |
| Grounding with Bing Search                  | Gives agents standard access to Bing's search capabilities.                                                                                                                                | Scenarios requiring broad knowledge access. |
| Grounding with Bing Custom Search (preview) | Allows agents to search within a configurable set of public web domains. You define the parts of the web you want to draw from so users only see relevant results from domains you choose. | Scenarios requiring information management. |

<Callout type="note">
  See [best practices](../../concepts/tool-best-practice) for information on optimizing tool usage.
</Callout>

## Code examples

<Callout type="note">
  * You need the latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code) for details.
  * For SDK samples, use the project connection name. For REST samples, use the project connection ID in the format `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`.
</Callout>

<ZonePivot pivot="python">
  ### Quick verification

  Before running the full samples, verify your Bing connection exists:

  ```python
  import os

  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential
  from dotenv import load_dotenv

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
  ):
      print("Connected to project.")

      # Verify Bing connection exists
      connection_name = os.environ.get("BING_PROJECT_CONNECTION_NAME")
      if connection_name:
          try:
              conn = project_client.connections.get(connection_name)
              print(f"Bing connection verified: {conn.name}")
              print(f"Connection ID: {conn.id}")
          except Exception as e:
              print(f"Bing connection '{connection_name}' not found: {e}")
      else:
          # List available connections to help find the right one
          print("BING_PROJECT_CONNECTION_NAME not set. Available connections:")
          for conn in project_client.connections.list():
              print(f"  - {conn.name}")
  ```

  If this code runs without errors, your credentials and Bing connection are configured correctly.

  ### Full samples

  The following examples demonstrate how to create an agent with Grounding with Bing Search tools, and how to use the agent to respond to user queries.

  #### Grounding with Bing Search

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      BingGroundingAgentTool,
      BingGroundingSearchToolParameters,
      BingGroundingSearchConfiguration,
  )

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
      project_client.get_openai_client() as openai_client,
  ):
      # Get connection ID from connection name
      bing_connection = project_client.connections.get(
          os.environ["BING_PROJECT_CONNECTION_NAME"],
      )
      print(f"Grounding with Bing Search connection ID: {bing_connection.id}")

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              instructions="You are a helpful assistant.",
              tools=[
                  BingGroundingAgentTool(
                      bing_grounding=BingGroundingSearchToolParameters(
                          search_configurations=[
                              BingGroundingSearchConfiguration(
                                  project_connection_id=bing_connection.id
                              )
                          ]
                      )
                  )
              ],
          ),
          description="You are a helpful agent.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input="What is today's date and weather in Seattle?",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              if event.item.type == "message":
                  item = event.item
                  if item.content[-1].type == "output_text":
                      text_content = item.content[-1]
                      for annotation in text_content.annotations:
                          if annotation.type == "url_citation":
                              print(f"URL Citation: {annotation.url}")
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")
  ```

  ### What this code does

  This example creates an agent with grounding by using the Bing Search tool that can retrieve real-time information from the web. When you run the code:

  1. It creates an `AIProjectClient` and authenticates by using your Azure credentials.
  2. Creates an agent with the Bing grounding tool configured by using your Bing connection.
  3. Sends a query asking about current date and weather in Seattle.
  4. The agent uses the Bing grounding tool to search the web and streams the response.
  5. Extracts and displays URL citations from the search results.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `BING_PROJECT_CONNECTION_NAME`
  * Azure credentials configured for `DefaultAzureCredential`

  ### Expected output

  ```console
  Agent created (id: asst_abc123, name: MyAgent, version: 1)
  Follow-up response created with ID: resp_xyz789
  Delta: Today
  Delta: 's date
  Delta:  is December 12, 2025...
  Follow-up response done!
  URL Citation: https://www.weather.gov/seattle/
  Follow-up completed!
  Full response: Today's date is December 12, 2025, and the weather in Seattle is...
  ```

  ### Grounding with Bing Custom Search (preview)

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      BingCustomSearchAgentTool,
      BingCustomSearchToolParameters,
      BingCustomSearchConfiguration,
  )

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
      project_client.get_openai_client() as openai_client,
  ):
      # Get connection ID from connection name
      bing_custom_connection = project_client.connections.get(
          os.environ["BING_CUSTOM_SEARCH_PROJECT_CONNECTION_NAME"],
      )
      print(f"Grounding with Bing Custom Search connection ID: {bing_custom_connection.id}")

          bing_custom_search_tool = BingCustomSearchAgentTool(
          bing_custom_search_preview=BingCustomSearchToolParameters(
              search_configurations=[
                  BingCustomSearchConfiguration(
                      project_connection_id=bing_custom_connection.id,
                      instance_name=os.environ["BING_CUSTOM_SEARCH_INSTANCE_NAME"],
                  )
              ]
          )
      )

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              instructions="""You are a helpful agent that can use Bing Custom Search tools to assist users.
              Use the available Bing Custom Search tools to answer questions and perform tasks.""",
              tools=[bing_custom_search_tool],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      user_input = input(
          "Enter your question for the Bing Custom Search agent " "(e.g., 'Tell me more about foundry agent service'): \n"
      )

      # Send initial request that will trigger the Bing Custom Search tool
      stream_response = openai_client.responses.create(
          stream=True,
          input=user_input,
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              if event.item.type == "message":
                  item = event.item
                  if item.content[-1].type == "output_text":
                      text_content = item.content[-1]
                      for annotation in text_content.annotations:
                          if annotation.type == "url_citation":
                              print(
                                  f"URL Citation: {annotation.url}, "
                                  f"Start index: {annotation.start_index}, "
                                  f"End index: {annotation.end_index}"
                              )
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")
  ```

  **What this code does**

  This example creates an agent with Grounding with Bing Custom Search tool that searches within a configurable set of public web domains. When you run the code:

  1. It creates an `AIProjectClient` and authenticates by using your Azure credentials.
  2. Creates an agent with the Bing Custom Search tool configured by using your custom search instance.
  3. Prompts for user input asking about specific topics within your configured domains.
  4. The agent uses the Bing Custom Search tool to search only your specified domains and streams the response.
  5. Extracts and displays URL citations with start and end positions from the custom search results.

  **Required inputs**

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `BING_CUSTOM_SEARCH_PROJECT_CONNECTION_NAME`, `BING_CUSTOM_SEARCH_INSTANCE_NAME`
  * Azure credentials configured for `DefaultAzureCredential`
  * User input at runtime

  **Expected output**

  ```console
  Agent created (id: asst_abc123, name: MyAgent, version: 1)
  Enter your question for the Bing Custom Search agent (e.g., 'Tell me more about foundry agent service'):
  Tell me more about foundry agent service
  Follow-up response created with ID: resp_xyz789
  Delta: Microsoft
  Delta:  Foundry
  Delta:  Agent Service...
  Follow-up response done!
  URL Citation: https://learn.microsoft.com/azure/ai-foundry/agents, Start index: 45, End index: 120
  Follow-up completed!
  Full response: Microsoft Foundry Agent Service enables you to build...
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ### Quick verification

  Before running the full samples, verify your Bing connection exists:

  ```csharp
  using Azure.AI.Projects;
  using Azure.Identity;

  var projectEndpoint = System.Environment.GetEnvironmentVariable("AZURE_AI_PROJECT_ENDPOINT");
  var bingConnectionName = System.Environment.GetEnvironmentVariable("BING_PROJECT_CONNECTION_NAME");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Verify Bing connection exists
  try
  {
      AIProjectConnection conn = projectClient.Connections.GetConnection(connectionName: bingConnectionName);
      Console.WriteLine($"Bing connection verified: {conn.Name}");
      Console.WriteLine($"Connection ID: {conn.Id}");
  }
  catch (Exception ex)
  {
      Console.WriteLine($"Bing connection '{bingConnectionName}' not found: {ex.Message}");
      // List available connections
      Console.WriteLine("Available connections:");
      foreach (var conn in projectClient.Connections.GetConnections())
      {
          Console.WriteLine($"  - {conn.Name}");
      }
  }
  ```

  If this code runs without errors, your credentials and Bing connection are configured correctly.

  ### Full samples

  The following C# examples demonstrate how to create an agent with Grounding with Bing Search tool, and how to use the agent to respond to user queries. These examples use synchronous calls for simplicity. For asynchronous examples, see the [agent tools C# samples](https://github.com/Azure/azure-sdk-for-net/tree/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples).

  To enable your Agent to use Bing search API, use `BingGroundingAgentTool`.

  #### Grounding with Bing Search

  ```csharp
  // Read the environment variables, which will be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("AZURE_AI_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("AZURE_AI_MODEL_DEPLOYMENT_NAME");
  var bingConnectionName = System.Environment.GetEnvironmentVariable("BING_PROJECT_CONNECTION_NAME");

  // Create an instance of AIProjectClient.
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Get connection ID from connection name
  AIProjectConnection bingConnection = projectClient.Connections.GetConnection(connectionName: bingConnectionName);

  // Create the agent version with Bing grounding tool
  BingGroundingTool bingGroundingAgentTool = new(new BingGroundingSearchToolOptions(
    searchConfigurations: [new BingGroundingSearchConfiguration(projectConnectionId: bingConnection.Id)]
      )
  );
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful agent.",
      Tools = { bingGroundingAgentTool, }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Output the agent version info
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  ResponseResult response = responseClient.CreateResponse("How does wikipedia explain Euler's Identity?");

  // Extract and format URL citation annotations
  string citation = "";
  foreach (ResponseItem item in response.OutputItems)
  {
      if (item is MessageResponseItem messageItem)
      {
          foreach (ResponseContentPart content in messageItem.Content)
          {
              foreach (ResponseMessageAnnotation annotation in content.OutputTextAnnotations)
              {
                  if (annotation is UriCitationMessageAnnotation uriAnnotation)
                  {
                      citation = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                  }
              }
          }
      }
  }

  // Validate and print the response
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine($"{response.GetOutputText()}{citation}");

  // Clean up resources by deleting the agent version
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### What this code does

  This example creates an agent that uses the Grounding with Bing Search tool and demonstrates synchronous response handling. When you run the code:

  1. It creates an AIProjectClient by using your project endpoint.
  2. Retrieves the Bing connection configuration from your project.
  3. Creates an agent with the Bing grounding tool configured.
  4. Sends a query asking how Wikipedia explains Euler's Identity.
  5. The agent uses the Bing grounding tool to search and returns formatted results with URL citations.
  6. Cleans up by deleting the agent version.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `BING_PROJECT_CONNECTION_NAME`
  * Azure credentials configured for `DefaultAzureCredential`

  ### Expected output

  ```console
  Euler's identity is considered one of the most elegant equations in mathematics... [Euler's identity - Wikipedia](https://en.wikipedia.org/wiki/Euler%27s_identity)
  ```

  ## Grounding with Bing in streaming scenarios

  ```csharp
  // Read the environment variables, which will be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("AZURE_AI_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("AZURE_AI_MODEL_DEPLOYMENT_NAME");
  var bingConnectionName = System.Environment.GetEnvironmentVariable("BING_PROJECT_CONNECTION_NAME");

  // Create an instance of AIProjectClient
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Get connection ID from connection name
  AIProjectConnection bingConnection = projectClient.Connections.GetConnection(connectionName: bingConnectionName);

  // Create the agent version with Bing grounding tool
  BingGroundingTool bingGroundingAgentTool = new(new BingGroundingSearchToolOptions(
    searchConfigurations: [new BingGroundingSearchConfiguration(projectConnectionId: bingConnection.Id)]
      )
  );
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful agent.",
      Tools = { bingGroundingAgentTool }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Stream the response from the agent version
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  string annotation = "";
  string text = "";

  // Parse the streaming response and output the results
  foreach (StreamingResponseUpdate streamResponse in responseClient.CreateResponseStreaming("How does wikipedia explain Euler's Identity?"))
  {
      if (streamResponse is StreamingResponseCreatedUpdate createUpdate)
      {
          Console.WriteLine($"Stream response created with ID: {createUpdate.Response.Id}");
      }
      else if (streamResponse is StreamingResponseOutputTextDeltaUpdate textDelta)
      {
          Console.WriteLine($"Delta: {textDelta.Delta}");
      }
      else if (streamResponse is StreamingResponseOutputTextDoneUpdate textDoneUpdate)
      {
          text = textDoneUpdate.Text;
      }
      else if (streamResponse is StreamingResponseOutputItemDoneUpdate itemDoneUpdate)
      {
          if (annotation.Length == 0)
          {
              // Extract and format URL citation annotations
              if (itemDoneUpdate.Item is MessageResponseItem messageItem)
              {
                  foreach (ResponseContentPart content in messageItem.Content)
                  {
                      foreach (ResponseMessageAnnotation messageAnnotation in content.OutputTextAnnotations)
                      {
                          if (messageAnnotation is UriCitationMessageAnnotation uriAnnotation)
                          {
                              annotation = $" [{uriAnnotation.Title}]({uriAnnotation.Uri})";
                          }
                      }
                  }
              }
          }
      }
      else if (streamResponse is StreamingResponseErrorUpdate errorUpdate)
      {
          throw new InvalidOperationException($"The stream has failed: {errorUpdate.Message}");
      }
  }
  Console.WriteLine($"{text}{annotation}");

  // Clean up resources by deleting the agent version
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### What this code does

  This example creates an agent with grounding by using the Bing Search tool and demonstrates streaming response handling. When you run the code:

  1. It creates an AIProjectClient by using your project endpoint.
  2. Retrieves the Bing connection configuration from your project.
  3. Creates an agent with the Bing grounding tool configured.
  4. Sends a query asking how Wikipedia explains Euler's Identity.
  5. The agent uses the Bing grounding tool and streams the response in real-time.
  6. Processes streaming events including delta text updates and citation extraction.
  7. Cleans up by deleting the agent version.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `BING_PROJECT_CONNECTION_NAME`
  * Azure credentials configured for `DefaultAzureCredential`

  ### Expected output

  ```console
  Stream response created with ID: resp_xyz789
  Delta: Euler
  Delta: 's
  Delta:  Identity
  Delta:  is one of the most...
  Euler's Identity is one of the most elegant equations in mathematics... [Euler's identity - Wikipedia](https://en.wikipedia.org/wiki/Euler%27s_identity)
  ```
</ZonePivot>

<ZonePivot pivot="rest">
  The following REST API examples demonstrate how to use Grounding with Bing Search and Grounding with Bing Custom Search (preview) tools to respond to user queries.

  ### Grounding with Bing Search

  ### Authentication setup

  Before running REST API calls, configure authentication:

  1. Set environment variables:

     * `AZURE_AI_PROJECT_ENDPOINT`: Your Foundry project endpoint URL.
     * `API_VERSION`: API version (for example, `2025-11-15-preview`).
     * `AZURE_AI_MODEL_DEPLOYMENT_NAME`: Your deployed model name.
     * `BING_PROJECT_CONNECTION_ID`: Your Grounding with Bing Search project connection ID.

  * `AGENT_TOKEN`: A bearer token for your user or service principal.

  1. Obtain a bearer token:
     ```azurecli
     az account get-access-token --resource https://ai.azure.com --query accessToken -o tsv
     ```

  Save the output as the `AGENT_TOKEN` environment variable.

  ```bash
  curl --request POST \
    --url "$AZURE_AI_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
    "model": "'$AZURE_AI_MODEL_DEPLOYMENT_NAME'",
    "input": "How does Wikipedia explain Euler\u0027s identity?",
    "tool_choice": "required",
    "tools": [
      {
        "type": "bing_grounding",
        "bing_grounding": {
          "search_configurations": [
            {
              "project_connection_id": "'$BING_PROJECT_CONNECTION_ID'",
              "count": 7,
              "market": "en-US",
              "set_lang": "en",
              "freshness": "7d"
            }
          ]
        }
      }
    ]
  }'
  ```

  ### What this code does

  This REST API request creates a response using Grounding with Bing Search. The request:

  1. Sends a POST request to the Foundry responses endpoint.
  2. Includes the model deployment and user input in the request body.
  3. Configures the Bing grounding tool with search parameters (count, market, language, freshness).
  4. Returns a response with web search results and citations.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `API_VERSION`, `AGENT_TOKEN`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `BING_PROJECT_CONNECTION_ID`.
  * Valid bearer token with appropriate permissions.

  ### Expected output

  JSON response with:

  * `id`: Response identifier
  * `output_text`: Generated text with grounded information
  * `citations`: Array of URL citations used to generate the response

  ### Grounding with Bing Custom Search (preview)

  ```bash
  curl --request POST \
    --url "$AZURE_AI_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
    "model": "'$AZURE_AI_MODEL_DEPLOYMENT_NAME'",
    "input": "How does Wikipedia explain Euler\u0027s identity?",
    "tool_choice": "required",
    "tools": [
      {
        "type": "bing_custom_search_preview",
        "bing_custom_search_preview": {
          "search_configurations": [
            {
              "project_connection_id": "'$BING_CUSTOM_SEARCH_PROJECT_CONNECTION_ID'",
              "instance_name": "'$BING_CUSTOM_SEARCH_INSTANCE_NAME'",
              "count": 7,
              "market": "en-US",
              "set_lang": "en",
              "freshness": "7d"
            }
          ]
        }
      }
    ]
  }'
  ```

  ### What this code does

  This REST API request creates a response using Grounding with Bing Custom Search. The request:

  1. Sends a POST request to the Foundry responses endpoint.
  2. Includes the model deployment and user input in the request body.
  3. Configures the Bing Custom Search tool with your instance name and search parameters.
  4. Returns a response with custom search results limited to your configured domains.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `API_VERSION`, `AGENT_TOKEN`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `BING_CUSTOM_SEARCH_PROJECT_CONNECTION_ID`, `BING_CUSTOM_SEARCH_INSTANCE_NAME`
  * Valid bearer token with appropriate permissions.
  * Bing Custom Search instance already configured with target domains

  ### Expected output

  JSON response with:

  * `id`: Response identifier
  * `output_text`: Generated text with information from your custom domain set
  * `citations`: Array of URL citations from your configured domains
</ZonePivot>

<ZonePivot pivot="typescript">
  ### Quick verification

  Before running the full samples, verify your Bing connection exists:

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const bingConnectionName = process.env["BING_PROJECT_CONNECTION_NAME"] || "<bing connection name>";

  async function verifyConnection(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    console.log("Connected to project.");

    try {
      const conn = await project.connections.get(bingConnectionName);
      console.log(`Bing connection verified: ${conn.name}`);
      console.log(`Connection ID: ${conn.id}`);
    } catch (error) {
      console.log(`Bing connection '${bingConnectionName}' not found: ${error}`);
      // List available connections
      console.log("Available connections:");
      for await (const conn of project.connections.list()) {
        console.log(`  - ${conn.name}`);
      }
    }
  }

  verifyConnection().catch(console.error);
  ```

  If this code runs without errors, your credentials and Bing connection are configured correctly.

  ### Full samples

  The following TypeScript examples demonstrate how to create an agent with Grounding with Bing Search and Grounding with Bing Custom Search (preview) tools, and how to use the agent to respond to user queries. For JavaScript examples, see the [agent tools JavaScript samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools) in the Azure SDK for JavaScript repository on GitHub.

  #### Grounding with Bing Search

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
    process.env["AZURE_AI_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const bingConnectionName =
    process.env["BING_PROJECT_CONNECTION_NAME"] || "<bing connection name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Get connection ID from connection name
    const bingConnection = await project.connections.get(bingConnectionName);
    console.log(`Bing connection ID: ${bingConnection.id}`);

    console.log("Creating agent with Bing grounding tool...");

    const agent = await project.agents.createVersion("MyBingGroundingAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions: "You are a helpful assistant.",
      tools: [
        {
          type: "bing_grounding",
          bing_grounding: {
            search_configurations: [
              {
                project_connection_id: bingConnection.id,
              },
            ],
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Send request that requires current information from the web
    console.log("\nSending request to Bing grounding agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: "What is today's date and weather in Seattle?",
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        if (event.item.type === "message") {
          const item = event.item;
          if (item.content && item.content.length > 0) {
            const lastContent = item.content[item.content.length - 1];
            if (lastContent.type === "output_text" && lastContent.annotations) {
              for (const annotation of lastContent.annotations) {
                if (annotation.type === "url_citation") {
                  console.log(
                    `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                  );
                }
              }
            }
          }
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nBing grounding agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  **What this code does**

  This example creates an agent with grounding by using the Bing Search tool that can retrieve real-time information from the web. When you run the code:

  1. It creates an `AIProjectClient` and authenticates by using your Azure credentials.
  2. Creates an agent with the Bing grounding tool configured by using your Bing connection.
  3. Sends a query asking about current date and weather in Seattle with tool choice set to "required".
  4. The agent uses the Bing grounding tool to search the web and streams the response.
  5. Processes streaming events and extracts URL citations with their positions in the text.
  6. Cleans up by deleting the agent version.

  **Required inputs**

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `BING_PROJECT_CONNECTION_NAME`
  * Azure credentials configured for `DefaultAzureCredential`

  **Expected output**

  ```console
  Creating agent with Bing grounding tool...
  Agent created (id: asst_abc123, name: MyBingGroundingAgent, version: 1)

  Sending request to Bing grounding agent with streaming...
  Follow-up response created with ID: resp_xyz789
  Today's date is December 12, 2025, and the weather in Seattle...

  Follow-up response done!
  URL Citation: https://www.weather.gov/seattle/, Start index: 45, End index: 120

  Follow-up completed!

  Cleaning up resources...
  Agent deleted

  Bing grounding agent sample completed!
  ```

  ### Grounding with Bing Custom Search (preview)

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as readline from "readline";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
    process.env["AZURE_AI_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const bingCustomSearchConnectionName =
    process.env["BING_CUSTOM_SEARCH_PROJECT_CONNECTION_NAME"] ||
    "<bing custom search connection name>";
  const bingCustomSearchInstanceName =
    process.env["BING_CUSTOM_SEARCH_INSTANCE_NAME"] || "<bing custom search instance name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Get connection ID from connection name
    const bingCustomConnection = await project.connections.get(bingCustomSearchConnectionName);
    console.log(`Bing Custom Search connection ID: ${bingCustomConnection.id}`);

    console.log("Creating agent with Bing Custom Search tool...");

    const agent = await project.agents.createVersion("MyAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a helpful agent that can use Bing Custom Search tools to assist users. Use the available Bing Custom Search tools to answer questions and perform tasks.",
      tools: [
        {
          type: "bing_custom_search_preview",
          bing_custom_search_preview: {
            search_configurations: [
              {
                project_connection_id: bingCustomConnection.id,
                instance_name: bingCustomSearchInstanceName,
              },
            ],
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Prompt user for input
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    });

    const userInput = await new Promise<string>((resolve) => {
      rl.question(
        "Enter your question for the Bing Custom Search agent (e.g., 'Tell me more about foundry agent service'): \n",
        (answer) => {
          rl.close();
          resolve(answer);
        },
      );
    });

    // Send initial request that will trigger the Bing Custom Search tool
    console.log("\nSending request to Bing Custom Search agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: userInput,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        if (event.item.type === "message") {
          const item = event.item;
          if (item.content && item.content.length > 0) {
            const lastContent = item.content[item.content.length - 1];
            if (lastContent.type === "output_text" && lastContent.annotations) {
              for (const annotation of lastContent.annotations) {
                if (annotation.type === "url_citation") {
                  console.log(
                    `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                  );
                }
              }
            }
          }
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nBing Custom Search agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  **What this code does**

  This example creates an agent with Grounding with Bing Custom Search tool that searches within your configured domains. When you run the code:

  1. It creates an `AIProjectClient` and authenticates by using your Azure credentials.
  2. Creates an agent with the Bing Custom Search tool configured by using your custom search instance.
  3. Prompts for user input at runtime through the command line.
  4. The agent uses the Bing Custom Search tool to search only your specified domains and streams the response.
  5. Processes streaming events and extracts URL citations with their positions in the text.
  6. Cleans up by deleting the agent version.

  **Required inputs**

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `BING_CUSTOM_SEARCH_PROJECT_CONNECTION_NAME`, `BING_CUSTOM_SEARCH_INSTANCE_NAME`
  * Azure credentials configured for `DefaultAzureCredential`
  * User input provided at runtime via console

  **Expected output**

  ```console
  Creating agent with Bing Custom Search tool...
  Agent created (id: asst_abc123, name: MyAgent, version: 1)
  Enter your question for the Bing Custom Search agent (e.g., 'Tell me more about foundry agent service'):
  Tell me more about foundry agent service

  Sending request to Bing Custom Search agent with streaming...
  Follow-up response created with ID: resp_xyz789
  Microsoft Foundry Agent Service enables you to build...

  Follow-up response done!
  URL Citation: https://learn.microsoft.com/azure/ai-foundry/agents, Start index: 0, End index: 89

  Follow-up completed!

  Cleaning up resources...
  Agent deleted

  Bing Custom Search agent sample completed!
  ```
</ZonePivot>

## How it works

The user query is the message that an end user sends to an agent, such as *"should I take an umbrella with me today? I'm in Seattle."* Instructions are the system message a developer can provide to share context and provide instructions to the AI model on how to use various tools or behave.

When a user sends a query, the customer's AI model deployment first processes it (using the provided instructions) to later perform a Bing search query (which is [visible to developers](#how-to-display-search-results)). Grounding with Bing returns relevant search results to the customer's model deployment, which then generates the final output.

<Callout type="note">
  When you use Grounding with Bing Search or Grounding with Bing Custom Search, the only information sent to Bing is the Bing search query, tool parameters, and your resource key. The service doesn't send any end user-specific information. Your resource key is sent to Bing solely for billing and rate limiting purposes.
</Callout>

Authorization happens between the Grounding with Bing Search or Grounding with Bing Custom Search service and Foundry Agent Service. Any Bing search query that the service generates and sends to Bing for the purposes of grounding is transferred, along with the resource key, outside of the Azure compliance boundary to the Grounding with Bing Search service. Grounding with Bing Search is subject to Bing's terms and doesn't have the same compliance standards and certifications as the Agent Service, as described in the [Terms of Use](https://www.microsoft.com/bing/apis/grounding-legal-enterprise). You're responsible for assessing whether the use of Grounding with Bing Search or Grounding with Bing Custom Search in your agent meets your needs and requirements.

Transactions with your Grounding with Bing resource are counted by the number of tool calls per run. You can see how many tool calls are made from the run step.

Developers and end users don't have access to raw content returned from Grounding with Bing Search. The model response, however, includes citations with links to the websites used to generate the response, and a link to the Bing query used for the search. You can retrieve the **model response** by accessing the data in the conversation that was created. These two *references* must be retained and displayed in the exact form provided by Microsoft, as per Grounding with Bing Search's [Use and Display Requirements](https://www.microsoft.com/bing/apis/grounding-legal-enterprise#use-and-display-requirements). See the [how to display Grounding with Bing Search results](#how-to-display-search-results) section for details.

## How to display search results

According to Grounding with Bing's [terms of use and use and display requirements](https://www.microsoft.com/bing/apis/grounding-legal-enterprise#use-and-display-requirements#use-and-display-requirements), you need to display both website URLs and Bing search query URLs in your custom interface. You can find this information in the API response, in the `arguments` parameter. To render the webpage, replace the endpoint of Bing search query URLs with `www.bing.com` and your Bing search query URL would look like `https://www.bing.com/search?q={search query}`.

![A screenshot showing citations for Bing search results.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/bing/website-citations.png)

## Grounding with Bing Custom Search configuration

Grounding with Bing Custom Search is a powerful tool that you can use to select a subspace of the web to limit your agent’s grounding knowledge. Here are a few tips to help you take full advantage of this capability:

* If you own a public site that you want to include in the search but Bing hasn't indexed, see the [Bing Webmaster Guidelines](https://www.bing.com/webmasters/help/webmasters-guidelines-30fba23a) for details about getting your site indexed. The webmaster documentation also provides details about getting Bing to crawl your site if the index is out of date.

* You need at least the contributor role for the Bing Custom Search resource to create a configuration.

* You can only block certain domains and perform a search against the rest of the web (a competitor's site, for example).

* Grounding with Bing Custom Search only returns results for domains and webpages that are public and indexed by Bing.

  * Domain (for example, `https://www.microsoft.com`)
  * Domain and path (for example, `https://www.microsoft.com/surface`)
  * Webpage (for example, `https://www.microsoft.com/en-us/p/surface-earbuds/8r9cpq146064`)

## Optional parameters

When you add the Grounding with Bing Search or Grounding with Bing Custom Search tool to your agent, you can pass the following parameters. These parameters will impact the tool output, and the AI model might not fully use all of the outputs. See the code examples for information on API version support and how to pass these parameters.

| Name        | Value                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | Type            | Required |
| ----------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------- | -------- |
| `count`     | The number of search results to return in the response. The default is 5 and the maximum value is 50. The actual number delivered may be less than requested. It is possible for multiple pages to include some overlap in results. This parameter affects only web page results. It's possible that AI model might not use all search results returned by Bing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | `UnsignedShort` | No       |
| `freshness` | Filter search results by the following case-insensitive age values: **Day**: Return webpages that Bing discovered within the last 24 hours. **Week**: Return webpages that Bing discovered within the last 7 days. **Month**: Return webpages that Bing discovered within the last 30 days. To get articles discovered by Bing during a specific timeframe, specify a date range in the form: `YYYY-MM-DD..YYYY-MM-DD`. For example, `freshness=2019-02-01..2019-05-30`. To limit the results to a single date, set this parameter to a specific date. For example, `freshness=2019-02-04`.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | String          | No       |
| `market`    | The market where the results come from. Typically, `mkt` is the country/region where the user is making the request from. However, it could be a different country/region if the user is not located in a country/region where Bing delivers results. The market must be in the form: `<language>-<country/region>`. For example, `en-US`. The string is case insensitive. For a list of possible market values, see [Market codes](https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/reference/market-codes). If known, you are encouraged to always specify the market. Specifying the market helps Bing route the request and return an appropriate and optimal response. If you specify a market that is not listed in Market codes, Bing uses a best fit market code based on an internal mapping that is subject to change.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | String          | No       |
| `set_lang`  | The language to use for user interface strings. You may specify the language using either a 2-letter or 4-letter code. Using 4-letter codes is preferred. For a list of supported language codes, see [Bing supported languages](https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/reference/market-codes#bing-supported-language-codes). Bing loads the localized strings if `setlang` contains a valid 2-letter neutral culture code (`fr`) or a valid 4-letter specific culture code (`fr-ca`). For example, for `fr-ca`, Bing loads the `fr` neutral culture code strings. If `setlang` is not valid (for example, `zh`) or Bing doesn’t support the language (for example, `af`, `af-na`), Bing defaults to `en` (English). To specify the 2-letter code, set this parameter to an ISO 639-1 language code. To specify the 4-letter code, use the form `<language>-<country/region>` where `<language>` is an ISO 639-1 language code (neutral culture) and `<country/region>` is an ISO 3166 country/region (specific culture) code. For example, use `en-US` for United States English. Although optional, you should always specify the language. Typically, you set `setLang` to the same language specified by `mkt` unless the user wants the user interface strings displayed in a different language. | String          | No       |

## Supported capabilities and known issues

* The Grounding with Bing Search tool is designed to retrieve real-time information from the web, not specific web domains. To retrieve information from specific domains, use the Grounding with Bing Custom Search tool.
* Don't ask the model to **summarize** an entire web page.
* Within one run, the AI model evaluates the tool outputs and might decide to invoke the tool again for more information and context. The AI model might also decide which pieces of tool outputs are used to generate the response.
* Foundry Agent Service returns **AI model generated responses** as output, so end-to-end latency is impacted by model pre-processing and post-processing.
* The Grounding with Bing Search and Grounding with Bing Custom Search tools don't return the tool output to developers and end users.
* Grounding with Bing Search and Grounding with Bing Custom Search only work with agents that aren't using VPN or private endpoints. The agent must have normal network access.
* Use the default citations pattern (the links sent in `annotation`) for links from the Grounding with Bing tools. Don't ask the model to generate citation links.

## Troubleshooting

Use this section to resolve common issues when using Grounding with Bing Search tools.

### Connection ID format errors

**Problem**: Error message stating invalid connection ID format.

**Solution**: Verify your connection ID matches the required format:

```
/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}
```

Replace all placeholder values (including `{{` and `}}`) with your actual resource identifiers.

### Authentication failures

**Problem**: "Unauthorized" or "Forbidden" errors when creating agents or running queries.

**Solution**:

1. Verify you have the required RBAC roles:

   * **Contributor** or **Owner** role for creating Bing resources
   * **Azure AI Project Manager** role for creating project connections

2. Check that your Azure credentials are properly configured:

   * For Python/TypeScript: `DefaultAzureCredential` can authenticate
   * For REST: Bearer token is valid and not expired

3. Run `az login` to refresh your credentials if you're using Azure CLI

### Network connectivity problems

**Problem**: Grounding with Bing Search requests time out or can't connect.

**Solution**: Grounding with Bing Search and Grounding with Bing Custom Search don't work with VPN or Private Endpoints. Ensure:

* Your network has normal internet access.
* You're not using a VPN connection.
* Private Endpoints aren't configured for the agent service.
* Firewall rules allow outbound connections to Bing services.

### Custom search returns no results

**Problem**: Bing Custom Search returns empty results or doesn't find expected content.

**Solution**:

* Verify your custom search instance is properly configured with target domains.
* Ensure the domains you want to search are public and indexed by Bing.
* Check that the configured domains match your search query expectations.
* If your site isn't indexed, see [Bing Webmaster Guidelines](https://www.bing.com/webmasters/help/webmasters-guidelines-30fba23a) for indexing instructions.
* Wait for Bing to crawl recently added or updated content (can take several days).

### Missing or invalid environment variables

**Problem**: Code fails with `KeyError` or "environment variable not found" errors.

**Solution**: Ensure you set all required environment variables:

* `AZURE_AI_PROJECT_ENDPOINT`
* `AZURE_AI_MODEL_DEPLOYMENT_NAME`
* `BING_PROJECT_CONNECTION_ID` or `BING_CUSTOM_SEARCH_PROJECT_CONNECTION_ID`
* For custom search: `BING_CUSTOM_SEARCH_INSTANCE_NAME`
* For REST API: `API_VERSION`, `AGENT_TOKEN`

Create a `.env` file or set system environment variables with these values.

### Agent doesn't use the grounding tool

**Problem**: Agent responds without calling the Bing grounding tool.

**Solution**:

* Ensure your query requires current information that the model doesn't know.
* For explicit tool usage, set `tool_choice="required"` in your request (Python/TypeScript examples show this).
* Verify the tool is properly configured in the agent definition.
* Check agent instructions encourage using available tools for current information.

### Instance name not found for Grounding with Bing Custom Search tool

**Problem**:

```json
{"error": "Tool_User_Error", "message": "[bing_search] Failed to call Get Custom Search Instance with status 404: {\"error\":{\"code\":\"ResourceNotFound\",\"message\":\"Instance or Customer not found\",\"target\":\"instanceName or customerId\"}}."}
```

**Solution**:

* Ensure your instance name is in the Grounding with Bing Custom Search resource you are using.
* Double check if your instance name is spelled correctly.

## Manage Grounding with Bing Search and Grounding with Bing Custom Search

Admins can use RBAC role assignments to enable or disable the use of Grounding with Bing and Grounding with Bing Custom Search within the subscription or resource group.

1. The admin registers `Microsoft.Bing` in the Azure subscription. The admin needs permissions to perform the `/register/action` operation for the resource provider. The Contributor and Owner roles include this permission. For more information about how to register, see [Azure resource providers and types](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-providers-and-types).
2. After the admin registers `Microsoft.Bing`, users with permissions can create, delete, or retrieve the resource key for a Grounding with Bing and/or Grounding with Bing Custom Search resource. These users need the **Contributor** or **Owner** role at the subscription or resource group level.
3. After creating a Grounding with Bing and/or Grounding with Bing Custom Search resource, users with permissions can create a Microsoft Foundry connection to connect to the resource and use it as a tool in Foundry Agent Service. These users need at least the **Azure AI Project Manager** role.

### Disable use of Grounding with Bing Search and Grounding with Bing Custom Search

1. The admin needs the **Owner** or **Contributor** role in the subscription.
2. The admin deletes all Grounding with Bing Search and Grounding with Bing Custom Search resources in the subscription.
3. The admin unregisters the `Microsoft.Bing` resource provider in the subscription (you can't unregister before deleting all resources). For more information, see [Azure resource providers and types](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/resource-providers-and-types).
4. The admin creates an Azure Policy to disallow creation of Grounding with Bing Search and Grounding with Bing Custom Search resources in their subscription, following the [sample](https://github.com/azure-ai-foundry/foundry-samples/blob/main/infrastructure/infrastructure-setup-bicep/05-custom-policy-definitions/deny-disallowed-connections.json).

## Next steps

* [Tool use best practices](../../concepts/tool-best-practice) - Learn optimization strategies for agent tools
* [Web search tool (preview)](web-search) - Use web search without configuring Bing tool parameters
* [Manage Grounding with Bing in Microsoft Foundry and Azure](../manage-grounding-with-bing) - Control and disable Grounding with Bing features
* [Connect OpenAPI tools to agents](openapi) - Integrate custom APIs with your agents
* [Discover tools in the Foundry Tools (preview)](../../concepts/tool-catalog) - Explore all available agent tools in Foundry Agent Service

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="note">
  See [best practices](../../concepts/tool-best-practice) for information on optimizing tool usage.
</Callout>

Use the [**Microsoft Fabric data agent**](https://go.microsoft.com/fwlink/?linkid=2312815) with Foundry Agent Service to analyze enterprise data in chat. The Fabric data agent turns enterprise data into a conversational question and answer experience.

First, build and publish a Fabric data agent. Then, connect your Fabric data agent with the published endpoint. When a user sends a query, the agent determines if it should use the Fabric data agent. If so, it uses the end user's identity to generate queries over data they have access to. Lastly, the agent generates responses based on queries returned from the Fabric data agent. By using identity passthrough (On-Behalf-Of) authorization, this integration simplifies access to enterprise data in Fabric while maintaining robust security, ensuring proper access control and enterprise-grade protection.

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

<Callout type="note">
  * The model you select during agent setup is only used for orchestration and response generation. It doesn't affect which model the Fabric data agent uses for NL2SQL.
  * To help your agent invoke the Fabric tool reliably, include clear tool guidance in your agent instructions (for example, "For customer and product sales data, use the Fabric tool"). You can also force tool use with `tool_choice`.
</Callout>

* Create and publish a [Fabric data agent](https://go.microsoft.com/fwlink/?linkid=2312910).

* Assign developers and end users at least the `Azure AI User` Azure RBAC role. For more information, see [Azure role-based access control in Foundry](../../../concepts/rbac-foundry).

* Give developers and end users at least `READ` access to the Fabric data agent and the underlying data sources it connects to.

* Ensure your Fabric data agent and Foundry project are in the same tenant.

* Use user identity authentication. Service principal authentication isn't supported for the Fabric data agent.

* Get these values before you run the samples:

  * Your Foundry project endpoint: `AZURE_AI_PROJECT_ENDPOINT`.
  * Your model deployment name: `AZURE_AI_MODEL_DEPLOYMENT_NAME`.
  * Your Fabric connection ID (project connection ID): `FABRIC_PROJECT_CONNECTION_ID`.

* For the REST sample, also set:

  * `API_VERSION`.

  * `AGENT_TOKEN` (a bearer token). You can get a temporary token with Azure CLI:

    ```azurecli
    az account get-access-token --scope https://ai.azure.com/.default
    ```

  ## Set up the Microsoft Fabric connection

  Before you run the samples, create a project connection to your Fabric data agent.

  1. In Microsoft Fabric, open your data agent.
  2. Copy the `workspace_id` and `artifact_id` values from the URL.

  The URL path looks similar to `.../groups/<workspace_id>/aiskills/<artifact_id>...`. Both values are GUIDs.

  1. In the Foundry portal, open your project.
  2. In the left pane, select **Management center**, and then select **Connected resources**.
  3. Create a connection of type **Microsoft Fabric**.
  4. Enter the `workspace_id` and `artifact_id` values.
  5. Save the connection, and then copy the connection **ID**.

  Use the connection ID as the value for `FABRIC_PROJECT_CONNECTION_ID`. The value looks like `/subscriptions/<subscriptionId>/resourceGroups/<resourceGroupName>/providers/Microsoft.CognitiveServices/accounts/<foundryAccountName>/projects/<foundryProjectName>/connections/<connectionName>`.

  ## Identity passthrough and access control

  This integration uses identity passthrough (On-Behalf-Of). The Fabric tool runs queries by using the identity of the signed-in user.

  * Give each end user access to the Fabric data agent and its underlying data sources, or the tool call fails.
  * Use user identity authentication. Service principal authentication isn't supported for the Fabric data agent.
  * For more information about how agent identity works, see [Agent identity](../../concepts/agent-identity).

## Code example

<Callout type="note">
  * To run this code, you need the latest prerelease package. For more information, see [Get ready to code](../../../quickstarts/get-started-code#get-ready-to-code).
  * Your connection ID should be in the format of `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`.
</Callout>

<ZonePivot pivot="python">
  ### Quick verification

  Before running the full sample, verify your Fabric connection exists:

  ```python
  import os

  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential
  from dotenv import load_dotenv

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
  ):
      print("Connected to project.")

      # Verify Fabric connection exists
      connection_name = os.environ.get("FABRIC_PROJECT_CONNECTION_NAME")
      if connection_name:
          try:
              conn = project_client.connections.get(connection_name)
              print(f"Fabric connection verified: {conn.name}")
              print(f"Connection ID: {conn.id}")
          except Exception as e:
              print(f"Fabric connection '{connection_name}' not found: {e}")
      else:
          # List available connections to help find the right one
          print("FABRIC_PROJECT_CONNECTION_NAME not set. Available connections:")
          for conn in project_client.connections.list():
              print(f"  - {conn.name}")
  ```

  If this code runs without errors, your credentials and Fabric connection are configured correctly.

  ### Full sample

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      MicrosoftFabricAgentTool,
      FabricDataAgentToolParameters,
      ToolProjectConnection,
  )

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
      project_client.get_openai_client() as openai_client,
  ):
      # Get connection ID from connection name
      fabric_connection = project_client.connections.get(
          os.environ["FABRIC_PROJECT_CONNECTION_NAME"],
      )
      print(f"Fabric connection ID: {fabric_connection.id}")

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              instructions="You are a helpful assistant.",
              tools=[
                  MicrosoftFabricAgentTool(
                      fabric_dataagent_preview=FabricDataAgentToolParameters(
                          project_connections=[
                              ToolProjectConnection(project_connection_id=fabric_connection.id)
                          ]
                      )
                  )
              ],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      user_input = input("Enter your question for Fabric (e.g., 'Tell me about sales records'): \n")

      response = openai_client.responses.create(
          tool_choice="required",
          input=user_input,
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      print(f"Response output: {response.output_text}")
  ```

  ### What this code does

  1. Creates an `AIProjectClient` using `DefaultAzureCredential`.
  2. Creates an agent version configured with the Fabric data agent tool.
  3. Prompts you for a question.
  4. Calls the Responses API with `tool_choice="required"` to force tool use.
  5. Prints the agent response.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `FABRIC_PROJECT_CONNECTION_ID`.
  * Authentication: `DefaultAzureCredential` must be able to obtain a token (for example, via `az login`).

  ### Expected output

  * A line confirming agent creation.
  * A line that starts with `Response output:` followed by the response text.

  For more details, see the [full Python sample for Fabric data agent](https://github.com/Azure/azure-sdk-for-python/blob/main/sdk/ai/azure-ai-agents/samples/agents_tools/sample_agents_fabric.py).
</ZonePivot>

<ZonePivot pivot="csharp">
  ### Quick verification

  Before running the full sample, verify your Fabric connection exists:

  ```csharp
  using Azure.AI.Projects;
  using Azure.Identity;

  var projectEndpoint = System.Environment.GetEnvironmentVariable("AZURE_AI_PROJECT_ENDPOINT");
  var fabricConnectionName = System.Environment.GetEnvironmentVariable("FABRIC_PROJECT_CONNECTION_NAME");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Verify Fabric connection exists
  try
  {
      AIProjectConnection conn = projectClient.Connections.GetConnection(connectionName: fabricConnectionName);
      Console.WriteLine($"Fabric connection verified: {conn.Name}");
      Console.WriteLine($"Connection ID: {conn.Id}");
  }
  catch (Exception ex)
  {
      Console.WriteLine($"Fabric connection '{fabricConnectionName}' not found: {ex.Message}");
      // List available connections
      Console.WriteLine("Available connections:");
      foreach (var conn in projectClient.Connections.GetConnections())
      {
          Console.WriteLine($"  - {conn.Name}");
      }
  }
  ```

  If this code runs without errors, your credentials and Fabric connection are configured correctly.

  ### Full sample

  To enable your agent to access the Fabric data agent, use `MicrosoftFabricAgentTool`.

  ```csharp
  // Create an Agent client and read the environment variables, which will be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("AZURE_AI_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("AZURE_AI_MODEL_DEPLOYMENT_NAME");
  var fabricConnectionName = System.Environment.GetEnvironmentVariable("FABRIC_PROJECT_CONNECTION_NAME");
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Get connection ID from connection name
  AIProjectConnection fabricConnection = projectClient.Connections.GetConnection(connectionName: fabricConnectionName);

  FabricDataAgentToolOptions fabricToolOption = new()
  {
      ProjectConnections = { new ToolProjectConnection(projectConnectionId: fabricConnection.Id) }
  };
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant.",
      Tools = { new MicrosoftFabricPreviewTool(fabricToolOption), }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create the response and make sure we are always using tool.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  CreateResponseOptions responseOptions = new()
  {
      ToolChoice = ResponseToolChoice.CreateRequiredChoice(),
      InputItems = { ResponseItem.CreateUserMessageItem("What was the number of public holidays in Norway in 2024?") },
  };
  ResponseResult response = responseClient.CreateResponse(options: responseOptions);

  // Print the Agent output.
  Assert.That(response.Status, Is.EqualTo(ResponseStatus.Completed));
  Console.WriteLine(response.GetOutputText());

  // Delete the Agent version to clean up resources.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### What this code does

  1. Creates an `AIProjectClient` using `DefaultAzureCredential`.
  2. Configures the Fabric data agent tool by using your project connection ID.
  3. Creates an agent version.
  4. Sends a question through the agent and forces tool usage.
  5. Writes the response text and deletes the agent version.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `FABRIC_PROJECT_CONNECTION_ID`.
  * Authentication: `DefaultAzureCredential` must be able to obtain a token (for example, via `az login`).

  ### Expected output

  * The response text printed to the console. For the sample question, the response should include the number of public holidays (for example, `62`).
</ZonePivot>

<ZonePivot pivot="typescript">
  ### Quick verification

  Before running the full sample, verify your Fabric connection exists:

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const fabricConnectionName = process.env["FABRIC_PROJECT_CONNECTION_NAME"] || "<fabric connection name>";

  async function verifyConnection(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    console.log("Connected to project.");

    try {
      const conn = await project.connections.get(fabricConnectionName);
      console.log(`Fabric connection verified: ${conn.name}`);
      console.log(`Connection ID: ${conn.id}`);
    } catch (error) {
      console.log(`Fabric connection '${fabricConnectionName}' not found: ${error}`);
      // List available connections
      console.log("Available connections:");
      for await (const conn of project.connections.list()) {
        console.log(`  - ${conn.name}`);
      }
    }
  }

  verifyConnection().catch(console.error);
  ```

  If this code runs without errors, your credentials and Fabric connection are configured correctly.

  ### Full sample

  The following TypeScript example demonstrates how to create an AI agent with Microsoft Fabric capabilities by using the `MicrosoftFabricAgentTool` and synchronous Azure AI Projects client. The agent can query Fabric data sources and provide responses based on data analysis. For a JavaScript version of this sample, see the [JavaScript sample for Fabric data agent](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentFabric.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as readline from "readline";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName =
    process.env["AZURE_AI_MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const fabricConnectionName =
    process.env["FABRIC_PROJECT_CONNECTION_NAME"] || "<fabric connection name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    // Get connection ID from connection name
    const fabricConnection = await project.connections.get(fabricConnectionName);
    console.log(`Fabric connection ID: ${fabricConnection.id}`);

    console.log("Creating agent with Microsoft Fabric tool...");

    // Define Microsoft Fabric tool that connects to Fabric data sources
    const agent = await project.agents.createVersion("MyFabricAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions: "You are a helpful assistant.",
      tools: [
        {
          type: "fabric_dataagent_preview",
          fabric_dataagent_preview: {
            project_connections: [
              {
                project_connection_id: fabricConnection.id,
              },
            ],
          },
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Prompt user for input
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    });

    const userInput = await new Promise<string>((resolve) => {
      rl.question(
        "Enter your question for Fabric (e.g., 'Tell me about sales records'): \n",
        (answer) => {
          rl.close();
          resolve(answer);
        },
      );
    });

    console.log("\nSending request to Fabric agent...");
    const response = await openAIClient.responses.create(
      {
        input: userInput,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    console.log(`\nResponse output: ${response.output_text}`);

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nMicrosoft Fabric agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### What this code does

  1. Creates an `AIProjectClient` using `DefaultAzureCredential`.
  2. Creates an agent version configured with the Fabric data agent tool.
  3. Prompts you for a question.
  4. Calls the Responses API with `tool_choice: "required"` to force tool use.
  5. Prints the response output text.
  6. Deletes the agent version.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `FABRIC_PROJECT_CONNECTION_ID`.
  * Authentication: `DefaultAzureCredential` must be able to obtain a token (for example, via `az login`).

  ### Expected output

  * A line confirming agent creation.
  * A line that starts with `Response output:` followed by the response text.
  * A final confirmation that the agent was deleted.
</ZonePivot>

<ZonePivot pivot="rest">
  The following example shows how to call the Foundry Agent REST API by using the Fabric data agent tool.

  <Callout type="important">
    `AGENT_TOKEN` is a credential. Keep it secret and avoid checking it into source control.
  </Callout>

  ```bash
  curl --request POST \
    --url "$AZURE_AI_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    --data '{
    "model": "'$AZURE_AI_MODEL_DEPLOYMENT_NAME'",
    "input": "Tell me about sales records for the last quarter.",
    "tool_choice": "required",
    "tools": [
      {
        "type": "fabric_dataagent_preview",
        "fabric_dataagent_preview": {
          "project_connections": [
            {
              "project_connection_id": "'$FABRIC_PROJECT_CONNECTION_ID'"
            }
          ]
        }
      }
    ]
  }'
  ```

  ### What this code does

  1. Calls the Responses API.
  2. Configures the request to use the Fabric data agent tool.
  3. Forces tool usage by using `tool_choice`.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `API_VERSION`, `AGENT_TOKEN`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`, `FABRIC_PROJECT_CONNECTION_ID`.

  ### Expected output

  * A `200` response with a JSON body that contains the model output.
</ZonePivot>

## Troubleshooting

| Issue                                                           | Cause                                                                         | Resolution                                                                                                                                                |
| --------------------------------------------------------------- | ----------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Artifact Id should not be empty and needs to be a valid GUID.` | Fabric connection created with invalid `workspace_id` or `artifact_id`        | Recreate the Fabric connection. Copy `workspace_id` and `artifact_id` from the data agent URL path `.../groups/<workspace_id>/aiskills/<artifact_id>...`. |
| `Can't add messages to thread_... while a run ... is active.`   | A run is still active for the thread                                          | Start a new conversation or wait for the active run to finish, then try again.                                                                            |
| `unauthorized`                                                  | End user lacks access to the Fabric data agent or its underlying data sources | Grant the end user access in Fabric, and confirm you're using user identity authentication.                                                               |
| `Cannot find the requested item` or `configuration not found`   | Fabric data agent isn't published or its configuration changed                | Publish the Fabric data agent and confirm it's active and its data sources are valid.                                                                     |
| Connection timeout errors                                       | Network latency or Fabric service delays                                      | Increase timeout settings in your client configuration. Consider implementing retry logic with exponential backoff.                                       |
| Data query returns empty results                                | Query doesn't match available data                                            | Verify the data sources in the Fabric data agent contain the expected data. Test queries directly in Fabric first.                                        |
| `Invalid workspace ID format`                                   | Workspace ID isn't a valid GUID                                               | Copy the exact workspace GUID from the Fabric URL or portal. Don't modify the ID format.                                                                  |
| Agent doesn't use the Fabric tool                               | Tool not properly configured or prompt doesn't trigger it                     | Verify the Fabric tool is enabled in the agent definition. Update the prompt to reference data that requires Fabric access.                               |

## Next steps

<Callout type="nextstepaction">
  [Tool use best practices](../../concepts/tool-best-practice)
</Callout>

<Callout type="nextstepaction">
  [Agent identity](../../concepts/agent-identity)
</Callout>

<Callout type="nextstepaction">
  [Get started with the SDK](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true)
</Callout>

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

Agents need context from scattered enterprise content to accurately answer questions. With Foundry IQ, you can create a configurable, multi-source *knowledge base* that provides agents with permission-aware responses based on your organization's data.

A knowledge base consists of *knowledge sources* (connections to internal and external data stores) and parameters that control retrieval behavior. Multiple agents can share the same knowledge base. When an agent queries the knowledge base, Foundry IQ uses *agentic retrieval* to process the query, retrieve relevant information, enforce user permissions, and return grounded answers with citations.

## Capabilities

* Connect one knowledge base to multiple agents. Supported knowledge sources include internal data stores (such as Azure Blob Storage, SharePoint, and OneLake) and public web data.

* Automate document chunking, vector embedding generation, and metadata extraction for indexed knowledge sources. Schedule recurring indexer runs for incremental data refresh.

* Issue keyword, vector, or hybrid queries across indexed and remote knowledge sources.

* Use the agentic retrieval engine with a large language model (LLM) to plan queries, select sources, run parallel searches, and aggregate results.

* Return extractive data with citations so agents can reason over raw content and trace answers to source documents.

* Synchronize access control lists (ACLs) for supported sources and honor Microsoft Purview sensitivity labels. Enforce permissions at query time so agents return only authorized content.

* Run queries under the caller's Microsoft Entra identity for end-to-end permission enforcement.

## Components

A Foundry IQ knowledge base contains knowledge sources and uses agentic retrieval to process queries. Azure AI Search provides the underlying indexing and retrieval infrastructure.

| Component                                                                                                       | Description                                                                                                                                                                                                                                  |
| --------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [Knowledge base](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-create-knowledge-base) | Top-level resource that orchestrates agentic retrieval. Defines which knowledge sources to query and parameters that control retrieval behavior, including the retrieval reasoning effort (minimal, low, or medium) for LLM processing.      |
| [Knowledge sources](https://learn.microsoft.com/en-us/azure/search/agentic-knowledge-source-overview)           | Connections to indexed or remote content. A knowledge base references one or more knowledge sources.                                                                                                                                         |
| [Agentic retrieval](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-overview)                  | Multi-query pipeline that decomposes complex questions into subqueries, executes them in parallel, semantically reranks results, and returns unified responses. Uses an optional LLM from Azure OpenAI in Foundry Models for query planning. |

You can use Foundry IQ knowledge bases in Foundry Agent Service, Microsoft Agent Framework, or any custom application by calling the knowledge base APIs from Azure AI Search.

## Workflow

You can set up Foundry IQ through a portal or programmatically. The following steps outline the typical workflow for both approaches.

<Tabs>
  <Tab title="Portal">
    1. Sign in to the [Microsoft Foundry (new) portal](../../what-is-foundry#microsoft-foundry-portals).

    2. Create a project or select an existing project.

    3. From the top menu, select **Build**.

    4. On the **Knowledge** tab:

       1. Create or connect to an existing search service that supports agentic retrieval.
       2. Create a knowledge base by adding one knowledge source at a time.
       3. Configure knowledge base properties for retrieval behavior.

    5. On the **Agents** tab:

       1. Create or select an existing agent.
       2. Connect to your knowledge base.
       3. Use the playground to send messages and refine your agent.

    <Callout type="note">
      * The playground provides a simplified workflow for proof-of-concept testing. When you move to code, configure managed identities and permissions to meet your organization's security requirements.

      * You can use the [Azure portal](https://learn.microsoft.com/en-us/azure/search/get-started-portal-agentic-retrieval) to create knowledge bases and knowledge sources, but agent configuration and integration must be done in the Microsoft Foundry (new) portal or programmatically.
    </Callout>
  </Tab>

  <Tab title="Programmatic">
    1. [Create knowledge sources](https://learn.microsoft.com/en-us/azure/search/agentic-knowledge-source-overview).

    2. [Create a knowledge base](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-create-knowledge-base) that references your knowledge sources.

    3. [Connect an agent](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/foundry-iq-connect) to your knowledge base.

    4. Send messages and refine your agent.

    <Callout type="note">
      For centralized guidance on these steps, see [Tutorial: Build an end-to-end agentic retrieval solution](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-create-pipeline).
    </Callout>
  </Tab>
</Tabs>

## Relationship to Fabric IQ and Work IQ

Microsoft provides three IQ workloads that give agents access to different aspects of your organization:

* [Fabric IQ](https://learn.microsoft.com/en-us/fabric/iq/overview) is a semantic intelligence layer for Microsoft Fabric. It models business data (ontologies, semantic models, and graphs) so agents can reason over analytics in OneLake and Power BI.

* [Work IQ](https://learn.microsoft.com/en-us/microsoft-365-copilot/extensibility/workiq-overview) is a contextual intelligence layer for Microsoft 365. It captures collaboration signals from documents, meetings, chats, and workflows, providing agents with insight into how your organization operates.

* [Foundry IQ](#capabilities) is a managed knowledge layer for enterprise data. It connects structured and unstructured data across Azure, SharePoint, OneLake, and the web so agents can access permission-aware knowledge.

Each IQ workload is standalone, but you can use them together to provide comprehensive organizational context for agents.

## Get started

* [Watch this session](https://www.youtube.com/watch?v=slDdNIQCJBQ) for an introduction to Foundry IQ, and then [watch this video](https://www.youtube.com/watch?v=uDVkcZwB0EU) for a deep dive.

* For minimum costs and proof-of-concept testing, start with the Microsoft Foundry (new) portal. You can use the free tier for Azure AI Search and a free allocation of tokens for agentic retrieval. [Watch this video](https://www.youtube.com/watch?v=bHL1jbWjJUc) for a quick demonstration of the portal.

* For step-by-step integration guidance, learn how to [connect a Foundry IQ knowledge base to Foundry Agent Service](../how-to/foundry-iq-connect).

* Review application code in the [Azure OpenAI demo](https://learn.microsoft.com/en-us/samples/azure-samples/azure-search-openai-demo/azure-search-openai-demo/), which uses agentic retrieval.

Find answers to common questions about [Foundry IQ](what-is-foundry-iq).

## General

### What is Foundry IQ?

Foundry IQ enables agents to access, process, and act on knowledge from anywhere.

With Foundry IQ, you create a *knowledge base* that connects to one or more *knowledge sources*. The *agentic retrieval* engine processes queries, and an optional large language model (LLM) from Azure OpenAI in Foundry Models adds query planning and reasoning. Agents built in Foundry Agent Service call the knowledge base to retrieve relevant content.

### What is the difference between Foundry IQ and agentic retrieval?

Foundry IQ consists of knowledge bases, knowledge sources, and native integrations with Azure OpenAI in Foundry Models and Foundry Agent Service. The Microsoft Foundry (new) portal offers a streamlined, end-to-end setup experience, but you can also create the Foundry IQ components programmatically.

Agentic retrieval is the multi-query retrieval engine that powers Foundry IQ knowledge bases. For custom solutions, you can use agentic retrieval directly via the Azure AI Search APIs.

### How is Foundry IQ different from existing RAG patterns or Azure OpenAI On Your Data?

One Foundry IQ knowledge base provides access to multiple sources, removing the need to connect each agent to each source individually.

The agentic retrieval engine plans which sources to query and performs iterative search if initial results don't meet relevance standards. Indexing and data synchronization are triggered automatically. Iterative search depends on specifying a medium retrieval reasoning effort in the knowledge base or on a per-request basis.

## Components and requirements

### Is Azure AI Search required for Foundry IQ?

Yes. Foundry IQ is built on [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/)'s agentic retrieval capabilities. You must create a knowledge base in Azure AI Search to use Foundry IQ.

### Is Foundry Agent Service required for Foundry IQ?

No. You can call knowledge bases from Foundry Agent Service, Microsoft Agent Framework, or any application that supports the knowledge base APIs from Azure AI Search. However, Foundry Agent Service provides a turnkey agent hosting platform with built-in support for Foundry IQ knowledge bases.

### Do Foundry IQ and agentic retrieval have a hard dependency on LLMs?

LLM usage for query planning is optional but recommended. Without an LLM, you can use the minimal retrieval reasoning effort in agentic retrieval to merge results from multiple queries across knowledge sources.

To benefit from parallel processing of multiple subqueries, LLM query planning is required. The agentic retrieval engine uses LLM query planning in low and medium retrieval reasoning efforts for more thorough retrieval.

### Which LLM models are supported for query planning?

Only gpt-4o, gpt-4.1, and gpt-5 series models from Azure OpenAI in Foundry Models are supported for query planning. For the full list, see [Supported models](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-create-knowledge-base#supported-models). You specify the model deployment when you create the knowledge base.

## Knowledge bases and knowledge sources

### What is a knowledge base?

A [knowledge base](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-create-knowledge-base) is a top-level object that groups one or more knowledge sources under a single endpoint. Its configuration controls how sources are selected (via retrieval instructions and reasoning effort) and how results are returned (via output mode and answer instructions). An LLM connection enables query planning and answer synthesis.

At query time, the agentic retrieval engine uses this configuration to process requests. User-level access controls are only enforced when the knowledge source supports document-level permissions and when those permissions have been explicitly configured for synchronization. Unless support for out-of-the-box user permissions is explicitly stated in the knowledge source documentation, document-level access controls aren't automatically honored.

### What knowledge sources are supported?

There are two types of [knowledge sources](https://learn.microsoft.com/en-us/azure/search/agentic-knowledge-source-overview):

* **Indexed knowledge sources** ingest data into a search index and automatically handle chunking, vectorization, metadata extraction, and access control list (ACL) synchronization. Supported indexed sources include Azure Blob Storage, OneLake, SharePoint, and existing search indexes.

* **Remote knowledge sources** don't ingest or store data. Instead, they issue on-demand queries to the external system at retrieval time. Supported remote sources include SharePoint (via Copilot Retrieval API) and web (via Grounding with Bing).

### How often is indexed data refreshed?

Indexed knowledge sources use Azure AI Search indexers for data ingestion. You can schedule recurring indexer runs for incremental data refresh. The frequency depends on your indexer schedule configuration.

Remote knowledge sources query external systems on demand, so data is always current.

## Agentic retrieval

### What is agentic retrieval?

[Agentic retrieval](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-overview) is the retrieval engine that processes requests against a knowledge base.

It selects which knowledge sources to query, routes requests appropriately, and can automatically iterate when initial results are incomplete. When the knowledge base includes an LLM, agentic retrieval also performs query planning and higher-level reasoning to refine the search process. You can control the cost and latency of the search process by setting the retrieval reasoning effort on the knowledge base.

Benchmarks show that agentic retrieval achieves approximately [36% higher response quality](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/foundry-iq-boost-response-relevance-by-36-with-agentic-retrieval/4470720) than traditional single-shot RAG.

### What is query planning in agentic retrieval?

Query planning is the process by which an LLM breaks down a complex query into smaller, more focused subqueries for broader coverage of your search corpus. It also includes the logic for selecting one knowledge source over another.

You can influence knowledge source selection by adding descriptions to indexed knowledge sources and retrieval instructions to your knowledge base. For example, you might specify "use the employee-handbook-index for questions about time off" and "use the health-insurance-index for questions about medical coverage."

### How is query planning invoked?

Query planning is invoked when you specify the low or medium retrieval reasoning effort in a knowledge base.

### What is the retrieval reasoning effort?

The [retrieval reasoning effort](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-set-retrieval-reasoning-effort) determines how much LLM-driven planning is used during agentic retrieval. Levels range from minimal to medium, influencing how deeply the system interprets the query, selects sources, and decides whether more retrieval steps are needed.

### What are the limits for each retrieval reasoning effort level?

Limits vary by reasoning effort level:

* **Minimal**: Up to 10 knowledge sources. No LLM processing, no query planning, and no answer synthesis.

* **Low**: Up to three knowledge sources and three subqueries. Supports answer synthesis with a 5,000-token budget.

* **Medium**: Up to five knowledge sources and five subqueries. Supports iterative search and answer synthesis with a 10,000-token budget.

For more information, see [Agentic retrieval limits](https://learn.microsoft.com/en-us/azure/search/search-limits-quotas-capacity#agentic-retrieval-limits).

### What is answer synthesis? Should I use it for a Foundry IQ knowledge base?

[Answer synthesis](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-answer-synthesis) uses an LLM to generate a complete, natural-language answer based on retrieved content. Answer synthesis is required for web knowledge sources.

For most Foundry IQ scenarios, use extractive data instead of answer synthesis. Extractive data returns raw content that agents can reason over and incorporate into their responses. Reserve answer synthesis for standalone applications where the retrieval output goes directly to users without agent processing.

## Availability and pricing

### Where is Foundry IQ available? How is it billed?

Foundry IQ is subject to the [regional availability and billing](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-overview#availability-and-pricing) of its underlying services: Azure AI Search and, if applicable, Azure OpenAI in Foundry Models.

### Is Foundry IQ free to use?

Azure AI Search offers a free pricing tier and a free token allocation for agentic retrieval. Foundry Agent Service doesn't charge for agent instances.

After you exhaust the free allocation, agentic retrieval is billed based on token consumption in Azure AI Search. LLM usage for query planning and answer synthesis incurs separate charges from Azure OpenAI in Foundry Models.

## Security and governance

### How does Foundry IQ handle permissions?

Permission enforcement varies by knowledge source. Depending on the data source, indexed knowledge sources can support document-level security through ACLs, role-based access control, or both. At query time, results are filtered based on the user's identity.

Remote SharePoint knowledge sources enforce permissions directly via the Copilot Retrieval API, with out-of-the-box support for ACLs and Microsoft Purview sensitivity labels.

### What authentication methods are supported?

For connections between Azure AI Search and other Azure services (such as Azure OpenAI or Azure Blob Storage), use managed identities with Microsoft Entra ID (recommended) or API keys.

Remote SharePoint knowledge sources require end users to have a valid Copilot license.

## Troubleshooting

### Why isn't my agent returning results from my knowledge base?

Common causes include:

* Missing tool invocation in agent instructions.
* Permission issues between the agent and knowledge base.
* Incorrect project connection configuration.
* Empty or misconfigured knowledge sources.

For help isolating the issue, see [Troubleshooting](../how-to/foundry-iq-connect#troubleshooting).

### How do I debug agentic retrieval queries?

[Use the chat playground](https://learn.microsoft.com/en-us/azure/search/get-started-portal-agentic-retrieval#test-agentic-retrieval) in the Azure portal, which shows query plans, subqueries, and retrieval steps for your knowledge base. [Enable diagnostic logging](https://learn.microsoft.com/en-us/azure/search/search-monitor-enable-logging) for detailed request/response data.

## Related products

### How does Foundry IQ relate to Fabric IQ and Work IQ?

Microsoft provides three IQ workloads for agent-native systems: Fabric IQ for business analytics, Work IQ for Microsoft 365 collaboration, and Foundry IQ for enterprise knowledge.

Each IQ workload is standalone, but they can work together to answer virtually any organization-specific question from an agent.

### What is the difference between Copilot knowledge sources and Foundry IQ knowledge sources?

The concept is the same: connecting agents to enterprise data. However, the supported data sources differ by platform and aren't interoperable. In other words, you can't use Foundry IQ knowledge sources in Copilot, and you can't use Copilot knowledge sources in Foundry IQ.

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

In this article, you learn how to connect a knowledge base in Foundry IQ to an agent in Foundry Agent Service. The connection uses the [Model Context Protocol (MCP)](tools/model-context-protocol) to facilitate tool calls. When invoked by the agent, the knowledge base orchestrates the following operations:

* Plans and decomposes a user query into subqueries.
* Processes the subqueries simultaneously using keyword, vector, or hybrid techniques.
* Applies semantic reranking to identify the most relevant results.
* Synthesizes the results into a unified response with source references.

The agent uses the response to ground its answers in enterprise data or web sources, ensuring factual accuracy and transparency through source attribution.

For an end-to-end example of integrating Azure AI Search and Foundry Agent Service for knowledge retrieval, see the [agentic-retrieval-pipeline-example](https://github.com/Azure-Samples/azure-search-python-samples/tree/main/agentic-retrieval-pipeline-example) Python sample on GitHub.

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | -      | -              | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* An [Azure AI Search service](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal) with a [knowledge base](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-create-knowledge-base) containing one or more [knowledge sources](https://learn.microsoft.com/en-us/azure/search/agentic-knowledge-source-overview).

* A [Microsoft Foundry project](../../how-to/create-projects) with an [LLM deployment](../../foundry-models/how-to/create-model-deployments), such as `gpt-4.1-mini`.

* [Authentication and permissions](#authentication-and-permissions) on your search service and project.

* The latest preview Python SDK or the 2025-11-01-preview REST API version.

  ```bash
  pip install azure-ai-projects azure-identity requests
  ```

### Authentication and permissions

We recommend role-based access control for production deployments. If roles aren't feasible, skip this section and use key-based authentication instead.

<Tabs>
  <Tab title="Microsoft Foundry">
    * On the parent resource of your project, you need the **Azure AI User** role to access model deployments and create agents. **Owners** automatically get this role when they create the resource. Other users need a specific role assignment. For more information, see [Role-based access control in Foundry portal](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/rbac-foundry).

    * On the parent resource of your project, you need the **Azure AI Project Manager** role to create a project connection for MCP authentication and either **Azure AI User** or **Azure AI Project Manager** to use the MCP tool in agents.

    * On your project, create a system-assigned managed identity for interactions with Azure AI Search.
  </Tab>

  <Tab title="Azure AI Search">
    * On your search service, assign the **Search Index Data Reader** role to your project's managed identity for read-only access to search indexes.

    * If your agent needs to write documents to search indexes, also assign the **Search Index Data Contributor** role.

    * For indexed content with access control lists (ACLs), include [permission metadata fields](https://learn.microsoft.com/en-us/azure/search/search-document-level-access-overview) in your search index and pass user tokens via the `x-ms-query-source-authorization` header at query time to filter results based on the user's identity. For more information, see [Query-time ACL and RBAC enforcement](https://learn.microsoft.com/en-us/azure/search/search-query-access-control-rbac-enforcement).

    * For remote SharePoint knowledge sources, the `x-ms-query-source-authorization` header passes the user's identity, enabling SharePoint to enforce document permissions at query time. Content isn't indexed. Instead, SharePoint applies permissions directly via the Copilot Retrieval API. For more information, see [Create a remote SharePoint knowledge source](https://learn.microsoft.com/en-us/azure/search/agentic-knowledge-source-how-to-sharepoint-remote).
  </Tab>
</Tabs>

### Required values

Use the following values in the code samples.

| Value                                                | Where to get it                                                                                  | Example                                                                                                         |
| ---------------------------------------------------- | ------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------- |
| Project endpoint (`project_endpoint`)                | Find it in your project details in the Microsoft Foundry portal.                                 | `https://your-resource.services.ai.azure.com/api/projects/your-project`                                         |
| Project resource ID (`project_resource_id`)          | Copy the project ARM resource ID from Azure portal or use Azure CLI to query the resource ID.    | `/subscriptions/.../resourceGroups/.../providers/Microsoft.MachineLearningServices/workspaces/.../projects/...` |
| Azure AI Search endpoint (`search_service_endpoint`) | Find it on your Azure AI Search service **Overview** page (the service URL) in the Azure portal. | `https://your-search-service.search.windows.net`                                                                |
| Knowledge base name (`knowledge_base_name`)          | Use the knowledge base name you created in Azure AI Search.                                      | `hr-policy-kb`                                                                                                  |
| Project connection name (`project_connection_name`)  | Choose a name for the project connection you create.                                             | `my-kb-mcp-connection`                                                                                          |
| Agent name (`agent_name`)                            | Choose a name for the agent version you create.                                                  | `hr-assistant`                                                                                                  |
| Model deployment name (`deployed_LLM`)               | Find it in your Microsoft Foundry project model deployments.                                     | `gpt-4.1-mini`                                                                                                  |

<Callout type="tip">
  We recommend you store the project endpoint, search endpoint, and knowledge base name in a `.env` file for local development.
</Callout>

## Create a project connection

Create a `RemoteTool` connection on your Microsoft Foundry project. This connection uses the project's managed identity to target the MCP endpoint of the knowledge base, allowing the agent to securely communicate with Azure AI Search for retrieval operations.

<Callout type="note">
  The `RemoteTool` category and `ProjectManagedIdentity` authentication type are specific to Microsoft Foundry project connections.
</Callout>

<Tabs>
  <Tab title="Python">
    ```python
    import requests
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider

    # Provide connection details
    credential = DefaultAzureCredential()
    project_resource_id = "{project_resource_id}" # e.g. /subscriptions/{subscription}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{account_name}/projects/{project_name}
    project_connection_name = "{project_connection_name}"
    mcp_endpoint = "{search_service_endpoint}/knowledgebases/{knowledge_base_name}/mcp?api-version=2025-11-01-preview" # This endpoint enables the MCP connection between the agent and knowledge base

    # Get bearer token for authentication
    bearer_token_provider = get_bearer_token_provider(credential, "https://management.azure.com/.default")
    headers = {
      "Authorization": f"Bearer {bearer_token_provider()}",
    }

    # Create project connection
    response = requests.put(
      f"https://management.azure.com{project_resource_id}/connections/{project_connection_name}?api-version=2025-10-01-preview",
      headers = headers,
      json = {
        "name": project_connection_name,
        "type": "Microsoft.MachineLearningServices/workspaces/connections",
        "properties": {
          "authType": "ProjectManagedIdentity",
          "category": "RemoteTool",
          "target": mcp_endpoint,
          "isSharedToAll": True,
          "audience": "https://search.azure.com/",
          "metadata": { "ApiType": "Azure" }
        }
      }
    )

    response.raise_for_status()
    print(f"Connection '{project_connection_name}' created or updated successfully.")
    ```
  </Tab>

  <Tab title="REST">
    Use the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/what-is-azure-cli) to get an access token for Azure Resource Manager:

    ```azurecli
    az account get-access-token --scope https://management.azure.com/.default --query accessToken -o tsv
    ```

    Create the project connection by making a `PUT` request to Azure Resource Manager:

    ```HTTP
    PUT https://management.azure.com/{project_resource_id}/connections/{project_connection_name}?api-version=2025-10-01-preview
    Authorization: Bearer {management_access_token}
    Content-Type: application/json

    {
      "name": "{project_connection_name}",
      "type": "Microsoft.MachineLearningServices/workspaces/connections",
      "properties": {
        "authType": "ProjectManagedIdentity",
        "category": "RemoteTool",
        "target": "{search_service_endpoint}/knowledgebases/{knowledge_base_name}/mcp?api-version=2025-11-01-preview", // This endpoint enables the MCP connection between the agent and knowledge base
        "isSharedToAll": true,
        "audience": "https://search.azure.com/",
        "metadata": {
          "ApiType": "Azure"
        }
      }
    }
    ```
  </Tab>
</Tabs>

## Optimize agent instructions for knowledge retrieval

To improve knowledge base invocations and produce citation-backed answers, start with instructions like the following:

```plaintext
You are a helpful assistant.

Use the knowledge base tool to answer user questions.
If the knowledge base doesn't contain the answer, respond with "I don't know".

When you use information from the knowledge base, include citations to the retrieved sources.
```

This instruction template optimizes for:

* **Higher MCP tool invocation rates**: Explicit directives ensure the agent consistently calls the knowledge base tool rather than relying on its training data.
* **Clear source attribution**: Citations make it easier to validate where information came from.

<Callout type="tip">
  While this template provides a strong foundation, evaluate and iterate on the instructions based on your specific use case and objectives. Test different variations to find what works best for your scenario.
</Callout>

## Create an agent with the MCP tool

Create an agent that integrates the knowledge base as an MCP tool. The agent uses a system prompt to instruct when and how to call the knowledge base. It follows instructions on how to answer questions and automatically maintains its tool configuration and settings across conversation sessions.

Add the knowledge base MCP tool with the project connection you previously created. This tool orchestrates query planning, decomposition, and retrieval across configured knowledge sources. The agent uses this tool to answer queries.

<Callout type="note">
  Azure AI Search knowledge bases expose the `knowledge_base_retrieve` MCP tool for agent integration. This is the only tool currently supported for use with Foundry Agent Service.
</Callout>

<Tabs>
  <Tab title="Python">
    ```python
    from azure.ai.projects import AIProjectClient
    from azure.ai.projects.models import PromptAgentDefinition, MCPTool
    from azure.identity import DefaultAzureCredential

    # Provide agent configuration details
    credential = DefaultAzureCredential()
    mcp_endpoint = "{search_service_endpoint}/knowledgebases/{knowledge_base_name}/mcp?api-version=2025-11-01-preview"
    project_endpoint = "{project_endpoint}" # e.g. https://your-foundry-resource.services.ai.azure.com/api/projects/your-foundry-project
    project_connection_name = "{project_connection_name}"
    agent_name = "{agent_name}"
    agent_model = "{deployed_LLM}" # e.g. gpt-4.1-mini

    # Create project client
    project_client = AIProjectClient(endpoint = project_endpoint, credential = credential)

    # Define agent instructions (see "Optimize agent instructions" section for guidance)
    instructions = """
    You are a helpful assistant that must use the knowledge base to answer all the questions from user. You must never answer from your own knowledge under any circumstances.
    Every answer must always provide annotations for using the MCP knowledge base tool and render them as: `【message_idx:search_idx†source_name】`
    If you cannot find the answer in the provided knowledge base you must respond with "I don't know".
    """

    # Create MCP tool with knowledge base connection
    mcp_kb_tool = MCPTool(
        server_label = "knowledge-base",
        server_url = mcp_endpoint,
        require_approval = "never",
        allowed_tools = ["knowledge_base_retrieve"],
        project_connection_id = project_connection_name
    )

    # Create agent with MCP tool
    agent = project_client.agents.create_version(
        agent_name = agent_name,
        definition = PromptAgentDefinition(
            model = agent_model,
            instructions = instructions,
            tools = [mcp_kb_tool]
        )
    )

    print(f"Agent '{agent_name}' created or updated successfully.")
    ```
  </Tab>

  <Tab title="REST">
    Get an access token for Microsoft Foundry:

    ```azurecli
    az account get-access-token --scope https://ai.azure.com/.default --query accessToken -o tsv
    ```

    Create the agent by sending a `POST` request to Foundry Agent Service:

    ```HTTP
    POST {project_endpoint}/agents/{agent_name}/versions?api-version=2025-11-15-preview
    Authorization: Bearer {foundry_access_token}
    Content-Type: application/json

    {
      "definition": {
        "model": "{deployed_llm}",
        "instructions": "\nYou are a helpful assistant that must use the knowledge base to answer all the questions from user. You must never answer from your own knowledge under any circumstances.\nEvery answer must always provide annotations for using the MCP knowledge base tool and render them as: `【message_idx:search_idx†source_name】`\nIf you cannot find the answer in the provided knowledge base you must respond with \"I don't know\".\n",
        "tools": [
          {
            "server_label": "knowledge-base",
            "server_url": "{search_service_endpoint}/knowledgebases/{knowledge_base_name}/mcp?api-version=2025-11-01-preview",
            "require_approval": "never",
            "allowed_tools": [
              "knowledge_base_retrieve"
            ],
            "project_connection_id": "{project_connection_name}",
            "type": "mcp"
          }
        ],
        "kind": "prompt"
      }
    }
    ```
  </Tab>
</Tabs>

### Connect to a remote SharePoint knowledge source

<Callout type="important">
  In this preview, Foundry Agent Service doesn't support per-request headers for MCP tools. Headers set in agent definitions apply to all invocations and can't vary by user or request.

  For per-user authorization, use the [Azure OpenAI Responses API](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/responses) instead.
</Callout>

Optionally, if your knowledge base includes a [remote SharePoint knowledge source](https://learn.microsoft.com/en-us/azure/search/agentic-knowledge-source-how-to-sharepoint-remote), you must also include the `x-ms-query-source-authorization` header in the MCP tool connection.

<Tabs>
  <Tab title="Python">
    ```python
    from azure.identity import get_bearer_token_provider

    # Create MCP tool with SharePoint authorization header
    mcp_kb_tool = MCPTool(
        server_label = "knowledge-base",
        server_url = mcp_endpoint,
        require_approval = "never",
        allowed_tools = ["knowledge_base_retrieve"],
        project_connection_id = project_connection_name,
        headers = {
            "x-ms-query-source-authorization": get_bearer_token_provider(credential, "https://search.azure.com/.default")()
        }
    )
    ```
  </Tab>

  <Tab title="REST">
    Get an access token for Azure AI Search:

    ```azurecli
    az account get-access-token --scope https://search.azure.com/.default --query accessToken --output tsv
    ```

    Provide the header and token in the MCP tool configuration:

    ```HTTP
        "tools": [
          {
            "server_label": "knowledge-base",
            "server_url": "{search_service_endpoint}/knowledgebases/{knowledge_base_name}/mcp?api-version=2025-11-01-preview",
            "require_approval": "never",
            "allowed_tools": [
              "knowledge_base_retrieve"
            ],
            "project_connection_id": "{project_connection_name}",
            "type": "mcp",
            "headers": {
                "x-ms-query-source-authorization": "{search-bearer-token}"
            }
          }
        ]
    ```
  </Tab>
</Tabs>

## Invoke the agent with a query

Create a conversation session and send a user query to the agent. When appropriate, the agent orchestrates calls to the MCP tool to retrieve relevant content from the knowledge base. The agent then synthesizes this content into a natural-language response that cites the source documents.

<Tabs>
  <Tab title="Python">
    ```python
    # Get the OpenAI client for responses and conversations
    openai_client = project_client.get_openai_client()

    # Create conversation
    conversation = openai_client.conversations.create()

    # Send request to trigger the MCP tool
    response = openai_client.responses.create(
        conversation = conversation.id,
        input = """
            Why do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown?
            Why is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?
        """,
        extra_body = {"agent": {"name": agent.name, "type": "agent_reference"}},
    )

    print(f"Response: {response.output_text}")
    ```

    The output should be similar to the following (truncated for brevity):

    ```
    Response: Suburban belts display larger December brightening than urban cores, even
    though absolute light levels are higher downtown, primarily because holiday lights
    increase most dramatically in the suburbs and outskirts of major cities. This is due
    to more yard space and a prevalence of single-family homes in suburban areas...

    The Phoenix nighttime street grid is sharply visible from space due to the city's
    layout along a regular grid of city blocks and streets with extensive street lighting...

    References:
    - earth_at_night_508_page_174, earth_at_night_508_page_176 (Holiday lighting)
    - earth_at_night_508_page_104, earth_at_night_508_page_105 (Phoenix grid visibility)
    ```
  </Tab>

  <Tab title="REST">
    Send an empty `POST` request to create a conversation session:

    ```HTTP
    ### Create conversation
    POST {project_endpoint}/openai/conversations?api-version=2025-11-15-preview
    Authorization: Bearer {foundry_access_token}
    Content-Type: application/json

    {}
    ```

    The response includes a conversation `id`, which you can use to send a query to the agent:

    ```HTTP
    ### Send request to trigger the MCP tool
    POST {project_endpoint}/openai/responses?api-version=2025-11-15-preview
    Authorization: Bearer {foundry_access_token}
    Content-Type: application/json

    {
        "conversation": "{conversation_id}",
        "input": "\nWhy do suburban belts display larger December brightening than urban cores even though absolute light levels are higher downtown?\nWhy is the Phoenix nighttime street grid is so sharply visible from space, whereas large stretches of the interstate between midwestern cities remain comparatively dim?\n",
        "agent": {
            "name": "{agent_name}",
            "type": "agent_reference"
        }
    }
    ```

    The response includes metadata about the agent execution, tool calls, and the generated output. The most relevant part of the response is the `text` in the `content` field, which should be similar to the following (truncated for brevity):

    ```
    Suburban belts display larger December brightening in nighttime lights than urban
    cores, primarily because suburban areas have more yard space and single-family homes
    where holiday lighting decorations are more commonly used...

    The Phoenix nighttime street grid is sharply visible due to the regular, planned
    layout of city blocks with extensive street lighting, shopping centers, and
    commercial properties along major streets...

    References:
    - earth_at_night_508_page_174_verbalized, earth_at_night_508_page_176_verbalized
    - earth_at_night_508_page_104_verbalized, earth_at_night_508_page_105_verbalized
    ```
  </Tab>
</Tabs>

## Delete the agent and project connection

<Tabs>
  <Tab title="Python">
    ```python
    # Delete the agent
    project_client.agents.delete_version(agent.name, agent.version)
    print(f"Agent '{agent.name}' version '{agent.version}' deleted successfully.")

    # Delete the project connection (Azure Resource Manager)
    import requests
    from azure.identity import DefaultAzureCredential, get_bearer_token_provider

    credential = DefaultAzureCredential()
    project_resource_id = "{project_resource_id}"
    project_connection_name = "{project_connection_name}"

    bearer_token_provider = get_bearer_token_provider(credential, "https://management.azure.com/.default")
    headers = {"Authorization": f"Bearer {bearer_token_provider()}"}

    response = requests.delete(
      f"https://management.azure.com{project_resource_id}/connections/{project_connection_name}?api-version=2025-10-01-preview",
      headers=headers,
    )
    response.raise_for_status()
    print(f"Project connection '{project_connection_name}' deleted successfully.")
    ```
  </Tab>

  <Tab title="REST">
    ```HTTP
    ### Delete the agent
    DELETE {project_endpoint}/agents/{agent_name}?api-version=2025-11-15-preview
    Authorization: Bearer {foundry_access_token}

    ### Delete the project connection
    DELETE https://management.azure.com/{project_resource_id}/connections/{project_connection_name}?api-version=2025-10-01-preview
    Authorization: Bearer {management_access_token}
    ```
  </Tab>
</Tabs>

<Callout type="note">
  Deleting your agent and project connection doesn't delete your knowledge base or its knowledge sources. You must delete these objects separately on your Azure AI Search service. For more information, see [Delete a knowledge base](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-create-knowledge-base?#delete-a-knowledge-base) and [Delete a knowledge source](https://learn.microsoft.com/en-us/azure/search/agentic-knowledge-source-how-to-search-index#delete-a-knowledge-source).
</Callout>

## Troubleshooting

This section helps you troubleshoot common issues when connecting Foundry Agent Service to a Foundry IQ knowledge base.

### Authorization failures (401/403)

* If you get a 403 from Azure AI Search, confirm the project's managed identity has the **Search Index Data Reader** role on the search service (and **Search Index Data Contributor** if you write to indexes).
* If you get a 403 from Azure Resource Manager when you create or delete the project connection, confirm your user or service principal has permissions on the Microsoft Foundry resource and project.
* If you use keyless authentication, confirm your environment is signed in to the correct tenant and subscription.

### MCP endpoint errors (400/404)

* Confirm `search_service_endpoint` is the Azure AI Search service URL, such as `https://<name>.search.windows.net`.
* Confirm `knowledge_base_name` matches the knowledge base you created in Azure AI Search.
* Confirm you use the `2025-11-01-preview` API version for the knowledge base MCP endpoint.

### The agent doesn't ground answers

* Confirm the agent has the MCP tool configured and `allowed_tools` includes `knowledge_base_retrieve`.
* Update your agent instructions to explicitly require using the knowledge base and to return "I don't know" when retrieval doesn't contain the answer.

## Related content

* [Create a knowledge base in Azure AI Search](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-create-knowledge-base)
* [Tutorial: Build an end-to-end agentic retrieval solution](https://learn.microsoft.com/en-us/azure/search/agentic-retrieval-how-to-create-pipeline)
* [Foundry IQ: Unlocking ubiquitous knowledge for agents](https://techcommunity.microsoft.com/blog/azure-ai-foundry-blog/foundry-iq-unlocking-ubiquitous-knowledge-for-agents/4470812)
* [Tool best practices](../concepts/tool-best-practice)

Azure Speech in Foundry Tools lets your agent convert speech to text and generate speech audio from text. You connect the tool by adding a remote Model Context Protocol (MCP) server to your agent in Foundry Agent Service.

<Callout type="important">
  The Speech MCP tool doesn't support [Network-secured Microsoft Foundry](../virtual-networks). For more information, see [Connect to Model Context Protocol servers](model-context-protocol).
</Callout>

## Prerequisites

* An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* A [Microsoft Foundry resource](../../../../ai-services/multi-service-resource) created in a [supported region](../../../../ai-services/speech-service/regions). Your Foundry resource includes speech capabilities and is used by the Speech MCP server. The Speech MCP Server is available in all regions where Foundry Agent Service supports [MCP tools](../../concepts/tool-best-practice#tool-support-by-region-and-model).

## Usage support

This article shows how to connect the tool in Foundry portal.

If you want to work with code, see [Connect to Model Context Protocol servers](model-context-protocol) for SDK examples in Python, C#, and JavaScript.

## Security and privacy

Treat your Speech resource key and storage SAS URLs as secrets:

* Don’t paste keys or SAS URLs into agent prompts, chat transcripts, screenshots, or source control.
* Use the shortest practical SAS expiry time.
* Scope SAS URLs to the minimum required resource (for example, a single container).
* Rotate keys periodically as a security best practice, or immediately if you suspect they're exposed.

## Set up storage

You need an Azure Storage account to store input audio files for speech-to-text processing and receive output audio files from text-to-speech processing. [Create an Azure Storage account](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-create?tabs=azure-portal).

Ensure your user account has the **Storage Blob Data Contributor** role assigned for the storage account, so you can create SAS URLs later on.

Create one or more blob containers to store the input and output audio files.

## Create an agent

1. Go to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs).
2. In the upper-right menu, select **Build**.
3. In the left pane, select **Agents**, and then select **Create agent**.
4. Enter a name and description, and then select **Create**.

## Connect the Azure Speech tool to your agent

1. In your agent, open the agent playground.

2. Under **Tools**, select **Add** -> **Add a new tool**.

3. In **Select a tool**, select the **Catalog** tab.

4. Search for **Azure Speech MCP Server**, select it, and then select **Create**.

5. On the setup page, fill in the following fields:

   * **Parameters** -> `foundry-resource-name`: Enter the name of the Foundry resource you created in the prerequisites.
   * **Authorization** -> `Bearer` (API Key): Enter a key from your Foundry resource. You can use either `KEY1` or `KEY2` from your resource’s **Keys and Endpoint** page in the Azure portal.
   * **Authorization** -> `X-Blob-Container-Url`: Generate a SAS URL for your storage container with read and write permissions, and then enter it here. The service stores audio output files in this container.

6. Select **Connect** to add the remote Speech MCP server as a tool for your agent.

   After connecting, the Speech tool appears in your agent's **Tools** list with a connected status.

## Test the Azure Speech tool

In the agent playground chat, enter `What can you do?`.

<Callout type="tip">
  Select a capable base model for best results.
</Callout>

The agent lists its available capabilities, including the newly added Speech Capabilities such as speech-to-text and text-to-speech. This confirms that the remote Speech MCP server is successfully connected.

### Test speech-to-text

The Speech tool can convert an audio file to text. The audio file can be stored in Azure Blob Storage and accessed with a SAS URL, or it can be any publicly accessible URL to an audio file.

<Callout type="note">
  Supported audio formats include WAV, MP3, OGG, FLAC, and other common formats. For best results with speech recognition, use WAV files with 16 kHz sample rate and 16-bit depth.
</Callout>

1. Upload your audio file to your Azure blob storage container.

2. Generate a SAS URL for the file:

   1. Select your uploaded audio file.
   2. In **Properties**, select **Generate SAS**.
   3. Set the shortest practical expiry time, and then select **Generate SAS token and URL**.

3. Copy the SAS URL. Then use it in one of the following example prompts in the agent chat window:

   * `Recognize this English audio file located in <blob SAS URL>`
   * `Recognize the audio file located in <blob SAS URL> with these phrase hints: "Azure, OpenAI, Cognitive Services, Lucy" to improve accuracy.`
   * `Convert this audio file located in <blob SAS URL> into text and summarize it for me.`
   * `Recognize this French audio file located in <blob SAS URL> with detailed output format.`
   * `Recognize this Hindi audio file located in <blob SAS URL> and remove profanity.`

4. View the output text in the chat window.

### Test text-to-speech

Start a new chat in the agent playground, and use one of the following example prompts. Replace the placeholder with your own text:

* `Convert text to speech: <your text to speak>`
* `Synthesize speech from "<your text to speak>"`
* `Generate speech audio from text "<your text to speak>"`
* `Convert text to speech with Chinese language: <your text to speak>`
* `Synthesize speech with voice en-US-JennyNeural from text <your text to speak>`

The output audio is saved as a WAV file in your blob container. An audio link is displayed in the chat window. Select it to listen to the output.

## Troubleshooting

| Issue                                                           | Likely cause                                                       | Resolution                                                                                      |
| --------------------------------------------------------------- | ------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------- |
| You can’t find **Azure Speech MCP Server** in the tool catalog. | The tool isn’t available for your tenant, region, or scenario.     | Confirm your Foundry resource is created in a supported region and try again.                   |
| **Connect** fails with authorization errors.                    | The API key is incorrect or expired.                               | Recopy `KEY1` or `KEY2` from your resource’s **Keys and Endpoint** page. Rotate keys if needed. |
| Speech output audio link doesn’t work.                          | The container SAS URL is invalid, expired, or missing permissions. | Regenerate the container SAS URL with read and write permissions and a valid expiry time.       |
| Speech-to-text can’t access the audio file.                     | The file SAS URL is invalid or expired.                            | Regenerate the file SAS URL and retry the prompt.                                               |

## Next steps

* Learn more about MCP connections: [Connect to Model Context Protocol servers](model-context-protocol).
* Review tool usage guidance: [Tool best practices](../../concepts/tool-best-practice).

The following Speech features are available in the Foundry (new) portal:

* [Speech MCP server](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/azure-ai-speech?view=foundry)
* [Speech to text quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text?context=%2Fazure%2Fai-foundry%2Fcontext%2Fcontext\&view=foundry\&pivots=ai-foundry)
* [Text to speech quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-text-to-speech?context=%2Fazure%2Fai-foundry%2Fcontext%2Fcontext\&view=foundry\&pivots=ai-foundry)
* [Text to speech avatar quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/text-to-speech-avatar/batch-synthesis-avatar?context=%2Fazure%2Fai-foundry%2Fcontext%2Fcontext\&view=foundry\&pivots=ai-foundry)
* [Voice live quickstart](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/voice-live-quickstart?context=%2Fazure%2Fai-foundry%2Fcontext%2Fcontext\&view=foundry\&pivots=ai-foundry-portal)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="note">
  When you use a [Network Secured Microsoft Foundry](../virtual-networks), you can't use private MCP servers deployed in the same virtual network. You can only use publicly accessible MCP servers.
</Callout>

Connect your Foundry agents to [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) servers by using the MCP tool. This extends agent capabilities with external tools and data sources. By connecting to remote MCP server endpoints, your agents can access tools hosted by developers and organizations that MCP-compatible clients like Foundry Agent Service can use.

MCP is an open standard that defines how applications provide tools and contextual data to large language models (LLMs). It enables consistent, scalable integration of external tools into model workflows.

In this article, you learn how to:

* Add a remote MCP server as a tool.
* Authenticate to an MCP server by using a project connection.
* Review and approve MCP tool calls.
* Troubleshoot common MCP integration issues.

For conceptual details about how MCP integration works, see [How it works](#how-it-works).

### Usage support

The following table shows SDK and setup support for MCP connections.

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

Before you begin, ensure you have:

* An Azure subscription with an active Microsoft Foundry project.

* Azure role-based access control (RBAC): Contributor or Owner role on the Foundry project.

* The latest prerelease SDK package for your language. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true) for installation details.

* Azure credentials configured for authentication (such as `DefaultAzureCredential`).

* Environment variables configured:

  * `AZURE_AI_PROJECT_ENDPOINT` or `FOUNDRY_PROJECT_ENDPOINT`: Your project endpoint URL.
  * `AZURE_AI_MODEL_DEPLOYMENT_NAME` or `MODEL_DEPLOYMENT_NAME`: Your model deployment name.
  * `MCP_PROJECT_CONNECTION_NAME`: Your MCP project connection name.

* Access to a remote MCP server endpoint (such as GitHub's MCP server at `https://api.githubcopilot.com/mcp`).

## Authentication

Many MCP servers require authentication.

In Foundry Agent Service, use a project connection to store authentication details (for example, API keys or bearer tokens) instead of hard-coding credentials in your app.

To learn about supported authentication options (key-based, Microsoft Entra identities, and OAuth identity passthrough), see [MCP server authentication](../mcp-authentication).

<Callout type="note">
  Set `project_connection_id` to the ID of your project connection.
</Callout>

## Considerations for using non-Microsoft services and servers

You're subject to the terms between you and the service provider when you use connected non-Microsoft services. When you connect to a non-Microsoft service, you pass some of your data (such as prompt content) to the non-Microsoft service, or your application might receive data from the non-Microsoft service. You're responsible for your use of non-Microsoft services and data, along with any charges associated with that use.

Third parties, not Microsoft, create the remote MCP servers that you decide to use with the MCP tool described in this article. Microsoft doesn't test or verify these servers. Microsoft has no responsibility to you or others in relation to your use of any remote MCP servers.

Carefully review and track what MCP servers you add to Foundry Agent Service. Rely on servers hosted by trusted service providers themselves rather than proxies.

The MCP tool allows you to pass custom headers, such as authentication keys or schemas, that a remote MCP server might need. Review all data that you share with remote MCP servers and log the data for auditing purposes. Be aware of non-Microsoft practices for retention and location of data.

## Best practices

For general guidance on tool usage, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

When you use MCP servers, follow these practices:

* Prefer an allow-list of tools by using `allowed_tools`.
* Require approval for high-risk operations (especially tools that write data or change resources).
* Review the requested tool name and arguments before you approve.
* Log approvals and tool calls for auditing and troubleshooting.

## Create an agent in Python with the MCP tool

Use the following code sample to create an agent and call the function. You need the latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true) for details.

<ZonePivot pivot="python">
  ### Quick verification

  Before running the full sample, verify your project connection (optional, for authenticated MCP servers):

  ```python
  import os

  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential
  from dotenv import load_dotenv

  load_dotenv()

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
  ):
      print("Connected to project.")

      # Verify MCP connection exists (optional - only needed for authenticated servers)
      connection_name = os.environ.get("MCP_PROJECT_CONNECTION_NAME")
      if connection_name:
          try:
              conn = project_client.connections.get(connection_name)
              print(f"MCP connection verified: {conn.name}")
          except Exception as e:
              print(f"MCP connection '{connection_name}' not found: {e}")
      else:
          print("MCP_PROJECT_CONNECTION_NAME not set (optional for unauthenticated servers).")
          print("Available connections:")
          for conn in project_client.connections.list():
              print(f"  - {conn.name}")
  ```

  If this code runs without errors, your credentials are configured correctly. For authenticated MCP servers, ensure your connection exists.

  ### Full sample

  The following example shows how to use the GitHub MCP server as a tool for an agent.

  ```python
  import os
  import json
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition, MCPTool, Tool
  from openai.types.responses.response_input_param import McpApprovalResponse, ResponseInputParam

  load_dotenv()

  endpoint = os.environ["AZURE_AI_PROJECT_ENDPOINT"]

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=endpoint, credential=credential) as project_client,
      project_client.get_openai_client() as openai_client,
  ):

      # [START tool_declaration]
      tool = MCPTool(
          server_label="api-specs",
          server_url="https://api.githubcopilot.com/mcp",
          require_approval="always",
          project_connection_id=os.getenv("MCP_PROJECT_CONNECTION_NAME"),
      )
      # [END tool_declaration]

      # Create a prompt agent with MCP tool capabilities
      agent = project_client.agents.create_version(
          agent_name="MyAgent7",
          definition=PromptAgentDefinition(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              instructions="Use MCP tools as needed",
              tools=[tool],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      # Create a conversation to maintain context across multiple interactions
      conversation = openai_client.conversations.create()
      print(f"Created conversation (id: {conversation.id})")

      # Send initial request that will trigger the MCP tool
      response = openai_client.responses.create(
          conversation=conversation.id,
          input="What is my username in my GitHub profile?",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      # Process any MCP approval requests that were generated
      input_list: ResponseInputParam = []
      for item in response.output:
          if item.type == "mcp_approval_request" and item.id:
              print("MCP approval requested")
              print(f"  Server: {item.server_label}")
              print(f"  Tool: {getattr(item, 'name', '<unknown>')}")
              print(
                  f"  Arguments: {json.dumps(getattr(item, 'arguments', None), indent=2, default=str)}"
              )

              # Approve only after you review the tool call.
              # In production, implement your own approval UX and policy.
              should_approve = (
                  input("Approve this MCP tool call? (y/N): ").strip().lower() == "y"
              )
              input_list.append(
                  McpApprovalResponse(
                      type="mcp_approval_response",
                      approve=should_approve,
                      approval_request_id=item.id,
                  )
              )

      print("Final input:")
      print(input_list)

      # Send the approval response back to continue the agent's work
      # This allows the MCP tool to access the GitHub repository and complete the original request
      response = openai_client.responses.create(
          input=input_list,
          previous_response_id=response.id,
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      print(f"Response: {response.output_text}")

      # Clean up resources by deleting the agent version
      # This prevents accumulation of unused agent versions in your project
      project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
      print("Agent deleted")
  ```

  ### Expected output

  The following example shows the expected output when you run the sample:

  ```console
  Agent created (id: <agent-id>, name: MyAgent7, version: 1)
  Created conversation (id: <conversation-id>)
  Final input:
  [McpApprovalResponse(type='mcp_approval_response', approve=True, approval_request_id='<approval-request-id>')]
  Response: Your GitHub username is "example-username".
  Agent deleted
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Create an agent with MCP tool

  The following example shows how to use the GitHub MCP server as a tool for an agent. The example uses synchronous methods to create an agent. For asynchronous methods, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample19_MCP.md) in the Azure SDK for .NET repository on GitHub.

  ```csharp
  // Create project client and read the environment variables that are used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME");
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Create Agent with the `MCPTool`. Note that in this scenario
  // GlobalMcpToolCallApprovalPolicy.AlwaysRequireApproval is used,
  // which means that any calls to the MCP server must be approved.
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful agent that can use MCP tools to assist users. Use the available MCP tools to answer questions and perform tasks.",
      Tools = { ResponseTool.CreateMcpTool(
          serverLabel: "api-specs",
          serverUri: new Uri("https://gitmcp.io/Azure/azure-rest-api-specs"),
          toolCallApprovalPolicy: new McpToolCallApprovalPolicy(GlobalMcpToolCallApprovalPolicy.AlwaysRequireApproval
      )) }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // If the tool approval is required, the response item is
  // of `McpToolCallApprovalRequestItem` type and contains all
  // the information about tool call. This example checks that
  // the server label is "api-specs" and approves the tool call.
  // All other calls are denied because they should not occur for
  // the current configuration.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  CreateResponseOptions nextResponseOptions = new([ResponseItem.CreateUserMessageItem("Please summarize the Azure REST API specifications README")]);
  ResponseResult latestResponse = null;

  while (nextResponseOptions is not null)
  {
      latestResponse = responseClient.CreateResponse(nextResponseOptions);
      nextResponseOptions = null;

      foreach (ResponseItem responseItem in latestResponse.OutputItems)
      {
          if (responseItem is McpToolCallApprovalRequestItem mcpToolCall)
          {
              nextResponseOptions = new CreateResponseOptions()
              {
                  PreviousResponseId = latestResponse.Id,
              };
              if (string.Equals(mcpToolCall.ServerLabel, "api-specs"))
              {
                  Console.WriteLine($"Approval requested for {mcpToolCall.ServerLabel} (tool: {mcpToolCall.ToolName})");
                  Console.Write("Approve this MCP tool call? (y/N): ");
                  bool approved = string.Equals(Console.ReadLine(), "y", StringComparison.OrdinalIgnoreCase);
                  nextResponseOptions.InputItems.Add(ResponseItem.CreateMcpApprovalResponseItem(approvalRequestId: mcpToolCall.Id, approved: approved));
              }
              else
              {
                  Console.WriteLine($"Rejecting unknown call {mcpToolCall.ServerLabel}...");
                  nextResponseOptions.InputItems.Add(ResponseItem.CreateMcpApprovalResponseItem(approvalRequestId: mcpToolCall.Id, approved: false));
              }
          }
      }
  }

  // Output the final response from the agent.
  Console.WriteLine(latestResponse.GetOutputText());

  // Clean up resources by deleting the agent version.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected output

  The following example shows the expected output when you run the sample:

  ```console
  Approval requested for api-specs...
  Response: The Azure REST API specifications repository contains the OpenAPI specifications for Azure services. It is
  organized by service and includes guidelines for contributing new specifications. The repository is intended for use by developers building tools and services that interact with Azure APIs.
  ```

  ## Create an agent with the MCP tool using project connection authentication

  ### Quick verification

  Before running the full sample, verify your project connection (optional, for authenticated MCP servers):

  ```csharp
  using Azure.AI.Projects;
  using Azure.Identity;

  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var mcpConnectionName = System.Environment.GetEnvironmentVariable("MCP_PROJECT_CONNECTION_NAME");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  Console.WriteLine("Connected to project.");

  // Verify MCP connection exists (optional - only needed for authenticated servers)
  if (!string.IsNullOrEmpty(mcpConnectionName))
  {
      try
      {
          AIProjectConnection conn = projectClient.Connections.GetConnection(connectionName: mcpConnectionName);
          Console.WriteLine($"MCP connection verified: {conn.Name}");
      }
      catch (Exception ex)
      {
          Console.WriteLine($"MCP connection '{mcpConnectionName}' not found: {ex.Message}");
      }
  }
  else
  {
      Console.WriteLine("MCP_PROJECT_CONNECTION_NAME not set (optional for unauthenticated servers).");
      Console.WriteLine("Available connections:");
      foreach (var conn in projectClient.Connections.GetConnections())
      {
          Console.WriteLine($"  - {conn.Name}");
      }
  }
  ```

  If this code runs without errors, your credentials are configured correctly. For authenticated MCP servers, ensure your connection exists.

  ### Full sample

  In this example, you learn how to authenticate to the GitHub MCP server and use it as a tool for an agent. The example uses synchronous methods to create an agent. For asynchronous methods, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample20_MCP_Connection.md) in the Azure SDK for .NET repository on GitHub.

  #### Set up project connection

  Before running the sample:

  1. Sign in to your GitHub profile.
  2. Select the profile picture at the upper right corner.
  3. Select **Settings**.
  4. In the left panel, select **Developer Settings** and **Personal access tokens > Tokens (classic)**.
  5. At the top, select **Generate new token**, enter your password, and create a token that can read public repositories.
     * **Important:** Save the token, or keep the page open as once the page is closed, token can't be shown again.
  6. In the Azure portal, open Microsoft Foundry.
  7. In the left panel, select **Management center** and then select **Connected resources**.
  8. Create new connection of **Custom keys** type.
  9. Name it and add a key value pair.
  10. Set the key name to `Authorization` and the value should have a form of `Bearer your_github_token`.

  ### Code sample to create the agent

  ```csharp
  // Create project client and read the environment variables to be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME");
  var mcpConnectionName = System.Environment.GetEnvironmentVariable("MCP_PROJECT_CONNECTION_NAME");
  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Create an agent with the MCPTool. Note that, in this scenario,
  // GlobalMcpToolCallApprovalPolicy.AlwaysRequireApproval is used.
  // This means that any calls to the MCP server must be approved.
  // The ProjectConnectionId property is then set on the McpTool
  // so agent can authenticate with GitHub.
  McpTool tool = ResponseTool.CreateMcpTool(
          serverLabel: "api-specs",
          serverUri: new Uri("https://api.githubcopilot.com/mcp"),
          toolCallApprovalPolicy: new McpToolCallApprovalPolicy(GlobalMcpToolCallApprovalPolicy.AlwaysRequireApproval
      ));
  tool.ProjectConnectionId = mcpConnectionName;
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful agent that can use MCP tools to assist users. Use the available MCP tools to answer questions and perform tasks.",
      Tools = { tool }
  };
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // If the tool approval is required, the response item is
  // of McpToolCallApprovalRequestItem type and contains all
  // the information about tool call. This example checks that
  // the server label is "api-specs" and approves the tool call,
  // All other calls are denied because they shouldn't happen given
  // the current configuration.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

  CreateResponseOptions nextResponseOptions = new([ResponseItem.CreateUserMessageItem("What is my username in my GitHub profile?")]);
  ResponseResult latestResponse = null;

  while (nextResponseOptions is not null)
  {
      latestResponse = responseClient.CreateResponse(nextResponseOptions);
      nextResponseOptions = null;

      foreach (ResponseItem responseItem in latestResponse.OutputItems)
      {
          if (responseItem is McpToolCallApprovalRequestItem mcpToolCall)
          {
              nextResponseOptions = new()
              {
                  PreviousResponseId = latestResponse.Id,
              };
              if (string.Equals(mcpToolCall.ServerLabel, "api-specs"))
              {
                  Console.WriteLine($"Approval requested for {mcpToolCall.ServerLabel} (tool: {mcpToolCall.ToolName})");
                  Console.Write("Approve this MCP tool call? (y/N): ");
                  bool approved = string.Equals(Console.ReadLine(), "y", StringComparison.OrdinalIgnoreCase);
                  nextResponseOptions.InputItems.Add(ResponseItem.CreateMcpApprovalResponseItem(approvalRequestId: mcpToolCall.Id, approved: approved));
              }
              else
              {
                  Console.WriteLine($"Rejecting unknown call {mcpToolCall.ServerLabel}...");
                  nextResponseOptions.InputItems.Add(ResponseItem.CreateMcpApprovalResponseItem(approvalRequestId: mcpToolCall.Id, approved: false));
              }
          }
      }
  }

  // Output the final response from the agent.
  Console.WriteLine(latestResponse.GetOutputText());

  // Clean up resources by deleting the agent version.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected output

  The following example shows the expected output when you run the sample:

  ```console
  Approval requested for api-specs...
  Response: Your GitHub username is "example-username".
  ```
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Create an agent in TypeScript with the MCP tool

  The following TypeScript sample demonstrates how to create an agent with MCP tool capabilities, send requests that trigger MCP approval workflows, handle approval requests, and clean up resources. For a JavaScript version, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentMcp.js) on the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import OpenAI from "openai";
  import "dotenv/config";

  import * as readline from "readline";

  import * as readline from "readline";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";

  export async function main(): Promise<void> {
    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with MCP tool...");

    // Define MCP tool that connects to Azure REST API specifications GitHub repository
    // The tool requires approval for each operation to ensure user control over external requests
    const agent = await project.agents.createVersion("agent-mcp", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a helpful agent that can use MCP tools to assist users. Use the available MCP tools to answer questions and perform tasks.",
      tools: [
        {
          type: "mcp",
          server_label: "api-specs",
          server_url: "https://gitmcp.io/Azure/azure-rest-api-specs",
          require_approval: "always",
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Create a conversation thread to maintain context across multiple interactions
    console.log("\nCreating conversation...");
    const conversation = await openAIClient.conversations.create();
    console.log(`Created conversation (id: ${conversation.id})`);

    // Send initial request that will trigger the MCP tool to access Azure REST API specs
    // This will generate an approval request since requireApproval="always"
    console.log("\nSending request that will trigger MCP approval...");
    const response = await openAIClient.responses.create(
      {
        conversation: conversation.id,
        input: "Please summarize the Azure REST API specifications Readme",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
      },
    );

    // Process any MCP approval requests that were generated
    // When requireApproval="always", the agent will request permission before accessing external resources
    const inputList: OpenAI.Responses.ResponseInputItem.McpApprovalResponse[] = [];

    const rl = readline.createInterface({ input: process.stdin, output: process.stdout });
    const ask = (q: string) => new Promise<string>((resolve) => rl.question(q, resolve));
    for (const item of response.output) {
      if (item.type === "mcp_approval_request") {
        if (item.server_label === "api-specs" && item.id) {
          console.log(`\nReceived MCP approval request (id: ${item.id})`);
          console.log(`  Server: ${item.server_label}`);
          console.log(`  Tool: ${item.name}`);

          // Approve only after you review the tool call.
          // In production, implement your own approval UX and policy.
          const answer = (await ask("Approve this MCP tool call? (y/N): ")).trim().toLowerCase();
          const approve = answer === "y";
          inputList.push({
            type: "mcp_approval_response",
            approval_request_id: item.id,
            approve,
          });
        }
      }
    }

    rl.close();

    console.log(`\nProcessing ${inputList.length} approval request(s)`);
    console.log("Final input:");
    console.log(JSON.stringify(inputList, null, 2));

    // Send the approval response back to continue the agent's work
    // This allows the MCP tool to access the GitHub repository and complete the original request
    console.log("\nSending approval response...");
    const finalResponse = await openAIClient.responses.create(
      {
        input: inputList,
        previous_response_id: response.id,
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
      },
    );

    console.log(`\nResponse: ${finalResponse.output_text}`);

    // Clean up resources by deleting the agent version and conversation
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await openAIClient.conversations.delete(conversation.id);
    console.log("Conversation deleted");

    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nMCP sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  The following example shows the expected output when you run the sample:

  ```console
  Creating agent with MCP tool...
  Agent created (id: <agent-id>, name: agent-mcp, version: 1)

  Creating conversation...
  Created conversation (id: <conversation-id>)

  Sending request that will trigger MCP approval...

  Received MCP approval request (id: <approval-request-id>)
    Server: api-specs
    Tool: get-readme

  Processing 1 approval request(s)
  Final input:
  [
    {
      "type": "mcp_approval_response",
      "approval_request_id": "<approval-request-id>",
      "approve": true
    }
  ]

  Sending approval response...

  Response: The Azure REST API specifications repository contains the OpenAPI specifications for Azure services. It is organized by service and includes guidelines for contributing new specifications. The repository is intended for use by developers building tools and services that interact with Azure APIs.

  Cleaning up resources...
  Conversation deleted
  Agent deleted

  MCP sample completed!
  ```

  ## Create an agent with the MCP tool using project connection authentication

  ### Quick verification

  Before running the full sample, verify your project connection (optional, for authenticated MCP servers):

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const mcpConnectionName = process.env["MCP_PROJECT_CONNECTION_NAME"] || "";

  async function verifyConnection(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    console.log("Connected to project.");

    // Verify MCP connection exists (optional - only needed for authenticated servers)
    if (mcpConnectionName) {
      try {
        const conn = await project.connections.get(mcpConnectionName);
        console.log(`MCP connection verified: ${conn.name}`);
      } catch (error) {
        console.log(`MCP connection '${mcpConnectionName}' not found: ${error}`);
      }
    } else {
      console.log("MCP_PROJECT_CONNECTION_NAME not set (optional for unauthenticated servers).");
      console.log("Available connections:");
      for await (const conn of project.connections.list()) {
        console.log(`  - ${conn.name}`);
      }
    }
  }

  verifyConnection().catch(console.error);
  ```

  If this code runs without errors, your credentials are configured correctly. For authenticated MCP servers, ensure your connection exists.

  ### Full sample

  The following TypeScript sample demonstrates how to create an agent with MCP tool capabilities using project connection authentication, send requests that trigger MCP approval workflows, handle approval requests, and clean up resources. For a JavaScript version, see the [sample code](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentMcpConnectionAuth.js) on the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import OpenAI from "openai";
  import "dotenv/config";

  const projectEndpoint = process.env["FOUNDRY_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const mcpConnectionName = process.env["MCP_PROJECT_CONNECTION_NAME"] || "";

  export async function main(): Promise<void> {
    // Create AI Project client
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with MCP tool using project connection...");

    // Define MCP tool that connects to GitHub Copilot API with project connection authentication
    // The project connection should have Authorization header configured with "Bearer <GitHub PAT token>"
    // Token can be created at https://github.com/settings/personal-access-tokens/new
    const agent = await project.agents.createVersion("agent-mcp-connection-auth", {
      kind: "prompt",
      model: deploymentName,
      instructions: "Use MCP tools as needed",
      tools: [
        {
          type: "mcp",
          server_label: "api-specs",
          server_url: "https://api.githubcopilot.com/mcp",
          require_approval: "always",
          project_connection_id: mcpConnectionName,
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Create a conversation thread to maintain context across multiple interactions
    console.log("\nCreating conversation...");
    const conversation = await openAIClient.conversations.create();
    console.log(`Created conversation (id: ${conversation.id})`);

    // Send initial request that will trigger the MCP tool
    console.log("\nSending request that will trigger MCP approval...");
    const response = await openAIClient.responses.create(
      {
        conversation: conversation.id,
        input: "What is my username in my GitHub profile?",
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
      },
    );

    // Process any MCP approval requests that were generated
    const inputList: OpenAI.Responses.ResponseInputItem.McpApprovalResponse[] = [];

    const rl = readline.createInterface({ input: process.stdin, output: process.stdout });
    const ask = (q: string) => new Promise<string>((resolve) => rl.question(q, resolve));
    for (const item of response.output) {
      if (item.type === "mcp_approval_request") {
        if (item.server_label === "api-specs" && item.id) {
          console.log(`\nReceived MCP approval request (id: ${item.id})`);
          console.log(`  Server: ${item.server_label}`);
          console.log(`  Tool: ${item.name}`);

          // Approve only after you review the tool call.
          // In production, implement your own approval UX and policy.
          const answer = (await ask("Approve this MCP tool call? (y/N): ")).trim().toLowerCase();
          const approve = answer === "y";
          inputList.push({
            type: "mcp_approval_response",
            approval_request_id: item.id,
            approve,
          });
        }
      }
    }

    rl.close();

    console.log(`\nProcessing ${inputList.length} approval request(s)`);
    console.log("Final input:");
    console.log(JSON.stringify(inputList, null, 2));

    // Send the approval response back to continue the agent's work
    // This allows the MCP tool to access the GitHub repository and complete the original request
    console.log("\nSending approval response...");
    const finalResponse = await openAIClient.responses.create(
      {
        input: inputList,
        previous_response_id: response.id,
      },
      {
        body: { agent: { name: agent.name, type: "agent_reference" } },
      },
    );

    console.log(`\nResponse: ${finalResponse.output_text}`);

    // Clean up resources by deleting the agent version and conversation
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await openAIClient.conversations.delete(conversation.id);
    console.log("Conversation deleted");

    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nMCP with project connection sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  The following example shows the expected output when you run the sample:

  ```console
  Creating agent with MCP tool using project connection...
  Agent created (id: <agent-id>, name: agent-mcp-connection-auth, version: 1)
  Creating conversation...
  Created conversation (id: <conversation-id>)
  Sending request that will trigger MCP approval...
  Received MCP approval request (id: <approval-request-id>)
    Server: api-specs
    Tool: get-github-username
  Processing 1 approval request(s)
  Final input:
  [
    {
      "type": "mcp_approval_response",
      "approval_request_id": "<approval-request-id>",
      "approve": true
    }
  ]
  Sending approval response...
  Response: Your GitHub username is "example-username".
  Cleaning up resources...
  Conversation deleted
  Agent deleted
  MCP with project connection sample completed!
  ```
</ZonePivot>

<ZonePivot pivot="rest">
  ## Create a response that uses the MCP tool (REST API)

  The following example shows how to call an MCP tool by using the Responses API. If the response includes an output item with `type` set to `mcp_approval_request`, send a follow-up request that includes a `mcp_approval_response` item.

  ### Prerequisites

  Set these environment variables:

  * `PROJECT_ENDPOINT`: Your project endpoint URL.
  * `API_VERSION`: The API version (for example, `2025-11-15-preview`).
  * `AGENT_TOKEN`: A bearer token for Foundry.
  * `MODEL_DEPLOYMENT_NAME`: Your model deployment name.
  * `MCP_PROJECT_CONNECTION_NAME` (optional): Your MCP project connection name.

  If your MCP server doesn't require authentication, omit `project_connection_id` from the request body.

  <Callout type="note">
    For REST API, you need to first retrieve the connection ID from the connection name using the Connections API, then pass the ID to the MCP tool configuration.
  </Callout>

  <Callout type="tip">
    For details on the MCP tool schema and approval items, see [OpenAI.MCPTool](../../../reference/foundry-project-rest-preview#openaimcptool) and the MCP approval item types in the REST reference.
  </Callout>

  ```bash
  curl --request POST \
    --url "$PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
    "model": "'$MODEL_DEPLOYMENT_NAME'",
    "input": "What is my username in my GitHub profile?",
    "tools": [
      {
        "type": "mcp",
        "server_label": "github",
        "server_url": "https://api.githubcopilot.com/mcp",
        "project_connection_id": "'$MCP_PROJECT_CONNECTION_NAME'"
      }
    ]
  }'
  ```

  If the response includes an output item with `type` set to `mcp_approval_request`, copy the approval request item `id` as `APPROVAL_REQUEST_ID`. Also copy the top-level response `id` as `PREVIOUS_RESPONSE_ID`.

  ### 2. Send an approval response

  ```bash
  curl --request POST \
    --url "$PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
    "previous_response_id": "'$PREVIOUS_RESPONSE_ID'",
    "input": [
      {
        "type": "mcp_approval_response",
        "approval_request_id": "'$APPROVAL_REQUEST_ID'",
        "approve": true
      }
    ]
  }'
  ```
</ZonePivot>

## How it works

You need to bring a remote MCP server (an existing MCP server endpoint) to Foundry Agent Service. You can bring multiple remote MCP servers by adding them as tools. For each tool, you need to provide a unique `server_label` value within the same agent and a `server_url` value that points to the remote MCP server. Be sure to carefully review which MCP servers you add to Foundry Agent Service.

For more information on using MCP, see:

* [Security Best Practices](https://modelcontextprotocol.io/specification/draft/basic/security_best_practices) on the Model Context Protocol website.
* [Understanding and mitigating security risks in MCP implementations](https://techcommunity.microsoft.com/blog/microsoft-security-blog/understanding-and-mitigating-security-risks-in-mcp-implementations/4404667) in the Microsoft Security Community Blog.

## Set up the MCP connection

The following steps outline how to connect to a remote MCP server from Foundry Agent Service:

1. Find the remote MCP server that you want to connect to, such as the GitHub MCP server. Create or update a Foundry agent with an `mcp` tool by using the following information:

   1. `server_url`: The URL of the MCP server, such as `https://api.githubcopilot.com/mcp/`.

   2. `server_label`: A unique identifier of this MCP server to the agent, such as `github`.

   3. `allowed_tools`: An optional list of tools that this agent can access and use. If you don't provide this value, the default value includes all of the tools in the MCP server.

   4. `require_approval`: Optionally determine whether approval is required. The default value is `always`. Supported values are:

      * `always`: A developer needs to provide approval for every call. If you don't provide a value, this one is the default.
      * `never`: No approval is required.
      * `{"never":[<tool_name_1>, <tool_name_2>]}`: You provide a list of tools that don't require approval.
      * `{"always":[<tool_name_1>, <tool_name_2>]}`: You provide a list of tools that require approval.

2. `project_connection_id`: The project connection ID that stores authentication and other connection details for the MCP server.

3. If the model tries to invoke a tool in your MCP server with approval required, you get a response output item type as `mcp_approval_request`. In the response output item, you can get more details on which tool in the MCP server is called and arguments to be passed. Review the tool and arguments so that you can make an informed decision for approval.

4. Submit your approval to the agent by using `response_id` and setting `approve` to `true`.

## Known limitations

* **Non-streaming MCP tool call timeout**: Non-streaming MCP tool calls have a timeout of 120 seconds. If your MCP server takes longer than 120 seconds to respond, the call fails. To avoid timeouts, ensure that your MCP server responds within this limit. If your use case requires longer processing times, consider optimizing the server-side logic or breaking the operation into smaller steps.

## Common questions and errors

The following are common issues that you might encounter when using MCP tools with Foundry Agent Service:

* "Invalid tool schema":

  An invalid tool schema usually happens if you have `anyOf` or `allOf` in your MCP server definition, or if a parameter can take multiple types of values. Update your MCP server definition and try again.

* "Unauthorized" or "Forbidden" from the MCP server:

  Confirm the MCP server supports your authentication method, and verify the credentials stored in your project connection. For GitHub, use least-privilege tokens and rotate them regularly.

* The model never calls your MCP tool:

  Confirm your agent instructions encourage tool usage, and verify `server_label`, `server_url`, and `allowed_tools` values. If you set `allowed_tools`, make sure the tool name matches what the MCP server exposes.

* The agent never continues after approval:

  Confirm you send a follow-up request with `previous_response_id` set to the original response ID, and that you use the approval request item ID as `approval_request_id`.

## Host a local MCP server

The Agent Service runtime only accepts a remote MCP server endpoint. If you want to add tools from a local MCP server, you need to self-host it on [Azure Container Apps](https://github.com/Azure-Samples/mcp-container-ts) or [Azure Functions](https://github.com/Azure-Samples/mcp-sdk-functions-hosting-python/tree/main) to get a remote MCP server endpoint. Consider the following factors when hosting local MCP servers in the cloud:

| Local MCP server setup     | Hosting in Azure Container Apps                                                     | Hosting in Azure Functions                                                   |
| -------------------------- | ----------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| **Transport**              | HTTP POST/GET endpoints required.                                                   | HTTP streamable required.                                                    |
| **Code changes**           | Container rebuild required.                                                         | Azure Functions-specific configuration files required in the root directory. |
| **Authentication**         | Custom authentication implementation required.                                      | Key-based only. OAuth needs API Management.                                  |
| **Language**               | Any language that runs in Linux containers (Python, Node.js, .NET, TypeScript, Go). | Python, Node.js, Java, .NET only.                                            |
| **Container requirements** | Linux (linux/amd64) only. No privileged containers.                                 | Containerized servers aren't supported.                                      |
| **Dependencies**           | All dependencies must be in container image.                                        | OS-level dependencies (such as Playwright) aren't supported.                 |
| **State**                  | Stateless only.                                                                     | Stateless only.                                                              |
| **UVX/NPX**                | Supported.                                                                          | Not supported. `npx` start commands not supported.                           |

## Related content

* [Get started with agents using code](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true)
* [MCP server authentication](../mcp-authentication)
* [Build and register a Model Context Protocol (MCP) server](../../../mcp/build-your-own-mcp-server)
* [MCP tool REST reference](../../../reference/foundry-project-rest-preview#openaimcptool)
* [Security Best Practices for MCP](https://modelcontextprotocol.io/specification/draft/basic/security_best_practices)
* [Understanding and mitigating security risks in MCP implementations](https://techcommunity.microsoft.com/blog/microsoft-security-blog/understanding-and-mitigating-security-risks-in-mcp-implementations/4404667)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

Most Model Context Protocol (MCP) servers require authentication to access the server and its underlying service. Proper authentication ensures your agents can securely connect to MCP servers, invoke their tools, and access protected resources while maintaining appropriate access controls.

In this article, you:

* Choose an authentication method based on your security requirements
* Configure key-based, Microsoft Entra, or OAuth authentication
* Set up and validate your MCP server connection

<Callout type="note">
  If you don't already have an account with the MCP server publisher, create one through the publisher's website.
</Callout>

## Prerequisites

Before you begin, you need:

* Access to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and a project. If you don't have one, see [Create projects in Foundry](../../how-to/create-projects).

* Permissions to create project connections and configure agents. For details, see [Role-based access control in the Foundry portal](../../concepts/rbac-foundry).

* The remote MCP server endpoint URL you want to connect to.

* Credentials for your selected authentication method:

  * Key-based authentication: an API key, personal access token (PAT), or other token.
  * Microsoft Entra authentication: role assignments for the agent identity or project managed identity on the underlying service.
  * OAuth identity passthrough: managed OAuth configuration or an OAuth app registration (custom OAuth).

## Choose an authentication method

In general, two authentication scenarios exist:

* **Shared authentication**: Every user of the agent uses the same identity to authenticate to the MCP server. User context doesn't persist.
* **Individual authentication**: Each user authenticates with their own account so their user context persists.

Use the following guidance to choose a method:

| Your goal                                                                   | Recommended method                                         |
| --------------------------------------------------------------------------- | ---------------------------------------------------------- |
| Use one shared identity for all users                                       | Key-based authentication or Microsoft Entra authentication |
| Preserve each user's identity and permissions                               | OAuth identity passthrough                                 |
| Avoid managing secrets when the underlying service supports Microsoft Entra | Microsoft Entra authentication                             |
| Connect to an MCP server that doesn't require auth                          | Unauthenticated access                                     |

<Callout type="tip">
  When in doubt, start with Microsoft Entra authentication if the MCP server supports it. Microsoft Entra authentication eliminates the need to manage secrets and provides built-in token rotation.
</Callout>

## Supported authentication methods

| Method                                     | Description                                                                                                                | User context persists |
| ------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------- | --------------------- |
| Key-based                                  | Provide an API key or access token to authenticate with the MCP server.                                                    | No                    |
| Microsoft Entra - agent identity           | Use the agent identity to authenticate with the MCP server. Assign the required roles on the underlying service.           | No                    |
| Microsoft Entra - project managed identity | Use the project managed identity to authenticate with the MCP server. Assign the required roles on the underlying service. | No                    |
| OAuth identity passthrough                 | Prompt users interacting with your agent to sign in and authorize access to the MCP server.                                | Yes                   |
| Unauthenticated access                     | Use this method only when the MCP server doesn't require authentication.                                                   | No                    |

## Key-based authentication

Use key-based authentication when the MCP server requires an API key, personal access token, or similar credential, and you don't need to preserve individual user context.

<Callout type="note">
  People who have access to the project can access an API key stored in a project connection. Store only shared secrets in a project connection. For user-specific access, use OAuth identity passthrough.
</Callout>

Pass an API key, a personal access token (PAT), or other credentials to MCP servers that support key-based authentication. For improved security, store shared credentials in a project connection instead of passing them at runtime.

When you connect an MCP server to an agent in the Foundry portal, Foundry creates a project connection for you. Provide the credential name and credential value. For example, if you're connecting to the GitHub MCP server, you might provide:

* Credential name: `Authorization`
* Credential value: `Bearer <your-personal-access-token>`

When the agent invokes the MCP server, Agent Service retrieves the credentials from the project connection and passes them to the MCP server.

For security:

* Use least-privilege credentials where possible.
* Rotate tokens regularly.
* Restrict access to projects that contain shared secrets.

## Microsoft Entra authentication

Use Microsoft Entra authentication when the MCP server (and its underlying service) supports Microsoft Entra tokens. This method eliminates the need to manage secrets and provides automatic token rotation.

### Use agent identity authentication

Use agent identity when you want authentication scoped to a specific agent. This approach is ideal when you have multiple agents that need different levels of access to the same MCP server.

Use your agent identity to authenticate with MCP servers that support agent identity authentication. If you create your agent by using Agent Service, you automatically assign an agent identity to it.

Before publishing, all agents in your Foundry project share the same agent identity. After you publish an agent, the agent gets a unique agent identity.

Make sure the agent identity has the required role assignments on the underlying service that powers the MCP server.

When the agent invokes the MCP server, Agent Service uses the available agent identity to request an authorization token and passes it to the MCP server.

### Use project managed identity authentication

Use project managed identity when you want all agents in a project to share the same access level, or when the MCP server requires a managed identity rather than an agent identity.

Use your Foundry project's managed identity to authenticate with MCP servers that support managed identity authentication.

Make sure the project managed identity has the required role assignments on the underlying service that powers the MCP server.

When the agent invokes the MCP server, Agent Service uses the project's managed identity to request an authorization token and passes it to the MCP server.

## OAuth identity passthrough

<Callout type="note">
  To use OAuth identity passthrough, users interacting with your agent need at least the **Azure AI User** role on the project.
</Callout>

OAuth identity passthrough is available for authentication to Microsoft and non-Microsoft MCP servers and underlying services that are compliant with OAuth, including Microsoft Entra.

Use [OAuth identity](https://learn.microsoft.com/en-us/entra/architecture/auth-oauth2) passthrough to prompt users interacting with your agent to sign in to the MCP server and its underlying service. Agent Service securely stores the user's credentials and uses them only within the context of the agent communicating with the MCP server.

When you use OAuth identity passthrough, Agent Service generates a consent link the first time a particular user needs to authorize access. After the user signs in and consents, the agent can discover and invoke tools on the MCP server with that user's credentials.

Agent Service supports two OAuth options: **managed OAuth** and **custom OAuth**.

* With managed OAuth, Microsoft or the MCP server publisher manages the OAuth app.
* With custom OAuth, you bring your own OAuth app registration.

<Callout type="note">
  If you use custom OAuth, you receive a redirect URL after configuration. Add the redirect URL to your OAuth app so Agent Service can complete the flow.
</Callout>

When you set up **custom OAuth**, provide the following information:

* Client ID: required
* Client secret: optional (depends on your OAuth app)
* Auth URL: required
* Refresh URL: required (if you don't have a separate refresh URL, you can use the token URL instead)
* Token URL: required
* Scopes: optional

### Flow using OAuth identity passthrough

The scope of OAuth is per tool (connection) name per Foundry project. Each new user using a new tool (connection) in a Foundry project is prompted to provide consent.

* When a user first tries to use a new tool in a Foundry project, the response output shares the consent link in `response.output_item`. You can find the consent link in item type `oauth_consent_request`, under `consent_link`. Surface this consent link to the user.

  ```json
  "type":"response.output_item.done",
  "sequence_number":7,
  "output_index":1,
  "item":{
      "type":"oauth_consent_request",
      "id":"oauthreq_10b0f026610e2b76006981547b53d48190840179e52f39a0aa",
      "created_by":{},
      "consent_link":"https://logic-swedencentral-001.consent.azure-apihub.net/login?data=xxxx"
  }
  ```

  See an example: ![Screenshot that shows the consent dialog in the Foundry portal.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/mcp/foundry-open-consent.png)

* The user is prompted to sign in and give consent after reviewing the access needed. After giving consent successfully, the user sees a dialog like this example: ![Screenshot that shows the confirmation dialog after giving OAuth consent in the Foundry portal.](https://learn.microsoft.com/azure/ai-foundry/agents/default/media/mcp/foundry-close-me.png)

* After the user has closed the dialog, you need to submit another response with the previous response id

  ```python
  # Requires: azure-ai-projects >= 1.0.0
  from azure.ai.projects import AIProjectClient
  from azure.identity import DefaultAzureCredential

  # Submit another response after user consent
  response = client.responses.create(
       previous_response_id="YOUR_PREVIOUS_RESPONSE_ID",
       input=user_input,
       extra_body={
           "agent": {"name": agent.name, "type": "agent_reference"},
           "tool_choice": "required",
           "stream": True
       },
  )
  ```

Once the user has signed in and given consent once, they don't need to give consent in the future.

<Callout type="note">
  If the user declines consent, the MCP tool call fails and returns an error. Your application should handle this case gracefully and inform the user that the tool requires authorization to function.
</Callout>

### Bring your own Microsoft Entra app registration

<Callout type="note">
  Agent 365 MCP servers are only available to [Frontier tenants](https://adoption.microsoft.com/en-us/copilot/frontier-program/).
</Callout>

To use identity passthrough with Microsoft services, bring your own [Microsoft Entra app registration](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-register-app). By bringing your own Microsoft Entra app registration, you control what permissions you grant.

The following steps use the Agent 365 MCP server as an example:

1. Follow the [app registration guide](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-register-app) to create a Microsoft Entra app and get the client ID and client secret.

2. Grant [scoped permissions](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-configure-app-access-web-apis) to your Microsoft Entra app.

   For Agent 365 MCP servers, go to **Manage** > **API Permissions** and search for **Agent 365 Tools**. If you can't find it, search for `ea9ffc3e-8a23-4a7d-836d-234d7c7565c1`. Assign the permissions you need and grant admin consent for your tenant.

   Here are the permissions for each MCP server:

   * Microsoft Outlook Mail MCP Server (Frontier): `McpServers.Mail.All`
   * Microsoft Outlook Calendar MCP Server (Frontier): `McpServers.Calendar.All`
   * Microsoft Teams MCP Server (Frontier): `McpServers.Teams.All`
   * Microsoft 365 User Profile MCP Server (Frontier): `McpServers.Me.All`
   * Microsoft SharePoint and OneDrive MCP Server (Frontier): `McpServers.OneDriveSharepoint.All`
   * Microsoft SharePoint Lists MCP Server (Frontier): `McpServers.SharepointLists.All`
   * Microsoft Word MCP Server (Frontier): `McpServers.Word.All`
   * Microsoft 365 Copilot (Search) MCP Server (Frontier): `McpServers.CopilotMCP.All`
   * Microsoft 365 Admin Center MCP Server (Frontier): `McpServers.M365Admin.All`
   * Microsoft Dataverse MCP Server (Frontier): `McpServers.Dataverse.All`

3. Go back to [Foundry portal](https://ai.azure.com/build/tools) and configure your MCP server. Connect a tool, go to **Custom**, and then select **MCP**. Provide a name and MCP server endpoint, and then select **OAuth Identity Passthrough**:

   * client ID and client secret
   * token URL: `https://login.microsoftonline.com/{tenantId}/oauth2/v2.0/token`
   * auth URL: `https://login.microsoftonline.com/{tenantId}/oauth2/v2.0/authorize`
   * refresh URL: `https://login.microsoftonline.com/{tenantId}/oauth2/v2.0/token`
   * scopes: `ea9ffc3e-8a23-4a7d-836d-234d7c7565c1/{permission above}`

4. After you complete the configuration, you receive a [redirect URL](https://learn.microsoft.com/en-us/entra/identity-platform/how-to-add-redirect-uri). Add it to your Microsoft Entra app.

## Unauthenticated access

Use unauthenticated access only when the MCP server doesn't require authentication. This method is appropriate for public MCP servers that provide open access to their tools.

<Callout type="important">
  Even when authentication isn't required, ensure you understand the MCP server's terms of service and rate limits before connecting.
</Callout>

## Set up authentication for an MCP server

1. Identify the remote MCP server you want to connect to.

2. Create or select a project connection that stores the MCP server endpoint, authentication type, and any required credentials.

   If you connect the MCP server in the Foundry portal, the portal creates the project connection for you.

3. Create or update an agent with an `mcp` tool with the following information:

   1. `server_url`: The URL of the MCP server. For example, `https://api.githubcopilot.com/mcp/`.

   2. `server_label`: A unique identifier of this MCP server to the agent. For example, `github`.

   3. `require_approval`: Optionally determine whether approval is required. Supported values are:

      * `always`: A developer needs to provide approval for every call. If you don't provide a value, this value is the default.
      * `never`: No approval is required.
      * `{"never":[<tool_name_1>, <tool_name_2>]}`: You provide a list of tools that don't require approval.
      * `{"always":[<tool_name_1>, <tool_name_2>]}`: You provide a list of tools that require approval.

   4. `project_connection_id`: The connection name that stores the MCP server endpoint, authentication selection, and relevant information. If you provide different endpoints in the connection versus `server_url`, the endpoint in the connection is used.

4. Run the agent.

5. If the model tries to invoke a tool in your MCP server with approval required or the user needs to sign in for OAuth identity passthrough, review the response output:

   * Consent link: `oauth_consent_request`
   * Approval request: `mcp_approval_request`

   After the user signs in or you approve the call, submit another response to continue.

## Validate

After you configure authentication, verify the connection works correctly:

1. Trigger an MCP tool call from your agent by sending a prompt that causes the agent to use one of the MCP server's tools.

2. Confirm the tool call completes successfully. You should see the tool's output in the agent's response without authentication errors.

3. If you're using OAuth identity passthrough:

   * Confirm a new user receives a consent link (`oauth_consent_request` in the response).
   * After the user consents, confirm subsequent tool calls succeed without prompting for consent again.
   * Test with a different user to verify the per-user consent flow works correctly.

## Troubleshooting

| Issue                                                        | Cause                                                                                                               | Resolution                                                                                                                                                                                                               |
| ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| You don't get an `oauth_consent_request` when you expect one | The MCP tool isn't configured for OAuth identity passthrough, or the tool call didn't execute                       | Confirm the project connection is configured for OAuth identity passthrough, and make sure your prompt causes the agent to invoke the MCP tool.                                                                          |
| Consent completes but tool calls still fail                  | Missing access in the underlying service                                                                            | Confirm the user has access to the underlying service and has the **Azure AI User** role (or higher) on the project.                                                                                                     |
| Key-based authentication fails                               | Invalid or expired key or token, or the MCP server expects a different header name or value format                  | Regenerate or rotate the credential and update the project connection. Confirm the required header name and value format in the MCP server documentation.                                                                |
| Microsoft Entra authentication fails                         | The identity doesn't have required role assignments                                                                 | Assign the required roles to the agent identity or project managed identity on the underlying service, and then try again.                                                                                               |
| Tool calls are blocked unexpectedly                          | `require_approval` is set to `always` (default), or the configuration requires approval for the tool you're calling | Update `require_approval` to match your approval requirements.                                                                                                                                                           |
| MCP server returns "unauthorized" despite valid credentials  | The credential header name or format doesn't match what the MCP server expects                                      | Check the MCP server's documentation for the exact header name (for example, `Authorization`, `X-API-Key`, or `Api-Key`) and value format (for example, `Bearer <token>` vs. just `<token>`).                            |
| OAuth tokens expire and tool calls fail after some time      | The refresh token is invalid or the refresh URL is incorrect                                                        | Verify the refresh URL is correct. If you used the token URL as the refresh URL, confirm the OAuth provider supports token refresh at that endpoint. The user might need to consent again if refresh tokens are revoked. |

## Host a local MCP server

If you developed a custom MCP server or want to use an open-source MCP server that runs locally, you need to host it in the cloud before connecting it to Agent Service.

The Agent Service runtime only accepts a remote MCP server endpoint. If you want to add tools from a local MCP server, you need to self-host it on [Azure Container Apps](https://learn.microsoft.com/en-us/samples/azure-samples/mcp-container-ts/mcp-container-ts/) or [Azure Functions](https://github.com/Azure-Samples/mcp-sdk-functions-hosting-python/tree/main) to get a remote MCP server endpoint. Consider the following points when attempting to host local MCP servers in the cloud:

|   Local MCP server setup   |                           Hosting in Azure Container Apps                           |                                                                                                                                                                           Hosting in Azure Functions                                                                                                                                                                           |
| :------------------------: | :---------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|        **Transport**       |                          HTTP POST/GET endpoints required.                          |                                                                                                                                      HTTP streamable required (responses must support chunked transfer encoding for SSE-style streaming).                                                                                                                                      |
|      **Code changes**      |                            Container requires a rebuild.                            |                                                                                                                                                  Azure Functions-specific configuration files required in the root directory.                                                                                                                                                  |
|     **Authentication**     |                    Custom authentication implementation required.                   | Use [built-in authentication](https://learn.microsoft.com/en-us/azure/app-service/configure-authentication-mcp?toc=%2Fazure%2Fazure-functions%2Ftoc.json) or custom code. Azure Functions requires a key by default, but you can [disable the key requirement in host.json](https://learn.microsoft.com/en-us/azure/azure-functions/functions-bindings-mcp#hostjson-settings). |
|     **Language stack**     | Any language that runs in Linux containers (Python, Node.js, .NET, TypeScript, Go). |                                                                                                                                                                  Python, Node.js, TypeScript, Java, .NET only.                                                                                                                                                                 |
| **Container requirements** |          Linux (linux/amd64) only. Privileged containers aren't supported.          |                                                                                                                                                                     Containerized servers aren't supported.                                                                                                                                                                    |
|      **Dependencies**      |                   All dependencies must be in the container image.                  |                                                                                                                                                          OS-level dependencies (such as Playwright) aren't supported.                                                                                                                                                          |
|          **State**         |                                   Stateless only.                                   |                                                                                                                                                                                 Stateless only.                                                                                                                                                                                |
|         **UVX/NPX**        |                                      Supported.                                     |                                                                                                                                                            Not supported. The `npx` start commands aren't supported.                                                                                                                                                           |

## Next steps

* [Connect to Model Context Protocol servers](tools/model-context-protocol)
* [Agent identity concepts in Foundry](../concepts/agent-identity)
* [Role-based access control in the Foundry portal](../../concepts/rbac-foundry)
* [Add a connection in Foundry](../../how-to/connections-add)

The [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) provides a standard interface for AI agents to interact with APIs and external services. When you need to integrate private or internal enterprise systems that don't have existing MCP server implementations, you can build your own custom server. This article shows you how to create a remote MCP server using Azure Functions, register it in a private organizational tool catalog using Azure API Center, and connect it to Foundry Agent Service.

This approach enables you to securely integrate internal APIs and services into the Microsoft Foundry ecosystem, allowing agents to call your enterprise-specific tools through a standardized MCP interface.

## Prerequisites

* A Foundry project with Agent Service enabled. For setup instructions, see [Quickstart: Get started with Agent Service](../agents/quickstart).

* An Azure subscription and permissions to create resources. At minimum, you typically need the Contributor role on the target resource group.

* [Python](https://www.python.org/downloads/) version 3.11 or higher installed on your local development machine.

* [Azure Functions Core Tools](https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local?pivots=programming-language-python#install-the-azure-functions-core-tools) version 4.0.7030 or higher.

* [Azure Developer CLI](https://aka.ms/azd) installed for deployment automation.

* For local development and debugging:

  * [Visual Studio Code](https://code.visualstudio.com/)
  * [Azure Functions extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-azurefunctions) for Visual Studio Code

* An [Azure API Center resource](https://learn.microsoft.com/en-us/azure/api-center/overview) (optional, required only for organizational tool catalog registration).

<Callout type="note">
  Agent Service connects only to publicly accessible MCP server endpoints.
</Callout>

## Understand the request flow

The high-level flow looks like this:

1. You deploy an MCP server (this article uses Azure Functions) that exposes one or more MCP tools.
2. You optionally register the server in Azure API Center so it shows up in an organizational tool catalog.
3. In Foundry portal, you connect the MCP server to Agent Service.
4. When an agent needs a tool, Agent Service calls your MCP server endpoint.
5. Your MCP server validates the request, calls your internal API, and returns the tool result.

## Build an MCP server by using Azure Functions

Azure Functions is a serverless compute service that provides scale-to-zero capability, burst scaling, and enterprise features including identity-based access and virtual networking. The lightweight programming model makes it straightforward to build MCP servers so you can focus on implementing your business logic rather than infrastructure management.

1. Open a terminal or command prompt and navigate to the folder where you want to create your project.

2. Run the `azd init` command to initialize the project from [this sample MCP server template](https://github.com/Azure-Samples/remote-mcp-functions-python):

   ```bash
   azd init --template remote-mcp-functions-python -e mcpserver-python
   ```

3. Review the sample code structure. The template includes:

   * Function definitions for MCP endpoints.
   * Configuration for authentication and authorization.
   * Deployment scripts for Azure.

4. Customize the MCP server functions to expose your specific APIs and services. Modify the function code to implement the tools and capabilities your agents need.

5. Test your MCP server locally by using the Azure Functions Core Tools:

   ```bash
   func start
   ```

6. Deploy your MCP server to Azure by using the Azure Developer CLI:

   ```bash
   azd up
   ```

   Follow the prompts to select your Azure subscription and resource group.

7. After deployment completes, save the following information for later steps:

   * Remote MCP server endpoint: `https://{function_app_name}.azurewebsites.net/runtime/webhooks/mcp`
   * Authentication information: For access key authentication, note the `mcp_extension` system key in the Azure portal.

   If you prefer a CLI workflow to retrieve function access keys, see [Work with access keys in Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/function-keys-how-to?tabs=azure-cli#get-your-function-access-keys).

For additional implementation details including advanced authentication patterns and troubleshooting, refer to the tutorial [Host an MCP server on Azure Functions](https://learn.microsoft.com/en-us/azure/azure-functions/functions-mcp-tutorial?tabs=mcp-extension\&pivots=programming-language-python).

## Secure your MCP server endpoint

Before you share your MCP server with others, define and apply a security baseline:

* Require authentication. Avoid anonymous access unless your scenario explicitly needs it.
* Treat credentials as secrets. Don't hard-code keys in code or check them into source control. Store secrets in a secure store such as [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
* Implement least privilege for downstream calls. If your MCP server calls internal APIs, scope permissions to only what the exposed tools need.
* Log and monitor tool calls. Use Azure Functions logging to trace requests and troubleshoot failures.

For Agent Service authentication patterns (for example, key-based authentication, Microsoft Entra identities, and OAuth identity passthrough), see [MCP server authentication](../agents/how-to/mcp-authentication).

For governance and operational guidance when you run MCP tools, see [Foundry MCP Server best practices and security guidance](security-best-practices).

## Register your MCP server in the organizational tool catalog

When you register your MCP server in Azure API Center, you create a private organizational tool catalog. This step is optional but recommended for sharing MCP servers across your organization with consistent governance and discoverability.

To register your MCP server:

1. Sign in to the [Azure portal](https://portal.azure.com) and go to your Azure API Center resource.

   <Callout type="tip">
     The API Center name becomes your private tool catalog name in the registry filter. Choose an informative name that helps users identify your organization's tool catalog.
   </Callout>

2. In the left navigation pane, expand **Inventory** and select **Assets**.

3. Select **Register an asset** and choose **MCP server**.

4. Provide the required information about your MCP server.

5. Configure environments and deployments following the tutorial: [Add environments and deployments for APIs in Azure API Center](https://learn.microsoft.com/en-us/azure/api-center/configure-environments-deployments).

6. Configure authentication for your MCP server (optional):

   In the left navigation pane of your API Center resource, select **Governance** > **Authorization**.

   ![Screenshot showing the Azure API Center authorization configuration page with Governance menu expanded.](https://learn.microsoft.com/azure/ai-foundry/default/media/build-your-own-mcp-server/azure-api-center-authorization-page.png)

7. Select **Add configuration**.

8. Choose the security scheme that matches your MCP server requirements:

   * **API Key**: Developers provide the API key during tool configuration in Foundry
   * **OAuth**: Configure OAuth 2.0 authentication parameters
   * **HTTP**: Configure bearer token authorization

9. Provide the required authentication details for your selected scheme.

   <Callout type="note">
     If you choose API Key authentication, the key you store in Azure Key Vault isn't automatically used in Foundry. Developers must provide the API key when configuring the MCP server connection.
   </Callout>

10. Configure access management (optional):

    a. Go to your registered MCP server in API Center.

    b. Select **Details** > **Versions** > **Manage Access (preview)**.

    c. Configure which users or groups can access this MCP server through the organizational catalog.

After registration, your MCP server appears in the Foundry tool catalog with the governance and authentication settings you configured.

## Connect the MCP server to Agent Service

You can connect your MCP server to Agent Service through the organizational tool catalog (if you registered it) or as a custom MCP tool.

### Connect using the organizational tool catalog

If you registered your MCP server in Azure API Center, users with appropriate access can discover and configure it:

1. In [Foundry portal](https://ai.azure.com), go to your project.

2. Go to **Build** > **Tools** or open Agent Builder.

3. Browse the organizational tool catalog to find your registered MCP server.

4. Follow the configuration guidance displayed in the tool catalog to add the server to your agent.

### Connect using a custom MCP tool

If you don't register your MCP server in the organizational catalog, add it directly as a custom tool:

1. In [Foundry portal](https://ai.azure.com), go to your project.

2. Go to **Build** > **Tools** or open Agent Builder.

3. Select **Add tool** > **Custom** > **Model Context Protocol**.

4. Enter your MCP server details:

   * **Name**: Unique name for your remote MCP server
   * **Remote MCP Server endpoint**: Enter your remote MCP server endpoint URL (for example, `https://{function_app_name}.azurewebsites.net/runtime/webhooks/mcp`)
   * **Authentication**: Select the authentication method. For **Key-based** authentication, provide the following credential:
     * **Credential**: `"x-functions-key": "{mcp_extension_system_key}"`

5. Select **Connect** to register the custom MCP tool.

For detailed configuration steps (including project connections and approval workflows), see [Connect to Model Context Protocol servers (preview)](../agents/how-to/tools/model-context-protocol).

After connecting your MCP server, agents in your Foundry project can call the tools and functions exposed by your custom server.

## Verify the MCP server works end to end

After you deploy and connect the server, verify that the server is discoverable and that an agent can successfully invoke a tool.

1. In Foundry portal, confirm the MCP server appears in your project tool list.

2. Create an agent (or open an existing agent) and add the MCP server tool.

3. Run a prompt that should require one of your MCP tools.

4. If approval is enabled, review the tool name and arguments, then approve the call.

5. Confirm the tool call succeeds.

   If the tool call fails, open the Function App logs in Azure portal to confirm the MCP endpoint was invoked and to diagnose errors.

## Troubleshooting

Here are some common issues you might encounter when building and connecting your MCP server:

* **MCP server connection fails**: Confirm the server URL is publicly reachable and uses the MCP webhook path (`/runtime/webhooks/mcp`). Check the Function App logs in Azure portal for errors.
* **Authentication errors (401/403)**: Verify you're using the correct key or token for the authentication method you selected. Rotate keys that might have been exposed, and update any saved credentials.
* **Tool discovery problems**: If you registered the server in Azure API Center, confirm the API is published and you have access to it. If you added a custom tool, confirm the endpoint URL is correct.
* **Tool call succeeds but an internal API fails**: Review your MCP server logs to confirm what request was sent to the downstream API. Verify the MCP server identity or API credentials have the required permissions.

## Clean up resources

When you're done, delete Azure resources created by the template to avoid ongoing charges.

1. In your MCP server project folder, run:

   ```bash
   azd down --purge
   ```

2. If you registered the server in Azure API Center, remove the API entry if you no longer need it.

## Related content

* [MCP server authentication](../agents/how-to/mcp-authentication)
* [Get started with Foundry MCP Server (preview) using Visual Studio Code](get-started)
* [Foundry MCP Server best practices and security guidance](security-best-practices)
* [Explore available tools and example prompts for Foundry MCP Server (preview)](available-tools)
* [Add environments and deployments in Azure API Center](https://learn.microsoft.com/en-us/azure/api-center/configure-environments-deployments)
* [Azure Functions Python developer guide](https://learn.microsoft.com/en-us/azure/azure-functions/functions-reference-python)

<Callout type="important">
  Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).
</Callout>

<Callout type="note">
  For information on optimizing tool usage, see [best practices](../../concepts/tool-best-practice).
</Callout>

You can extend the capabilities of your Microsoft Foundry agent by adding an Agent2Agent (A2A) agent endpoint that supports the [A2A protocol](https://a2a-protocol.org/latest/). The A2A tool enables agent-to-agent communication, making it easier to share context between Foundry agents and external agent endpoints through a standardized protocol. This guide shows you how to configure and use the A2A tool in your Foundry Agent Service.

Connecting agents via the A2A tool versus a multi-agent workflow:

* **Using the A2A tool**: When Agent A calls Agent B through the A2A tool, Agent B's answer goes back to Agent A. Agent A then summarizes the answer and generates a response for the user. Agent A keeps control and continues to handle future user input.
* **Using a multi-agent workflow**: When Agent A calls Agent B through a workflow or other multi-agent orchestration, Agent B takes full responsibility for answering the user. Agent A is out of the loop. Agent B handles all subsequent user input. For more information, see [Build a workflow in Microsoft Foundry](../../concepts/workflow).

## Usage support

The following table shows SDK and setup support. A checkmark (✔️) indicates support; a dash (-) indicates the feature isn't available.

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

* An Azure subscription with an active Foundry project.

* A model deployment (for example, gpt-4) in your Foundry project.

* Required Azure role: On the Foundry resource, **Contributor** or **Owner** for management and **Azure AI User** for building an agent.

* SDK installation:

  * Python: `pip install azure-ai-projects[agents]` (latest prerelease)
  * C#: `Azure.AI.Projects` NuGet package
  * TypeScript: `@azure/ai-projects` npm package

* Environment variables configured:

  * `FOUNDRY_PROJECT_ENDPOINT`: Your project endpoint URL.
  * `FOUNDRY_MODEL_DEPLOYMENT_NAME`: Your model deployment name.
  * `A2A_PROJECT_CONNECTION_NAME`: Your A2A connection name (created in the Foundry portal).
  * `A2A_BASE_URI` (optional): The base URI for the A2A endpoint.

* An A2A connection configured in your Foundry project. For connection setup and REST examples, see [Create an A2A connection](#create-an-a2a-connection).

## Create an A2A connection

Create a project connection for your A2A endpoint so you can store authentication securely and reuse it across agent versions.

For details about supported authentication approaches, see [Agent2Agent (A2A) authentication](../../concepts/agent-to-agent-authentication).

### Create the connection in the Foundry portal

1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.![](https://learn.microsoft.com/azure/ai-foundry/agents/media/version-banner/new-foundry.png)
2. Select **Tools**.
3. Select **Connect tool**.
4. Select the **Custom** tab.
5. Select **Agent2Agent (A2A)**, and then select **Create**.
6. Enter a **Name** and an **A2A Agent Endpoint**.
7. Under **Authentication**, select an authentication method. For key-based authentication, set the credential name (for example, `x-api-key`) and the corresponding secret value.

### Get the connection identifier for code

Store your connection name in the `A2A_PROJECT_CONNECTION_NAME` environment variable. Your code uses this name to retrieve the full connection ID at runtime:

* **Python/C#/TypeScript**: Call `project.connections.get(connection_name)` to get the connection object, then access `connection.id`.
* **REST API**: Include the connection ID in the `project_connection_id` field of the A2A tool definition.

## Verify your connection

Before running the full sample, confirm your environment setup is correct. This verification script checks that your credentials work and the A2A connection exists in your project.

```python
import os

from azure.ai.projects import AIProjectClient
from azure.identity import DefaultAzureCredential
from dotenv import load_dotenv

load_dotenv()

with (
    DefaultAzureCredential() as credential,
    AIProjectClient(endpoint=os.environ["FOUNDRY_PROJECT_ENDPOINT"], credential=credential) as project_client,
):
    print("Connected to project.")

    # Verify A2A connection exists
    connection_name = os.environ.get("A2A_PROJECT_CONNECTION_NAME")
    if connection_name:
        try:
            conn = project_client.connections.get(connection_name)
            print(f"A2A connection verified: {conn.name}")
        except Exception as e:
            print(f"A2A connection '{connection_name}' not found: {e}")
    else:
        # List available connections to help find the right one
        print("A2A_PROJECT_CONNECTION_NAME not set. Available connections:")
        for conn in project_client.connections.list():
            print(f"  - {conn.name}")
```

If this code runs without errors, your credentials and A2A connection are configured correctly.

## Code example

<Callout type="note">
  You need the latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code) for details.
</Callout>

<ZonePivot pivot="python">
  ## Create an agent with the A2A tool

  ```python
  import os
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import (
      PromptAgentDefinition,
      A2ATool,
  )

  load_dotenv()

  endpoint = os.environ["FOUNDRY_PROJECT_ENDPOINT"]

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=endpoint, credential=credential) as project_client,
      project_client.get_openai_client() as openai_client,
  ):
      a2a_connection = project_client.connections.get(
          os.environ["A2A_PROJECT_CONNECTION_NAME"],
      )

      tool = A2ATool(
          project_connection_id=a2a_connection.id,
      )

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["FOUNDRY_MODEL_DEPLOYMENT_NAME"],
              instructions="You are a helpful assistant.",
              tools=[tool],
          ),
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      user_input = input("Enter your question (e.g., 'What can the secondary agent do?'): \n")

      stream_response = openai_client.responses.create(
          stream=True,
          tool_choice="required",
          input=user_input,
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )

      for event in stream_response:
          if event.type == "response.created":
              print(f"Follow-up response created with ID: {event.response.id}")
          elif event.type == "response.output_text.delta":
              print(f"Delta: {event.delta}")
          elif event.type == "response.text.done":
              print(f"\nFollow-up response done!")
          elif event.type == "response.output_item.done":
              item = event.item
              if item.type == "remote_function_call":
                  print(f"Call ID: {getattr(item, 'call_id')}")
                  print(f"Label: {getattr(item, 'label')}")
          elif event.type == "response.completed":
              print(f"\nFollow-up completed!")
              print(f"Full response: {event.response.output_text}")

      print("\nCleaning up...")
      project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
      print("Agent deleted")
  ```

  ### Expected output

  The agent responds with information about the secondary agent's capabilities, demonstrating successful A2A communication. You see streaming delta text as the response is generated, followed by completion messages. The output includes the follow-up response ID, text deltas, and a final summary of what the secondary agent can do.
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Create an agent with the A2A tool

  This example creates an agent that can call a remote A2A endpoint. For the connection setup steps, see [Create an A2A connection](#create-an-a2a-connection).

  ```csharp
  // Create an Agent client and read the environment variables, which will be used in the next steps.
  var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
  var modelDeploymentName = System.Environment.GetEnvironmentVariable("FOUNDRY_MODEL_DEPLOYMENT_NAME");
  var a2aConnectionName = System.Environment.GetEnvironmentVariable("A2A_PROJECT_CONNECTION_NAME");
  var a2aBaseUri = System.Environment.GetEnvironmentVariable("A2A_BASE_URI");

  AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

  // Create the A2ATool and provide it with the A2A connection ID.
  AIProjectConnection a2aConnection = projectClient.Connections.GetConnection(connectionName: a2aConnectionName);
  A2APreviewTool a2aTool = new()
  {
      ProjectConnectionId = a2aConnection.Id
  };
  if (!string.Equals(a2aConnection.Type.ToString(), "RemoteA2A"))
  {
      if (a2aBaseUri is null)
      {
          throw new InvalidOperationException($"The connection {a2aConnection.Name} is of {a2aConnection.Type.ToString()} type and does not carry the A2A service base URI. Please provide this value through A2A_BASE_URI environment variable.");
      }
      // Provide the service endpoint as a baseUri parameter
      // if the connection is not of a RemoteA2A type.
      a2aTool.BaseUri = new Uri(a2aBaseUri);
  }
  PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
  {
      Instructions = "You are a helpful assistant.",
      Tools = { a2aTool }
  };
  // Create the Agent version with the A2A tool.
  AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
      agentName: "myAgent",
      options: new(agentDefinition));

  // Create the response and make sure we are always using tool.
  ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
  CreateResponseOptions responseOptions = new()
  {
      ToolChoice = ResponseToolChoice.CreateRequiredChoice(),
      InputItems = { ResponseItem.CreateUserMessageItem("What can the secondary agent do?") },
  };
  ResponseResult response = responseClient.CreateResponse(responseOptions);

  // Print the Agent output.
  if (response.Status != ResponseStatus.Completed)
  {
      throw new InvalidOperationException($"Response did not complete. Status: {response.Status}");
  }
  Console.WriteLine(response.GetOutputText());

  // Clean up the created Agent version.
  projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
  ```

  ### Expected output

  The console displays the agent's response text from the A2A endpoint. After completion, the agent version is deleted to clean up resources.
</ZonePivot>

<ZonePivot pivot="rest-api">
  ## Create an A2A connection by using the REST API

  Use these examples to create a project connection that stores your authentication information.

  To get an access token for the Azure Resource Manager endpoint:

  ```azurecli
  az account get-access-token --scope https://management.azure.com/.default --query accessToken -o tsv
  ```

  ### Key-based

  ```bash
  curl --request PUT \
    --url 'https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group_name}}/providers/Microsoft.CognitiveServices/accounts/{{foundry_account_name}}/projects/{{project_name}}/connections/{{connection_name}}?api-version=2025-04-01-preview' \
    --header 'Authorization: Bearer {{token}}' \
    --header 'Content-Type: application/json' \
    --data '{
      "tags": null,
      "location": null,
      "name": "{{connection_name}}",
      "type": "Microsoft.MachineLearningServices/workspaces/connections",
      "properties": {
        "authType": "CustomKeys",
        "group": "ServicesAndApps",
        "category": "RemoteA2A",
        "expiryTime": null,
        "target": "{{a2a_endpoint}}",
        "isSharedToAll": true,
        "sharedUserList": [],
        "Credentials": {
          "Keys": {
            "{{key_name}}": "{{key_value}}"
          }
        },
        "metadata": {
          "ApiType": "Azure"
        }
      }
    }'
  ```

  ### Managed OAuth Identity Passthrough

  This option is supported when you select **Managed OAuth** in the Foundry tool catalog.

  ```bash
  curl --request PUT \
    --url 'https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group_name}}/providers/Microsoft.CognitiveServices/accounts/{{foundry_account_name}}/projects/{{project_name}}/connections/{{connection_name}}?api-version=2025-04-01-preview' \
    --header 'Authorization: Bearer {{token}}' \
    --header 'Content-Type: application/json' \
    --data '{
      "tags": null,
      "location": null,
      "name": "{{connection_name}}",
      "type": "Microsoft.MachineLearningServices/workspaces/connections",
      "properties": {
        "authType": "OAuth2",
        "group": "ServicesAndApps",
        "category": "RemoteA2A",
        "expiryTime": null,
        "target": "{{a2a_endpoint}}",
        "isSharedToAll": true,
        "sharedUserList": [],
        "useCustomConnector": false,
        "connectorName": "{{connector_name}}",
        "Credentials": {},
        "metadata": {
          "ApiType": "Azure"
        }
      }
    }'
  ```

  ### Custom OAuth Identity Passthrough

  Custom OAuth doesn't support the update operation. Create a new connection if you want to update certain values.

  If your OAuth app doesn't require a client secret, omit `ClientSecret`.

  ```bash
  curl --request PUT \
    --url 'https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group_name}}/providers/Microsoft.CognitiveServices/accounts/{{foundry_account_name}}/projects/{{project_name}}/connections/{{connection_name}}?api-version=2025-04-01-preview' \
    --header 'Authorization: Bearer {{token}}' \
    --header 'Content-Type: application/json' \
    --data '{
      "tags": null,
      "location": null,
      "name": "{{connection_name}}",
      "type": "Microsoft.MachineLearningServices/workspaces/connections",
      "properties": {
        "authType": "OAuth2",
        "group": "ServicesAndApps",
        "category": "RemoteA2A",
        "expiryTime": null,
        "target": "{{a2a_endpoint}}",
        "isSharedToAll": true,
        "sharedUserList": [],
        "TokenUrl": "{{token_url}}",
        "AuthorizationUrl": "{{authorization_url}}",
        "RefreshUrl": "{{refresh_url}}",
        "Scopes": [
          "{{scope}}"
        ],
        "Credentials": {
          "ClientId": "{{client_id}}",
          "ClientSecret": "{{client_secret}}"
        },
        "metadata": {
          "ApiType": "Azure"
        }
      }
    }'
  ```

  ### Foundry Project Managed Identity

  ```bash
  curl --request PUT \
    --url 'https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group_name}}/providers/Microsoft.CognitiveServices/accounts/{{foundry_account_name}}/projects/{{project_name}}/connections/{{connection_name}}?api-version=2025-04-01-preview' \
    --header 'Authorization: Bearer {{token}}' \
    --header 'Content-Type: application/json' \
    --data '{
      "tags": null,
      "location": null,
      "name": "{{connection_name}}",
      "type": "Microsoft.MachineLearningServices/workspaces/connections",
      "properties": {
        "authType": "ProjectManagedIdentity",
        "group": "ServicesAndApps",
        "category": "RemoteA2A",
        "expiryTime": null,
        "target": "{{a2a_endpoint}}",
        "isSharedToAll": true,
        "sharedUserList": [],
        "audience": "{{audience}}",
        "Credentials": {},
        "metadata": {
          "ApiType": "Azure"
        }
      }
    }'
  ```

  ### Agent identity

  ```bash
  curl --request PUT \
    --url 'https://management.azure.com/subscriptions/{{subscription_id}}/resourceGroups/{{resource_group_name}}/providers/Microsoft.CognitiveServices/accounts/{{foundry_account_name}}/projects/{{project_name}}/connections/{{connection_name}}?api-version=2025-04-01-preview' \
    --header 'Authorization: Bearer {{token}}' \
    --header 'Content-Type: application/json' \
    --data '{
      "tags": null,
      "location": null,
      "name": "{{connection_name}}",
      "type": "Microsoft.MachineLearningServices/workspaces/connections",
      "properties": {
        "authType": "AgenticIdentity",
        "group": "ServicesAndApps",
        "category": "RemoteA2A",
        "expiryTime": null,
        "target": "{{a2a_endpoint}}",
        "isSharedToAll": true,
        "sharedUserList": [],
        "audience": "{{audience}}",
        "Credentials": {},
        "metadata": {
          "ApiType": "Azure"
        }
      }
    }'
  ```

  ## Add A2A tool to Foundry Agent Service

  ### Create an agent version with the A2A tool

  ```bash
  curl --request POST \
    --url $FOUNDRY_PROJECT_ENDPOINT/agents/$AGENTVERSION_NAME/versions?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H 'Content-Type: application/json' \
    -d '{
    "description": "Test agent version description",
    "definition": {
      "kind": "prompt",
      "model": "{{model}}",
      "tools": [
        {
           "type": "a2a_preview",
           "base_url": "{{a2a_endpoint}}",
           "project_connection_id": "{{project_connection_id}}"
        }
      ],
      "instructions": "You are a helpful agent."
    }
  }'
  ```

  To delete an agent version, send a `DELETE` request to the same endpoint with the agent name and version.
</ZonePivot>

<ZonePivot pivot="typescript">
  This sample demonstrates how to create an AI agent with A2A capabilities by using the `A2ATool` and the Azure AI Projects client. The agent can communicate with other agents and provide responses based on inter-agent interactions by using the A2A protocol.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import { AIProjectClient } from "@azure/ai-projects";
  import * as readline from "readline";
  import "dotenv/config";

  // Load environment variables
  const projectEndpoint = process.env.FOUNDRY_PROJECT_ENDPOINT || "<project endpoint>";
  const deploymentName = process.env.FOUNDRY_MODEL_DEPLOYMENT_NAME || "<model deployment name>";
  const a2aConnectionName = process.env.A2A_PROJECT_CONNECTION_NAME || "<a2a connection name>";

  export async function main(): Promise<void> {
    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with A2A tool...");

    // Get the A2A connection by name to retrieve its ID
    const a2aConnection = await project.connections.get(a2aConnectionName);

    // Create the agent with A2A tool
    const agent = await project.agents.createVersion("MyA2AAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions: "You are a helpful assistant.",
      // Define A2A tool for agent-to-agent communication
      tools: [
        {
          type: "a2a_preview",
          project_connection_id: a2aConnection.id,
        },
      ],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    // Prompt user for input
    const rl = readline.createInterface({
      input: process.stdin,
      output: process.stdout,
    });

    const userInput = await new Promise<string>((resolve) => {
      rl.question("Enter your question (e.g., 'What can the secondary agent do?'): \n", (answer) => {
        rl.close();
        resolve(answer);
      });
    });

    console.log("\nSending request to A2A agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input: userInput,
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        const item = event.item as any;
        if (item.type === "remote_function_call") {
          // Add your handling logic for remote function call items here
          const callId = item.call_id;
          const label = item.label;
          console.log(`Call ID: ${callId ?? "None"}`);
          console.log(`Label: ${label ?? "None"}`);
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nAgent-to-Agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### Expected output

  The console displays streamed response text as the A2A agent processes the request. You see the follow-up response ID, text deltas printed to stdout, and completion messages. The agent version is deleted after the interaction completes.
</ZonePivot>

## Troubleshooting

| Issue                                    | Cause                                                   | Resolution                                                                                                                                                                                           |
| ---------------------------------------- | ------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Agent doesn't invoke the A2A tool        | Agent definition doesn't include A2A tool configuration | Confirm your agent definition includes the A2A tool and that you configured the connection. If you're using responses, confirm you're not forcing a different tool.                                  |
| Agent doesn't invoke the A2A tool        | Prompt doesn't require remote agent                     | Update your prompt to require calling the remote agent, or remove conflicting tool choice settings.                                                                                                  |
| Authentication failures (401 or 403)     | Connection authentication type mismatch                 | Confirm the connection's authentication type matches your endpoint requirements. For key-based auth, confirm the credential name matches what the endpoint expects (`x-api-key` or `Authorization`). |
| SDK sample can't find the connection     | Environment variable mismatch                           | Confirm `A2A_PROJECT_CONNECTION_NAME` matches the connection name in Foundry.                                                                                                                        |
| Network or TLS errors                    | Endpoint unreachable or invalid certificate             | Confirm the endpoint is publicly reachable and uses a valid TLS certificate. Check firewall rules and proxy settings.                                                                                |
| Remote agent returns unexpected response | Response format incompatibility                         | Confirm the remote agent follows A2A protocol specifications. Check that response content types match expected formats.                                                                              |
| Connection timeout                       | Remote agent slow to respond                            | Increase timeout settings or verify the remote agent's performance. Consider implementing retry logic with exponential backoff.                                                                      |
| Missing A2A tool in response             | Tool not enabled for the agent                          | Recreate the agent with the A2A tool explicitly enabled, and verify the connection is active and properly configured.                                                                                |

## Considerations for using non-Microsoft services

You're subject to the terms between you and the service provider when you use connected non-Microsoft services and servers ("non-Microsoft services"). Under your agreement governing use of Microsoft Online services, non-Microsoft services are non-Microsoft Products. When you connect to a non-Microsoft service, you pass some of your data (such as prompt content) to the non-Microsoft services, or your application might receive data from the non-Microsoft services. You're responsible for your use of non-Microsoft services and data, along with any charges associated with that use.

Third parties, not Microsoft, create the non-Microsoft services, including A2A agent endpoints, that you decide to use with the A2A tool described in this article. Microsoft didn't test or verify these A2A agent endpoints. Microsoft has no responsibility to you or others in relation to your use of any non-Microsoft services.

Carefully review and track the A2A agent endpoints you add to Foundry Agent Service. Rely on endpoints hosted by trusted service providers themselves rather than proxies.

The A2A tool allows you to pass custom headers, such as authentication keys or schemas, that an A2A agent endpoint might need. Review all data that you share with non-Microsoft services, including A2A agent endpoints, and log the data for auditing purposes. Be aware of non-Microsoft practices for retention and location of data.

## Related content

* [Agent2Agent (A2A) authentication](../../concepts/agent-to-agent-authentication)
* [Build a workflow in Microsoft Foundry](../../concepts/workflow)
* [Best practices for tools](../../concepts/tool-best-practice)
* [Foundry project REST API (preview)](../../../reference/foundry-project-rest-preview)

The Agent2Agent (A2A) protocol enables your agents to invoke other agents. Most A2A endpoints require authentication to access the endpoint and its underlying service. Configuring authentication ensures that only authorized users can invoke your A2A tools in Foundry Agent Service.

This article explains the authentication methods available for A2A connections and helps you choose the right approach for your scenario.

## Authentication scenarios

In general, there are two authentication scenarios:

* **Shared authentication**: Every user of your agent uses the same identity to authenticate to the A2A endpoint. Individual user context doesn't persist. This approach is ideal when all users should have the same level of access. For example, if you build a chat agent to retrieve information from Azure Cosmos DB for your organization, you might want every user to access the same shared container without requiring individual sign-in.
* **Individual authentication**: Each user of your agent authenticates with their own account, so their user context persists across interactions. This approach is essential when actions should be scoped to the user's permissions. For example, if you build a coding agent that retrieves commits and pull requests from GitHub, you want each developer to sign in with their own GitHub account so they only see repositories they have access to.

## Prerequisites

Before you choose an authentication method, you need:

* Access to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and a project. If you don't have one, see [Create projects in Foundry](../../how-to/create-projects).

* The **Azure AI User** role or higher on your project. This role grants permissions to create project connections and configure agents. For details, see [Role-based access control in the Foundry portal](../../concepts/rbac-foundry).

* The A2A endpoint URL you want to connect to. Contact the endpoint publisher to confirm which authentication methods the endpoint supports.

* Credentials for your selected authentication method:

  * **Key-based**: An API key, personal access token (PAT), or other secret token from the endpoint publisher.
  * **Microsoft Entra authentication**: Role assignments for the agent identity or project managed identity on the underlying service. The specific roles depend on the service (for example, **Cosmos DB Data Reader** for Azure Cosmos DB).

## Choose an authentication method

The authentication method you choose depends on whether you need shared or individual user context, and what authentication protocols the A2A endpoint supports.

Use the following table to choose the right method for your scenario:

| Your goal                                                                   | Recommended method                                         |
| --------------------------------------------------------------------------- | ---------------------------------------------------------- |
| Use one shared identity for all users                                       | Key-based authentication or Microsoft Entra authentication |
| Preserve each user's identity and permissions                               | OAuth identity passthrough                                 |
| Avoid managing secrets when the underlying service supports Microsoft Entra | Microsoft Entra authentication                             |
| Connect to an A2A endpoint that doesn't require auth                        | Unauthenticated access                                     |

## Supported authentication methods

The following table summarizes the authentication methods available for A2A connections:

| Method                                        | Description                                                                                                                                                                                | User context persists |
| --------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------- |
| Key-based                                     | Provide an API key or access token to authenticate with the A2A endpoint. Best for endpoints that use simple token-based authentication.                                                   | No                    |
| Microsoft Entra ID - agent identity           | Use the agent's managed identity to authenticate. Requires role assignments on the underlying service. Best for Azure services that support managed identities.                            | No                    |
| Microsoft Entra ID - project managed identity | Use the project's managed identity to authenticate. Requires role assignments on the underlying service. Use this option when you want all agents in a project to share the same identity. | No                    |
| OAuth identity passthrough                    | Prompt users to sign in and authorize access to the A2A endpoint. Required when you need per-user permissions.                                                                             | Yes                   |
| Unauthenticated access                        | No authentication required. Use this method only for A2A endpoints that are publicly accessible or don't require authentication.                                                           | No                    |

## Key-based authentication

<Callout type="note">
  Anyone with access to the project can access secrets stored in a project connection. Store only shared secrets in project connections. For user-specific access, use OAuth identity passthrough instead.
</Callout>

Use key-based authentication when the A2A endpoint accepts an API key, a personal access token (PAT), or another secret credential. For improved security, store shared credentials in a project connection instead of passing them at runtime.

When you connect your A2A endpoint to an agent in the Foundry portal, Foundry creates a project connection for you. Provide the credential name (the HTTP header name) and credential value (the header value). The format depends on what the endpoint expects.

**Common credential formats:**

| Endpoint type     | Credential name        | Credential value      |
| ----------------- | ---------------------- | --------------------- |
| Bearer token      | `Authorization`        | `Bearer <your-token>` |
| API key in header | `x-api-key`            | `<your-api-key>`      |
| Custom header     | `<custom-header-name>` | `<your-secret-value>` |

When the agent invokes the A2A endpoint, Agent Service retrieves the credentials from the project connection and includes them in the request headers.

### Security best practices for key-based authentication

* **Use least-privilege credentials**: Request only the minimum permissions needed for the agent's tasks.
* **Rotate tokens regularly**: Set a reminder to regenerate tokens before they expire.
* **Restrict project access**: Limit who can access projects that contain shared secrets.
* **Audit credential usage**: Monitor project connection access in your Azure activity logs.

## Microsoft Entra ID authentication

Use Microsoft Entra ID authentication when the A2A endpoint and its underlying service accept Microsoft Entra ID tokens. This method eliminates the need to manage secrets because Azure handles token acquisition and renewal automatically.

### Agent identity

Use your agent's managed identity to authenticate with A2A endpoints that support Microsoft Entra ID authentication. When you create an agent in Agent Service, the agent automatically receives a managed identity.

**Identity lifecycle:**

* **Before publishing**: All agents in the same project share a common identity. This simplifies development and testing.
* **After publishing**: Each published agent receives a unique identity. This provides isolation and enables granular access control.

For more information about agent identity lifecycle, see [Agent identity concepts in Microsoft Foundry](agent-identity).

**To configure agent identity authentication:**

1. Identify the underlying service that powers the A2A endpoint (for example, Azure Cosmos DB or Azure Storage).
2. Assign the required roles to the agent identity on that service. The specific roles depend on the service and the operations your agent needs to perform.
3. Configure the A2A connection to use agent identity authentication.

When the agent invokes the A2A endpoint, Agent Service uses the agent identity to request an authorization token from Microsoft Entra ID and includes it in the request.

### Foundry project managed identity

Use your Foundry project's managed identity to authenticate with A2A endpoints. This option is useful when you want all agents in a project to share the same identity for accessing resources.

**To configure project managed identity authentication:**

1. Identify the underlying service that powers the A2A endpoint.
2. Assign the required roles to the project's managed identity on that service.
3. Configure the A2A connection to use project managed identity authentication.

When the agent invokes the A2A endpoint, Agent Service uses the project's managed identity to request an authorization token from Microsoft Entra ID and includes it in the request.

## OAuth identity passthrough

<Callout type="note">
  To use OAuth identity passthrough, users interacting with your agent need at least the **Azure AI User** role on the project.
</Callout>

OAuth identity passthrough enables your agent to act on behalf of individual users. Use this method when actions should be scoped to each user's permissions, such as accessing their personal files, repositories, or other protected resources.

OAuth identity passthrough works with Microsoft and non-Microsoft A2A endpoints that support OAuth 2.0, including services that use Microsoft Entra ID.

### How OAuth identity passthrough works

1. **First interaction**: When a user first interacts with your agent, Agent Service generates a consent link.
2. **User consent**: The user opens the link, signs in to the underlying service, and authorizes the agent to access their data.
3. **Token storage**: Agent Service securely stores the user's OAuth tokens (access token and refresh token). These tokens are scoped to that specific user and agent combination.
4. **Subsequent requests**: When the agent invokes the A2A endpoint, Agent Service includes the user's access token in the request. If the access token expires, Agent Service uses the refresh token to obtain a new one.

### OAuth token types

OAuth uses two types of tokens:

| Token type        | Purpose                                                               | Lifetime                                                        |
| ----------------- | --------------------------------------------------------------------- | --------------------------------------------------------------- |
| **Access token**  | Authorizes API calls to the underlying service                        | Short-lived (typically 1 hour) to limit exposure if compromised |
| **Refresh token** | Obtains new access tokens without requiring the user to sign in again | Longer-lived (hours to weeks, or until revoked)                 |

OAuth scopes define what the agent can access and do on the user's behalf. The scopes are specified when you configure the connection and are presented to the user during the consent flow. For more information about OAuth, see the [Microsoft security documentation](https://www.microsoft.com/security/business/security-101/what-is-oauth).

### Managed OAuth vs. custom OAuth

Agent Service supports two OAuth configuration options:

| Option            | Description                                                                                       | When to use                                                                         |
| ----------------- | ------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |
| **Managed OAuth** | Microsoft or the A2A endpoint publisher manages the OAuth app registration.                       | Use when available. Simplifies setup and reduces configuration errors.              |
| **Custom OAuth**  | You provide your own OAuth app registration from Microsoft Entra ID or another identity provider. | Use when managed OAuth isn't available, or when you need custom scopes or branding. |

**To configure custom OAuth**, provide the following information:

* **Client ID**: The application ID from your OAuth app registration.
* **Client secret** (if required): The secret associated with your app registration.
* **Authorization URL**: The endpoint where users authorize access.
* **Token URL**: The endpoint where Agent Service exchanges the authorization code for tokens.
* **Refresh URL**: The endpoint for refreshing expired access tokens.
* **Scopes**: The permissions your agent needs (for example, `repo` for GitHub or `Files.Read` for Microsoft Graph).

<Callout type="important">
  If you use custom OAuth, you receive a redirect URL from Agent Service. Add this URL to your OAuth app registration's allowed redirect URIs so Agent Service can complete the authorization flow.
</Callout>

## Unauthenticated access

Use unauthenticated access only when the A2A endpoint is publicly accessible and doesn't require authentication. This option is rare in production scenarios but might be appropriate for:

* Public APIs that don't require authentication
* Internal development or testing endpoints
* Endpoints protected by network-level security (such as private endpoints) instead of authentication

## Set up authentication for an A2A connection

Follow these steps to configure authentication for an A2A connection:

1. **Identify the A2A endpoint and supported authentication methods**. Contact the endpoint publisher or check the endpoint documentation to determine which authentication methods are supported.

2. **Gather the required credentials** based on your chosen authentication method:

   * **Key-based**: Obtain the API key or token from the endpoint publisher.
   * **Microsoft Entra ID**: Identify the required role assignments for the underlying service.
   * **OAuth**: Determine whether managed OAuth is available, or gather your custom OAuth app registration details.

3. **Create a project connection** in the Foundry portal. The connection stores the A2A endpoint URL, authentication method, and credentials.

   * For general connection guidance, see [Add a new connection to your project](../../how-to/connections-add).
   * For A2A-specific configuration, see [Add an A2A agent endpoint to Foundry Agent Service](../how-to/tools/agent-to-agent).

4. **Configure role assignments** (Microsoft Entra ID authentication only). Assign the required roles to the agent identity or project managed identity on the underlying service.

5. **Add the A2A tool to your agent**. Reference the project connection you created and configure which tools from the A2A endpoint your agent can invoke.

## Validate authentication

After you configure authentication, test the connection to confirm it works correctly.

### Validate key-based or Microsoft Entra ID authentication

1. Open your agent in the Foundry portal.
2. Start a conversation and trigger an action that invokes the A2A tool.
3. Confirm the tool call completes successfully. If the call fails, check the error message and see [Troubleshooting](#troubleshooting).

### Validate OAuth identity passthrough

1. Open your agent in the Foundry portal using a test user account that hasn't previously consented.
2. Start a conversation and trigger an action that invokes the A2A tool.
3. Confirm that a consent link appears in the agent's response.
4. Open the consent link and sign in with the test user's credentials.
5. Authorize the requested permissions.
6. Return to the agent and trigger the A2A tool again.
7. Confirm the tool call completes successfully using the test user's credentials.
8. (Optional) Test with another user account to confirm consent flows work for multiple users.

## Troubleshooting

Use the following table to diagnose and resolve common authentication issues:

| Issue                                                         | Possible cause                                                                                | Resolution                                                                                                                                                             |
| ------------------------------------------------------------- | --------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Key-based authentication fails with 401 Unauthorized          | Invalid or expired token                                                                      | Regenerate the token from the endpoint publisher and update the project connection.                                                                                    |
| Key-based authentication fails with 400 Bad Request           | Incorrect header name or value format                                                         | Check the endpoint documentation for the expected header format. Common formats include `Authorization: Bearer <token>` and `x-api-key: <key>`.                        |
| Microsoft Entra ID authentication fails with 403 Forbidden    | The identity doesn't have the required role assignments                                       | Assign the required roles to the agent identity or project managed identity on the underlying service. Role assignment changes can take up to 10 minutes to propagate. |
| Microsoft Entra ID authentication fails with 401 Unauthorized | The underlying service doesn't accept Microsoft Entra ID tokens, or the audience is incorrect | Confirm the underlying service supports Microsoft Entra ID authentication. Check that the A2A endpoint is configured to accept tokens for the correct audience.        |
| Consent completes but tool calls fail                         | The user doesn't have permissions in the underlying service                                   | Confirm the user has the required permissions in the underlying service. Also confirm the user has at least the **Azure AI User** role on the Foundry project.         |
| No consent link appears for OAuth                             | OAuth identity passthrough isn't configured, or the agent didn't invoke the A2A tool          | Verify the project connection is configured for OAuth identity passthrough. Trigger an action that invokes the A2A tool.                                               |
| Consent link appears but sign-in fails                        | Custom OAuth configuration is incorrect                                                       | For custom OAuth, verify the authorization URL, client ID, and redirect URL are correct. Confirm the redirect URL is added to your OAuth app registration.             |
| Refresh token expired                                         | User hasn't interacted with the agent for an extended period                                  | The user needs to go through the consent flow again. This is expected behavior for security.                                                                           |

## Related content

* [Add an A2A agent endpoint to Foundry Agent Service](../how-to/tools/agent-to-agent): Step-by-step guide to configure an A2A tool for your agent.
* [Agent identity concepts in Microsoft Foundry](agent-identity): Learn how agent identities work and their lifecycle.
* [Role-based access control for Microsoft Foundry](../../concepts/rbac-foundry): Understand the roles and permissions available in Foundry.
* [Add a new connection to your project](../../how-to/connections-add): General guidance for creating project connections.

Connect your Microsoft Foundry agents to external APIs using OpenAPI 3.0 specifications. Agents that connect to OpenAPI tools can call external services, retrieve real-time data, and extend their capabilities beyond built-in functions.

[OpenAPI specifications](https://spec.openapis.org/oas/latest.html) define a standard way to describe HTTP APIs so you can integrate existing services with your agents. Microsoft Foundry supports three authentication methods: `anonymous`, `API key`, and `managed identity`. For help choosing an authentication method, see [Choose an authentication method](#choose-an-authentication-method).

### Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | ✔️             | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

Before you begin, make sure you have:

* An Azure subscription with the right permissions.

* Azure RBAC role: Contributor or Owner on the Foundry project.

* A Foundry project created with an endpoint configured.

* An AI model deployed in your project.

* A [basic or standard agent environment](../../environment-setup).

* SDK installed for your preferred language:

  * Python: `azure-ai-projects` (latest prerelease version)
  * C#: `Azure.AI.Projects.OpenAI`
  * TypeScript/JavaScript: `@azure/ai-projects`

### Environment variables

| Variable                          | Description                                                                    |
| --------------------------------- | ------------------------------------------------------------------------------ |
| `AZURE_AI_PROJECT_ENDPOINT`       | Your Foundry project endpoint URL (not the external OpenAPI service endpoint). |
| `AZURE_AI_MODEL_DEPLOYMENT_NAME`  | Your deployed model name.                                                      |
| `OPENAPI_PROJECT_CONNECTION_NAME` | (For API key auth) Your project connection name for the OpenAPI service.       |

* OpenAPI 3.0 specification file that meets these requirements:

  * Each function must have an `operationId` (required for the OpenAPI tool).
  * `operationId` should only contain letters, `-`, and `_`.
  * Use descriptive names to help models efficiently decide which function to use.
  * Supported content type: "application/json", "application/json-patch+json"

* For managed identity authentication: Reader role or higher on target service resources.

* For API key/token authentication: a project connection configured with your API key or token. See [Add a new connection to your project](../../../how-to/connections-add).

<Callout type="note">
  The `AZURE_AI_PROJECT_ENDPOINT` value refers to your Microsoft Foundry project endpoint, not the external OpenAPI service endpoint. You can find this endpoint in the Microsoft Foundry portal under your project’s Overview page. This endpoint is required to authenticate the agent service and is separate from any OpenAPI endpoints defined in your specification file.
</Callout>

## Understand limitations

* Your OpenAPI spec must include `operationId` for each operation, and `operationId` can include only letters, `-`, and `_`.
* Supported content types: `application/json`, `application/json-patch+json`.
* For API key authentication, use one API key security scheme per OpenAPI tool. If you need multiple security schemes, create multiple OpenAPI tools.

## Code example

<Callout type="note">
  * You need the latest prerelease package. See the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true) for details.
  * If you use API key for authentication, your connection ID should be in the format of `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`.
</Callout>

<Callout type="important">
  **For API key authentication to work**, your OpenAPI specification file must include:

  1. A `securitySchemes` section with your API key configuration, such as the header name and parameter name.
  2. A `security` section that references the security scheme.
  3. A project connection configured with the matching key name and value.

  Without these configurations, the API key isn't included in requests. For detailed setup instructions, see the [Authenticate with API key](#authenticate-with-api-key) section.

  You can also use token-based authentication (for example, a Bearer token) by storing the token in a project connection. For Bearer token auth, create a **Custom keys** connection with key set to `Authorization` and value set to `Bearer <token>` (replace `<token>` with your actual token). The word `Bearer` followed by a space must be included in the value. For details, see [Set up a Bearer token connection](#set-up-a-bearer-token-connection).
</Callout>

<ZonePivot pivot="python">
  ### Quick verification

  First, verify your environment is configured correctly:

  ```python
  # Verify authentication and project connection
  import os
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from dotenv import load_dotenv

  load_dotenv()

  endpoint = os.environ["AZURE_AI_PROJECT_ENDPOINT"]

  with DefaultAzureCredential() as credential, \
       AIProjectClient(endpoint=endpoint, credential=credential) as project_client:
      print(f"Successfully connected to project")
  ```

  If this command runs without errors, you're ready to create an agent with OpenAPI tools.

  ### Complete example

  ```python
  # Import required libraries
  import os
  import jsonref
  from dotenv import load_dotenv
  from azure.identity import DefaultAzureCredential
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition

  load_dotenv()

  endpoint = os.environ["AZURE_AI_PROJECT_ENDPOINT"]

  with (
      DefaultAzureCredential() as credential,
      AIProjectClient(endpoint=endpoint, credential=credential) as project_client,
      project_client.get_openai_client() as openai_client,
  ):

      weather_asset_file_path = os.path.abspath(os.path.join(os.path.dirname(__file__), "../assets/weather_openapi.json"))

      with open(weather_asset_file_path, "r") as f:
          openapi_weather = jsonref.loads(f.read())


      # Initialize agent OpenApi tool using the read in OpenAPI spec
      weather_tool = {
          "type": "openapi",
          "openapi":{
              "name": "weather",
              "spec": openapi_weather,
              "auth": {
                  "type": "anonymous"
              },
          }
      }

      # If you want to use key-based authentication
      # IMPORTANT: Your OpenAPI spec must include securitySchemes and security sections
      # Example spec structure for API key auth:
      # {
      #   "components": {
      #     "securitySchemes": {
      #       "apiKeyHeader": {
      #         "type": "apiKey",
      #         "name": "x-api-key",  # This must match the key name in your project connection
      #         "in": "header"
      #       }
      #     }
      #   },
      #   "security": [{"apiKeyHeader": []}]
      # }
      #
      # For Bearer token authentication, use this securitySchemes structure instead:
      # {
      #   "components": {
      #     "securitySchemes": {
      #       "bearerAuth": {
      #         "type": "apiKey",
      #         "name": "Authorization",
      #         "in": "header"
      #       }
      #     }
      #   },
      #   "security": [{"bearerAuth": []}]
      # }
      # Then set connection key = "Authorization" and value = "Bearer <token>"
      # The word "Bearer" followed by a space MUST be included in the value.

      openapi_connection = project_client.connections.get(os.environ["OPENAPI_PROJECT_CONNECTION_NAME"])
      connection_id = openapi_connection.id
      print(f"OpenAPI connection ID: {connection_id}")

      openapi_key_auth_tool={
          "type": "openapi",
          "openapi":{
              "name": "TOOL_NAME",
              "spec": SPEC_NAME,  # Must include securitySchemes and security sections
              "auth": {
                    "type": "project_connection",
                    "security_scheme": {
                        "project_connection_id": connection_id
                    }
              },
          }
      }

      # If you want to use Managed Identity authentication
      openapi_mi_auth_tool={
        "type": "openapi",
        "openapi":{
            "name": "TOOL_NAME",
            "description": "",
            "spec": SPEC_NAME,
            "auth": {
                "type": "managed_identity",
                "security_scheme": {
                    "audience": ""  #audience to the service, such as https://ai.azure.com
                }
                },
        }
    }

      agent = project_client.agents.create_version(
          agent_name="MyAgent23",
          definition=PromptAgentDefinition(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              instructions="You are a helpful assistant.",
              tools=[weather_tool],
          ),
          description="You are a helpful assistant.",
      )
      print(f"Agent created (id: {agent.id}, name: {agent.name}, version: {agent.version})")

      response = openai_client.responses.create(
          input="What's the weather in Seattle?",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )
      print(f"Response created: {response.output_text}")

      print("\nCleaning up...")
      project_client.agents.delete_version(agent_name=agent.name, agent_version=agent.version)
      print("Agent deleted")
  ```

  ### What this code does

  This example creates an agent with an OpenAPI tool that calls the wttr.in weather API using anonymous authentication. When you run the code:

  1. It loads the weather OpenAPI specification from a local JSON file.
  2. Creates an agent with the weather tool configured for anonymous access.
  3. Sends a query asking about Seattle's weather.
  4. The agent uses the OpenAPI tool to call the weather API and returns formatted results.
  5. Cleans up by deleting the agent version.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`
  * Local file: `weather_openapi.json` (OpenAPI specification)

  ### Expected output

  ```console
  Agent created (id: asst_abc123, name: MyAgent23, version: 1)
  Response created: The weather in Seattle is currently cloudy with a temperature of 52°F (11°C)...

  Cleaning up...
  Agent deleted
  ```

  ### Common errors

  * `FileNotFoundError`: OpenAPI specification file not found at specified path
  * `KeyError`: Missing required environment variables
  * `AuthenticationError`: Invalid credentials or insufficient permissions, or missing `securitySchemes` in OpenAPI spec for API key authentication
  * Invalid `operationId` format in OpenAPI spec causes tool registration failure
  * **API key not injected**: Verify your OpenAPI spec includes both `securitySchemes` and `security` sections, and that the key name matches your project connection
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Sample of using Agents with OpenAPI tool

  This example demonstrates how to use services described by an [OpenAPI specification](https://spec.openapis.org/oas/latest.html) by using an agent. It uses the [wttr.in](https://wttr.in/:help) service to get weather and its specification file [weather\_openapi.json](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/ai/Azure.AI.Agents.Persistent/tests/Samples/weather_openapi.json). This example uses synchronous methods of the Azure AI Projects client library. For an example that uses asynchronous methods, see the [sample](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample21_OpenAPI.md) in the Azure SDK for .NET repository on GitHub.

  ```csharp
  class OpenAPIDemo
  {
      // Utility method to get the OpenAPI specification file from the Assets folder.
      private static string GetFile([CallerFilePath] string pth = "")
      {
          var dirName = Path.GetDirectoryName(pth) ?? "";
          return Path.Combine(dirName, "Assets", "weather_openapi.json");
      }

      public static void Main()
      {
          // First, create an agent client and read the environment variables, which will be used in the next steps.
          var projectEndpoint = System.Environment.GetEnvironmentVariable("PROJECT_ENDPOINT");
          var modelDeploymentName = System.Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME");
          AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

          // Create an Agent with `OpenAPIAgentTool` and anonymous authentication.
          string filePath = GetFile();
          OpenAPIFunctionDefinition toolDefinition = new(
              name: "get_weather",
              spec: BinaryData.FromBytes(BinaryData.FromBytes(File.ReadAllBytes(filePath))),
              auth: new OpenAPIAnonymousAuthenticationDetails()
          );
          toolDefinition.Description = "Retrieve weather information for a location.";
          OpenAPITool openapiTool = new(toolDefinition);

          // Create the agent definition and the agent version.
          PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
          {
              Instructions = "You are a helpful assistant.",
              Tools = { openapiTool }
          };
          AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
              agentName: "myAgent",
              options: new(agentDefinition));

          // Create a response object and ask the question about the weather in Seattle, WA.
          ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
          ResponseResult response = responseClient.CreateResponse(
                  userInputText: "Use the OpenAPI tool to print out, what is the weather in Seattle, WA today."
              );
          Console.WriteLine(response.GetOutputText());

          // Finally, delete all the resources created in this sample.
          projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
      }
  }
  ```

  ### What this code does

  This C# example creates an agent with an OpenAPI tool that retrieves weather information from wttr.in by using anonymous authentication. When you run the code:

  1. It reads the weather OpenAPI specification from a local JSON file.
  2. Creates an agent with the weather tool configured.
  3. Sends a request asking about Seattle's weather using the OpenAPI tool.
  4. The agent calls the weather API and returns the results.
  5. Cleans up by deleting the agent.

  ### Required inputs

  * Environment variables: `PROJECT_ENDPOINT`, `MODEL_DEPLOYMENT_NAME`
  * Local file: `Assets/weather_openapi.json` (OpenAPI specification)

  ### Expected output

  ```console
  The weather in Seattle, WA today is cloudy with temperatures around 52°F...
  ```

  ### Common errors

  * `FileNotFoundException`: OpenAPI specification file not found in Assets folder
  * `ArgumentNullException`: Missing required environment variables
  * `UnauthorizedAccessException`: Invalid credentials or insufficient RBAC permissions
  * **API key not injected**: Verify your OpenAPI spec includes both `securitySchemes` (in `components`) and `security` sections with matching scheme names

  ## Sample of using Agents with OpenAPI tool on Web service, requiring authentication

  In this example, you use services with an OpenAPI specification by using the agent in a scenario that requires authentication. You use the TripAdvisor specification.

  The TripAdvisor service requires key-based authentication. To create a connection in the Azure portal, open Microsoft Foundry and, at the left panel select **Management center** and then select **Connected resources**. Finally, create new connection of **Custom keys** type. Name it `tripadvisor` and add a key value pair. Add key named `key` and enter a value with your TripAdvisor key.

  ```csharp
  class OpenAPIConnectedDemo
  {
      // Utility method to get the OpenAPI specification file from the Assets folder.
      private static string GetFile([CallerFilePath] string pth = "")
      {
          var dirName = Path.GetDirectoryName(pth) ?? "";
          return Path.Combine(dirName, "Assets", "tripadvisor_openapi.json");
      }

      public static void Main()
      {
          // First, we need to create agent client and read the environment variables, which will be used in the next steps.
          var projectEndpoint = System.Environment.GetEnvironmentVariable("PROJECT_ENDPOINT");
          var modelDeploymentName = System.Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME");
          AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());

          // Create an Agent with `OpenAPIAgentTool` and authentication by project connection security scheme.
          string filePath = GetFile();
          AIProjectConnection tripadvisorConnection = projectClient.Connections.GetConnection("tripadvisor");
          OpenAPIFunctionDefinition toolDefinition = new(
              name: "tripadvisor",
              spec: BinaryData.FromBytes(BinaryData.FromBytes(File.ReadAllBytes(filePath))),
              auth: new OpenAPIProjectConnectionAuthenticationDetails(new OpenAPIProjectConnectionSecurityScheme(
                  projectConnectionId: tripadvisorConnection.Id
              ))
          );
          toolDefinition.Description = "Trip Advisor API to get travel information.";
          OpenAPITool openapiTool = new(toolDefinition);

          // Create the agent definition and the agent version.
          PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
          {
              Instructions = "You are a helpful assistant.",
              Tools = { openapiTool }
          };
          AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
              agentName: "myAgent",
              options: new(agentDefinition));

          // Create a response object and ask the question about the hotels in France.
          // Test the Web service access before you run production scenarios.
          // It can be done by setting:
          // ToolChoice = ResponseToolChoice.CreateRequiredChoice()`
          // in the ResponseCreationOptions. This setting will
          // force Agent to use tool and will trigger the error if it is not accessible.
          ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);
          CreateResponseOptions responseOptions = new()
          {
              ToolChoice = ResponseToolChoice.CreateRequiredChoice(),
              InputItems =
              {
                  ResponseItem.CreateUserMessageItem("Recommend me 5 top hotels in paris, France."),
              }
          };
          ResponseResult response = responseClient.CreateResponse(
              options: responseOptions
          );
          Console.WriteLine(response.GetOutputText());

          // Finally, delete all the resources we have created in this sample.
          projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
      }
  }
  ```

  ### What this code does

  This C# example demonstrates using an OpenAPI tool with API key authentication through a project connection. When you run the code:

  1. It loads the TripAdvisor OpenAPI specification from a local file.
  2. Retrieves the `tripadvisor` project connection containing your API key.
  3. Creates an agent with the TripAdvisor tool configured to use the connection for authentication.
  4. Sends a request for hotel recommendations in Paris.
  5. The agent calls the TripAdvisor API using your stored API key and returns results.
  6. Cleans up by deleting the agent.

  ### Required inputs

  * Environment variables: `PROJECT_ENDPOINT`, `MODEL_DEPLOYMENT_NAME`
  * Local file: `Assets/tripadvisor_openapi.json`
  * Project connection: `tripadvisor` with valid API key configured

  ### Expected output

  ```console
  Here are 5 top hotels in Paris, France:
  1. Hotel Name - Rating: 4.5/5, Location: ...
  2. Hotel Name - Rating: 4.4/5, Location: ...
  ...
  ```

  ### Common errors

  * `ConnectionNotFoundException`: No project connection named `tripadvisor` found.
  * `AuthenticationException`: Invalid API key in project connection, or missing/incorrect `securitySchemes` configuration in OpenAPI spec.
  * Tool not used: Verify `ToolChoice = ResponseToolChoice.CreateRequiredChoice()` forces tool usage.
  * **API key not passed to API**: Ensure the OpenAPI spec has proper `securitySchemes` and `security` sections configured.
</ZonePivot>

<ZonePivot pivot="rest">
  The following examples show how to call an OpenAPI tool by using the REST API.

  ### Anonymous authentication

  ```bash
  curl --request POST \
    --url "$AZURE_AI_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    --header "Authorization: Bearer $AGENT_TOKEN" \
    --header "Content-Type: application/json" \
    --data '{
      "model": "'$AZURE_AI_MODEL_DEPLOYMENT_NAME'",
      "input": "Use the OpenAPI tool to get the weather in Seattle, WA today.",
      "tools": [
        {
          "type": "openapi",
          "openapi": {
            "name": "weather",
            "description": "Tool to get weather data",
            "auth": { "type": "anonymous" },
            "spec": {
              "openapi": "3.1.0",
              "info": {
                "title": "get weather data",
                "description": "Retrieves current weather data for a location.",
                "version": "v1.0.0"
              },
              "servers": [{ "url": "https://wttr.in" }],
              "paths": {
                "/{location}": {
                  "get": {
                    "description": "Get weather information for a specific location",
                    "operationId": "GetCurrentWeather",
                    "parameters": [
                      {
                        "name": "location",
                        "in": "path",
                        "description": "City or location to retrieve the weather for",
                        "required": true,
                        "schema": { "type": "string" }
                      },
                      {
                        "name": "format",
                        "in": "query",
                        "description": "Format in which to return data. Always use 3.",
                        "required": true,
                        "schema": { "type": "integer", "default": 3 }
                      }
                    ],
                    "responses": {
                      "200": {
                        "description": "Successful response",
                        "content": {
                          "text/plain": {
                            "schema": { "type": "string" }
                          }
                        }
                      },
                      "404": { "description": "Location not found" }
                    }
                  }
                }
              }
            }
          }
        }
      ]
    }'
  ```

  ### API key authentication (project connection)

  ```bash
  curl --request POST \
    --url "$AZURE_AI_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    --header "Authorization: Bearer $AGENT_TOKEN" \
    --header "Content-Type: application/json" \
    --data '{
      "model": "'$AZURE_AI_MODEL_DEPLOYMENT_NAME'",
      "input": "Use the OpenAPI tool to get the weather in Seattle, WA today.",
      "tools": [
        {
          "type": "openapi",
          "openapi": {
            "name": "weather",
            "description": "Tool to get weather data",
            "auth": {
              "type": "project_connection",
              "security_scheme": {
                "project_connection_id": "'$WEATHER_APP_PROJECT_CONNECTION_ID'"
              }
            },
            "spec": {
              "openapi": "3.1.0",
              "info": {
                "title": "get weather data",
                "description": "Retrieves current weather data for a location.",
                "version": "v1.0.0"
              },
              "servers": [{ "url": "https://wttr.in" }],
              "paths": {
                "/{location}": {
                  "get": {
                    "description": "Get weather information for a specific location",
                    "operationId": "GetCurrentWeather",
                    "parameters": [
                      {
                        "name": "location",
                        "in": "path",
                        "description": "City or location to retrieve the weather for",
                        "required": true,
                        "schema": { "type": "string" }
                      },
                      {
                        "name": "format",
                        "in": "query",
                        "description": "Format in which to return data. Always use 3.",
                        "required": true,
                        "schema": { "type": "integer", "default": 3 }
                      }
                    ],
                    "responses": {
                      "200": {
                        "description": "Successful response",
                        "content": {
                          "text/plain": {
                            "schema": { "type": "string" }
                          }
                        }
                      },
                      "404": { "description": "Location not found" }
                    }
                  }
                }
              },
              "components": {
                "securitySchemes": {
                  "apiKeyHeader": {
                    "type": "apiKey",
                    "name": "x-api-key",
                    "in": "header"
                  }
                }
              },
              "security": [
                { "apiKeyHeader": [] }
              ]
            }
          }
        }
      ]
    }'
  ```

  ### Managed identity authentication

  ```bash
  curl --request POST \
    --url "$AZURE_AI_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION" \
    --header "Authorization: Bearer $AGENT_TOKEN" \
    --header "Content-Type: application/json" \
    --data '{
      "model": "'$AZURE_AI_MODEL_DEPLOYMENT_NAME'",
      "input": "Use the OpenAPI tool to get the weather in Seattle, WA today.",
      "tools": [
        {
          "type": "openapi",
          "openapi": {
            "name": "weather",
            "description": "Tool to get weather data",
            "auth": {
              "type": "managed_identity",
              "security_scheme": {
                "audience": "'$MANAGED_IDENTITY_AUDIENCE'"
              }
            },
            "spec": {
              "openapi": "3.1.0",
              "info": {
                "title": "get weather data",
                "description": "Retrieves current weather data for a location.",
                "version": "v1.0.0"
              },
              "servers": [{ "url": "https://wttr.in" }],
              "paths": {
                "/{location}": {
                  "get": {
                    "description": "Get weather information for a specific location",
                    "operationId": "GetCurrentWeather",
                    "parameters": [
                      {
                        "name": "location",
                        "in": "path",
                        "description": "City or location to retrieve the weather for",
                        "required": true,
                        "schema": { "type": "string" }
                      },
                      {
                        "name": "format",
                        "in": "query",
                        "description": "Format in which to return data. Always use 3.",
                        "required": true,
                        "schema": { "type": "integer", "default": 3 }
                      }
                    ],
                    "responses": {
                      "200": {
                        "description": "Successful response",
                        "content": {
                          "text/plain": {
                            "schema": { "type": "string" }
                          }
                        }
                      },
                      "404": { "description": "Location not found" }
                    }
                  }
                }
              }
            }
          }
        }
      ]
    }'
  ```

  ### What this code does

  This REST API example shows how to call an OpenAPI tool with different authentication methods. The request:

  1. Sends a query to the agent asking about Seattle's weather.
  2. Includes the OpenAPI tool definition inline with the weather API specification.
  3. Shows three authentication options (anonymous, API key via project connection, managed identity) as commented alternatives.
  4. The agent uses the tool to call the weather API and returns formatted results.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `API_VERSION`, `AGENT_TOKEN`, `AZURE_AI_MODEL_DEPLOYMENT_NAME`.
  * For API key auth: `WEATHER_APP_PROJECT_CONNECTION_ID`.
  * For managed identity auth: `MANAGED_IDENTITY_AUDIENCE`.
  * Inline OpenAPI specification in request body.

  ### Expected output

  ```json
  {
    "id": "resp_abc123",
    "object": "response",
    "output": [
      {
        "type": "message",
        "content": [
          {
            "type": "text",
            "text": "The weather in Seattle, WA today is cloudy with a temperature of 52°F (11°C)..."
          }
        ]
      }
    ]
  }
  ```

  ### Common errors

  * `401 Unauthorized`: Invalid or missing `AGENT_TOKEN`, or API key not injected because `securitySchemes` and `security` are missing in your OpenAPI spec
  * `404 Not Found`: Incorrect endpoint or model deployment name
  * `400 Bad Request`: Malformed OpenAPI specification or invalid auth configuration
  * **API key not sent with request**: Verify the `components.securitySchemes` section in your OpenAPI spec is properly configured (not empty) and matches your project connection key name
</ZonePivot>

<ZonePivot pivot="typescript">
  ## Create an agent with OpenAPI tool capabilities

  The following TypeScript code example demonstrates how to create an AI agent with OpenAPI tool capabilities by using the `OpenApiAgentTool` and synchronous Azure AI Projects client. The agent can call external APIs defined by OpenAPI specifications. For a JavaScript version of this example, see the [sample](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentOpenApi.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import {
    AIProjectClient,
    OpenApiAgentTool,
    OpenApiFunctionDefinition,
    OpenApiAnonymousAuthDetails,
  } from "@azure/ai-projects";
  import * as fs from "fs";
  import * as path from "path";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const weatherSpecPath = path.resolve(__dirname, "../assets", "weather_openapi.json");

  function loadOpenApiSpec(specPath: string): unknown {
    if (!fs.existsSync(specPath)) {
      throw new Error(`OpenAPI specification not found at: ${specPath}`);
    }

    try {
      const data = fs.readFileSync(specPath, "utf-8");
      return JSON.parse(data);
    } catch (error) {
      throw new Error(`Failed to read or parse OpenAPI specification at ${specPath}: ${error}`);
    }
  }

  function createWeatherTool(spec: unknown): OpenApiAgentTool {
    const auth: OpenApiAnonymousAuthDetails = { type: "anonymous" };
    const definition: OpenApiFunctionDefinition = {
      name: "get_weather",
      description: "Retrieve weather information for a location using wttr.in",
      spec,
      auth,
    };

    return {
      type: "openapi",
      openapi: definition,
    };
  }

  export async function main(): Promise<void> {
    console.log("Loading OpenAPI specifications from assets directory...");
    const weatherSpec = loadOpenApiSpec(weatherSpecPath);

    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with OpenAPI tool...");

    const agent = await project.agents.createVersion("MyOpenApiAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a helpful assistant that can call external APIs defined by OpenAPI specs to answer user questions.",
      tools: [createWeatherTool(weatherSpec)],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    console.log("\nSending request to OpenAPI-enabled agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input:
          "What's the weather in Seattle and how should I plan my outfit for the day based on the forecast?",
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        const item = event.item as any;
        if (item.type === "message") {
          const content = item.content?.[item.content.length - 1];
          if (content?.type === "output_text" && content.annotations) {
            for (const annotation of content.annotations) {
              if (annotation.type === "url_citation") {
                console.log(
                  `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                );
              }
            }
          }
        } else if (item.type === "tool_call") {
          console.log(`Tool call completed: ${item.name ?? "unknown"}`);
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nOpenAPI agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### What this code does

  This TypeScript example creates an agent with an OpenAPI tool for weather data by using anonymous authentication. When you run the code:

  1. It loads the weather OpenAPI specification from a local JSON file.
  2. Creates an agent with the weather tool configured.
  3. Sends a streaming request asking about Seattle's weather and outfit planning.
  4. Processes the streaming response and displays deltas as they arrive.
  5. It forces tool usage by using `tool_choice: "required"` to ensure the API is called.
  6. Cleans up by deleting the agent.

  ## Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `MODEL_DEPLOYMENT_NAME`
  * Local file: `../assets/weather_openapi.json` (OpenAPI specification)

  ### Expected output

  ```console
  Loading OpenAPI specifications from assets directory...
  Creating agent with OpenAPI tool...
  Agent created (id: asst_abc123, name: MyOpenApiAgent, version: 1)

  Sending request to OpenAPI-enabled agent with streaming...
  Follow-up response created with ID: resp_xyz789
  The weather in Seattle is currently...
  Tool call completed: get_weather

  Follow-up completed!

  Cleaning up resources...
  Agent deleted

  OpenAPI agent sample completed!
  ```

  ### Common errors

  * `Error: OpenAPI specification not found`: File path incorrect or file missing
  * Missing environment variables causes initialization failure
  * `AuthenticationError`: Invalid Azure credentials
  * **API key not working**: If switching from anonymous to API key auth, ensure your OpenAPI spec has `securitySchemes` and `security` properly configured

  ## Create an agent that uses OpenAPI tools authenticated with a project connection

  The following TypeScript code example demonstrates how to create an AI agent that uses OpenAPI tools authenticated through a project connection. The agent loads the TripAdvisor OpenAPI specification from local assets and can invoke the API through the configured project connection. For a JavaScript version of this example, see the [sample](https://github.com/Azure/azure-sdk-for-js/blob/main/sdk/ai/ai-projects/samples/v2-beta/javascript/agents/tools/agentOpenApiConnectionAuth.js) in the Azure SDK for JavaScript repository on GitHub.

  ```typescript
  import { DefaultAzureCredential } from "@azure/identity";
  import {
    AIProjectClient,
    OpenApiAgentTool,
    OpenApiFunctionDefinition,
    OpenApiProjectConnectionAuthDetails,
  } from "@azure/ai-projects";
  import * as fs from "fs";
  import * as path from "path";
  import "dotenv/config";

  const projectEndpoint = process.env["AZURE_AI_PROJECT_ENDPOINT"] || "<project endpoint>";
  const deploymentName = process.env["MODEL_DEPLOYMENT_NAME"] || "<model deployment name>";
  const tripAdvisorProjectConnectionId =
    process.env["TRIPADVISOR_PROJECT_CONNECTION_ID"] || "<tripadvisor project connection id>";
  const tripAdvisorSpecPath = path.resolve(__dirname, "../assets", "tripadvisor_openapi.json");

  function loadOpenApiSpec(specPath: string): unknown {
    if (!fs.existsSync(specPath)) {
      throw new Error(`OpenAPI specification not found at: ${specPath}`);
    }

    try {
      const data = fs.readFileSync(specPath, "utf-8");
      return JSON.parse(data);
    } catch (error) {
      throw new Error(`Failed to read or parse OpenAPI specification at ${specPath}: ${error}`);
    }
  }

  function createTripAdvisorTool(spec: unknown): OpenApiAgentTool {
    const auth: OpenApiProjectConnectionAuthDetails = {
      type: "project_connection",
      security_scheme: {
        project_connection_id: tripAdvisorProjectConnectionId,
      },
    };

    const definition: OpenApiFunctionDefinition = {
      name: "get_tripadvisor_location_details",
      description:
        "Fetch TripAdvisor location details, reviews, or photos using the Content API via project connection auth.",
      spec,
      auth,
    };

    return {
      type: "openapi",
      openapi: definition,
    };
  }

  export async function main(): Promise<void> {
    console.log("Loading TripAdvisor OpenAPI specification from assets directory...");
    const tripAdvisorSpec = loadOpenApiSpec(tripAdvisorSpecPath);

    const project = new AIProjectClient(projectEndpoint, new DefaultAzureCredential());
    const openAIClient = await project.getOpenAIClient();

    console.log("Creating agent with OpenAPI project-connection tool...");

    const agent = await project.agents.createVersion("MyOpenApiConnectionAgent", {
      kind: "prompt",
      model: deploymentName,
      instructions:
        "You are a travel assistant that consults the TripAdvisor Content API via project connection to answer user questions about locations.",
      tools: [createTripAdvisorTool(tripAdvisorSpec)],
    });
    console.log(`Agent created (id: ${agent.id}, name: ${agent.name}, version: ${agent.version})`);

    console.log("\nSending request to TripAdvisor OpenAPI agent with streaming...");
    const streamResponse = await openAIClient.responses.create(
      {
        input:
          "Provide a quick overview of the TripAdvisor location 293919 including its name, rating, and review count.",
        stream: true,
      },
      {
        body: {
          agent: { name: agent.name, type: "agent_reference" },
          tool_choice: "required",
        },
      },
    );

    // Process the streaming response
    for await (const event of streamResponse) {
      if (event.type === "response.created") {
        console.log(`Follow-up response created with ID: ${event.response.id}`);
      } else if (event.type === "response.output_text.delta") {
        process.stdout.write(event.delta);
      } else if (event.type === "response.output_text.done") {
        console.log("\n\nFollow-up response done!");
      } else if (event.type === "response.output_item.done") {
        const item = event.item as any;
        if (item.type === "message") {
          const content = item.content?.[item.content.length - 1];
          if (content?.type === "output_text" && content.annotations) {
            for (const annotation of content.annotations) {
              if (annotation.type === "url_citation") {
                console.log(
                  `URL Citation: ${annotation.url}, Start index: ${annotation.start_index}, End index: ${annotation.end_index}`,
                );
              }
            }
          }
        } else if (item.type === "tool_call") {
          console.log(`Tool call completed: ${item.name ?? "unknown"}`);
        }
      } else if (event.type === "response.completed") {
        console.log("\nFollow-up completed!");
      }
    }

    // Clean up resources by deleting the agent version
    // This prevents accumulation of unused resources in your project
    console.log("\nCleaning up resources...");
    await project.agents.deleteVersion(agent.name, agent.version);
    console.log("Agent deleted");

    console.log("\nTripAdvisor OpenAPI agent sample completed!");
  }

  main().catch((err) => {
    console.error("The sample encountered an error:", err);
  });
  ```

  ### What this code does

  This TypeScript example demonstrates using an OpenAPI tool with API key authentication through a project connection. When you run the code:

  1. It loads the TripAdvisor OpenAPI specification from a local file.
  2. It configures authentication by using the `TRIPADVISOR_PROJECT_CONNECTION_ID` environment variable.
  3. It creates an agent with the TripAdvisor tool that uses the project connection for API key authentication.
  4. It sends a streaming request for TripAdvisor location details.
  5. It forces tool usage by using `tool_choice: "required"` to ensure the API is called.
  6. It processes and displays the streaming response.
  7. It cleans up by deleting the agent.

  ### Required inputs

  * Environment variables: `AZURE_AI_PROJECT_ENDPOINT`, `MODEL_DEPLOYMENT_NAME`, `TRIPADVISOR_PROJECT_CONNECTION_ID`
  * Local file: `../assets/tripadvisor_openapi.json`
  * Project connection configured with TripAdvisor API key

  ### Expected output

  ```console
  Loading TripAdvisor OpenAPI specification from assets directory...
  Creating agent with OpenAPI project-connection tool...
  Agent created (id: asst_abc123, name: MyOpenApiConnectionAgent, version: 1)

  Sending request to TripAdvisor OpenAPI agent with streaming...
  Follow-up response created with ID: resp_xyz789
  Location 293919 is the Eiffel Tower in Paris, France. It has a rating of 4.5 stars with over 140,000 reviews...
  Tool call completed: get_tripadvisor_location_details

  Follow-up completed!

  Cleaning up resources...
  Agent deleted

  TripAdvisor OpenAPI agent sample completed!
  ```

  ### Common errors

  * `Error: OpenAPI specification not found`: Check the file path.
  * Connection not found: Verify `TRIPADVISOR_PROJECT_CONNECTION_ID` is correct and connection exists.
  * `AuthenticationException`: Invalid API key in project connection.
  * **API key not injected in requests**: Your OpenAPI spec must include proper `securitySchemes` (under `components`) and `security` sections. The key name in `securitySchemes` must match the key in your project connection.
  * `Content type is not supported`: Currently, only these two content types are supported: `application/json` and `application/json-patch+json`.
</ZonePivot>

## Security and data considerations

When you connect an agent to an OpenAPI tool, the agent can send request parameters derived from user input to the target API.

* Use project connections for secrets (API keys and tokens). Avoid putting secrets in an OpenAPI spec file or source code.
* Review what data the API receives and what it returns before you use the tool in production.
* Use least-privilege access. For managed identity, assign only the roles the target service requires.

## Authenticate with API key

By using API key authentication, you can authenticate your OpenAPI spec by using various methods such as an API key or Bearer token. You can use only one API key security schema per OpenAPI spec. If you need multiple security schemas, create multiple OpenAPI spec tools.

1. Update your OpenAPI spec security schemas. It has a `securitySchemes` section and one scheme of type `apiKey`. For example:

   ```json
    "securitySchemes": {
        "apiKeyHeader": {
                "type": "apiKey",
                "name": "x-api-key",
                "in": "header"
            }
    }
   ```

   You usually only need to update the `name` field, which corresponds to the name of `key` in the connection. If the security schemes include multiple schemes, keep only one of them.

2. Update your OpenAPI spec to include a `security` section:

   ```json
   "security": [
        {
        "apiKeyHeader": []
        }
    ]
   ```

3. Remove any parameter in the OpenAPI spec that needs API key, because API key is stored and passed through a connection, as described later in this article.

4. Create a connection to store your API key.

5. Go to the [Foundry portal](https://ai.azure.com/nextgen?cid=learnDocs) and open your project.

6. Create or select a connection that stores the secret. See [Add a new connection to your project](../../../how-to/connections-add).

   <Callout type="note">
     If you regenerate the API key at a later date, you need to update the connection with the new key.
   </Callout>

7. Enter the following information

   * key: `name` field of your security scheme. In this example, it should be `x-api-key`

     ```json
            "securitySchemes": {
               "apiKeyHeader": {
                         "type": "apiKey",
                         "name": "x-api-key",
                         "in": "header"
                     }
             }
     ```

   * value: YOUR\_API\_KEY

8. After you create a connection, you can use it through the SDK or REST API. Use the tabs at the top of this article to see code examples.

## Set up a Bearer token connection

You can use token-based authentication (for example, a Bearer token) with the same `project_connection` auth type used for API keys. The key difference is how you configure both the OpenAPI spec and the project connection.

1. Update your OpenAPI spec `securitySchemes` to use `Authorization` as the header name:

   ```json
   "securitySchemes": {
       "bearerAuth": {
           "type": "apiKey",
           "name": "Authorization",
           "in": "header"
       }
   }
   ```

2. Add a `security` section that references the scheme:

   ```json
   "security": [
       {
           "bearerAuth": []
       }
   ]
   ```

3. Create a **Custom keys** connection in your Foundry project:

   1. Go to the [Foundry portal](https://ai.azure.com/nextgen?cid=learnDocs) and open your project.

   2. Create or select a connection that stores the secret. See [Add a new connection to your project](../../../how-to/connections-add).

   3. Enter the following values:

      * **key**: `Authorization` (must match the `name` field in your `securitySchemes`)
      * **value**: `Bearer <token>` (replace `<token>` with your actual token)

   <Callout type="important">
     The value must include the word `Bearer` followed by a space before the token. For example: `Bearer eyJhbGciOiJSUzI1NiIs...`. If you omit `Bearer `, the API receives a raw token without the required authorization scheme prefix, and the request fails.
   </Callout>

4. After you create the connection, use it with the `project_connection` auth type in your code, the same way you would for API key authentication. The connection ID uses the same format: `/subscriptions/{{subscriptionID}}/resourceGroups/{{resourceGroupName}}/providers/Microsoft.CognitiveServices/accounts/{{foundryAccountName}}/projects/{{foundryProjectName}}/connections/{{foundryConnectionName}}`.

## Authenticate by using managed identity (Microsoft Entra ID)

[Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/fundamentals/what-is-entra) is a cloud-based identity and access management service that your employees can use to access external resources. By using Microsoft Entra ID, you can add extra security to your APIs without needing to use API keys. When you set up managed identity authentication, the agent authenticates through the Foundry tool it uses.

To set up authentication by using Managed Identity:

1. Make sure your Foundry resource has system assigned managed identity enabled.

   ![A screenshot showing the managed identity selector in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/managed-identity-portal.png)

2. Create a resource for the service you want to connect to through OpenAPI spec.

3. Assign proper access to the resource.

   1. Select **Access Control** for your resource.

   2. Select **Add** and then **add role assignment** at the top of the screen.

      ![A screenshot showing the role assignment selector in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/agents/media/tools/role-assignment-portal.png)

   3. Select the proper role assignment needed, usually it requires at least the *READER* role. Then select **Next**.

   4. Select **Managed identity** and then select **select members**.

   5. In the managed identity dropdown menu, search for **Foundry Account** and then select the Foundry account of your agent.

   6. Select **Finish**.

4. When you finish the setup, you can continue by using the tool through the Foundry portal, SDK, or REST API. Use the tabs at the top of this article to see code samples.

## Troubleshoot common errors

| Symptom                                    | Likely cause                                                   | Resolution                                                                                                                                                                        |
| ------------------------------------------ | -------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| API key isn't included in requests.        | OpenAPI spec missing `securitySchemes` or `security` sections. | Verify your OpenAPI spec includes both `components.securitySchemes` and a top-level `security` section. Ensure the scheme `name` matches the key name in your project connection. |
| Agent doesn't call the OpenAPI tool.       | Tool choice not set or `operationId` not descriptive.          | Use `tool_choice="required"` to force tool invocation. Ensure `operationId` values are descriptive so the model can choose the right operation.                                   |
| Authentication fails for managed identity. | Managed identity not enabled or missing role assignment.       | Enable system-assigned managed identity on your Foundry resource. Assign the required role (Reader or higher) on the target service.                                              |
| Request fails with 400 Bad Request.        | OpenAPI spec doesn't match actual API.                         | Validate your OpenAPI spec against the actual API. Check parameter names, types, and required fields.                                                                             |
| Request fails with 401 Unauthorized.       | API key or token invalid or expired.                           | Regenerate the API key/token and update your project connection. Verify the connection ID is correct.                                                                             |
| Tool returns unexpected response format.   | Response schema not defined in OpenAPI spec.                   | Add response schemas to your OpenAPI spec for better model understanding.                                                                                                         |
| `operationId` validation error.            | Invalid characters in `operationId`.                           | Use only letters, `-`, and `_` in `operationId` values. Remove numbers and special characters.                                                                                    |
| Connection not found error.                | Connection name or ID mismatch.                                | Verify `OPENAPI_PROJECT_CONNECTION_NAME` matches the connection name in your Foundry project.                                                                                     |
| Bearer token not sent correctly.           | Connection value missing `Bearer `prefix.                      | Set the connection value to `Bearer <token>` (with the word `Bearer` and a space before the token). Verify the OpenAPI spec `securitySchemes` uses `"name": "Authorization"`.     |

## Choose an authentication method

The following table helps you choose the right authentication method for your OpenAPI tool:

| Authentication method | Best for                                             | Setup complexity |
| --------------------- | ---------------------------------------------------- | ---------------- |
| Anonymous             | Public APIs with no authentication                   | Low              |
| API key               | Third-party APIs with key-based access               | Medium           |
| Managed identity      | Azure services and Microsoft Entra ID-protected APIs | Medium-High      |

## Related content

* [Add a new connection to your project](../../../how-to/connections-add)
* [Set up your environment for Foundry Agent Service](../../environment-setup)
* [Agents REST API (preview)](../../../reference/foundry-project-rest-preview)

Microsoft Foundry agents support function calling, which lets you extend agents with custom capabilities. Define a function with its name, parameters, and description, and the agent can request your app to call it. Your app executes the function and returns the output. The agent then uses the result to continue the conversation with accurate, real-time data from your systems.

<Callout type="important">
  Runs expire 10 minutes after creation. Submit your tool outputs before they expire.
</Callout>

You can run agents with function tools in the Microsoft Foundry portal. However, the portal doesn't support adding, removing, or updating function definitions on an agent. Use the SDK or REST API to configure function tools.

## Usage support

| Microsoft Foundry support | Python SDK | C# SDK | JavaScript SDK | Java SDK | REST API | Basic agent setup | Standard agent setup |
| ------------------------- | ---------- | ------ | -------------- | -------- | -------- | ----------------- | -------------------- |
| ✔️                        | ✔️         | ✔️     | -              | -        | ✔️       | ✔️                | ✔️                   |

## Prerequisites

Before you start, make sure you have:

* A [basic or standard agent environment](../../environment-setup).
* A Foundry project and a deployed model.
* The latest prerelease SDK package for your language (`azure-ai-projects>=2.0.0b1` for Python, `Azure.AI.Projects.OpenAI` prerelease for .NET). For installation and authentication steps, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true).

### Environment variables

Each language uses different environment variable names. Use one set consistently.

| Language | Project endpoint                    | Model deployment name            |
| -------- | ----------------------------------- | -------------------------------- |
| Python   | `AZURE_AI_PROJECT_ENDPOINT`         | `AZURE_AI_MODEL_DEPLOYMENT_NAME` |
| C#       | `FOUNDRY_PROJECT_ENDPOINT`          | `MODEL_DEPLOYMENT_NAME`          |
| REST API | `AZURE_AI_FOUNDRY_PROJECT_ENDPOINT` | (use the request body field)     |

<Callout type="tip">
  If you use `DefaultAzureCredential`, sign in by using `az login` before running the samples.
</Callout>

### Quick verification

If you're not sure your authentication and endpoint are set up correctly, run the following snippet first.

```python
import os

from azure.ai.projects import AIProjectClient
from azure.identity import DefaultAzureCredential
from dotenv import load_dotenv

load_dotenv()

with (
    DefaultAzureCredential() as credential,
    AIProjectClient(endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"], credential=credential) as project_client,
):
    print("Connected to project.")
```

## Create an agent with function tools

Function calling follows this pattern:

1. **Define function tools** — Describe each function's name, parameters, and purpose.
2. **Create an agent** — Register the agent with your function definitions.
3. **Send a prompt** — The agent analyzes the prompt and requests function calls if needed.
4. **Execute and return** — Your app runs the function and submits the output back to the agent.
5. **Get the final response** — The agent uses your function output to complete its response.

<Callout type="note">
  You need the latest prerelease package. For more information, see the [quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true#get-ready-to-code).
</Callout>

<ZonePivot pivot="python">
  Use the following code sample to create an agent, handle a function call, and return tool output back to the agent.

  ```python
  import os
  import json
  from dotenv import load_dotenv
  from azure.ai.projects import AIProjectClient
  from azure.ai.projects.models import PromptAgentDefinition, Tool, FunctionTool
  from azure.identity import DefaultAzureCredential
  from openai.types.responses.response_input_param import FunctionCallOutput, ResponseInputParam

  load_dotenv()

  # Define a function tool for the model to use
  func_tool = FunctionTool(
      name="get_horoscope",
      parameters={
          "type": "object",
          "properties": {
              "sign": {
                  "type": "string",
                  "description": "An astrological sign like Taurus or Aquarius",
              },
          },
          "required": ["sign"],
          "additionalProperties": False,
      },
      description="Get today's horoscope for an astrological sign.",
      strict=True,
  )

  tools: list[Tool] = [func_tool]


  def get_horoscope(sign: str) -> str:
      """Generate a horoscope for the given astrological sign."""
      return f"{sign}: Next Tuesday you will befriend a baby otter."


  project_client = AIProjectClient(
      endpoint=os.environ["AZURE_AI_PROJECT_ENDPOINT"],
      credential=DefaultAzureCredential(),
  )


  with project_client:

      agent = project_client.agents.create_version(
          agent_name="MyAgent",
          definition=PromptAgentDefinition(
              model=os.environ["AZURE_AI_MODEL_DEPLOYMENT_NAME"],
              instructions="You are a helpful assistant that can use function tools.",
              tools=tools,
          ),
      )

      openai_client = project_client.get_openai_client()

      # Prompt the model with tools defined
      response = openai_client.responses.create(
          input="What is my horoscope? I am an Aquarius.",
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )
      print(f"Response output: {response.output_text}")

      input_list: ResponseInputParam = []
      # Process function calls
      for item in response.output:
          if item.type == "function_call":
              if item.name == "get_horoscope":
                  # Execute the function logic for get_horoscope
                  horoscope = get_horoscope(**json.loads(item.arguments))

                  # Provide function call results to the model
                  input_list.append(
                      FunctionCallOutput(
                          type="function_call_output",
                          call_id=item.call_id,
                          output=json.dumps({"horoscope": horoscope}),
                      )
                  )

      print("Final input:")
      print(input_list)

      response = openai_client.responses.create(
          input=input_list,
          previous_response_id=response.id,
          extra_body={"agent": {"name": agent.name, "type": "agent_reference"}},
      )
  ```

  ### Expected output

  The following example shows the expected output:

  ```console
  Response output:
  Final input:
  [FunctionCallOutput(type='function_call_output', call_id='call_abc123', output='{"horoscope": "Aquarius: Next Tuesday you will befriend a baby otter."}')]
  ```
</ZonePivot>

<ZonePivot pivot="csharp">
  ## Use agents with functions example

  In this example, you use local functions with agents. Use the functions to give the Agent specific information in response to a user question. The code in this example is synchronous. For an asynchronous example, see the [sample code](https://github.com/Azure/azure-sdk-for-net/blob/feature/ai-foundry/agents-v2/sdk/ai/Azure.AI.Projects.OpenAI/samples/Sample9_Function.md) example in the Azure SDK for .NET repository on GitHub.

  ```csharp
  class FunctionCallingDemo
  {
      // Define three functions:
      //   1. GetUserFavoriteCity always returns "Seattle, WA".
      //   2. GetCityNickname handles only "Seattle, WA"
      //      and throws an exception for other city names.
      //   3. GetWeatherAtLocation returns the weather in Seattle, WA.

      /// Example of a function that defines no parameters but
      /// returns the user's favorite city.
      private static string GetUserFavoriteCity() => "Seattle, WA";

      /// <summary>
      /// Example of a function with a single required parameter
      /// </summary>
      /// <param name="location">The location to get nickname for.</param>
      /// <returns>The city nickname.</returns>
      /// <exception cref="NotImplementedException"></exception>
      private static string GetCityNickname(string location) => location switch
      {
          "Seattle, WA" => "The Emerald City",
          _ => throw new NotImplementedException(),
      };

      /// <summary>
      /// Example of a function with one required and one optional, enum parameter
      /// </summary>
      /// <param name="location">Get weather for location.</param>
      /// <param name="temperatureUnit">"c" or "f"</param>
      /// <returns>The weather in selected location.</returns>
      /// <exception cref="NotImplementedException"></exception>
      public static string GetWeatherAtLocation(string location, string temperatureUnit = "f") => location switch
      {
          "Seattle, WA" => temperatureUnit == "f" ? "70f" : "21c",
          _ => throw new NotImplementedException()
      };

      // For each function, create FunctionTool, which defines the function name, description, and parameters.
      public static readonly FunctionTool getUserFavoriteCityTool = ResponseTool.CreateFunctionTool(
          functionName: "getUserFavoriteCity",
          functionDescription: "Gets the user's favorite city.",
          functionParameters: BinaryData.FromString("{}"),
          strictModeEnabled: false
      );

      public static readonly FunctionTool getCityNicknameTool = ResponseTool.CreateFunctionTool(
          functionName: "getCityNickname",
          functionDescription: "Gets the nickname of a city, e.g. 'LA' for 'Los Angeles, CA'.",
          functionParameters: BinaryData.FromObjectAsJson(
              new
              {
                  Type = "object",
                  Properties = new
                  {
                      Location = new
                      {
                          Type = "string",
                          Description = "The city and state, e.g. San Francisco, CA",
                      },
                  },
                  Required = new[] { "location" },
              },
              new JsonSerializerOptions() { PropertyNamingPolicy = JsonNamingPolicy.CamelCase }
          ),
          strictModeEnabled: false
      );

      private static readonly FunctionTool getCurrentWeatherAtLocationTool = ResponseTool.CreateFunctionTool(
          functionName: "getCurrentWeatherAtLocation",
          functionDescription: "Gets the current weather at a provided location.",
          functionParameters: BinaryData.FromObjectAsJson(
               new
               {
                   Type = "object",
                   Properties = new
                   {
                       Location = new
                       {
                           Type = "string",
                           Description = "The city and state, e.g. San Francisco, CA",
                       },
                       Unit = new
                       {
                           Type = "string",
                           Enum = new[] { "c", "f" },
                       },
                   },
                   Required = new[] { "location" },
               },
              new JsonSerializerOptions() { PropertyNamingPolicy = JsonNamingPolicy.CamelCase }
          ),
          strictModeEnabled: false
      );

      // Create the method GetResolvedToolOutput.
      // It runs the preceding functions and wraps the output in a ResponseItem object.
      private static FunctionCallOutputResponseItem GetResolvedToolOutput(FunctionCallResponseItem item)
      {
          if (item.FunctionName == getUserFavoriteCityTool.FunctionName)
          {
              return ResponseItem.CreateFunctionCallOutputItem(item.CallId, GetUserFavoriteCity());
          }
          using JsonDocument argumentsJson = JsonDocument.Parse(item.FunctionArguments);
          if (item.FunctionName == getCityNicknameTool.FunctionName)
          {
              string locationArgument = argumentsJson.RootElement.GetProperty("location").GetString();
              return ResponseItem.CreateFunctionCallOutputItem(item.CallId, GetCityNickname(locationArgument));
          }
          if (item.FunctionName == getCurrentWeatherAtLocationTool.FunctionName)
          {
              string locationArgument = argumentsJson.RootElement.GetProperty("location").GetString();
              if (argumentsJson.RootElement.TryGetProperty("unit", out JsonElement unitElement))
              {
                  string unitArgument = unitElement.GetString();
                  return ResponseItem.CreateFunctionCallOutputItem(item.CallId, GetWeatherAtLocation(locationArgument, unitArgument));
              }
              return ResponseItem.CreateFunctionCallOutputItem(item.CallId, GetWeatherAtLocation(locationArgument));
          }
          return null;
      }

      public static void Main()
      {
          // Create project client and read the environment variables that will be used in the next steps.
          var projectEndpoint = System.Environment.GetEnvironmentVariable("FOUNDRY_PROJECT_ENDPOINT");
          var modelDeploymentName = System.Environment.GetEnvironmentVariable("MODEL_DEPLOYMENT_NAME");
          AIProjectClient projectClient = new(endpoint: new Uri(projectEndpoint), tokenProvider: new DefaultAzureCredential());
          // Create an agent version with the defined functions as tools.
          PromptAgentDefinition agentDefinition = new(model: modelDeploymentName)
          {
              Instructions = "You are a weather bot. Use the provided functions to help answer questions. "
                      + "Customize your responses to the user's preferences as much as possible and use friendly "
                      + "nicknames for cities whenever possible.",
              Tools = { getUserFavoriteCityTool, getCityNicknameTool, getCurrentWeatherAtLocationTool }
          };
          AgentVersion agentVersion = projectClient.Agents.CreateAgentVersion(
              agentName: "myAgent",
              options: new(agentDefinition));

          // If the local function call is required, the response item is of type FunctionCallResponseItem.
          // It contains the function name needed by the Agent. In this case, use the helper method
          // GetResolvedToolOutput to get the FunctionCallOutputResponseItem with the function call result.
          // To provide the right answer, supply all the response items to the CreateResponse call.
          // At the end, output the function's response.
          ResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForAgent(agentVersion.Name);

          ResponseItem request = ResponseItem.CreateUserMessageItem("What's the weather like in my favorite city?");
          var inputItems = new List<ResponseItem> { request };
          string previousResponseId = null;
          bool functionCalled = false;
          ResponseResult response;
          do
          {
              response = responseClient.CreateResponse(
                  previousResponseId: previousResponseId,
                  inputItems: inputItems);
              previousResponseId = response.Id;
              inputItems.Clear();
              functionCalled = false;
              foreach (ResponseItem responseItem in response.OutputItems)
              {
                  inputItems.Add(responseItem);
                  if (responseItem is FunctionCallResponseItem functionToolCall)
                  {
                      Console.WriteLine($"Calling {functionToolCall.FunctionName}...");
                      inputItems.Add(GetResolvedToolOutput(functionToolCall));
                      functionCalled = true;
                  }
              }
          } while (functionCalled);
          Console.WriteLine(response.GetOutputText());

          // Remove all the resources created in this sample.
          projectClient.Agents.DeleteAgentVersion(agentName: agentVersion.Name, agentVersion: agentVersion.Version);
      }
  }
  ```

  ### Expected output

  The following example shows the expected output:

  ```console
  Calling getUserFavoriteCity...
  Calling getCityNickname...
  Calling getCurrentWeatherAtLocation...
  Your favorite city, Seattle, WA, is also known as The Emerald City. The current weather there is 70f.
  ```
</ZonePivot>

<ZonePivot pivot="rest">
  There are two ways to use function calling in Foundry Agent Service.

  1. Create a `response`. When you need the agent to call functions again, create another `response`.
  2. Create a `conversation`, then create multiple conversation items. Each conversation item corresponds to one `response`.

  Set the following environment variables before running the examples:

  ```bash
  export AGENT_TOKEN=$(az account get-access-token --resource https://cognitiveservices.azure.com --query accessToken -o tsv)
  export API_VERSION="2025-11-15-preview"
  ```

  ## Define a function for your agent to call

  Start by defining a function for your agent to call. When you create a function for an agent to call, describe its structure and any required parameters in a docstring. For example functions, see the other SDK languages.

  ## Create an agent version

  ```bash
  curl --request POST \
    --url $AZURE_AI_FOUNDRY_PROJECT_ENDPOINT/agents/$AGENTVERSION_NAME/versions?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
          "description": "Test agent version with function calling",
      "metadata": { "env": "test", "owner": "user" },
      "definition": {
        "kind": "prompt",
        "model": {{model}},
        "instructions": "You are a helpful agent.",
        "tools": [
          {
            "type": "function",
            "name": "getCurrentWeather",
            "description": "Get the current weather in a location",
            "parameters": {
              "type": "object",
              "properties": {
                "location": {"type": "string", "description": "The city and state e.g. San Francisco, CA"},
                "unit": {"type": "string", "enum": ["c", "f"]}
              },
              "required": ["location"]
            }
          }
        ]
      }
    }'
  ```

  ## Create a conversation

  ```bash
  curl --request POST \
    --url $AZURE_AI_FOUNDRY_PROJECT_ENDPOINT/openai/conversations?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d ''
  ```

  ## Create a response

  ```bash
  curl --request POST \
    --url $AZURE_AI_FOUNDRY_PROJECT_ENDPOINT/openai/responses?api-version=$API_VERSION \
    -H "Authorization: Bearer $AGENT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{
      "model": {{model}},
      "conversation": {{conversation.id}},
      "input": [{
          "type": "message",
          "role": "user",
          "content": [
              {
                  "type": "input_text",
                  "text": "What's the weather in Dar es Salaam, Tanzania?"
              }
          ]
      }],
      "tools": [
        {
          "type": "function",
          "name": "getCurrentWeather",
          "description": "Get the current weather in a location",
          "parameters": {
            "type": "object",
            "properties": {
              "location": {"type": "string", "description": "The city and state e.g. San Francisco, CA"},
              "unit": {"type": "string", "enum": ["c", "f"]}
            },
            "required": ["location"]
          }
        }
      ],
      "stream": true
    }
  '
  ```

  ### Expected output

  The response contains a function call item that you need to process:

  ```json
  {
    "output": [
      {
        "type": "function_call",
        "call_id": "call_xyz789",
        "name": "getCurrentWeather",
        "arguments": "{\"location\": \"Dar es Salaam, Tanzania\", \"unit\": \"c\"}"
      }
    ]
  }
  ```

  After you process the function call and provide the output back to the agent, the final response includes the weather information in natural language.
</ZonePivot>

## Verify function calling works

Use these checks to confirm function calling is working:

1. Your first response contains an output item with `type` set to `function_call`.
2. Your app executes the requested function by using the returned arguments.
3. Your app submits a follow-up response that includes a `function_call_output` item and references the previous response, and the agent returns a natural-language answer.

If you use tracing in Microsoft Foundry, confirm the tool invocation occurred. For guidance on validating tool invocation and controlling tool usage, see [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice).

## Security and data considerations

* Treat tool arguments and tool outputs as untrusted input. Validate and sanitize values before using them.
* Don't pass secrets (API keys, tokens, connection strings) in tool output. Return only the data the model needs.
* Apply least privilege to the identity used by `DefaultAzureCredential`.
* Avoid side effects unless you explicitly intend them. For example, restrict function tools to safe operations, or require explicit user confirmation for actions that change data.
* For long-running operations, return a status immediately and implement polling. The 10-minute run expiration applies to total elapsed time, not individual function execution.

## Troubleshooting

| Issue                                            | Likely cause                                     | Resolution                                                                                                      |
| ------------------------------------------------ | ------------------------------------------------ | --------------------------------------------------------------------------------------------------------------- |
| Agent returns function call but no final answer. | Tool output not returned to model.               | Execute the function, then call `responses.create` with the tool output and `previous_response_id` to continue. |
| No function call occurs.                         | Function not in agent definition or poor naming. | Confirm the function tool is added to the agent. Use clear, descriptive names and parameter descriptions.       |
| Arguments aren't valid JSON.                     | Schema mismatch or model hallucination.          | Verify JSON schema uses correct types and required properties. Handle parsing errors gracefully in your app.    |
| Required fields are missing.                     | Schema doesn't enforce required properties.      | Add `"required": [...]` array to your parameter schema. Set `strict: true` for stricter validation.             |
| Tool outputs fail due to expiration.             | Run expired (10-minute limit).                   | Return tool outputs promptly. For slow operations, return a status and poll separately.                         |
| Function called with wrong parameters.           | Ambiguous function description.                  | Improve the function `description` field. Add detailed parameter descriptions with examples.                    |
| Multiple function calls in one response.         | Model determined multiple functions needed.      | Handle each function call in the output array. Return all results in a single `responses.create` call.          |
| Function not visible in Foundry portal.          | Portal doesn't execute function calls.           | Test function calling via SDK or REST API. The portal shows agents but doesn't invoke functions.                |

## Clean up resources

When you finish testing, delete the resources you created to avoid ongoing costs:

* Delete the agent version.
* Delete conversations created for testing.

## Related content

* [Best practices for using tools in Microsoft Foundry Agent Service](../../concepts/tool-best-practice)
* [Connect OpenAPI tools to Microsoft Foundry agents](openapi)
* [Microsoft Foundry Quickstart](../../../quickstarts/get-started-code?view=foundry\&preserve-view=true)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

This article lists a selection of Microsoft Foundry Models sold directly by Azure along with their capabilities, [deployment types, and regions of availability](deployment-types), excluding [deprecated and legacy models](../../concepts/model-lifecycle-retirement#deprecated). To see a list of Azure OpenAI models that are supported by the Foundry Agent Service, see [Models supported by Agent Service](../../agents/concepts/model-region-support).

Models sold directly by Azure include all Azure OpenAI models and specific, selected models from top providers. These models are billed through your Azure subscription, covered by Azure service-level agreements, and supported by Microsoft. For models offered by partners outside of this list, see [Foundry Models from partners and community](models-from-partners).

Use the tabs at the top of this page to switch between [Azure OpenAI models](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai) and [other model collections](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-direct-others) from providers like Cohere, DeepSeek, Meta, Mistral AI, and xAI.

Foundry Models are available for standard deployment to a Foundry resource.

To learn more about attributes of Foundry Models sold directly by Azure, see [Explore Foundry Models](../../concepts/foundry-models-overview#models-sold-directly-by-azure).

<ZonePivot pivot="azure-openai">
  <Callout type="note">
    Foundry Models sold directly by Azure also include select models from top model providers, such as:

    * Black Forest Labs: `FLUX.2-pro`, `FLUX.1-Kontext-pro`, `FLUX-1.1-pro`
    * Cohere: `Cohere-command-a`, `embed-v-4-0`, `Cohere-rerank-v4.0-pro`, `Cohere-rerank-v4.0-fast`
    * DeepSeek: `DeepSeek-V3.2`, `DeepSeek-V3.2-Speciale`, `DeepSeek-V3.1`, `DeepSeek-V3-0324`, `DeepSeek-R1-0528`, `DeepSeek-R1`
    * Moonshot AI: `Kimi-K2.5`, `Kimi-K2-Thinking`
    * Meta: `Llama-4-Maverick-17B-128E-Instruct-FP8`, `Llama-3.3-70B-Instruct`
    * Microsoft: `MAI-DS-R1`, `model-router`
    * Mistral: `mistral-document-ai-2505`, `Mistral-Large-3`
    * xAI: `grok-code-fast-1`, `grok-3`, `grok-3-mini`, `grok-4-fast-reasoning`, `grok-4-fast-non-reasoning`, `grok-4`

    To learn about these models, switch to [Other model collections](models-sold-directly-by-azure?pivots=azure-direct-others) at the top of this article.
  </Callout>

  ## Azure OpenAI in Microsoft Foundry models

  Azure OpenAI is powered by a diverse set of models with different capabilities and price points. Model availability varies by region and cloud. For Azure Government model availability, refer to [Azure OpenAI in Azure Government](../../openai/azure-government).

  | Models                                                                                                                                                                                                                                | Description                                                                                                                                                                                     |
  | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | [GPT-5.2 series](models-sold-directly-by-azure#gpt-52)                                                                                                                                                                                | **NEW** `gpt-5.2-codex`, `gpt-5.2`, `gpt-5.2-chat` (**Preview**)                                                                                                                                |
  | [GPT-5.1 series](models-sold-directly-by-azure#gpt-51)                                                                                                                                                                                | **NEW** `gpt-5.1`, `gpt-5.1-chat`, `gpt-5.1-codex`, `gpt-5.1-codex-mini`                                                                                                                        |
  | [Sora](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai\&tabs=global-standard-aoai%2Cstandard-chat-completions%2Cglobal-standard#video-generation-models) | **NEW** sora-2                                                                                                                                                                                  |
  | [GPT-5 series](models-sold-directly-by-azure#gpt-5)                                                                                                                                                                                   | gpt-5, gpt-5-mini, gpt-5-nano, gpt-5-chat                                                                                                                                                       |
  | [gpt-oss](models-sold-directly-by-azure#gpt-oss)                                                                                                                                                                                      | open-weight reasoning models                                                                                                                                                                    |
  | [codex-mini](models-sold-directly-by-azure#o-series-models)                                                                                                                                                                           | Fine-tuned version of o4-mini.                                                                                                                                                                  |
  | [GPT-4.1 series](models-sold-directly-by-azure#gpt-41-series)                                                                                                                                                                         | gpt-4.1, gpt-4.1-mini, gpt-4.1-nano                                                                                                                                                             |
  | [computer-use-preview](models-sold-directly-by-azure#computer-use-preview)                                                                                                                                                            | An experimental model trained for use with the Responses API computer use tool.                                                                                                                 |
  | [o-series models](models-sold-directly-by-azure#o-series-models)                                                                                                                                                                      | [Reasoning models](../../openai/how-to/reasoning) with advanced problem solving and increased focus and capability.                                                                             |
  | [GPT-4o, GPT-4o mini, and GPT-4 Turbo](models-sold-directly-by-azure#gpt-4o-and-gpt-4-turbo)                                                                                                                                          | Capable Azure OpenAI models with multimodal versions, which can accept both text and images as input.                                                                                           |
  | [Embeddings](models-sold-directly-by-azure#embeddings)                                                                                                                                                                                | A set of models that can convert text into numerical vector form to facilitate text similarity.                                                                                                 |
  | [Image generation](models-sold-directly-by-azure#image-generation-models)                                                                                                                                                             | A series of models that can generate original images from natural language.                                                                                                                     |
  | [`Video generation`](models-sold-directly-by-azure#video-generation-models)                                                                                                                                                           | A model that can generate original video scenes from text instructions.                                                                                                                         |
  | [Audio](models-sold-directly-by-azure#audio-models)                                                                                                                                                                                   | A series of models for speech to text, translation, and text to speech. GPT-4o audio models support either low latency *speech in, speech out* conversational interactions or audio generation. |

  ## GPT-5.2

  ### Region availability

  | Model           | Region                                                                |
  | --------------- | --------------------------------------------------------------------- |
  | `gpt-5.2`       | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.2-chat`  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.2-codex` | East US2 & Sweden Central (Global Standard)                           |

  * **[Registration is required for access to gpt-5.2 and gpt-5.2-codex](https://aka.ms/oai/gpt5access).**

  Access will be granted based on Microsoft's eligibility criteria. Customers who previously applied and received access to a limited access model, don't need to reapply as their approved subscriptions will automatically be granted access upon model release.

  | Model ID                                | Description                                                                                                                                                                                                                                                                                                                                                                | Context Window                         | Max Output Tokens | Training Data (up to) |
  | --------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------- | :---------------- | :-------------------: |
  | `gpt-5.2-codex` (2026-01-14)            | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning). - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |                       |
  | `gpt-5.2` (2025-12-11)                  | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                                  | 400,000 Input: 272,000 Output: 128,000 | 128,000           |      August 2025      |
  | `gpt-5.2-chat` (2025-12-11) **Preview** | - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs - Functions, tools, and parallel tool calling.                                                                                                                                                                                                                              | 128,000 Input: 111,616 Output: 16,384  | 16,384            |      August 2025      |

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  ## GPT-5.1

  ### Region availability

  | Model                | Region                                                                |
  | -------------------- | --------------------------------------------------------------------- |
  | `gpt-5.1`            | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.1-chat`       | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.1-codex`      | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.1-codex-mini` | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.1-codex-max`  | See the [models table](#model-summary-table-and-region-availability). |

  * **[Registration is required for access to gpt-5.1, gpt-5.1-codex, and gpt-5.1-codex-max](https://aka.ms/oai/gpt5access).**

  Access will be granted based on Microsoft's eligibility criteria. Customers who previously applied and received access to a limited access model, don't need to reapply as their approved subscriptions will automatically be granted access upon model release.

  | Model ID                                | Description                                                                                                                                                                                                                                                                                              | Context Window                         | Max Output Tokens | Training Data (up to) |
  | --------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------- | :---------------- | :-------------------: |
  | `gpt-5.1` (2025-11-13)                  | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |
  | `gpt-5.1-chat` (2025-11-13) **Preview** | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs - Functions, tools, and parallel tool calling.                                                                                                               | 128,000 Input: 111,616 Output: 16,384  | 16,384            |   September 30, 2024  |
  | `gpt-5.1-codex` (2025-11-13)            | - [Responses API](../../openai/how-to/responses) only. - Text and image processing - Structured outputs. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning) - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |
  | `gpt-5.1-codex-mini` (2025-11-13)       | - [Responses API](../../openai/how-to/responses) only. - Text and image processing - Structured outputs. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning) - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |
  | `gpt-5.1-codex-max` (2025-12-04)        | - [Responses API](../../openai/how-to/responses) only. - Text and image processing - Structured outputs. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning) - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  <Callout type="important">
    * `gpt-5.1` `reasoning_effort` defaults to `none`. When upgrading from previous reasoning models to `gpt-5.1`, keep in mind that you may need to update your code to explicitly pass a `reasoning_effort` level if you want reasoning to occur.

    * `gpt-5.1-chat` adds built-in reasoning capabilities. Like other [reasoning models](../../openai/how-to/reasoning) it does not support parameters like `temperature`. If you upgrade from using `gpt-5-chat` (which is not a reasoning model) to `gpt-5.1-chat` make sure you remove any custom parameters like `temperature` from your code which are not supported by reasoning models.

    * `gpt-5.1-codex-max` adds support for setting `reasoning_effort` to `xhigh`. Reasoning effort `none` is not supported with `gpt-5.1-codex-max`.
  </Callout>

  ## GPT-5

  ### Region availability

  | Model                      | Region                                                                |
  | -------------------------- | --------------------------------------------------------------------- |
  | `gpt-5` (2025-08-07)       | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-mini` (2025-08-07)  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-nano` (2025-08-07)  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-chat` (2025-08-07)  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-chat` (2025-10-03)  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-codex` (2025-09-11) | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-pro` (2025-10-06)   | See the [models table](#model-summary-table-and-region-availability). |

  * **[Registration is required for access to the gpt-5-pro, gpt-5, & gpt-5-codex models](https://aka.ms/oai/gpt5access).**

  * `gpt-5-mini`, `gpt-5-nano`, and `gpt-5-chat` do not require registration.

  Access will be granted based on Microsoft's eligibility criteria. Customers who previously applied and received access to `o3`, don't need to reapply as their approved subscriptions will automatically be granted access upon model release.

  | Model ID                               | Description                                                                                                                                                                                                                                                                                                                                               | Context Window                         | Max Output Tokens | Training Data (up to) |
  | -------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------- | :---------------- | :-------------------: |
  | `gpt-5` (2025-08-07)                   | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                 | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |
  | `gpt-5-mini` (2025-08-07)              | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                 | 400,000 Input: 272,000 Output: 128,000 | 128,000           |      May 31, 2024     |
  | `gpt-5-nano` (2025-08-07)              | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                 | 400,000 Input: 272,000 Output: 128,000 | 128,000           |      May 31, 2024     |
  | `gpt-5-chat` (2025-08-07) **Preview**  | - Chat Completions API. - [Responses API](../../openai/how-to/responses). - **Input**: Text/Image - **Output**: Text only                                                                                                                                                                                                                                 | 128,000                                | 16,384            |   September 30, 2024  |
  | `gpt-5-chat` (2025-10-03) **Preview**1 | - Chat Completions API. - [Responses API](../../openai/how-to/responses). - **Input**: Text/Image - **Output**: Text only                                                                                                                                                                                                                                 | 128,000                                | 16,384            |   September 30, 2024  |
  | `gpt-5-codex` (2025-09-11)             | - [Responses API](../../openai/how-to/responses) only. - **Input**: Text/Image - **Output**: Text only - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning) - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |           -           |
  | `gpt-5-pro` (2025-10-06)               | - [Reasoning](../../openai/how-to/reasoning) - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions and tools - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                                                                  | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |

  <Callout type="note">
    1 `gpt-5-chat` version `2025-10-03` introduces a significant enhancement focused on emotional intelligence and mental health capabilities. This upgrade integrates specialized datasets and refined response strategies to improve the model's ability to:

    * **Understand and interpret emotional context** more accurately, enabling nuanced and empathetic interactions.
    * **Provide supportive, responsible responses** in conversations related to mental health, ensuring sensitivity and adherence to best practices.

    These improvements aim to make GPT-5-chat more context-aware, human-centric, and reliable in scenarios where emotional tone and well-being considerations are critical.
  </Callout>

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  ## gpt-oss

  ### Region availability

  | Model          | Region                   |
  | -------------- | ------------------------ |
  | `gpt-oss-120b` | All Azure OpenAI regions |

  ### Capabilities

  | Model ID                 | Description                                                                                                                                                                                                                                                                         | Context Window | Max Output Tokens | Training Data (up to) |
  | ------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------- | :---------------- | :-------------------: |
  | `gpt-oss-120b` (Preview) | - Text in/text out only - Chat Completions API - Streaming - Function calling - Structured outputs - Reasoning - Available for deployment1 and via [managed compute](../../how-to/deploy-models-managed)                                                                            | 131,072        | 131,072           |      May 31, 2024     |
  | `gpt-oss-20b` (Preview)  | - Text in/text out only - Chat Completions API - Streaming - Function calling - Structured outputs - Reasoning - Available via [managed compute](../../how-to/deploy-models-managed) and [Foundry Local](../../foundry-local/get-started#optional-run-the-latest-gpt-oss-20b-model) | 131,072        | 131,072           |      May 31, 2024     |

  1 Unlike other Azure OpenAI models `gpt-oss-120b` requires a [Foundry project](https://learn.microsoft.com/en-us/azure/ai-foundry/quickstarts/get-started-code?tabs=azure-ai-foundry) to deploy the model.

  ### Deploy with code

  ```cli
  az cognitiveservices account deployment create \
    --name "Foundry-project-resource" \
    --resource-group "test-rg" \
    --deployment-name "gpt-oss-120b" \
    --model-name "gpt-oss-120b" \
    --model-version "1" \
    --model-format "OpenAI-OSS" \
    --sku-capacity 10 \
    --sku-name "GlobalStandard"
  ```

  ## GPT-4.1 series

  ### Region availability

  | Model                       | Region                                                                |
  | --------------------------- | --------------------------------------------------------------------- |
  | `gpt-4.1` (2025-04-14)      | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-4.1-nano` (2025-04-14) | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-4.1-mini` (2025-04-14) | See the [models table](#model-summary-table-and-region-availability). |

  ### Capabilities

  <Callout type="important">
    A known issue is affecting all GPT 4.1 series models. Large tool or function call definitions that exceed 300,000 tokens will result in failures, even though the 1 million token context limit of the models wasn't reached.

    The errors can vary based on API call and underlying payload characteristics.

    Here are the error messages for the Chat Completions API:

    * `Error code: 400 - {'error': {'message': "This model's maximum context length is 300000 tokens. However, your messages resulted in 350564 tokens (100 in the messages, 350464 in the functions). Please reduce the length of the messages or functions.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}`

    * `Error code: 400 - {'error': {'message': "Invalid 'tools[0].function.description': string too long. Expected a string with maximum length 1048576, but got a string with length 2778531 instead.", 'type': 'invalid_request_error', 'param': 'tools[0].function.description', 'code': 'string_above_max_length'}}`

    Here's the error message for the Responses API:

    * `Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 if you keep seeing this error. (Please include the request ID d2008353-291d-428f-adc1-defb5d9fb109 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}`
  </Callout>

  | Model ID                    | Description                                                                                                                                        | Context window                                                                                   | Max output tokens | Training data (up to) |
  | --------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------- | :---------------- | :-------------------: |
  | `gpt-4.1` (2025-04-14)      | - Text and image input - Text output - Chat completions API - Responses API - Streaming - Function calling - Structured outputs (chat completions) | - 1,047,576 - 128,000 (standard & provisioned managed deployments) - 300,000 (batch deployments) | 32,768            |      May 31, 2024     |
  | `gpt-4.1-nano` (2025-04-14) | - Text and image input - Text output - Chat completions API - Responses API - Streaming - Function calling - Structured outputs (chat completions) | - 1,047,576 - 128,000 (standard & provisioned managed deployments) - 300,000 (batch deployments) | 32,768            |      May 31, 2024     |
  | `gpt-4.1-mini` (2025-04-14) | - Text and image input - Text output - Chat completions API - Responses API - Streaming - Function calling - Structured outputs (chat completions) | - 1,047,576 - 128,000 (standard & provisioned managed deployments) - 300,000 (batch deployments) | 32,768            |      May 31, 2024     |

  ## computer-use-preview

  An experimental model trained for use with the [Responses API](../../openai/how-to/responses) computer use tool.

  It can be used with third-party libraries to allow the model to control mouse and keyboard input, while getting context from screenshots of the current environment.

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  Registration is required to access `computer-use-preview`. Access is granted based on Microsoft's eligibility criteria. Customers who have access to other limited access models still need to request access for this model.

  To request access, go to [`computer-use-preview` limited access model application](https://aka.ms/oai/cuaaccess). When access is granted, you need to create a deployment for the model.

  ### Region availability

  | Model                  | Region                                                                |
  | ---------------------- | --------------------------------------------------------------------- |
  | `computer-use-preview` | See the [models table](#model-summary-table-and-region-availability). |

  ### Capabilities

  | Model ID                            | Description                                                                                                                                                   | Context window | Max output tokens | Training data (up to) |
  | ----------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------- | :---------------- | :-------------------: |
  | `computer-use-preview` (2025-03-11) | Specialized model for use with the [Responses API](../../openai/how-to/responses) computer use tool - Tools - Streaming - Text (input/output) - Image (input) | 8,192          | 1,024             |      October 2023     |

  ## o-series models

  The Azure OpenAI o-series models are designed to tackle reasoning and problem-solving tasks with increased focus and capability. These models spend more time processing and understanding the user's request, making them exceptionally strong in areas like science, coding, and math, compared to previous iterations.

  | Model ID                  | Description                                                                                                                                                                                                                                                                                                                                        | Max request (tokens)           | Training data (up to) |
  | ------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------- | :-------------------: |
  | `codex-mini` (2025-05-16) | Fine-tuned version of `o4-mini`. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions and tools. [Full summary of capabilities](../../openai/how-to/reasoning).                                                                                                                        | Input: 200,000 Output: 100,000 |      May 31, 2024     |
  | `o3-pro` (2025-06-10)     | - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions and tools. [Full summary of capabilities](../../openai/how-to/reasoning).                                                                                                                                                         | Input: 200,000 Output: 100,000 |      May 31, 2024     |
  | `o4-mini` (2025-04-16)    | - *New* reasoning model, offering [enhanced reasoning abilities](../../openai/how-to/reasoning). - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions and tools. [Full summary of capabilities](../../openai/how-to/reasoning).                                | Input: 200,000 Output: 100,000 |      May 31, 2024     |
  | `o3` (2025-04-16)         | - *New* reasoning model, offering [enhanced reasoning abilities](../../openai/how-to/reasoning). - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. [Full summary of capabilities](../../openai/how-to/reasoning).        | Input: 200,000 Output: 100,000 |      May 31, 2024     |
  | `o3-mini` (2025-01-31)    | - [Enhanced reasoning abilities](../../openai/how-to/reasoning). - Structured outputs. - Text-only processing. - Functions and tools.                                                                                                                                                                                                              | Input: 200,000 Output: 100,000 |      October 2023     |
  | `o1` (2024-12-17)         | - [Enhanced reasoning abilities](../../openai/how-to/reasoning). - Structured outputs. - Text and image processing. - Functions and tools.                                                                                                                                                                                                         | Input: 200,000 Output: 100,000 |      October 2023     |
  | `o1-preview` (2024-09-12) | Older preview version.                                                                                                                                                                                                                                                                                                                             | Input: 128,000 Output: 32,768  |      October 2023     |
  | `o1-mini` (2024-09-12)    | A faster and more cost-efficient option in the o1 series, ideal for coding tasks that require speed and lower resource consumption. - Global Standard deployment available by default. - Standard (regional) deployments are currently only available for select customers who received access as part of the `o1-preview` limited access release. | Input: 128,000 Output: 65,536  |      October 2023     |

  To learn more about advanced o-series models, see [Getting started with reasoning models](../../openai/how-to/reasoning).

  ### Region availability

  | Model        | Region                                                                                                                                                                           |
  | ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `codex-mini` | East US2 & Sweden Central (Global Standard).                                                                                                                                     |
  | `o3-pro`     | East US2 & Sweden Central (Global Standard).                                                                                                                                     |
  | `o4-mini`    | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |
  | `o3`         | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |
  | `o3-mini`    | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |
  | `o1`         | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |
  | `o1-preview` | See the [models table](#model-summary-table-and-region-availability). This model is available only for customers who were granted access as part of the original limited access. |
  | `o1-mini`    | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |

  ## GPT-4o and GPT-4 Turbo

  GPT-4o integrates text and images in a single model, which enables it to handle multiple data types simultaneously. This multimodal approach enhances accuracy and responsiveness in human-computer interactions. GPT-4o matches GPT-4 Turbo in English text and coding tasks while offering superior performance in non-English language tasks and vision tasks, setting new benchmarks for AI capabilities.

  ## GPT-4 and GPT-4 Turbo models

  These models can be used only with the Chat Completions API.

  See [Model versions](../../openai/concepts/model-versions) to learn about how Azure OpenAI handles model version upgrades. See [Working with models](../../openai/how-to/working-with-models) to learn how to view and configure the model version settings of your GPT-4 deployments.

  | Model ID                                           | Description                                                                                                                                                                                                                                                                                                                     | Max request (tokens)          | Training data (up to) |
  | -------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :---------------------------- | :-------------------: |
  | `gpt-4o` (2024-11-20) GPT-4o (Omni)                | - Structured outputs. - Text and image processing. - JSON Mode. - Parallel function calling. - Enhanced accuracy and responsiveness. - Parity with English text and coding tasks compared to GPT-4 Turbo with Vision. - Superior performance in non-English languages and in vision tasks. - Enhanced creative writing ability. | Input: 128,000 Output: 16,384 |      October 2023     |
  | `gpt-4o` (2024-08-06) GPT-4o (Omni)                | - Structured outputs. - Text and image processing. - JSON Mode. - Parallel function calling. - Enhanced accuracy and responsiveness. - Parity with English text and coding tasks compared to GPT-4 Turbo with Vision. - Superior performance in non-English languages and in vision tasks.                                      | Input: 128,000 Output: 16,384 |      October 2023     |
  | `gpt-4o-mini` (2024-07-18) GPT-4o mini             | - Fast, inexpensive, capable model ideal for replacing GPT-3.5 Turbo series models. - Text and image processing. - JSON Mode. - Parallel function calling.                                                                                                                                                                      | Input: 128,000 Output: 16,384 |      October 2023     |
  | `gpt-4o` (2024-05-13) GPT-4o (Omni)                | - Text and image processing. - JSON Mode. - Parallel function calling. - Enhanced accuracy and responsiveness. - Parity with English text and coding tasks compared to GPT-4 Turbo with Vision. - Superior performance in non-English languages and in vision tasks.                                                            | Input: 128,000 Output: 4,096  |      October 2023     |
  | `gpt-4` (turbo-2024-04-09) GPT-4 Turbo with Vision | New generally available model. - Replacement for all previous GPT-4 preview models (`vision-preview`, `1106-Preview`, `0125-Preview`). - [Feature availability](#gpt-4o-and-gpt-4-turbo) is currently different, depending on the method of input and the deployment type.                                                      | Input: 128,000 Output: 4,096  |     December 2023     |

  <Callout type="caution">
    We don't recommend that you use preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  ## Embeddings

  `text-embedding-3-large` is the latest and most capable embedding model. You can't upgrade between embeddings models. To move from using `text-embedding-ada-002` to `text-embedding-3-large`, you need to generate new embeddings.

  * `text-embedding-3-large`
  * `text-embedding-3-small`
  * `text-embedding-ada-002`

  OpenAI reports that testing shows that both the large and small third generation embeddings models offer better average multi-language retrieval performance with the [MIRACL](https://github.com/project-miracl/miracl) benchmark. They still maintain performance for English tasks with the [MTEB](https://github.com/embeddings-benchmark/mteb) benchmark.

  | Evaluation benchmark | `text-embedding-ada-002` | `text-embedding-3-small` | `text-embedding-3-large` |
  | -------------------- | ------------------------ | ------------------------ | ------------------------ |
  | MIRACL average       | 31.4                     | 44.0                     | 54.9                     |
  | MTEB average         | 61.0                     | 62.3                     | 64.6                     |

  The third generation embeddings models support reducing the size of the embedding via a new `dimensions` parameter. Typically, larger embeddings are more expensive from a compute, memory, and storage perspective. When you can adjust the number of dimensions, you gain more control over overall cost and performance. The `dimensions` parameter isn't supported in all versions of the OpenAI 1.x Python library. To take advantage of this parameter, we recommend that you upgrade to the latest version: `pip install openai --upgrade`.

  OpenAI's MTEB benchmark testing found that even when the third generation model's dimensions are reduced to less than the 1,536 dimensions of `text-embeddings-ada-002`, performance remains slightly better.

  ## Image generation models

  The image generation models generate images from text prompts that the user provides. GPT-image-1 series models are in limited access preview. DALL-E 3 is generally available for use with the REST APIs. DALL-E 2 and DALL-E 3 with client SDKs are in preview.

  Registration is required to access `gpt-image-1`, `gpt-image-1-mini`, or `gpt-image-1.5`. Access is granted based on Microsoft's eligibility criteria. Customers who have access to other limited access models still need to request access for this model.

  To request access, fill out an application form: [Apply for GPT-image-1 access](https://aka.ms/oai/gptimage1access); [Apply for GPT-image-1.5 access](https://aka.ms/oai/gptimage1.5access). When access is granted, you need to create a deployment for the model.

  ### Region availability

  | Model              | Region                                                                                                                                                |
  | ------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `dall-e-3`         | East US Australia East Sweden Central                                                                                                                 |
  | `gpt-image-1`      | West US 3 (Global Standard) East US 2 (Global Standard) UAE North (Global Standard) Poland Central (Global Standard) Sweden Central (Global Standard) |
  | `gpt-image-1-mini` | West US 3 (Global Standard) East US 2 (Global Standard) UAE North (Global Standard) Poland Central (Global Standard) Sweden Central (Global Standard) |
  | `gpt-image-1.5`    | West US 3 (Global Standard) East US 2 (Global Standard) UAE North (Global Standard) Poland Central (Global Standard) Sweden Central (Global Standard) |

  ## Video generation models

  Sora is an AI model from OpenAI that can create realistic and imaginative video scenes from text instructions. Sora is in preview.

  ### Region availability

  | Model    | Region                                                       |
  | -------- | ------------------------------------------------------------ |
  | `sora`   | East US 2 (Global Standard) Sweden Central (Global Standard) |
  | `sora-2` | East US 2 (Global Standard) Sweden Central (Global Standard) |

  ## Audio models

  Audio models in Azure OpenAI are available via the `realtime`, `completions`, and `audio` APIs.

  ### GPT-4o audio models

  The GPT-4o audio models are part of the GPT-4o model family and support either low-latency, *speech in, speech out* conversational interactions or audio generation.

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  Details about maximum request tokens and training data are available in the following table:

  | Model ID                                                                                                                                                           | Description                                 | Max request (tokens)          | Training data (up to) |
  | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------- | ----------------------------- | --------------------- |
  | `gpt-4o-mini-audio-preview` (2024-12-17) GPT-4o audio                                                                                                              | Audio model for audio and text generation.  | Input: 128,000 Output: 16,384 | September 2023        |
  | `gpt-4o-audio-preview` (2024-12-17) GPT-4o audio                                                                                                                   | Audio model for audio and text generation.  | Input: 128,000 Output: 16,384 | September 2023        |
  | `gpt-4o-realtime-preview` (2025-06-03) GPT-4o audio                                                                                                                | Audio model for real-time audio processing. | Input: 128,000 Output: 4,096  | October 2023          |
  | `gpt-4o-realtime-preview` (2024-12-17) GPT-4o audio                                                                                                                | Audio model for real-time audio processing. | Input: 128,000 Output: 4,096  | October 2023          |
  | `gpt-4o-mini-realtime-preview` (2024-12-17) GPT-4o audio                                                                                                           | Audio model for real-time audio processing. | Input: 128,000 Output: 4,096  | October 2023          |
  | `gpt-realtime` (2025-08-28) (GA) `gpt-realtime-mini` (2025-10-06) `gpt-realtime-mini-2025-12-15` (2025-12-15) `gpt-audio`(2025-08-28) `gpt-audio-mini`(2025-10-06) | Audio model for real-time audio processing. | Input: 28,672 Output: 4,096   | October 2023          |

  To compare the availability of GPT-4o audio models across all regions, refer to the [models table](#global-standard-model-availability).

  ### Audio API

  The audio models via the `/audio` API can be used for speech to text, translation, and text to speech.

  #### Speech-to-text models

  | Model ID                            | Description                                                                                             | Max request (audio file size) |
  | ----------------------------------- | ------------------------------------------------------------------------------------------------------- | ----------------------------- |
  | `whisper`                           | General-purpose speech recognition model.                                                               | 25 MB                         |
  | `gpt-4o-transcribe`                 | Speech-to-text model powered by GPT-4o.                                                                 | 25 MB                         |
  | `gpt-4o-mini-transcribe`            | Speech-to-text model powered by GPT-4o mini.                                                            | 25 MB                         |
  | `gpt-4o-transcribe-diarize`         | Speech-to-text model with automatic speech recognition.                                                 | 25 MB                         |
  | `gpt-4o-mini-transcribe-2025-12-15` | Speech-to-text model with automatic speech recognition. Improved transcription accuracy and robustness. | 25 MB                         |

  #### Speech translation models

  | Model ID  | Description                               | Max request (audio file size) |
  | --------- | ----------------------------------------- | ----------------------------- |
  | `whisper` | General-purpose speech recognition model. | 25 MB                         |

  #### Text-to-speech models (preview)

  | Model ID                     | Description                                                                                                |
  | ---------------------------- | :--------------------------------------------------------------------------------------------------------- |
  | `tts`                        | Text-to-speech model optimized for speed.                                                                  |
  | `tts-hd`                     | Text-to-speech model optimized for quality.                                                                |
  | `gpt-4o-mini-tts`            | Text-to-speech model powered by GPT-4o mini. You can guide the voice to speak in a specific style or tone. |
  | `gpt-4o-mini-tts-2025-12-15` | Text-to-speech model powered by GPT-4o mini. You can guide the voice to speak in a specific style or tone. |

  ## Model summary table and region availability

  ### Models by deployment type

  Azure OpenAI provides customers with choices on the hosting structure that fits their business and usage patterns. The service offers two main types of deployment:

  * **Standard**: Has a global deployment option, routing traffic globally to provide higher throughput.
  * **Provisioned**: Also has a global deployment option, allowing customers to purchase and deploy provisioned throughput units across Azure global infrastructure.

  All deployments can perform the exact same inference operations, but the billing, scale, and performance are substantially different. To learn more about Azure OpenAI deployment types, see our [Deployment types guide](deployment-types).

  <Tabs>
    <Tab title="Global Standard">
      ### Global Standard model availability

      | **Region**         | **gpt-5.2-codex**, **2026-01-14** | **gpt-5.2**, **2025-12-11** | **gpt-5.2-chat**, **2025-12-11** | **gpt-5.1-codex-max**, **2025-12-04** | **gpt-5.1**, **2025-11-13** | **gpt-5.1-chat**, **2025-11-13** | **gpt-5.1-codex**, **2025-11-13** | **gpt-5.1-codex-mini**, **2025-11-13** | **gpt-5-pro**, **2025-10-06** | **gpt-5-codex**, **2025-09-15** | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **gpt-5-nano**, **2025-08-07** | **gpt-5-chat**, **2025-08-07** | **gpt-5-chat**, **2025-10-03** | **o3-pro**, **2025-06-10** | **codex-mini**, **2025-05-16** | **sora**, **2025-05-02** | **model-router**, **2025-08-07** | **model-router**, **2025-05-19** | **model-router**, **2025-11-18** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-image-1**, **2025-04-15** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **computer-use-preview**, **2025-03-11** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** | **text-embedding-3-small**, **1** | **text-embedding-3-large**, **1** | **text-embedding-ada-002**, **2** | **gpt-4o-realtime-preview**, **2024-12-17** | **gpt-4o-audio-preview**, **2024-12-17** | **gpt-4o-mini-realtime-preview**, **2024-12-17** | **gpt-4o-mini-audio-preview**, **2024-12-17** | **gpt-4o-transcribe**, **2025-03-20** | **gpt-4o-mini-tts**, **2025-12-15** | **gpt-4o-mini-tts**, **2025-03-20** | **gpt-4o-mini-transcribe**, **2025-12-15** | **gpt-4o-mini-transcribe**, **2025-03-20** | **gpt-image-1-mini**, **2025-10-06** | **gpt-audio-mini**, **2025-10-06** | **gpt-audio-mini**, **2025-12-15** | **gpt-image-1.5**, **2025-12-16** | **sora-2**, **2025-10-06** | **gpt-realtime-mini**, **2025-10-06** | **gpt-realtime-mini**, **2025-12-15** | **o3-deep-research**, **2025-06-26** | **gpt-realtime**, **2025-08-28** | **gpt-audio**, **2025-08-28** | **gpt-4o-transcribe-diarize**, **2025-10-15** |
      | :----------------- | :-------------------------------: | :-------------------------: | :------------------------------: | :-----------------------------------: | :-------------------------: | :------------------------------: | :-------------------------------: | :------------------------------------: | :---------------------------: | :-----------------------------: | :-----------------------: | :----------------------------: | :----------------------------: | :----------------------------: | :----------------------------: | :------------------------: | :----------------------------: | :----------------------: | :------------------------------: | :------------------------------: | :------------------------------: | :--------------------: | :-------------------------: | :-----------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :--------------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: | :-----------------------------------------: | :--------------------------------------: | :----------------------------------------------: | :-------------------------------------------: | :-----------------------------------: | :---------------------------------: | :---------------------------------: | :----------------------------------------: | :----------------------------------------: | :----------------------------------: | :--------------------------------: | :--------------------------------: | :-------------------------------: | :------------------------: | :-----------------------------------: | :-----------------------------------: | :----------------------------------: | :------------------------------: | :---------------------------: | :-------------------------------------------: |
      | australiaeast      |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | brazilsouth        |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | canadacentral      |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             -             |                -               |                -               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            -           |              -              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              -              |            -           |              -             |              ✅             |              -             |                ✅                |                 -                 |                 -                 |                 -                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | canadaeast         |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | centralus          |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               ✅               |                ✅                |             ✅             |                ✅               |                ✅               |                ✅               |                ✅               |              ✅             |                ✅               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 -                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  ✅                 |                  ✅                 |                 -                 |              -             |                   ✅                   |                   ✅                   |                   -                  |                 ✅                |               ✅               |                       -                       |
      | eastus             |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       ✅                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | eastus2            |                 ✅                 |              ✅              |                 ✅                |                   ✅                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               ✅               |                ✅                |             ✅             |                ✅               |                ✅               |                ✅               |                ✅               |              ✅             |                ✅               |             ✅            |                 ✅                |                 ✅                |                 ✅                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     ✅                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      ✅                      |                     ✅                    |                         ✅                        |                       ✅                       |                   ✅                   |                  ✅                  |                  ✅                  |                      ✅                     |                      ✅                     |                   ✅                  |                  ✅                 |                  ✅                 |                 ✅                 |              ✅             |                   ✅                   |                   ✅                   |                   -                  |                 ✅                |               ✅               |                       ✅                       |
      | francecentral      |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | germanywestcentral |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | italynorth         |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              -             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | japaneast          |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | koreacentral       |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | northcentralus     |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | norwayeast         |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   ✅                  |                 -                |               -               |                       -                       |
      | polandcentral      |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   ✅                  |                  -                 |                  -                 |                 ✅                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | southafricanorth   |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | southcentralus     |                 -                 |              ✅              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | southeastasia      |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              -              |            -           |              -             |              ✅             |              -             |                ✅                |                 -                 |                 -                 |                 -                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | southindia         |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     ✅                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | spaincentral       |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | swedencentral      |                 ✅                 |              ✅              |                 ✅                |                   ✅                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               ✅               |                ✅                |             ✅             |                ✅               |                ✅               |                ✅               |                ✅               |              ✅             |                ✅               |             ✅            |                 ✅                |                 ✅                |                 ✅                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     ✅                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      ✅                      |                     ✅                    |                         ✅                        |                       -                       |                   ✅                   |                  -                  |                  -                  |                      ✅                     |                      ✅                     |                   ✅                  |                  ✅                 |                  ✅                 |                 ✅                 |              ✅             |                   ✅                   |                   ✅                   |                   -                  |                 ✅                |               ✅               |                       ✅                       |
      | switzerlandnorth   |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | switzerlandwest    |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             -             |                -               |                -               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            -           |              -              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              -              |            -           |              -             |              ✅             |              -             |                ✅                |                 -                 |                 -                 |                 -                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | uaenorth           |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   ✅                  |                  -                 |                  -                 |                 ✅                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | uksouth            |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | westeurope         |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | westus             |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   ✅                  |                 -                |               -               |                       -                       |
      | westus3            |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   ✅                  |                  -                 |                  -                 |                 ✅                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |

      <Callout type="note">
        `o3-deep-research` is currently only available with Foundry Agent Service. To learn more, see the [Deep Research tool guidance](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/deep-research).
      </Callout>
    </Tab>

    <Tab title="Global Provisioned managed">
      ### Global Provisioned managed model availability

      | **Region**         | **gpt-5.2**, **2025-12-11** | **gpt-5.1**, **2025-11-13** | **gpt-5.1-codex**, **2025-11-13** | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** |
      | :----------------- | :-------------------------: | :-------------------------: | :-------------------------------: | :-----------------------: | :----------------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: |
      | australiaeast      |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | brazilsouth        |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | canadacentral      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | canadaeast         |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | centralus          |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus             |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus2            |              ✅              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | francecentral      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | germanywestcentral |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | italynorth         |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | japaneast          |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | koreacentral       |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | northcentralus     |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | norwayeast         |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | polandcentral      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southafricanorth   |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southcentralus     |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southeastasia      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southindia         |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | spaincentral       |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | swedencentral      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | switzerlandnorth   |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | switzerlandwest    |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | uaenorth           |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | uksouth            |              ✅              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westeurope         |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus             |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus3            |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
    </Tab>

    <Tab title="Global Batch">
      ### Global Batch model availability

      | **Region**         | **gpt-5.1**, **2025-11-13** | **gpt-5**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** |
      | :----------------- | :-------------------------: | :-----------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: |
      | australiaeast      |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | brazilsouth        |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | canadaeast         |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | centralus          |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus             |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus2            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | francecentral      |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | germanywestcentral |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | japaneast          |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | koreacentral       |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | northcentralus     |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | norwayeast         |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | polandcentral      |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | southafricanorth   |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | southcentralus     |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | southindia         |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | swedencentral      |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | switzerlandnorth   |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | uksouth            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | westeurope         |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus             |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus3            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
    </Tab>

    <Tab title="Data Zone Standard">
      ### Data Zone Standard model availability

      | **Region**         | **gpt-5.2**, **2025-12-11** | **gpt-5.1**, **2025-11-13** | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **gpt-5-nano**, **2025-08-07** | **model-router**, **2025-08-07** | **model-router**, **2025-05-19** | **model-router**, **2025-11-18** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** | **text-embedding-3-small**, **1** | **text-embedding-3-large**, **1** | **text-embedding-ada-002**, **2** |
      | :----------------- | :-------------------------: | :-------------------------: | :-----------------------: | :----------------------------: | :----------------------------: | :------------------------------: | :------------------------------: | :------------------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: |
      | centralus          |              ✅              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | eastus             |              -              |              ✅              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | eastus2            |              ✅              |              ✅              |             ✅             |                ✅               |                ✅               |                 ✅                |                 ✅                |                 ✅                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | francecentral      |              -              |              ✅              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | germanywestcentral |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | italynorth         |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              -             |              -             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | northcentralus     |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | polandcentral      |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | southcentralus     |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | spaincentral       |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | swedencentral      |              -              |              ✅              |             ✅             |                ✅               |                ✅               |                 ✅                |                 ✅                |                 ✅                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | westeurope         |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | westus             |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | westus3            |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |

      <Callout type="note">
        `o1-mini` is currently available to all customers for Global Standard deployment.

        Select customers were granted standard (regional) deployment access to `o1-mini` as part of the `o1-preview` limited access release. At this time, access to `o1-mini` standard (regional) deployments isn't being expanded.
      </Callout>
    </Tab>

    <Tab title="Data Zone Provisioned managed">
      ### Data Zone Provisioned managed model availability

      | **Region**         | **gpt-5.1**, **2025-11-13** | **gpt-5.1-codex**, **2025-11-13** | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** |
      | :----------------- | :-------------------------: | :-------------------------------: | :-----------------------: | :----------------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: |
      | eastus             |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus2            |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | francecentral      |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | germanywestcentral |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | italynorth         |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 -                |              -              |            -           |              -             |              -             |              -             |                -                |
      | northcentralus     |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | polandcentral      |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southcentralus     |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | spaincentral       |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | swedencentral      |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westeurope         |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus             |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus3            |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
    </Tab>

    <Tab title="Data Zone Batch">
      ### Data Zone Batch model availability

      | **Region**         | **gpt-5.1**, **2025-11-13** | **gpt-5**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** |
      | :----------------- | :-------------------------: | :-----------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :------------------------: | :------------------------: | :-----------------------------: |
      | centralus          |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | eastus             |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | eastus2            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | francecentral      |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | germanywestcentral |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | northcentralus     |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | polandcentral      |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | southcentralus     |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | swedencentral      |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | westeurope         |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | westus             |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | westus3            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
    </Tab>

    <Tab title="Standard">
      ### Standard deployment model availability

      | **Region**         | **sora**, **2025-05-02** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** | **text-embedding-3-small**, **1** | **text-embedding-3-large**, **1** | **text-embedding-ada-002**, **1** | **text-embedding-ada-002**, **2** | **dall-e-3**, **3.0** | **tts**, **001** | **tts-hd**, **001** | **whisper**, **001** |
      | :----------------- | :----------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: | :-------------------: | :--------------: | :-----------------: | :------------------: |
      | australiaeast      |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           ✅           |         -        |          -          |           -          |
      | brazilsouth        |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 -                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | canadaeast         |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | centralus          |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              ✅             |              ✅             |                -                |                 -                 |                 -                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | eastus             |             -            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                 ✅                 |           ✅           |         -        |          -          |           -          |
      | eastus2            |             ✅            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | francecentral      |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | germanywestcentral |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | japaneast          |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | koreacentral       |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | northcentralus     |             -            |              ✅              |              ✅              |                 ✅                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 -                 |                 -                 |                 -                 |                 ✅                 |           -           |         ✅        |          ✅          |           ✅          |
      | norwayeast         |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              ✅             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | polandcentral      |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | southafricanorth   |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | southcentralus     |             -            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 -                 |                 -                 |                 ✅                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | southeastasia      |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | southindia         |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | spaincentral       |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | swedencentral      |             -            |              ✅              |              ✅              |                 ✅                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           ✅           |         ✅        |          ✅          |           ✅          |
      | switzerlandnorth   |             -            |              -              |              ✅              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | uaenorth           |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | uksouth            |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | westeurope         |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 -                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | westus             |             -            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 -                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | westus3            |             -            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         ✅        |          ✅          |           -          |

      <Callout type="note">
        `o1-mini` is currently available to all customers for Global Standard deployment.

        Select customers were granted standard (regional) deployment access to `o1-mini` as part of the `o1-preview` limited access release. At this time, access to `o1-mini` standard (regional) deployments isn't being expanded.
      </Callout>
    </Tab>

    <Tab title="Provisioned managed">
      ### Provisioned deployment model availability

      | **Region**         | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** | **gpt-4**, **0613** | **gpt-4**, **1106-Preview** | **gpt-4**, **0125-Preview** | **gpt-4**, **turbo-2024-04-09** | **gpt-4-32k**, **0613** | **gpt-35-turbo**, **1106** | **gpt-35-turbo**, **0125** |
      | :----------------- | :-----------------------: | :----------------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: | :-----------------: | :-------------------------: | :-------------------------: | :-----------------------------: | :---------------------: | :------------------------: | :------------------------: |
      | australiaeast      |             -             |                -               |            ✅           |              -              |              ✅              |                 -                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | brazilsouth        |             -             |                -               |            -           |              -              |              ✅              |                 -                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                -                |            ✅            |              ✅             |              -             |
      | canadacentral      |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              -             |              -             |                ✅                |          ✅          |              -              |              -              |                -                |            ✅            |              -             |              ✅             |
      | canadaeast         |             ✅             |                -               |            -           |              -              |              -              |                 -                |                 -                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              -              |                ✅                |            -            |              ✅             |              -             |
      | centralus          |             -             |                -               |            -           |              ✅              |              ✅              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              ✅             |                -                |          -          |              -              |              -              |                -                |            ✅            |              -             |              ✅             |
      | eastus             |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | eastus2            |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | francecentral      |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              -             |                ✅                |          ✅          |              ✅              |              ✅              |                -                |            ✅            |              -             |              ✅             |
      | germanywestcentral |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              ✅             |                -                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              -             |
      | japaneast          |             ✅             |                -               |            -           |              ✅              |              ✅              |                 -                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |          -          |              ✅              |              ✅              |                ✅                |            -            |              -             |              ✅             |
      | koreacentral       |             ✅             |                ✅               |            -           |              -              |              ✅              |                 -                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              -              |              -              |                ✅                |            ✅            |              ✅             |              -             |
      | northcentralus     |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | norwayeast         |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              -             |                ✅                |          ✅          |              -              |              ✅              |                -                |            ✅            |              -             |              -             |
      | polandcentral      |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              -             |              ✅             |                -                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | southafricanorth   |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              -             |              -             |                -                |          ✅          |              ✅              |              -              |                ✅                |            ✅            |              ✅             |              -             |
      | southcentralus     |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              -             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | southeastasia      |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              -             |              ✅             |              ✅             |                ✅                |          -          |              -              |              -              |                -                |            -            |              -             |              -             |
      | southindia         |             ✅             |                -               |            -           |              -              |              ✅              |                 -                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                -                |            ✅            |              ✅             |              ✅             |
      | spaincentral       |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              -             |                ✅                |          -          |              -              |              -              |                -                |            -            |              -             |              ✅             |
      | swedencentral      |             -             |                -               |            -           |              -              |              ✅              |                 ✅                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | switzerlandnorth   |             -             |                -               |            -           |              -              |              ✅              |                 -                |                 -                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | switzerlandwest    |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              -             |              ✅             |              -             |                ✅                |          -          |              -              |              -              |                -                |            -            |              -             |              ✅             |
      | uaenorth           |             -             |                -               |            -           |              -              |              ✅              |                 -                |                 -                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                -                |          -          |              ✅              |              -              |                -                |            -            |              ✅             |              ✅             |
      | uksouth            |             -             |                -               |            -           |              -              |              ✅              |                 -                |                 ✅                |              -              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | westeurope         |             -             |                -               |            ✅           |              ✅              |              -              |                 -                |                 -                |              -              |            -           |              -             |              -             |              ✅             |                -                |          -          |              -              |              -              |                -                |            -            |              -             |              -             |
      | westus             |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | westus3            |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |

      <Callout type="note">
        The provisioned version of `gpt-4` version `turbo-2024-04-09` is currently limited to text only.
      </Callout>

      For more information on provisioned deployments, see [Provisioned guidance](../../openai/concepts/provisioned-throughput).
    </Tab>
  </Tabs>

  This table doesn't include fine-tuning regional availability information. Consult the [fine-tuning section](#fine-tuning-models) for this information.

  ### Embeddings models

  These models can be used only with Embedding API requests.

  <Callout type="note">
    `text-embedding-3-large` is the latest and most capable embedding model. You can't upgrade between embedding models. To migrate from using `text-embedding-ada-002` to `text-embedding-3-large`, you need to generate new embeddings.
  </Callout>

  | Model ID                             | Max request (tokens) | Output dimensions | Training data (up to) |
  | ------------------------------------ | -------------------- | :---------------: | :-------------------: |
  | `text-embedding-ada-002` (version 2) | 8,192                |       1,536       |        Sep 2021       |
  | `text-embedding-ada-002` (version 1) | 2,046                |       1,536       |        Sep 2021       |
  | `text-embedding-3-large`             | 8,192                |       3,072       |        Sep 2021       |
  | `text-embedding-3-small`             | 8,192                |       1,536       |        Sep 2021       |

  <Callout type="note">
    When you send an array of inputs for embedding, the maximum number of input items in the array per call to the embedding endpoint is 2,048.
  </Callout>

  ### Image generation models

  | Model ID           | Max request (characters) |
  | ------------------ | :----------------------: |
  | `gpt-image-1`      |           4,000          |
  | `gpt-image-1-mini` |           4,000          |
  | `gpt-image-1.5`    |           4,000          |
  | `dall-e-3`         |           4,000          |

  ### Video generation models

  | Model ID | Max Request (characters) |
  | -------- | :----------------------: |
  | sora     |           4,000          |

  ## Fine-tuning models

  <Callout type="note">
    The supported regions for fine-tuning might vary if you use Azure OpenAI models in a Microsoft Foundry project versus outside a project.
  </Callout>

  | Model ID                        | Standard regions                         | Global | Developer |                           Max request (tokens)                          | Training data (up to) | Modality                |
  | ------------------------------- | ---------------------------------------- | :----: | :-------: | :---------------------------------------------------------------------: | --------------------- | ----------------------- |
  | `gpt-4o-mini` (2024-07-18)      | North Central US Sweden Central          |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | Oct 2023              | Text to text            |
  | `gpt-4o` (2024-08-06)           | East US2 North Central US Sweden Central |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | Oct 2023              | Text and vision to text |
  | `gpt-4.1` (2025-04-14)          | North Central US Sweden Central          |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | May 2024              | Text and vision to text |
  | `gpt-4.1-mini` (2025-04-14)     | North Central US Sweden Central          |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | May 2024              | Text to text            |
  | `gpt-4.1-nano` (2025-04-14)     | North Central US Sweden Central          |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 32,768  | May 2024              | Text to text            |
  | `o4-mini` (2025-04-16)          | East US2 Sweden Central                  |    ✅   |     ❌     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | May 2024              | Text to text            |
  | `Ministral-3B` (preview) (2411) | Not supported                            |    ✅   |     ❌     | Input: 128,000 Output: Unknown Training example context length: Unknown | Unknown               | Text to text            |
  | `Qwen-32B` (preview)            | Not supported                            |    ✅   |     ❌     |    Input: 8,000 Output: 32,000 Training example context length: 8192    | July 2024             | Text to text            |

  <Callout type="note">
    Global training provides [more affordable](https://aka.ms/aoai-pricing) training per token, but doesn't offer [data residency](https://aka.ms/data-residency). It's currently available to Foundry resources in the following regions:

    * Australia East
    * Brazil South
    * Canada Central
    * Canada East
    * East US
    * East US2
    * France Central
    * Germany West Central
    * Italy North
    * Japan East *(no vision support)*
    * Korea Central
    * North Central US
    * Norway East
    * Poland Central *(no 4.1-nano support)*
    * Southeast Asia
    * South Africa North
    * South Central US
    * South India
    * Spain Central
    * Sweden Central
    * Switzerland West
    * Switzerland North
    * UK South
    * West Europe
    * West US
    * West US3
  </Callout>

  ## Assistants (preview)

  For Assistants, you need a combination of a supported model and a supported region. Certain tools and capabilities require the latest models. The following models are available in the Assistants API, SDK, and Foundry. The following table is for standard deployment. For information on provisioned throughput unit availability, see [Provisioned throughput](../../openai/concepts/provisioned-throughput). The listed models and regions can be used with both Assistants v1 and v2. You can use [Global Standard models](#global-standard-model-availability) if they're supported in the following regions.

  | Region        | gpt-4o, 2024-05-13 | gpt-4o, 2024-08-06 | gpt-4o-mini, 2024-07-18 | gpt-4, 0613 | gpt-4, 1106-Preview | gpt-4, 0125-Preview | gpt-4, turbo-2024-04-09 | gpt-4-32k, 0613 | gpt-35-turbo, 0613 | gpt-35-turbo, 1106 | gpt-35-turbo, 0125 | gpt-35-turbo-16k, 0613 |
  | :------------ | :----------------: | :----------------: | :---------------------: | :---------: | :-----------------: | :-----------------: | :---------------------: | :-------------: | :----------------: | :----------------: | :----------------: | :--------------------: |
  | australiaeast |          -         |          -         |            -            |      ✅      |          ✅          |          -          |            -            |        ✅        |          ✅         |          ✅         |          ✅         |            ✅           |
  | eastus        |          ✅         |          ✅         |            ✅            |      -      |          -          |          ✅          |            ✅            |        -        |          ✅         |          -         |          ✅         |            ✅           |
  | eastus2       |          ✅         |          ✅         |            ✅            |      -      |          ✅          |          -          |            ✅            |        -        |          ✅         |          -         |          ✅         |            ✅           |
  | francecentral |          -         |          -         |            -            |      ✅      |          ✅          |          -          |            -            |        ✅        |          ✅         |          ✅         |          -         |            ✅           |
  | japaneast     |          -         |          -         |            -            |      -      |          -          |          -          |            -            |        -        |          ✅         |          -         |          ✅         |            ✅           |
  | norwayeast    |          -         |          -         |            -            |      -      |          ✅          |          -          |            -            |        -        |          -         |          -         |          -         |            -           |
  | southindia    |          -         |          -         |            -            |      -      |          ✅          |          -          |            -            |        -        |          -         |          ✅         |          ✅         |            -           |
  | swedencentral |          ✅         |          ✅         |            ✅            |      ✅      |          ✅          |          -          |            ✅            |        ✅        |          ✅         |          ✅         |          -         |            ✅           |
  | uksouth       |          -         |          -         |            -            |      -      |          ✅          |          ✅          |            -            |        -        |          ✅         |          ✅         |          ✅         |            ✅           |
  | westus        |          ✅         |          ✅         |            ✅            |      -      |          ✅          |          -          |            ✅            |        -        |          -         |          ✅         |          ✅         |            -           |
  | westus3       |          ✅         |          ✅         |            ✅            |      -      |          ✅          |          -          |            ✅            |        -        |          -         |          -         |          ✅         |            -           |

  ## Model retirement

  For the latest information on model retirements, refer to the [model retirement guide](../../openai/concepts/model-retirements).

  ## Related content

  * [Foundry Models from partners and community](models-from-partners)
  * [Model retirement and deprecation](../../openai/concepts/model-retirements)
  * [Learn more about working with Azure OpenAI models](../../openai/how-to/working-with-models)
  * [Learn more about Azure OpenAI](models-sold-directly-by-azure)
  * [Learn more about fine-tuning Azure OpenAI models](../../openai/how-to/fine-tuning)
</ZonePivot>

<ZonePivot pivot="azure-direct-others">
  <Callout type="note">
    Foundry Models sold directly by Azure also include all Azure OpenAI models. To learn about these models, switch to the [Azure OpenAI models](models-sold-directly-by-azure?pivots=azure-openai) collection at the top of this article.
  </Callout>

  ## Black Forest Labs models sold directly by Azure

  The Black Forest Labs (BFL) collection of image generation models includes FLUX.2 \[pro] for image generation and editing through both text and image prompts, FLUX.1 Kontext \[pro] for in-context generation and editing, and FLUX1.1 \[pro] for text-to-image generation.

  You can run these models through the BFL service provider API and through the [images/generations and images/edits endpoints](../../openai/reference-preview).

  <Callout type="note">
    See the [GitHub sample for image generation with FLUX models in Microsoft Foundry](https://github.com/microsoft-foundry/foundry-samples/blob/main/samples/python/black-forest-labs/flux/README.md) and its associated [notebook](https://github.com/microsoft-foundry/foundry-samples/blob/main/samples/python/black-forest-labs/flux/AIFoundry_ImageGeneration_FLUX.ipynb) that showcases how to create high-quality images from textual prompts.
  </Callout>

  | Model                | Type & API endpoint                                                                                                                                                                                                                                                                                                                                                                                    | Capabilities                                                                                                                                                                                                                                                                                                                                                                                                 | Deployment type (region availability) |
  | -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------- |
  | `FLUX.2-pro`         | **Image generation** - [BFL service provider API](https://docs.bfl.ai/flux_2/flux2_text_to_image): `<resource-name>/providers/blackforestlabs/v1/flux-2-pro`                                                                                                                                                                                                                                           | - **Input:** text and image (32,000 tokens and up to 8 imagesi) - **Output:** One Image - **Tool calling:** No - **Response formats:** Image (PNG and JPG) - **Key features:** Multi-reference support for up to 8 imagesii; more grounded in real-world knowledge; greater output flexibility; enhanced performance - **Additional parameters:** *(In provider-specific API only)* Supports all parameters. | - Global standard (all regions)       |
  | `FLUX.1-Kontext-pro` | **Image generation** - [Image API](../../openai/reference-preview): `https://<resource-name>/openai/deployments/{deployment-id}/images/generations` and `https://<resource-name>/openai/deployments/{deployment-id}/images/edits` - [BFL service provider API](https://docs.bfl.ai/kontext/kontext_text_to_image): `<resource-name>/providers/blackforestlabs/v1/flux-kontext-pro?api-version=preview` | - **Input:** text and image (5,000 tokens and 1 image) - **Output:** One Image - **Tool calling:** No - **Response formats:** Image (PNG and JPG) - **Key features:** Character consistency, advanced editing - **Additional parameters:** *(In provider-specific API only)* `seed`, `aspect ratio`, `input_image`, `prompt_unsampling`, `safety_tolerance`, `output_format`                                 | - Global standard (all regions)       |
  | `FLUX-1.1-pro`       | **Image generation** - [Image API](../../openai/reference-preview): `https://<resource-name>/openai/deployments/{deployment-id}/images/generations` - [BFL service provider API](https://docs.bfl.ai/flux_models/flux_1_1_pro): `<resource-name>/providers/blackforestlabs/v1/flux-pro-1.1?api-version=preview`                                                                                        | - **Input:** text (5,000 tokens and 1 image) - **Output:** One Image - **Tool calling:** No - **Response formats:** Image (PNG and JPG) - **Key features:** Fast inference speed, strong prompt adherence, competitive pricing, scalable generation - **Additional parameters:** *(In provider-specific API only)* `width`, `height`, `prompt_unsampling`, `seed`, `safety_tolerance`, `output_format`       | - Global standard (all regions)       |

  i,ii Support for **multiple reference images (up to eight)** is available for FLUX.2\[pro] by using the API, but *not* in the playground. See the following [Code samples for FLUX.2\[pro\]](#code-samples-for-flux2pro).

  #### Code samples for FLUX.2\[pro]

  **Image generation**

  * Input: Text
  * Output: One image

  ```sh
  curl -X POST https://<your-resource-name>.api.cognitive.microsoft.com/providers/blackforestlabs/v1/flux-2-pro?api-version=preview \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer {API_KEY}" \
    -d '{
        "model": "FLUX.2-pro",
        "prompt": "A photograph of a red fox in an autumn forest",
        "width": 1024,
        "height": 1024,
        "seed": 42,
        "safety_tolerance": 2,
        "output_format": "jpeg"
      }'
  ```

  **Image editing**

  * Input: Up to eight bit-64 encoded images
  * Output: One image

  ```sh
  curl -X POST https://<your-resource-name>.api.cognitive.microsoft.com/providers/blackforestlabs/v1/flux-2-pro?api-version=preview \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer {API_KEY}" \
    -d '{
        "model": "FLUX.2-pro",
        "prompt": "Apply a cinematic, moody lighting effect to all photos. Make them look like scenes from a sci-fi noir film",
        "output_format": "jpeg",
        "input_image" : "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDA.......",
        "input_image_2" : "iVBORw0KGgoAAAANSUhEUgAABAAAAAQACAIAAADwf........"
      }'
  ```

  See [this model collection in Microsoft Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=black+forest+labs/?cid=learnDocs).

  ## Cohere models sold directly by Azure

  The Cohere family of models includes various models optimized for different use cases, including chat completions, rerank/text classification, and embeddings. Cohere models are optimized for various use cases that include reasoning, summarization, and question answering.

  | Model                                                                                                                                    | Type                         | Capabilities                                                                                                                                                                                                          | Deployment type (region availability)             |
  | ---------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
  | [Cohere-rerank-v4.0-pro](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-pro/version/1/registry/azureml-cohere/?cid=learnDocs)   | text classification (rerank) | - **Input:** text - **Output:** text - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `zh-cn`, `ar`, `vi`, `hi`, `ru`, `id`, and `nl` - **Tool calling:** No - **Response formats:** JSON                | - Global standard (all regions) - Managed compute |
  | [Cohere-rerank-v4.0-fast](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-fast/version/2/registry/azureml-cohere/?cid=learnDocs) | text classification (rerank) | - **Input:** text - **Output:** text - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `zh-cn`, `ar`, `vi`, `hi`, `ru`, `id`, and `nl` - **Tool calling:** No - **Response formats:** JSON                | - Global standard (all regions) - Managed compute |
  | `Cohere-command-a`                                                                                                                       | chat-completion              | - **Input:** text (131,072 tokens) - **Output:** text (8,182 tokens) - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `ko`, `zh-cn`, and `ar` - **Tool calling:** Yes - **Response formats:** Text, JSON | - Global standard (all regions)                   |
  | `embed-v-4-0`                                                                                                                            | embeddings                   | - **Input:** text (512 tokens) and images (2MM pixels) - **Output:** Vector (256, 512, 1024, 1536 dim.) - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `ko`, `zh-cn`, and `ar`                         | - Global standard (all regions)                   |

  See [the Cohere model collection in the Foundry portal](https://ai.azure.com/explore/models?selectedCollection=Cohere/?cid=learnDocs,cohere).

  ## DeepSeek models sold directly by Azure

  The DeepSeek family of models includes several reasoning models, which excel at reasoning tasks by using a step-by-step training process, such as language, scientific reasoning, and coding tasks.

  | Model                    | Type                                                                     | Capabilities                                                                                                                                                     | Deployment type (region availability)                              |
  | ------------------------ | ------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
  | `DeepSeek-V3.2-Speciale` | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (128,000 tokens) - **Output:** text (128,000 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text, JSON  | - Global standard (all regions)                                    |
  | `DeepSeek-V3.2`          | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (128,000 tokens) - **Output:** text (128,000 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text, JSON  | - Global standard (all regions)                                    |
  | `DeepSeek-V3.1`          | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (131,072 tokens) - **Output:** text (131,072 tokens) - **Languages:** `en` and `zh` - **Tool calling:** Yes - **Response formats:** Text, JSON | - Global standard (all regions)                                    |
  | `DeepSeek-R1-0528`       | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (163,840 tokens) - **Output:** text (163,840 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text        | - Global standard (all regions) - Global provisioned (all regions) |
  | `DeepSeek-V3-0324`       | chat-completion                                                          | - **Input:** text (131,072 tokens) - **Output:** text (131,072 tokens) - **Languages:** `en` and `zh` - **Tool calling:** Yes - **Response formats:** Text, JSON | - Global standard (all regions) - Global provisioned (all regions) |
  | `DeepSeek-R1`            | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (163,840 tokens) - **Output:** text (163,840 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text        | - Global standard (all regions) - Global provisioned (all regions) |

  See [this model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=DeepSeek/?cid=learnDocs).

  ## Meta models sold directly by Azure

  Meta Llama models and tools are a collection of pretrained and fine-tuned generative AI text and image reasoning models. Meta models range in scale to include:

  * Small language models (SLMs) like 1B and 3B Base and Instruct models for on-device and edge inferencing
  * Mid-size large language models (LLMs) like 7B, 8B, and 70B Base and Instruct models
  * High-performance models like Meta Llama 3.1-405B Instruct for synthetic data generation and distillation use cases.

  | Model                                    | Type            | Capabilities                                                                                                                                                                                                            | Deployment type (region availability) |
  | ---------------------------------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
  | `Llama-4-Maverick-17B-128E-Instruct-FP8` | chat-completion | - **Input:** text and images (1M tokens) - **Output:** text (1M tokens) - **Languages:** `ar`, `en`, `fr`, `de`, `hi`, `id`, `it`, `pt`, `es`, `tl`, `th`, and `vi` - **Tool calling:** No - **Response formats:** Text | - Global standard (all regions)       |
  | `Llama-3.3-70B-Instruct`                 | chat-completion | - **Input:** text (128,000 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en`, `de`, `fr`, `it`, `pt`, `hi`, `es`, and `th` - **Tool calling:** No - **Response formats:** Text                            | - Global standard (all regions)       |

  See [this model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Meta/?cid=learnDocs). You can also find several Meta models available [from partners and community](models-from-partners#meta).

  ## Microsoft models sold directly by Azure

  Microsoft models include various model groups such as Model Router, MAI models, Phi models, healthcare AI models, and more. See [the Microsoft model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Microsoft/?cid=learnDocs). You can also find several Microsoft models available [from partners and community](models-from-partners#microsoft).

  | Model                                                                                                                         | Type                                                                     | Capabilities                                                                                                                                                                                                                                       | Deployment type (region availability)                                                           |
  | ----------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
  | [model-router](https://ai.azure.com/resource/models/model-router/version/2025-11-18/registry/azureml-routers/?cid=learnDocs)1 | chat-completion                                                          | More details in [Model router overview](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/model-router). - **Input:** text, image - **Output:** text (max output tokens varies2) **Context window:** 200,0003 - **Languages:** `en` | - Global standard (East US 2, Sweden Central) - Data Zone standard4 (East US 2, Sweden Central) |
  | `MAI-DS-R1`                                                                                                                   | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (163,840 tokens) - **Output:** text (163,840 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text                                                                                          | - Global standard (all regions)                                                                 |

  1 **Model router version** `2025-11-18`. Earlier versions (`2025-08-07` and `2025-05-19`) are also available.

  2 **Max output tokens** varies for underlying models in the model router. For example, 32,768 (`GPT-4.1 series`), 100,000 (`o4-mini`), 128,000 (`gpt-5 reasoning models`), and 16,384 (`gpt-5-chat`).

  3 Larger **context windows** are compatible with *some* of the underlying models of the Model Router. That means an API call with a larger context succeeds only if the prompt gets routed to one of such models. Otherwise, the call fails.

  4 Billing for **Data Zone Standard** model router deployments begins no earlier than November 1, 2025.

  ## Mistral models sold directly by Azure

  | Model                      | Type            | Capabilities                                                                                                                                                                                 | Deployment type (region availability)                            |
  | -------------------------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
  | `Mistral-Large-3`          | chat-completion | - **Input:** text, image - **Output:** text - **Languages:** `en`, `fr`, `de`, `es`, `it`, `pt`, `nl`, `zh`, `ja`, `ko`, and `ar` - **Tool calling:** Yes - **Response formats:** Text, JSON | - Global standard (West US 3)                                    |
  | `mistral-document-ai-2505` | Image-to-Text   | - **Input:** image or PDF pages (30 pages, max 30MB PDF file) - **Output:** text - **Languages:** `en` - **Tool calling:** no - **Response formats:** Text, JSON, Markdown                   | - Global standard (all regions) - Data zone standard (US and EU) |

  See [the Mistral model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Mistral+AI/?cid=learnDocs). You can also find several Mistral models available [from partners and community](models-from-partners#mistral-ai).

  ## Moonshot AI models sold directly by Azure

  Moonshot AI models include Kimi K2.5 and Kimi K2 Thinking. Kimi K2.5 is a multimodal reasoning model that accepts text and image input, while Kimi K2 Thinking is the latest, most capable version of open-source thinking model.

  | Model              | Type                                                                     | Capabilities                                                                                                                                                         | Deployment type (region availability) |
  | ------------------ | ------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
  | `Kimi-K2.5`        | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text and image (262,144 tokens) - **Output:** text (262,144 tokens) - **Languages:** `en` and `zh` - **Tool calling:** Yes - **Response formats:** Text | - Global standard (all regions)       |
  | `Kimi-K2-Thinking` | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (262,144 tokens) - **Output:** text (262,144 tokens) - **Languages:** `en` and `zh` - **Tool calling:** Yes - **Response formats:** Text           | - Global standard (all regions)       |

  See [this model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Moonshot+ai/?cid=learnDocs).

  ## xAI models sold directly by Azure

  xAI's Grok models in Foundry Models include a diverse set of reasoning and non-reasoning models designed for enterprise use cases such as data extraction, coding, text summarization, and agentic applications. [Registration is required for access to grok-code-fast-1](https://aka.ms/xai/grok-code-fast-1) and [grok-4](https://aka.ms/xai/grok-4).

  | Model                       | Type            | Capabilities                                                                                                                                             | Deployment type (region availability)                     |
  | --------------------------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
  | `grok-4`                    | chat-completion | - **Input:** text (262,000 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text          | - Global standard (all regions)                           |
  | `grok-4-fast-reasoning`     | chat-completion | - **Input:** text, image (128,000 tokens) - **Output:** text (128,000 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text | - Global standard (all regions) - Data zone standard (US) |
  | `grok-4-fast-non-reasoning` | chat-completion | - **Input:** text, image (128,000 tokens) - **Output:** text (128,000 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text | - Global standard (all regions) - Data zone standard (US) |
  | `grok-code-fast-1`          | chat-completion | - **Input:** text (256,000 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text          | - Global standard (all regions)                           |
  | `grok-3`                    | chat-completion | - **Input:** text (131,072 tokens) - **Output:** text (131,072 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text        | - Global standard (all regions) - Data zone standard (US) |
  | `grok-3-mini`               | chat-completion | - **Input:** text (131,072 tokens) - **Output:** text (131,072 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text        | - Global standard (all regions) - Data zone standard (US) |

  See [the xAI model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=xAI/?cid=learnDocs).

  ## Model region availability by deployment type

  Foundry Models gives you choices for the hosting structure that fits your business and usage patterns. The service offers two main types of deployment:

  * **Standard**: Has a global deployment option, routing traffic globally to provide higher throughput.
  * **Provisioned**: Also has a global deployment option, allowing you to purchase and deploy provisioned throughput units across Azure global infrastructure.

  All deployments perform the same inference operations, but the billing, scale, and performance differ. For more information about deployment types, see [Deployment types in Foundry Models](deployment-types).

  <Tabs>
    <Tab title="Global Standard">
      ### Global Standard model availability

      | **Region**         | **DeepSeek-R1-0528** | **DeepSeek-R1** | **DeepSeek-V3-0324** | **DeepSeek-V3.1** | **FLUX.1-Kontext-pro** | **FLUX-1.1-pro** | **grok-4** | **grok-4-fast-reasoning** | **grok-4-fast-non-reasoning** | **grok-code-fast-1** | **grok-3** | **grok-3-mini** | **Llama-4-Maverick-17B-128E-Instruct-FP8** | **Llama-3.3-70B-Instruct** | **MAI-DS-R1** | **mistral-document-ai-2505** |
      | :----------------- | :------------------: | :-------------: | :------------------: | :---------------: | :--------------------: | :--------------: | :--------: | :-----------------------: | :---------------------------: | :------------------: | :--------: | :-------------: | :----------------------------------------: | :------------------------: | :-----------: | :--------------------------: |
      | australiaeast      |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | brazilsouth        |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | canadaeast         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | eastus             |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | eastus2            |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | francecentral      |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | germanywestcentral |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | italynorth         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | japaneast          |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | koreacentral       |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | northcentralus     |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | norwayeast         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | polandcentral      |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | southafricanorth   |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | southcentralus     |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | southindia         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | spaincentral       |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | swedencentral      |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | switzerlandnorth   |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | switzerlandwest    |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | uaenorth           |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | uksouth            |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | westeurope         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | westus             |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | westus3            |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
    </Tab>

    <Tab title="Global Provisioned managed">
      ### Global Provisioned managed model availability

      | **Region**         | **DeepSeek-R1-0528** | **DeepSeek-V3-0324** | **DeepSeek-R1** |
      | :----------------- | :------------------: | :------------------: | :-------------: |
      | australiaeast      |           ✅          |           ✅          |        ✅        |
      | brazilsouth        |           ✅          |           ✅          |        ✅        |
      | canadaeast         |           ✅          |           ✅          |        ✅        |
      | eastus             |           ✅          |           ✅          |        ✅        |
      | eastus2            |           ✅          |           ✅          |        ✅        |
      | francecentral      |           ✅          |           ✅          |        ✅        |
      | germanywestcentral |           ✅          |           ✅          |        ✅        |
      | italynorth         |           ✅          |           ✅          |        ✅        |
      | japaneast          |           ✅          |           ✅          |        ✅        |
      | koreacentral       |           ✅          |           ✅          |        ✅        |
      | northcentralus     |           ✅          |           ✅          |        ✅        |
      | norwayeast         |           ✅          |           ✅          |        ✅        |
      | polandcentral      |           ✅          |           ✅          |        ✅        |
      | southafricanorth   |           ✅          |           ✅          |        ✅        |
      | southcentralus     |           ✅          |           ✅          |        ✅        |
      | southindia         |           ✅          |           ✅          |        ✅        |
      | spaincentral       |           ✅          |           ✅          |        ✅        |
      | swedencentral      |           ✅          |           ✅          |        ✅        |
      | switzerlandnorth   |           ✅          |           ✅          |        ✅        |
      | switzerlandwest    |           ✅          |           ✅          |        ✅        |
      | uaenorth           |           ✅          |           ✅          |        ✅        |
      | uksouth            |           ✅          |           ✅          |        ✅        |
      | westeurope         |           ✅          |           ✅          |        ✅        |
      | westus             |           ✅          |           ✅          |        ✅        |
      | westus3            |           ✅          |           ✅          |        ✅        |
    </Tab>

    <Tab title="Data Zone Standard">
      ### Data Zone Standard model availability

      | **Region**         | **mistral-document-ai-2505** | **grok-4-fast-reasoning** | **grok-4-fast-non-reasoning** | **grok-3** | **grok-3-mini** |
      | :----------------- | :--------------------------: | :-----------------------: | :---------------------------: | :--------: | :-------------: |
      | eastus             |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | eastus2            |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | francecentral      |               ✅              |             -             |               -               |      -     |        -        |
      | germanywestcentral |               ✅              |             -             |               -               |      -     |        -        |
      | italynorth         |               ✅              |             -             |               -               |      -     |        -        |
      | northcentralus     |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | norwayeast         |               ✅              |             -             |               -               |      -     |        -        |
      | polandcentral      |               ✅              |             -             |               -               |      -     |        -        |
      | southcentralus     |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | spaincentral       |               ✅              |             -             |               -               |      -     |        -        |
      | swedencentral      |               ✅              |             -             |               -               |      -     |        -        |
      | switzerlandnorth   |               ✅              |             -             |               -               |      -     |        -        |
      | switzerlandwest    |               ✅              |             -             |               -               |      -     |        -        |
      | westeurope         |               ✅              |             -             |               -               |      -     |        -        |
      | westus             |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | westus3            |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
    </Tab>
  </Tabs>

  ## Related content

  * [Foundry Models from partners and community](models-from-partners)
  * [Model deprecation and retirement for Foundry Models](../../concepts/model-lifecycle-retirement)
  * [Deployment overview for Foundry Models](../../concepts/deployments-overview)
  * [Add and configure models to Foundry Models](../how-to/create-model-deployments)
  * [Deployment types in Foundry Models](deployment-types)
</ZonePivot>

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

This article lists a selection of Microsoft Foundry Models sold directly by Azure along with their capabilities, [deployment types, and regions of availability](deployment-types), excluding [deprecated and legacy models](../../concepts/model-lifecycle-retirement#deprecated). To see a list of Azure OpenAI models that are supported by the Foundry Agent Service, see [Models supported by Agent Service](../../agents/concepts/model-region-support).

Models sold directly by Azure include all Azure OpenAI models and specific, selected models from top providers. These models are billed through your Azure subscription, covered by Azure service-level agreements, and supported by Microsoft. For models offered by partners outside of this list, see [Foundry Models from partners and community](models-from-partners).

Use the tabs at the top of this page to switch between [Azure OpenAI models](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai) and [other model collections](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-direct-others) from providers like Cohere, DeepSeek, Meta, Mistral AI, and xAI.

Foundry Models are available for standard deployment to a Foundry resource.

To learn more about attributes of Foundry Models sold directly by Azure, see [Explore Foundry Models](../../concepts/foundry-models-overview#models-sold-directly-by-azure).

<ZonePivot pivot="azure-openai">
  <Callout type="note">
    Foundry Models sold directly by Azure also include select models from top model providers, such as:

    * Black Forest Labs: `FLUX.2-pro`, `FLUX.1-Kontext-pro`, `FLUX-1.1-pro`
    * Cohere: `Cohere-command-a`, `embed-v-4-0`, `Cohere-rerank-v4.0-pro`, `Cohere-rerank-v4.0-fast`
    * DeepSeek: `DeepSeek-V3.2`, `DeepSeek-V3.2-Speciale`, `DeepSeek-V3.1`, `DeepSeek-V3-0324`, `DeepSeek-R1-0528`, `DeepSeek-R1`
    * Moonshot AI: `Kimi-K2.5`, `Kimi-K2-Thinking`
    * Meta: `Llama-4-Maverick-17B-128E-Instruct-FP8`, `Llama-3.3-70B-Instruct`
    * Microsoft: `MAI-DS-R1`, `model-router`
    * Mistral: `mistral-document-ai-2505`, `Mistral-Large-3`
    * xAI: `grok-code-fast-1`, `grok-3`, `grok-3-mini`, `grok-4-fast-reasoning`, `grok-4-fast-non-reasoning`, `grok-4`

    To learn about these models, switch to [Other model collections](models-sold-directly-by-azure?pivots=azure-direct-others) at the top of this article.
  </Callout>

  ## Azure OpenAI in Microsoft Foundry models

  Azure OpenAI is powered by a diverse set of models with different capabilities and price points. Model availability varies by region and cloud. For Azure Government model availability, refer to [Azure OpenAI in Azure Government](../../openai/azure-government).

  | Models                                                                                                                                                                                                                                | Description                                                                                                                                                                                     |
  | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | [GPT-5.2 series](models-sold-directly-by-azure#gpt-52)                                                                                                                                                                                | **NEW** `gpt-5.2-codex`, `gpt-5.2`, `gpt-5.2-chat` (**Preview**)                                                                                                                                |
  | [GPT-5.1 series](models-sold-directly-by-azure#gpt-51)                                                                                                                                                                                | **NEW** `gpt-5.1`, `gpt-5.1-chat`, `gpt-5.1-codex`, `gpt-5.1-codex-mini`                                                                                                                        |
  | [Sora](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure?pivots=azure-openai\&tabs=global-standard-aoai%2Cstandard-chat-completions%2Cglobal-standard#video-generation-models) | **NEW** sora-2                                                                                                                                                                                  |
  | [GPT-5 series](models-sold-directly-by-azure#gpt-5)                                                                                                                                                                                   | gpt-5, gpt-5-mini, gpt-5-nano, gpt-5-chat                                                                                                                                                       |
  | [gpt-oss](models-sold-directly-by-azure#gpt-oss)                                                                                                                                                                                      | open-weight reasoning models                                                                                                                                                                    |
  | [codex-mini](models-sold-directly-by-azure#o-series-models)                                                                                                                                                                           | Fine-tuned version of o4-mini.                                                                                                                                                                  |
  | [GPT-4.1 series](models-sold-directly-by-azure#gpt-41-series)                                                                                                                                                                         | gpt-4.1, gpt-4.1-mini, gpt-4.1-nano                                                                                                                                                             |
  | [computer-use-preview](models-sold-directly-by-azure#computer-use-preview)                                                                                                                                                            | An experimental model trained for use with the Responses API computer use tool.                                                                                                                 |
  | [o-series models](models-sold-directly-by-azure#o-series-models)                                                                                                                                                                      | [Reasoning models](../../openai/how-to/reasoning) with advanced problem solving and increased focus and capability.                                                                             |
  | [GPT-4o, GPT-4o mini, and GPT-4 Turbo](models-sold-directly-by-azure#gpt-4o-and-gpt-4-turbo)                                                                                                                                          | Capable Azure OpenAI models with multimodal versions, which can accept both text and images as input.                                                                                           |
  | [Embeddings](models-sold-directly-by-azure#embeddings)                                                                                                                                                                                | A set of models that can convert text into numerical vector form to facilitate text similarity.                                                                                                 |
  | [Image generation](models-sold-directly-by-azure#image-generation-models)                                                                                                                                                             | A series of models that can generate original images from natural language.                                                                                                                     |
  | [`Video generation`](models-sold-directly-by-azure#video-generation-models)                                                                                                                                                           | A model that can generate original video scenes from text instructions.                                                                                                                         |
  | [Audio](models-sold-directly-by-azure#audio-models)                                                                                                                                                                                   | A series of models for speech to text, translation, and text to speech. GPT-4o audio models support either low latency *speech in, speech out* conversational interactions or audio generation. |

  ## GPT-5.2

  ### Region availability

  | Model           | Region                                                                |
  | --------------- | --------------------------------------------------------------------- |
  | `gpt-5.2`       | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.2-chat`  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.2-codex` | East US2 & Sweden Central (Global Standard)                           |

  * **[Registration is required for access to gpt-5.2 and gpt-5.2-codex](https://aka.ms/oai/gpt5access).**

  Access will be granted based on Microsoft's eligibility criteria. Customers who previously applied and received access to a limited access model, don't need to reapply as their approved subscriptions will automatically be granted access upon model release.

  | Model ID                                | Description                                                                                                                                                                                                                                                                                                                                                                | Context Window                         | Max Output Tokens | Training Data (up to) |
  | --------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------- | :---------------- | :-------------------: |
  | `gpt-5.2-codex` (2026-01-14)            | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning). - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |                       |
  | `gpt-5.2` (2025-12-11)                  | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                                  | 400,000 Input: 272,000 Output: 128,000 | 128,000           |      August 2025      |
  | `gpt-5.2-chat` (2025-12-11) **Preview** | - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs - Functions, tools, and parallel tool calling.                                                                                                                                                                                                                              | 128,000 Input: 111,616 Output: 16,384  | 16,384            |      August 2025      |

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  ## GPT-5.1

  ### Region availability

  | Model                | Region                                                                |
  | -------------------- | --------------------------------------------------------------------- |
  | `gpt-5.1`            | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.1-chat`       | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.1-codex`      | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.1-codex-mini` | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5.1-codex-max`  | See the [models table](#model-summary-table-and-region-availability). |

  * **[Registration is required for access to gpt-5.1, gpt-5.1-codex, and gpt-5.1-codex-max](https://aka.ms/oai/gpt5access).**

  Access will be granted based on Microsoft's eligibility criteria. Customers who previously applied and received access to a limited access model, don't need to reapply as their approved subscriptions will automatically be granted access upon model release.

  | Model ID                                | Description                                                                                                                                                                                                                                                                                              | Context Window                         | Max Output Tokens | Training Data (up to) |
  | --------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------- | :---------------- | :-------------------: |
  | `gpt-5.1` (2025-11-13)                  | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |
  | `gpt-5.1-chat` (2025-11-13) **Preview** | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs - Functions, tools, and parallel tool calling.                                                                                                               | 128,000 Input: 111,616 Output: 16,384  | 16,384            |   September 30, 2024  |
  | `gpt-5.1-codex` (2025-11-13)            | - [Responses API](../../openai/how-to/responses) only. - Text and image processing - Structured outputs. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning) - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |
  | `gpt-5.1-codex-mini` (2025-11-13)       | - [Responses API](../../openai/how-to/responses) only. - Text and image processing - Structured outputs. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning) - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |
  | `gpt-5.1-codex-max` (2025-12-04)        | - [Responses API](../../openai/how-to/responses) only. - Text and image processing - Structured outputs. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning) - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  <Callout type="important">
    * `gpt-5.1` `reasoning_effort` defaults to `none`. When upgrading from previous reasoning models to `gpt-5.1`, keep in mind that you may need to update your code to explicitly pass a `reasoning_effort` level if you want reasoning to occur.

    * `gpt-5.1-chat` adds built-in reasoning capabilities. Like other [reasoning models](../../openai/how-to/reasoning) it does not support parameters like `temperature`. If you upgrade from using `gpt-5-chat` (which is not a reasoning model) to `gpt-5.1-chat` make sure you remove any custom parameters like `temperature` from your code which are not supported by reasoning models.

    * `gpt-5.1-codex-max` adds support for setting `reasoning_effort` to `xhigh`. Reasoning effort `none` is not supported with `gpt-5.1-codex-max`.
  </Callout>

  ## GPT-5

  ### Region availability

  | Model                      | Region                                                                |
  | -------------------------- | --------------------------------------------------------------------- |
  | `gpt-5` (2025-08-07)       | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-mini` (2025-08-07)  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-nano` (2025-08-07)  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-chat` (2025-08-07)  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-chat` (2025-10-03)  | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-codex` (2025-09-11) | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-5-pro` (2025-10-06)   | See the [models table](#model-summary-table-and-region-availability). |

  * **[Registration is required for access to the gpt-5-pro, gpt-5, & gpt-5-codex models](https://aka.ms/oai/gpt5access).**

  * `gpt-5-mini`, `gpt-5-nano`, and `gpt-5-chat` do not require registration.

  Access will be granted based on Microsoft's eligibility criteria. Customers who previously applied and received access to `o3`, don't need to reapply as their approved subscriptions will automatically be granted access upon model release.

  | Model ID                               | Description                                                                                                                                                                                                                                                                                                                                               | Context Window                         | Max Output Tokens | Training Data (up to) |
  | -------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------- | :---------------- | :-------------------: |
  | `gpt-5` (2025-08-07)                   | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                 | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |
  | `gpt-5-mini` (2025-08-07)              | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                 | 400,000 Input: 272,000 Output: 128,000 | 128,000           |      May 31, 2024     |
  | `gpt-5-nano` (2025-08-07)              | - [Reasoning](../../openai/how-to/reasoning) - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                 | 400,000 Input: 272,000 Output: 128,000 | 128,000           |      May 31, 2024     |
  | `gpt-5-chat` (2025-08-07) **Preview**  | - Chat Completions API. - [Responses API](../../openai/how-to/responses). - **Input**: Text/Image - **Output**: Text only                                                                                                                                                                                                                                 | 128,000                                | 16,384            |   September 30, 2024  |
  | `gpt-5-chat` (2025-10-03) **Preview**1 | - Chat Completions API. - [Responses API](../../openai/how-to/responses). - **Input**: Text/Image - **Output**: Text only                                                                                                                                                                                                                                 | 128,000                                | 16,384            |   September 30, 2024  |
  | `gpt-5-codex` (2025-09-11)             | - [Responses API](../../openai/how-to/responses) only. - **Input**: Text/Image - **Output**: Text only - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. - [Full summary of capabilities](../../openai/how-to/reasoning) - Optimized for [Codex CLI & Codex VS Code extension](../../openai/how-to/codex) | 400,000 Input: 272,000 Output: 128,000 | 128,000           |           -           |
  | `gpt-5-pro` (2025-10-06)               | - [Reasoning](../../openai/how-to/reasoning) - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions and tools - [Full summary of capabilities](../../openai/how-to/reasoning).                                                                                                                  | 400,000 Input: 272,000 Output: 128,000 | 128,000           |   September 30, 2024  |

  <Callout type="note">
    1 `gpt-5-chat` version `2025-10-03` introduces a significant enhancement focused on emotional intelligence and mental health capabilities. This upgrade integrates specialized datasets and refined response strategies to improve the model's ability to:

    * **Understand and interpret emotional context** more accurately, enabling nuanced and empathetic interactions.
    * **Provide supportive, responsible responses** in conversations related to mental health, ensuring sensitivity and adherence to best practices.

    These improvements aim to make GPT-5-chat more context-aware, human-centric, and reliable in scenarios where emotional tone and well-being considerations are critical.
  </Callout>

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  ## gpt-oss

  ### Region availability

  | Model          | Region                   |
  | -------------- | ------------------------ |
  | `gpt-oss-120b` | All Azure OpenAI regions |

  ### Capabilities

  | Model ID                 | Description                                                                                                                                                                                                                                                                         | Context Window | Max Output Tokens | Training Data (up to) |
  | ------------------------ | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------- | :---------------- | :-------------------: |
  | `gpt-oss-120b` (Preview) | - Text in/text out only - Chat Completions API - Streaming - Function calling - Structured outputs - Reasoning - Available for deployment1 and via [managed compute](../../how-to/deploy-models-managed)                                                                            | 131,072        | 131,072           |      May 31, 2024     |
  | `gpt-oss-20b` (Preview)  | - Text in/text out only - Chat Completions API - Streaming - Function calling - Structured outputs - Reasoning - Available via [managed compute](../../how-to/deploy-models-managed) and [Foundry Local](../../foundry-local/get-started#optional-run-the-latest-gpt-oss-20b-model) | 131,072        | 131,072           |      May 31, 2024     |

  1 Unlike other Azure OpenAI models `gpt-oss-120b` requires a [Foundry project](https://learn.microsoft.com/en-us/azure/ai-foundry/quickstarts/get-started-code?tabs=azure-ai-foundry) to deploy the model.

  ### Deploy with code

  ```cli
  az cognitiveservices account deployment create \
    --name "Foundry-project-resource" \
    --resource-group "test-rg" \
    --deployment-name "gpt-oss-120b" \
    --model-name "gpt-oss-120b" \
    --model-version "1" \
    --model-format "OpenAI-OSS" \
    --sku-capacity 10 \
    --sku-name "GlobalStandard"
  ```

  ## GPT-4.1 series

  ### Region availability

  | Model                       | Region                                                                |
  | --------------------------- | --------------------------------------------------------------------- |
  | `gpt-4.1` (2025-04-14)      | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-4.1-nano` (2025-04-14) | See the [models table](#model-summary-table-and-region-availability). |
  | `gpt-4.1-mini` (2025-04-14) | See the [models table](#model-summary-table-and-region-availability). |

  ### Capabilities

  <Callout type="important">
    A known issue is affecting all GPT 4.1 series models. Large tool or function call definitions that exceed 300,000 tokens will result in failures, even though the 1 million token context limit of the models wasn't reached.

    The errors can vary based on API call and underlying payload characteristics.

    Here are the error messages for the Chat Completions API:

    * `Error code: 400 - {'error': {'message': "This model's maximum context length is 300000 tokens. However, your messages resulted in 350564 tokens (100 in the messages, 350464 in the functions). Please reduce the length of the messages or functions.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}`

    * `Error code: 400 - {'error': {'message': "Invalid 'tools[0].function.description': string too long. Expected a string with maximum length 1048576, but got a string with length 2778531 instead.", 'type': 'invalid_request_error', 'param': 'tools[0].function.description', 'code': 'string_above_max_length'}}`

    Here's the error message for the Responses API:

    * `Error code: 500 - {'error': {'message': 'The server had an error processing your request. Sorry about that! You can retry your request, or contact us through an Azure support request at: https://go.microsoft.com/fwlink/?linkid=2213926 if you keep seeing this error. (Please include the request ID d2008353-291d-428f-adc1-defb5d9fb109 in your email.)', 'type': 'server_error', 'param': None, 'code': None}}`
  </Callout>

  | Model ID                    | Description                                                                                                                                        | Context window                                                                                   | Max output tokens | Training data (up to) |
  | --------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------- | :---------------- | :-------------------: |
  | `gpt-4.1` (2025-04-14)      | - Text and image input - Text output - Chat completions API - Responses API - Streaming - Function calling - Structured outputs (chat completions) | - 1,047,576 - 128,000 (standard & provisioned managed deployments) - 300,000 (batch deployments) | 32,768            |      May 31, 2024     |
  | `gpt-4.1-nano` (2025-04-14) | - Text and image input - Text output - Chat completions API - Responses API - Streaming - Function calling - Structured outputs (chat completions) | - 1,047,576 - 128,000 (standard & provisioned managed deployments) - 300,000 (batch deployments) | 32,768            |      May 31, 2024     |
  | `gpt-4.1-mini` (2025-04-14) | - Text and image input - Text output - Chat completions API - Responses API - Streaming - Function calling - Structured outputs (chat completions) | - 1,047,576 - 128,000 (standard & provisioned managed deployments) - 300,000 (batch deployments) | 32,768            |      May 31, 2024     |

  ## computer-use-preview

  An experimental model trained for use with the [Responses API](../../openai/how-to/responses) computer use tool.

  It can be used with third-party libraries to allow the model to control mouse and keyboard input, while getting context from screenshots of the current environment.

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  Registration is required to access `computer-use-preview`. Access is granted based on Microsoft's eligibility criteria. Customers who have access to other limited access models still need to request access for this model.

  To request access, go to [`computer-use-preview` limited access model application](https://aka.ms/oai/cuaaccess). When access is granted, you need to create a deployment for the model.

  ### Region availability

  | Model                  | Region                                                                |
  | ---------------------- | --------------------------------------------------------------------- |
  | `computer-use-preview` | See the [models table](#model-summary-table-and-region-availability). |

  ### Capabilities

  | Model ID                            | Description                                                                                                                                                   | Context window | Max output tokens | Training data (up to) |
  | ----------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------- | :---------------- | :-------------------: |
  | `computer-use-preview` (2025-03-11) | Specialized model for use with the [Responses API](../../openai/how-to/responses) computer use tool - Tools - Streaming - Text (input/output) - Image (input) | 8,192          | 1,024             |      October 2023     |

  ## o-series models

  The Azure OpenAI o-series models are designed to tackle reasoning and problem-solving tasks with increased focus and capability. These models spend more time processing and understanding the user's request, making them exceptionally strong in areas like science, coding, and math, compared to previous iterations.

  | Model ID                  | Description                                                                                                                                                                                                                                                                                                                                        | Max request (tokens)           | Training data (up to) |
  | ------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------- | :-------------------: |
  | `codex-mini` (2025-05-16) | Fine-tuned version of `o4-mini`. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions and tools. [Full summary of capabilities](../../openai/how-to/reasoning).                                                                                                                        | Input: 200,000 Output: 100,000 |      May 31, 2024     |
  | `o3-pro` (2025-06-10)     | - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions and tools. [Full summary of capabilities](../../openai/how-to/reasoning).                                                                                                                                                         | Input: 200,000 Output: 100,000 |      May 31, 2024     |
  | `o4-mini` (2025-04-16)    | - *New* reasoning model, offering [enhanced reasoning abilities](../../openai/how-to/reasoning). - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions and tools. [Full summary of capabilities](../../openai/how-to/reasoning).                                | Input: 200,000 Output: 100,000 |      May 31, 2024     |
  | `o3` (2025-04-16)         | - *New* reasoning model, offering [enhanced reasoning abilities](../../openai/how-to/reasoning). - Chat Completions API. - [Responses API](../../openai/how-to/responses). - Structured outputs. - Text and image processing. - Functions, tools, and parallel tool calling. [Full summary of capabilities](../../openai/how-to/reasoning).        | Input: 200,000 Output: 100,000 |      May 31, 2024     |
  | `o3-mini` (2025-01-31)    | - [Enhanced reasoning abilities](../../openai/how-to/reasoning). - Structured outputs. - Text-only processing. - Functions and tools.                                                                                                                                                                                                              | Input: 200,000 Output: 100,000 |      October 2023     |
  | `o1` (2024-12-17)         | - [Enhanced reasoning abilities](../../openai/how-to/reasoning). - Structured outputs. - Text and image processing. - Functions and tools.                                                                                                                                                                                                         | Input: 200,000 Output: 100,000 |      October 2023     |
  | `o1-preview` (2024-09-12) | Older preview version.                                                                                                                                                                                                                                                                                                                             | Input: 128,000 Output: 32,768  |      October 2023     |
  | `o1-mini` (2024-09-12)    | A faster and more cost-efficient option in the o1 series, ideal for coding tasks that require speed and lower resource consumption. - Global Standard deployment available by default. - Standard (regional) deployments are currently only available for select customers who received access as part of the `o1-preview` limited access release. | Input: 128,000 Output: 65,536  |      October 2023     |

  To learn more about advanced o-series models, see [Getting started with reasoning models](../../openai/how-to/reasoning).

  ### Region availability

  | Model        | Region                                                                                                                                                                           |
  | ------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `codex-mini` | East US2 & Sweden Central (Global Standard).                                                                                                                                     |
  | `o3-pro`     | East US2 & Sweden Central (Global Standard).                                                                                                                                     |
  | `o4-mini`    | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |
  | `o3`         | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |
  | `o3-mini`    | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |
  | `o1`         | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |
  | `o1-preview` | See the [models table](#model-summary-table-and-region-availability). This model is available only for customers who were granted access as part of the original limited access. |
  | `o1-mini`    | See the [models table](#model-summary-table-and-region-availability).                                                                                                            |

  ## GPT-4o and GPT-4 Turbo

  GPT-4o integrates text and images in a single model, which enables it to handle multiple data types simultaneously. This multimodal approach enhances accuracy and responsiveness in human-computer interactions. GPT-4o matches GPT-4 Turbo in English text and coding tasks while offering superior performance in non-English language tasks and vision tasks, setting new benchmarks for AI capabilities.

  ## GPT-4 and GPT-4 Turbo models

  These models can be used only with the Chat Completions API.

  See [Model versions](../../openai/concepts/model-versions) to learn about how Azure OpenAI handles model version upgrades. See [Working with models](../../openai/how-to/working-with-models) to learn how to view and configure the model version settings of your GPT-4 deployments.

  | Model ID                                           | Description                                                                                                                                                                                                                                                                                                                     | Max request (tokens)          | Training data (up to) |
  | -------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :---------------------------- | :-------------------: |
  | `gpt-4o` (2024-11-20) GPT-4o (Omni)                | - Structured outputs. - Text and image processing. - JSON Mode. - Parallel function calling. - Enhanced accuracy and responsiveness. - Parity with English text and coding tasks compared to GPT-4 Turbo with Vision. - Superior performance in non-English languages and in vision tasks. - Enhanced creative writing ability. | Input: 128,000 Output: 16,384 |      October 2023     |
  | `gpt-4o` (2024-08-06) GPT-4o (Omni)                | - Structured outputs. - Text and image processing. - JSON Mode. - Parallel function calling. - Enhanced accuracy and responsiveness. - Parity with English text and coding tasks compared to GPT-4 Turbo with Vision. - Superior performance in non-English languages and in vision tasks.                                      | Input: 128,000 Output: 16,384 |      October 2023     |
  | `gpt-4o-mini` (2024-07-18) GPT-4o mini             | - Fast, inexpensive, capable model ideal for replacing GPT-3.5 Turbo series models. - Text and image processing. - JSON Mode. - Parallel function calling.                                                                                                                                                                      | Input: 128,000 Output: 16,384 |      October 2023     |
  | `gpt-4o` (2024-05-13) GPT-4o (Omni)                | - Text and image processing. - JSON Mode. - Parallel function calling. - Enhanced accuracy and responsiveness. - Parity with English text and coding tasks compared to GPT-4 Turbo with Vision. - Superior performance in non-English languages and in vision tasks.                                                            | Input: 128,000 Output: 4,096  |      October 2023     |
  | `gpt-4` (turbo-2024-04-09) GPT-4 Turbo with Vision | New generally available model. - Replacement for all previous GPT-4 preview models (`vision-preview`, `1106-Preview`, `0125-Preview`). - [Feature availability](#gpt-4o-and-gpt-4-turbo) is currently different, depending on the method of input and the deployment type.                                                      | Input: 128,000 Output: 4,096  |     December 2023     |

  <Callout type="caution">
    We don't recommend that you use preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  ## Embeddings

  `text-embedding-3-large` is the latest and most capable embedding model. You can't upgrade between embeddings models. To move from using `text-embedding-ada-002` to `text-embedding-3-large`, you need to generate new embeddings.

  * `text-embedding-3-large`
  * `text-embedding-3-small`
  * `text-embedding-ada-002`

  OpenAI reports that testing shows that both the large and small third generation embeddings models offer better average multi-language retrieval performance with the [MIRACL](https://github.com/project-miracl/miracl) benchmark. They still maintain performance for English tasks with the [MTEB](https://github.com/embeddings-benchmark/mteb) benchmark.

  | Evaluation benchmark | `text-embedding-ada-002` | `text-embedding-3-small` | `text-embedding-3-large` |
  | -------------------- | ------------------------ | ------------------------ | ------------------------ |
  | MIRACL average       | 31.4                     | 44.0                     | 54.9                     |
  | MTEB average         | 61.0                     | 62.3                     | 64.6                     |

  The third generation embeddings models support reducing the size of the embedding via a new `dimensions` parameter. Typically, larger embeddings are more expensive from a compute, memory, and storage perspective. When you can adjust the number of dimensions, you gain more control over overall cost and performance. The `dimensions` parameter isn't supported in all versions of the OpenAI 1.x Python library. To take advantage of this parameter, we recommend that you upgrade to the latest version: `pip install openai --upgrade`.

  OpenAI's MTEB benchmark testing found that even when the third generation model's dimensions are reduced to less than the 1,536 dimensions of `text-embeddings-ada-002`, performance remains slightly better.

  ## Image generation models

  The image generation models generate images from text prompts that the user provides. GPT-image-1 series models are in limited access preview. DALL-E 3 is generally available for use with the REST APIs. DALL-E 2 and DALL-E 3 with client SDKs are in preview.

  Registration is required to access `gpt-image-1`, `gpt-image-1-mini`, or `gpt-image-1.5`. Access is granted based on Microsoft's eligibility criteria. Customers who have access to other limited access models still need to request access for this model.

  To request access, fill out an application form: [Apply for GPT-image-1 access](https://aka.ms/oai/gptimage1access); [Apply for GPT-image-1.5 access](https://aka.ms/oai/gptimage1.5access). When access is granted, you need to create a deployment for the model.

  ### Region availability

  | Model              | Region                                                                                                                                                |
  | ------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------- |
  | `dall-e-3`         | East US Australia East Sweden Central                                                                                                                 |
  | `gpt-image-1`      | West US 3 (Global Standard) East US 2 (Global Standard) UAE North (Global Standard) Poland Central (Global Standard) Sweden Central (Global Standard) |
  | `gpt-image-1-mini` | West US 3 (Global Standard) East US 2 (Global Standard) UAE North (Global Standard) Poland Central (Global Standard) Sweden Central (Global Standard) |
  | `gpt-image-1.5`    | West US 3 (Global Standard) East US 2 (Global Standard) UAE North (Global Standard) Poland Central (Global Standard) Sweden Central (Global Standard) |

  ## Video generation models

  Sora is an AI model from OpenAI that can create realistic and imaginative video scenes from text instructions. Sora is in preview.

  ### Region availability

  | Model    | Region                                                       |
  | -------- | ------------------------------------------------------------ |
  | `sora`   | East US 2 (Global Standard) Sweden Central (Global Standard) |
  | `sora-2` | East US 2 (Global Standard) Sweden Central (Global Standard) |

  ## Audio models

  Audio models in Azure OpenAI are available via the `realtime`, `completions`, and `audio` APIs.

  ### GPT-4o audio models

  The GPT-4o audio models are part of the GPT-4o model family and support either low-latency, *speech in, speech out* conversational interactions or audio generation.

  <Callout type="caution">
    We don't recommend using preview models in production. We'll upgrade all deployments of preview models to either future preview versions or to the latest stable, generally available version. Models that are designated preview don't follow the standard Azure OpenAI model lifecycle.
  </Callout>

  Details about maximum request tokens and training data are available in the following table:

  | Model ID                                                                                                                                                           | Description                                 | Max request (tokens)          | Training data (up to) |
  | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------- | ----------------------------- | --------------------- |
  | `gpt-4o-mini-audio-preview` (2024-12-17) GPT-4o audio                                                                                                              | Audio model for audio and text generation.  | Input: 128,000 Output: 16,384 | September 2023        |
  | `gpt-4o-audio-preview` (2024-12-17) GPT-4o audio                                                                                                                   | Audio model for audio and text generation.  | Input: 128,000 Output: 16,384 | September 2023        |
  | `gpt-4o-realtime-preview` (2025-06-03) GPT-4o audio                                                                                                                | Audio model for real-time audio processing. | Input: 128,000 Output: 4,096  | October 2023          |
  | `gpt-4o-realtime-preview` (2024-12-17) GPT-4o audio                                                                                                                | Audio model for real-time audio processing. | Input: 128,000 Output: 4,096  | October 2023          |
  | `gpt-4o-mini-realtime-preview` (2024-12-17) GPT-4o audio                                                                                                           | Audio model for real-time audio processing. | Input: 128,000 Output: 4,096  | October 2023          |
  | `gpt-realtime` (2025-08-28) (GA) `gpt-realtime-mini` (2025-10-06) `gpt-realtime-mini-2025-12-15` (2025-12-15) `gpt-audio`(2025-08-28) `gpt-audio-mini`(2025-10-06) | Audio model for real-time audio processing. | Input: 28,672 Output: 4,096   | October 2023          |

  To compare the availability of GPT-4o audio models across all regions, refer to the [models table](#global-standard-model-availability).

  ### Audio API

  The audio models via the `/audio` API can be used for speech to text, translation, and text to speech.

  #### Speech-to-text models

  | Model ID                            | Description                                                                                             | Max request (audio file size) |
  | ----------------------------------- | ------------------------------------------------------------------------------------------------------- | ----------------------------- |
  | `whisper`                           | General-purpose speech recognition model.                                                               | 25 MB                         |
  | `gpt-4o-transcribe`                 | Speech-to-text model powered by GPT-4o.                                                                 | 25 MB                         |
  | `gpt-4o-mini-transcribe`            | Speech-to-text model powered by GPT-4o mini.                                                            | 25 MB                         |
  | `gpt-4o-transcribe-diarize`         | Speech-to-text model with automatic speech recognition.                                                 | 25 MB                         |
  | `gpt-4o-mini-transcribe-2025-12-15` | Speech-to-text model with automatic speech recognition. Improved transcription accuracy and robustness. | 25 MB                         |

  #### Speech translation models

  | Model ID  | Description                               | Max request (audio file size) |
  | --------- | ----------------------------------------- | ----------------------------- |
  | `whisper` | General-purpose speech recognition model. | 25 MB                         |

  #### Text-to-speech models (preview)

  | Model ID                     | Description                                                                                                |
  | ---------------------------- | :--------------------------------------------------------------------------------------------------------- |
  | `tts`                        | Text-to-speech model optimized for speed.                                                                  |
  | `tts-hd`                     | Text-to-speech model optimized for quality.                                                                |
  | `gpt-4o-mini-tts`            | Text-to-speech model powered by GPT-4o mini. You can guide the voice to speak in a specific style or tone. |
  | `gpt-4o-mini-tts-2025-12-15` | Text-to-speech model powered by GPT-4o mini. You can guide the voice to speak in a specific style or tone. |

  ## Model summary table and region availability

  ### Models by deployment type

  Azure OpenAI provides customers with choices on the hosting structure that fits their business and usage patterns. The service offers two main types of deployment:

  * **Standard**: Has a global deployment option, routing traffic globally to provide higher throughput.
  * **Provisioned**: Also has a global deployment option, allowing customers to purchase and deploy provisioned throughput units across Azure global infrastructure.

  All deployments can perform the exact same inference operations, but the billing, scale, and performance are substantially different. To learn more about Azure OpenAI deployment types, see our [Deployment types guide](deployment-types).

  <Tabs>
    <Tab title="Global Standard">
      ### Global Standard model availability

      | **Region**         | **gpt-5.2-codex**, **2026-01-14** | **gpt-5.2**, **2025-12-11** | **gpt-5.2-chat**, **2025-12-11** | **gpt-5.1-codex-max**, **2025-12-04** | **gpt-5.1**, **2025-11-13** | **gpt-5.1-chat**, **2025-11-13** | **gpt-5.1-codex**, **2025-11-13** | **gpt-5.1-codex-mini**, **2025-11-13** | **gpt-5-pro**, **2025-10-06** | **gpt-5-codex**, **2025-09-15** | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **gpt-5-nano**, **2025-08-07** | **gpt-5-chat**, **2025-08-07** | **gpt-5-chat**, **2025-10-03** | **o3-pro**, **2025-06-10** | **codex-mini**, **2025-05-16** | **sora**, **2025-05-02** | **model-router**, **2025-08-07** | **model-router**, **2025-05-19** | **model-router**, **2025-11-18** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-image-1**, **2025-04-15** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **computer-use-preview**, **2025-03-11** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** | **text-embedding-3-small**, **1** | **text-embedding-3-large**, **1** | **text-embedding-ada-002**, **2** | **gpt-4o-realtime-preview**, **2024-12-17** | **gpt-4o-audio-preview**, **2024-12-17** | **gpt-4o-mini-realtime-preview**, **2024-12-17** | **gpt-4o-mini-audio-preview**, **2024-12-17** | **gpt-4o-transcribe**, **2025-03-20** | **gpt-4o-mini-tts**, **2025-12-15** | **gpt-4o-mini-tts**, **2025-03-20** | **gpt-4o-mini-transcribe**, **2025-12-15** | **gpt-4o-mini-transcribe**, **2025-03-20** | **gpt-image-1-mini**, **2025-10-06** | **gpt-audio-mini**, **2025-10-06** | **gpt-audio-mini**, **2025-12-15** | **gpt-image-1.5**, **2025-12-16** | **sora-2**, **2025-10-06** | **gpt-realtime-mini**, **2025-10-06** | **gpt-realtime-mini**, **2025-12-15** | **o3-deep-research**, **2025-06-26** | **gpt-realtime**, **2025-08-28** | **gpt-audio**, **2025-08-28** | **gpt-4o-transcribe-diarize**, **2025-10-15** |
      | :----------------- | :-------------------------------: | :-------------------------: | :------------------------------: | :-----------------------------------: | :-------------------------: | :------------------------------: | :-------------------------------: | :------------------------------------: | :---------------------------: | :-----------------------------: | :-----------------------: | :----------------------------: | :----------------------------: | :----------------------------: | :----------------------------: | :------------------------: | :----------------------------: | :----------------------: | :------------------------------: | :------------------------------: | :------------------------------: | :--------------------: | :-------------------------: | :-----------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :--------------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: | :-----------------------------------------: | :--------------------------------------: | :----------------------------------------------: | :-------------------------------------------: | :-----------------------------------: | :---------------------------------: | :---------------------------------: | :----------------------------------------: | :----------------------------------------: | :----------------------------------: | :--------------------------------: | :--------------------------------: | :-------------------------------: | :------------------------: | :-----------------------------------: | :-----------------------------------: | :----------------------------------: | :------------------------------: | :---------------------------: | :-------------------------------------------: |
      | australiaeast      |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | brazilsouth        |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | canadacentral      |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             -             |                -               |                -               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            -           |              -              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              -              |            -           |              -             |              ✅             |              -             |                ✅                |                 -                 |                 -                 |                 -                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | canadaeast         |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | centralus          |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               ✅               |                ✅                |             ✅             |                ✅               |                ✅               |                ✅               |                ✅               |              ✅             |                ✅               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 -                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  ✅                 |                  ✅                 |                 -                 |              -             |                   ✅                   |                   ✅                   |                   -                  |                 ✅                |               ✅               |                       -                       |
      | eastus             |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       ✅                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | eastus2            |                 ✅                 |              ✅              |                 ✅                |                   ✅                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               ✅               |                ✅                |             ✅             |                ✅               |                ✅               |                ✅               |                ✅               |              ✅             |                ✅               |             ✅            |                 ✅                |                 ✅                |                 ✅                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     ✅                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      ✅                      |                     ✅                    |                         ✅                        |                       ✅                       |                   ✅                   |                  ✅                  |                  ✅                  |                      ✅                     |                      ✅                     |                   ✅                  |                  ✅                 |                  ✅                 |                 ✅                 |              ✅             |                   ✅                   |                   ✅                   |                   -                  |                 ✅                |               ✅               |                       ✅                       |
      | francecentral      |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | germanywestcentral |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | italynorth         |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              -             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | japaneast          |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | koreacentral       |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | northcentralus     |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | norwayeast         |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   ✅                  |                 -                |               -               |                       -                       |
      | polandcentral      |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   ✅                  |                  -                 |                  -                 |                 ✅                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | southafricanorth   |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | southcentralus     |                 -                 |              ✅              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | southeastasia      |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              -              |            -           |              -             |              ✅             |              -             |                ✅                |                 -                 |                 -                 |                 -                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | southindia         |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     ✅                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | spaincentral       |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | swedencentral      |                 ✅                 |              ✅              |                 ✅                |                   ✅                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               ✅               |                ✅                |             ✅             |                ✅               |                ✅               |                ✅               |                ✅               |              ✅             |                ✅               |             ✅            |                 ✅                |                 ✅                |                 ✅                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     ✅                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      ✅                      |                     ✅                    |                         ✅                        |                       -                       |                   ✅                   |                  -                  |                  -                  |                      ✅                     |                      ✅                     |                   ✅                  |                  ✅                 |                  ✅                 |                 ✅                 |              ✅             |                   ✅                   |                   ✅                   |                   -                  |                 ✅                |               ✅               |                       ✅                       |
      | switzerlandnorth   |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | switzerlandwest    |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             -             |                -               |                -               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            -           |              -              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              -              |            -           |              -             |              ✅             |              -             |                ✅                |                 -                 |                 -                 |                 -                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | uaenorth           |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   ✅                  |                  -                 |                  -                 |                 ✅                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | uksouth            |                 -                 |              -              |                 -                |                   -                   |              ✅              |                 ✅                |                 ✅                 |                    ✅                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | westeurope         |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |
      | westus             |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                -                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   -                  |                  -                 |                  -                 |                 -                 |              -             |                   -                   |                   -                   |                   ✅                  |                 -                |               -               |                       -                       |
      | westus3            |                 -                 |              -              |                 -                |                   -                   |              -              |                 -                |                 -                 |                    -                   |               -               |                -                |             ✅             |                ✅               |                ✅               |                -               |                -               |              -             |                -               |             -            |                 -                |                 -                |                 -                |            ✅           |              ✅              |                ✅                |              ✅              |                 ✅                |                 ✅                |                     -                    |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                      -                      |                     -                    |                         -                        |                       -                       |                   -                   |                  -                  |                  -                  |                      -                     |                      -                     |                   ✅                  |                  -                 |                  -                 |                 ✅                 |              -             |                   -                   |                   -                   |                   -                  |                 -                |               -               |                       -                       |

      <Callout type="note">
        `o3-deep-research` is currently only available with Foundry Agent Service. To learn more, see the [Deep Research tool guidance](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/tools/deep-research).
      </Callout>
    </Tab>

    <Tab title="Global Provisioned managed">
      ### Global Provisioned managed model availability

      | **Region**         | **gpt-5.2**, **2025-12-11** | **gpt-5.1**, **2025-11-13** | **gpt-5.1-codex**, **2025-11-13** | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** |
      | :----------------- | :-------------------------: | :-------------------------: | :-------------------------------: | :-----------------------: | :----------------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: |
      | australiaeast      |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | brazilsouth        |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | canadacentral      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | canadaeast         |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | centralus          |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus             |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus2            |              ✅              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | francecentral      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | germanywestcentral |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | italynorth         |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | japaneast          |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | koreacentral       |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | northcentralus     |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | norwayeast         |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | polandcentral      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southafricanorth   |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southcentralus     |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southeastasia      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southindia         |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | spaincentral       |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | swedencentral      |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | switzerlandnorth   |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | switzerlandwest    |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | uaenorth           |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | uksouth            |              ✅              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westeurope         |              -              |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus             |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus3            |              -              |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
    </Tab>

    <Tab title="Global Batch">
      ### Global Batch model availability

      | **Region**         | **gpt-5.1**, **2025-11-13** | **gpt-5**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** |
      | :----------------- | :-------------------------: | :-----------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: |
      | australiaeast      |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | brazilsouth        |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | canadaeast         |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | centralus          |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus             |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus2            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | francecentral      |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | germanywestcentral |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | japaneast          |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | koreacentral       |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | northcentralus     |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | norwayeast         |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | polandcentral      |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | southafricanorth   |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | southcentralus     |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | southindia         |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | swedencentral      |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | switzerlandnorth   |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | uksouth            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | westeurope         |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus             |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus3            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |              ✅             |                ✅                |
    </Tab>

    <Tab title="Data Zone Standard">
      ### Data Zone Standard model availability

      | **Region**         | **gpt-5.2**, **2025-12-11** | **gpt-5.1**, **2025-11-13** | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **gpt-5-nano**, **2025-08-07** | **model-router**, **2025-08-07** | **model-router**, **2025-05-19** | **model-router**, **2025-11-18** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** | **text-embedding-3-small**, **1** | **text-embedding-3-large**, **1** | **text-embedding-ada-002**, **2** |
      | :----------------- | :-------------------------: | :-------------------------: | :-----------------------: | :----------------------------: | :----------------------------: | :------------------------------: | :------------------------------: | :------------------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: |
      | centralus          |              ✅              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | eastus             |              -              |              ✅              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | eastus2            |              ✅              |              ✅              |             ✅             |                ✅               |                ✅               |                 ✅                |                 ✅                |                 ✅                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | francecentral      |              -              |              ✅              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | germanywestcentral |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | italynorth         |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              -             |              -             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | northcentralus     |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | polandcentral      |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | southcentralus     |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | spaincentral       |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | swedencentral      |              -              |              ✅              |             ✅             |                ✅               |                ✅               |                 ✅                |                 ✅                |                 ✅                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | westeurope         |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | westus             |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |
      | westus3            |              -              |              -              |             ✅             |                ✅               |                ✅               |                 -                |                 -                |                 -                |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |

      <Callout type="note">
        `o1-mini` is currently available to all customers for Global Standard deployment.

        Select customers were granted standard (regional) deployment access to `o1-mini` as part of the `o1-preview` limited access release. At this time, access to `o1-mini` standard (regional) deployments isn't being expanded.
      </Callout>
    </Tab>

    <Tab title="Data Zone Provisioned managed">
      ### Data Zone Provisioned managed model availability

      | **Region**         | **gpt-5.1**, **2025-11-13** | **gpt-5.1-codex**, **2025-11-13** | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** |
      | :----------------- | :-------------------------: | :-------------------------------: | :-----------------------: | :----------------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: |
      | eastus             |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | eastus2            |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | francecentral      |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | germanywestcentral |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | italynorth         |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 -                |              -              |            -           |              -             |              -             |              -             |                -                |
      | northcentralus     |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | polandcentral      |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | southcentralus     |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | spaincentral       |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | swedencentral      |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westeurope         |              ✅              |                 ✅                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus             |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
      | westus3            |              ✅              |                 -                 |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |
    </Tab>

    <Tab title="Data Zone Batch">
      ### Data Zone Batch model availability

      | **Region**         | **gpt-5.1**, **2025-11-13** | **gpt-5**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** |
      | :----------------- | :-------------------------: | :-----------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :------------------------: | :------------------------: | :-----------------------------: |
      | centralus          |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | eastus             |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | eastus2            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | francecentral      |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | germanywestcentral |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | northcentralus     |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | polandcentral      |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | southcentralus     |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | swedencentral      |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | westeurope         |              -              |             -             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |              ✅             |              ✅             |                ✅                |
      | westus             |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
      | westus3            |              ✅              |             ✅             |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |              ✅             |              ✅             |                ✅                |
    </Tab>

    <Tab title="Standard">
      ### Standard deployment model availability

      | **Region**         | **sora**, **2025-05-02** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** | **text-embedding-3-small**, **1** | **text-embedding-3-large**, **1** | **text-embedding-ada-002**, **1** | **text-embedding-ada-002**, **2** | **dall-e-3**, **3.0** | **tts**, **001** | **tts-hd**, **001** | **whisper**, **001** |
      | :----------------- | :----------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: | :-------------------------------: | :-------------------: | :--------------: | :-----------------: | :------------------: |
      | australiaeast      |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           ✅           |         -        |          -          |           -          |
      | brazilsouth        |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 -                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | canadaeast         |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | centralus          |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              ✅             |              ✅             |                -                |                 -                 |                 -                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | eastus             |             -            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 ✅                 |                 ✅                 |           ✅           |         -        |          -          |           -          |
      | eastus2            |             ✅            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | francecentral      |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | germanywestcentral |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | japaneast          |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | koreacentral       |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | northcentralus     |             -            |              ✅              |              ✅              |                 ✅                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 -                 |                 -                 |                 -                 |                 ✅                 |           -           |         ✅        |          ✅          |           ✅          |
      | norwayeast         |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              ✅             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | polandcentral      |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | southafricanorth   |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | southcentralus     |             -            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 -                 |                 -                 |                 ✅                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | southeastasia      |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | southindia         |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | spaincentral       |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 ✅                 |                 -                 |                 -                 |           -           |         -        |          -          |           -          |
      | swedencentral      |             -            |              ✅              |              ✅              |                 ✅                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           ✅           |         ✅        |          ✅          |           ✅          |
      | switzerlandnorth   |             -            |              -              |              ✅              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | uaenorth           |             -            |              -              |              -              |                 -                |                 -                |            -           |              -             |              -             |              -             |                -                |                 ✅                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | uksouth            |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              ✅             |                -                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | westeurope         |             -            |              -              |              -              |                 -                |                 ✅                |            -           |              -             |              -             |              -             |                -                |                 -                 |                 -                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           ✅          |
      | westus             |             -            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 ✅                 |                 -                 |                 -                 |                 ✅                 |           -           |         -        |          -          |           -          |
      | westus3            |             -            |              ✅              |              ✅              |                 -                |                 ✅                |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |                 -                 |                 ✅                 |                 -                 |                 ✅                 |           -           |         ✅        |          ✅          |           -          |

      <Callout type="note">
        `o1-mini` is currently available to all customers for Global Standard deployment.

        Select customers were granted standard (regional) deployment access to `o1-mini` as part of the `o1-preview` limited access release. At this time, access to `o1-mini` standard (regional) deployments isn't being expanded.
      </Callout>
    </Tab>

    <Tab title="Provisioned managed">
      ### Provisioned deployment model availability

      | **Region**         | **gpt-5**, **2025-08-07** | **gpt-5-mini**, **2025-08-07** | **o3**, **2025-04-16** | **o4-mini**, **2025-04-16** | **gpt-4.1**, **2025-04-14** | **gpt-4.1-nano**, **2025-04-14** | **gpt-4.1-mini**, **2025-04-14** | **o3-mini**, **2025-01-31** | **o1**, **2024-12-17** | **gpt-4o**, **2024-05-13** | **gpt-4o**, **2024-08-06** | **gpt-4o**, **2024-11-20** | **gpt-4o-mini**, **2024-07-18** | **gpt-4**, **0613** | **gpt-4**, **1106-Preview** | **gpt-4**, **0125-Preview** | **gpt-4**, **turbo-2024-04-09** | **gpt-4-32k**, **0613** | **gpt-35-turbo**, **1106** | **gpt-35-turbo**, **0125** |
      | :----------------- | :-----------------------: | :----------------------------: | :--------------------: | :-------------------------: | :-------------------------: | :------------------------------: | :------------------------------: | :-------------------------: | :--------------------: | :------------------------: | :------------------------: | :------------------------: | :-----------------------------: | :-----------------: | :-------------------------: | :-------------------------: | :-----------------------------: | :---------------------: | :------------------------: | :------------------------: |
      | australiaeast      |             -             |                -               |            ✅           |              -              |              ✅              |                 -                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | brazilsouth        |             -             |                -               |            -           |              -              |              ✅              |                 -                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                -                |            ✅            |              ✅             |              -             |
      | canadacentral      |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              -             |              -             |                ✅                |          ✅          |              -              |              -              |                -                |            ✅            |              -             |              ✅             |
      | canadaeast         |             ✅             |                -               |            -           |              -              |              -              |                 -                |                 -                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              -              |                ✅                |            -            |              ✅             |              -             |
      | centralus          |             -             |                -               |            -           |              ✅              |              ✅              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              ✅             |                -                |          -          |              -              |              -              |                -                |            ✅            |              -             |              ✅             |
      | eastus             |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | eastus2            |             ✅             |                ✅               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | francecentral      |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              -             |                ✅                |          ✅          |              ✅              |              ✅              |                -                |            ✅            |              -             |              ✅             |
      | germanywestcentral |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              ✅             |                -                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              -             |
      | japaneast          |             ✅             |                -               |            -           |              ✅              |              ✅              |                 -                |                 ✅                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |          -          |              ✅              |              ✅              |                ✅                |            -            |              -             |              ✅             |
      | koreacentral       |             ✅             |                ✅               |            -           |              -              |              ✅              |                 -                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              -              |              -              |                ✅                |            ✅            |              ✅             |              -             |
      | northcentralus     |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | norwayeast         |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              -             |                ✅                |          ✅          |              -              |              ✅              |                -                |            ✅            |              -             |              -             |
      | polandcentral      |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              -             |              ✅             |                -                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | southafricanorth   |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              -             |              -             |                -                |          ✅          |              ✅              |              -              |                ✅                |            ✅            |              ✅             |              -             |
      | southcentralus     |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              -             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | southeastasia      |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              -             |              ✅             |              ✅             |                ✅                |          -          |              -              |              -              |                -                |            -            |              -             |              -             |
      | southindia         |             ✅             |                -               |            -           |              -              |              ✅              |                 -                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                -                |            ✅            |              ✅             |              ✅             |
      | spaincentral       |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              ✅             |              ✅             |              -             |                ✅                |          -          |              -              |              -              |                -                |            -            |              -             |              ✅             |
      | swedencentral      |             -             |                -               |            -           |              -              |              ✅              |                 ✅                |                 ✅                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | switzerlandnorth   |             -             |                -               |            -           |              -              |              ✅              |                 -                |                 -                |              ✅              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | switzerlandwest    |             -             |                -               |            -           |              -              |              -              |                 -                |                 -                |              -              |            -           |              -             |              ✅             |              -             |                ✅                |          -          |              -              |              -              |                -                |            -            |              -             |              ✅             |
      | uaenorth           |             -             |                -               |            -           |              -              |              ✅              |                 -                |                 -                |              ✅              |            ✅           |              ✅             |              ✅             |              ✅             |                -                |          -          |              ✅              |              -              |                -                |            -            |              ✅             |              ✅             |
      | uksouth            |             -             |                -               |            -           |              -              |              ✅              |                 -                |                 ✅                |              -              |            ✅           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | westeurope         |             -             |                -               |            ✅           |              ✅              |              -              |                 -                |                 -                |              -              |            -           |              -             |              -             |              ✅             |                -                |          -          |              -              |              -              |                -                |            -            |              -             |              -             |
      | westus             |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |
      | westus3            |             ✅             |                -               |            ✅           |              ✅              |              ✅              |                 ✅                |                 ✅                |              -              |            -           |              ✅             |              ✅             |              ✅             |                ✅                |          ✅          |              ✅              |              ✅              |                ✅                |            ✅            |              ✅             |              ✅             |

      <Callout type="note">
        The provisioned version of `gpt-4` version `turbo-2024-04-09` is currently limited to text only.
      </Callout>

      For more information on provisioned deployments, see [Provisioned guidance](../../openai/concepts/provisioned-throughput).
    </Tab>
  </Tabs>

  This table doesn't include fine-tuning regional availability information. Consult the [fine-tuning section](#fine-tuning-models) for this information.

  ### Embeddings models

  These models can be used only with Embedding API requests.

  <Callout type="note">
    `text-embedding-3-large` is the latest and most capable embedding model. You can't upgrade between embedding models. To migrate from using `text-embedding-ada-002` to `text-embedding-3-large`, you need to generate new embeddings.
  </Callout>

  | Model ID                             | Max request (tokens) | Output dimensions | Training data (up to) |
  | ------------------------------------ | -------------------- | :---------------: | :-------------------: |
  | `text-embedding-ada-002` (version 2) | 8,192                |       1,536       |        Sep 2021       |
  | `text-embedding-ada-002` (version 1) | 2,046                |       1,536       |        Sep 2021       |
  | `text-embedding-3-large`             | 8,192                |       3,072       |        Sep 2021       |
  | `text-embedding-3-small`             | 8,192                |       1,536       |        Sep 2021       |

  <Callout type="note">
    When you send an array of inputs for embedding, the maximum number of input items in the array per call to the embedding endpoint is 2,048.
  </Callout>

  ### Image generation models

  | Model ID           | Max request (characters) |
  | ------------------ | :----------------------: |
  | `gpt-image-1`      |           4,000          |
  | `gpt-image-1-mini` |           4,000          |
  | `gpt-image-1.5`    |           4,000          |
  | `dall-e-3`         |           4,000          |

  ### Video generation models

  | Model ID | Max Request (characters) |
  | -------- | :----------------------: |
  | sora     |           4,000          |

  ## Fine-tuning models

  <Callout type="note">
    The supported regions for fine-tuning might vary if you use Azure OpenAI models in a Microsoft Foundry project versus outside a project.
  </Callout>

  | Model ID                        | Standard regions                         | Global | Developer |                           Max request (tokens)                          | Training data (up to) | Modality                |
  | ------------------------------- | ---------------------------------------- | :----: | :-------: | :---------------------------------------------------------------------: | --------------------- | ----------------------- |
  | `gpt-4o-mini` (2024-07-18)      | North Central US Sweden Central          |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | Oct 2023              | Text to text            |
  | `gpt-4o` (2024-08-06)           | East US2 North Central US Sweden Central |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | Oct 2023              | Text and vision to text |
  | `gpt-4.1` (2025-04-14)          | North Central US Sweden Central          |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | May 2024              | Text and vision to text |
  | `gpt-4.1-mini` (2025-04-14)     | North Central US Sweden Central          |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | May 2024              | Text to text            |
  | `gpt-4.1-nano` (2025-04-14)     | North Central US Sweden Central          |    ✅   |     ✅     |  Input: 128,000 Output: 16,384 Training example context length: 32,768  | May 2024              | Text to text            |
  | `o4-mini` (2025-04-16)          | East US2 Sweden Central                  |    ✅   |     ❌     |  Input: 128,000 Output: 16,384 Training example context length: 65,536  | May 2024              | Text to text            |
  | `Ministral-3B` (preview) (2411) | Not supported                            |    ✅   |     ❌     | Input: 128,000 Output: Unknown Training example context length: Unknown | Unknown               | Text to text            |
  | `Qwen-32B` (preview)            | Not supported                            |    ✅   |     ❌     |    Input: 8,000 Output: 32,000 Training example context length: 8192    | July 2024             | Text to text            |

  <Callout type="note">
    Global training provides [more affordable](https://aka.ms/aoai-pricing) training per token, but doesn't offer [data residency](https://aka.ms/data-residency). It's currently available to Foundry resources in the following regions:

    * Australia East
    * Brazil South
    * Canada Central
    * Canada East
    * East US
    * East US2
    * France Central
    * Germany West Central
    * Italy North
    * Japan East *(no vision support)*
    * Korea Central
    * North Central US
    * Norway East
    * Poland Central *(no 4.1-nano support)*
    * Southeast Asia
    * South Africa North
    * South Central US
    * South India
    * Spain Central
    * Sweden Central
    * Switzerland West
    * Switzerland North
    * UK South
    * West Europe
    * West US
    * West US3
  </Callout>

  ## Assistants (preview)

  For Assistants, you need a combination of a supported model and a supported region. Certain tools and capabilities require the latest models. The following models are available in the Assistants API, SDK, and Foundry. The following table is for standard deployment. For information on provisioned throughput unit availability, see [Provisioned throughput](../../openai/concepts/provisioned-throughput). The listed models and regions can be used with both Assistants v1 and v2. You can use [Global Standard models](#global-standard-model-availability) if they're supported in the following regions.

  | Region        | gpt-4o, 2024-05-13 | gpt-4o, 2024-08-06 | gpt-4o-mini, 2024-07-18 | gpt-4, 0613 | gpt-4, 1106-Preview | gpt-4, 0125-Preview | gpt-4, turbo-2024-04-09 | gpt-4-32k, 0613 | gpt-35-turbo, 0613 | gpt-35-turbo, 1106 | gpt-35-turbo, 0125 | gpt-35-turbo-16k, 0613 |
  | :------------ | :----------------: | :----------------: | :---------------------: | :---------: | :-----------------: | :-----------------: | :---------------------: | :-------------: | :----------------: | :----------------: | :----------------: | :--------------------: |
  | australiaeast |          -         |          -         |            -            |      ✅      |          ✅          |          -          |            -            |        ✅        |          ✅         |          ✅         |          ✅         |            ✅           |
  | eastus        |          ✅         |          ✅         |            ✅            |      -      |          -          |          ✅          |            ✅            |        -        |          ✅         |          -         |          ✅         |            ✅           |
  | eastus2       |          ✅         |          ✅         |            ✅            |      -      |          ✅          |          -          |            ✅            |        -        |          ✅         |          -         |          ✅         |            ✅           |
  | francecentral |          -         |          -         |            -            |      ✅      |          ✅          |          -          |            -            |        ✅        |          ✅         |          ✅         |          -         |            ✅           |
  | japaneast     |          -         |          -         |            -            |      -      |          -          |          -          |            -            |        -        |          ✅         |          -         |          ✅         |            ✅           |
  | norwayeast    |          -         |          -         |            -            |      -      |          ✅          |          -          |            -            |        -        |          -         |          -         |          -         |            -           |
  | southindia    |          -         |          -         |            -            |      -      |          ✅          |          -          |            -            |        -        |          -         |          ✅         |          ✅         |            -           |
  | swedencentral |          ✅         |          ✅         |            ✅            |      ✅      |          ✅          |          -          |            ✅            |        ✅        |          ✅         |          ✅         |          -         |            ✅           |
  | uksouth       |          -         |          -         |            -            |      -      |          ✅          |          ✅          |            -            |        -        |          ✅         |          ✅         |          ✅         |            ✅           |
  | westus        |          ✅         |          ✅         |            ✅            |      -      |          ✅          |          -          |            ✅            |        -        |          -         |          ✅         |          ✅         |            -           |
  | westus3       |          ✅         |          ✅         |            ✅            |      -      |          ✅          |          -          |            ✅            |        -        |          -         |          -         |          ✅         |            -           |

  ## Model retirement

  For the latest information on model retirements, refer to the [model retirement guide](../../openai/concepts/model-retirements).

  ## Related content

  * [Foundry Models from partners and community](models-from-partners)
  * [Model retirement and deprecation](../../openai/concepts/model-retirements)
  * [Learn more about working with Azure OpenAI models](../../openai/how-to/working-with-models)
  * [Learn more about Azure OpenAI](models-sold-directly-by-azure)
  * [Learn more about fine-tuning Azure OpenAI models](../../openai/how-to/fine-tuning)
</ZonePivot>

<ZonePivot pivot="azure-direct-others">
  <Callout type="note">
    Foundry Models sold directly by Azure also include all Azure OpenAI models. To learn about these models, switch to the [Azure OpenAI models](models-sold-directly-by-azure?pivots=azure-openai) collection at the top of this article.
  </Callout>

  ## Black Forest Labs models sold directly by Azure

  The Black Forest Labs (BFL) collection of image generation models includes FLUX.2 \[pro] for image generation and editing through both text and image prompts, FLUX.1 Kontext \[pro] for in-context generation and editing, and FLUX1.1 \[pro] for text-to-image generation.

  You can run these models through the BFL service provider API and through the [images/generations and images/edits endpoints](../../openai/reference-preview).

  <Callout type="note">
    See the [GitHub sample for image generation with FLUX models in Microsoft Foundry](https://github.com/microsoft-foundry/foundry-samples/blob/main/samples/python/black-forest-labs/flux/README.md) and its associated [notebook](https://github.com/microsoft-foundry/foundry-samples/blob/main/samples/python/black-forest-labs/flux/AIFoundry_ImageGeneration_FLUX.ipynb) that showcases how to create high-quality images from textual prompts.
  </Callout>

  | Model                | Type & API endpoint                                                                                                                                                                                                                                                                                                                                                                                    | Capabilities                                                                                                                                                                                                                                                                                                                                                                                                 | Deployment type (region availability) |
  | -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------- |
  | `FLUX.2-pro`         | **Image generation** - [BFL service provider API](https://docs.bfl.ai/flux_2/flux2_text_to_image): `<resource-name>/providers/blackforestlabs/v1/flux-2-pro`                                                                                                                                                                                                                                           | - **Input:** text and image (32,000 tokens and up to 8 imagesi) - **Output:** One Image - **Tool calling:** No - **Response formats:** Image (PNG and JPG) - **Key features:** Multi-reference support for up to 8 imagesii; more grounded in real-world knowledge; greater output flexibility; enhanced performance - **Additional parameters:** *(In provider-specific API only)* Supports all parameters. | - Global standard (all regions)       |
  | `FLUX.1-Kontext-pro` | **Image generation** - [Image API](../../openai/reference-preview): `https://<resource-name>/openai/deployments/{deployment-id}/images/generations` and `https://<resource-name>/openai/deployments/{deployment-id}/images/edits` - [BFL service provider API](https://docs.bfl.ai/kontext/kontext_text_to_image): `<resource-name>/providers/blackforestlabs/v1/flux-kontext-pro?api-version=preview` | - **Input:** text and image (5,000 tokens and 1 image) - **Output:** One Image - **Tool calling:** No - **Response formats:** Image (PNG and JPG) - **Key features:** Character consistency, advanced editing - **Additional parameters:** *(In provider-specific API only)* `seed`, `aspect ratio`, `input_image`, `prompt_unsampling`, `safety_tolerance`, `output_format`                                 | - Global standard (all regions)       |
  | `FLUX-1.1-pro`       | **Image generation** - [Image API](../../openai/reference-preview): `https://<resource-name>/openai/deployments/{deployment-id}/images/generations` - [BFL service provider API](https://docs.bfl.ai/flux_models/flux_1_1_pro): `<resource-name>/providers/blackforestlabs/v1/flux-pro-1.1?api-version=preview`                                                                                        | - **Input:** text (5,000 tokens and 1 image) - **Output:** One Image - **Tool calling:** No - **Response formats:** Image (PNG and JPG) - **Key features:** Fast inference speed, strong prompt adherence, competitive pricing, scalable generation - **Additional parameters:** *(In provider-specific API only)* `width`, `height`, `prompt_unsampling`, `seed`, `safety_tolerance`, `output_format`       | - Global standard (all regions)       |

  i,ii Support for **multiple reference images (up to eight)** is available for FLUX.2\[pro] by using the API, but *not* in the playground. See the following [Code samples for FLUX.2\[pro\]](#code-samples-for-flux2pro).

  #### Code samples for FLUX.2\[pro]

  **Image generation**

  * Input: Text
  * Output: One image

  ```sh
  curl -X POST https://<your-resource-name>.api.cognitive.microsoft.com/providers/blackforestlabs/v1/flux-2-pro?api-version=preview \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer {API_KEY}" \
    -d '{
        "model": "FLUX.2-pro",
        "prompt": "A photograph of a red fox in an autumn forest",
        "width": 1024,
        "height": 1024,
        "seed": 42,
        "safety_tolerance": 2,
        "output_format": "jpeg"
      }'
  ```

  **Image editing**

  * Input: Up to eight bit-64 encoded images
  * Output: One image

  ```sh
  curl -X POST https://<your-resource-name>.api.cognitive.microsoft.com/providers/blackforestlabs/v1/flux-2-pro?api-version=preview \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer {API_KEY}" \
    -d '{
        "model": "FLUX.2-pro",
        "prompt": "Apply a cinematic, moody lighting effect to all photos. Make them look like scenes from a sci-fi noir film",
        "output_format": "jpeg",
        "input_image" : "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDA.......",
        "input_image_2" : "iVBORw0KGgoAAAANSUhEUgAABAAAAAQACAIAAADwf........"
      }'
  ```

  See [this model collection in Microsoft Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=black+forest+labs/?cid=learnDocs).

  ## Cohere models sold directly by Azure

  The Cohere family of models includes various models optimized for different use cases, including chat completions, rerank/text classification, and embeddings. Cohere models are optimized for various use cases that include reasoning, summarization, and question answering.

  | Model                                                                                                                                    | Type                         | Capabilities                                                                                                                                                                                                          | Deployment type (region availability)             |
  | ---------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------- |
  | [Cohere-rerank-v4.0-pro](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-pro/version/1/registry/azureml-cohere/?cid=learnDocs)   | text classification (rerank) | - **Input:** text - **Output:** text - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `zh-cn`, `ar`, `vi`, `hi`, `ru`, `id`, and `nl` - **Tool calling:** No - **Response formats:** JSON                | - Global standard (all regions) - Managed compute |
  | [Cohere-rerank-v4.0-fast](https://ai.azure.com/resource/models/Cohere-rerank-v4.0-fast/version/2/registry/azureml-cohere/?cid=learnDocs) | text classification (rerank) | - **Input:** text - **Output:** text - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `zh-cn`, `ar`, `vi`, `hi`, `ru`, `id`, and `nl` - **Tool calling:** No - **Response formats:** JSON                | - Global standard (all regions) - Managed compute |
  | `Cohere-command-a`                                                                                                                       | chat-completion              | - **Input:** text (131,072 tokens) - **Output:** text (8,182 tokens) - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `ko`, `zh-cn`, and `ar` - **Tool calling:** Yes - **Response formats:** Text, JSON | - Global standard (all regions)                   |
  | `embed-v-4-0`                                                                                                                            | embeddings                   | - **Input:** text (512 tokens) and images (2MM pixels) - **Output:** Vector (256, 512, 1024, 1536 dim.) - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `ko`, `zh-cn`, and `ar`                         | - Global standard (all regions)                   |

  See [the Cohere model collection in the Foundry portal](https://ai.azure.com/explore/models?selectedCollection=Cohere/?cid=learnDocs,cohere).

  ## DeepSeek models sold directly by Azure

  The DeepSeek family of models includes several reasoning models, which excel at reasoning tasks by using a step-by-step training process, such as language, scientific reasoning, and coding tasks.

  | Model                    | Type                                                                     | Capabilities                                                                                                                                                     | Deployment type (region availability)                              |
  | ------------------------ | ------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
  | `DeepSeek-V3.2-Speciale` | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (128,000 tokens) - **Output:** text (128,000 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text, JSON  | - Global standard (all regions)                                    |
  | `DeepSeek-V3.2`          | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (128,000 tokens) - **Output:** text (128,000 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text, JSON  | - Global standard (all regions)                                    |
  | `DeepSeek-V3.1`          | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (131,072 tokens) - **Output:** text (131,072 tokens) - **Languages:** `en` and `zh` - **Tool calling:** Yes - **Response formats:** Text, JSON | - Global standard (all regions)                                    |
  | `DeepSeek-R1-0528`       | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (163,840 tokens) - **Output:** text (163,840 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text        | - Global standard (all regions) - Global provisioned (all regions) |
  | `DeepSeek-V3-0324`       | chat-completion                                                          | - **Input:** text (131,072 tokens) - **Output:** text (131,072 tokens) - **Languages:** `en` and `zh` - **Tool calling:** Yes - **Response formats:** Text, JSON | - Global standard (all regions) - Global provisioned (all regions) |
  | `DeepSeek-R1`            | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (163,840 tokens) - **Output:** text (163,840 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text        | - Global standard (all regions) - Global provisioned (all regions) |

  See [this model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=DeepSeek/?cid=learnDocs).

  ## Meta models sold directly by Azure

  Meta Llama models and tools are a collection of pretrained and fine-tuned generative AI text and image reasoning models. Meta models range in scale to include:

  * Small language models (SLMs) like 1B and 3B Base and Instruct models for on-device and edge inferencing
  * Mid-size large language models (LLMs) like 7B, 8B, and 70B Base and Instruct models
  * High-performance models like Meta Llama 3.1-405B Instruct for synthetic data generation and distillation use cases.

  | Model                                    | Type            | Capabilities                                                                                                                                                                                                            | Deployment type (region availability) |
  | ---------------------------------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
  | `Llama-4-Maverick-17B-128E-Instruct-FP8` | chat-completion | - **Input:** text and images (1M tokens) - **Output:** text (1M tokens) - **Languages:** `ar`, `en`, `fr`, `de`, `hi`, `id`, `it`, `pt`, `es`, `tl`, `th`, and `vi` - **Tool calling:** No - **Response formats:** Text | - Global standard (all regions)       |
  | `Llama-3.3-70B-Instruct`                 | chat-completion | - **Input:** text (128,000 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en`, `de`, `fr`, `it`, `pt`, `hi`, `es`, and `th` - **Tool calling:** No - **Response formats:** Text                            | - Global standard (all regions)       |

  See [this model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Meta/?cid=learnDocs). You can also find several Meta models available [from partners and community](models-from-partners#meta).

  ## Microsoft models sold directly by Azure

  Microsoft models include various model groups such as Model Router, MAI models, Phi models, healthcare AI models, and more. See [the Microsoft model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Microsoft/?cid=learnDocs). You can also find several Microsoft models available [from partners and community](models-from-partners#microsoft).

  | Model                                                                                                                         | Type                                                                     | Capabilities                                                                                                                                                                                                                                       | Deployment type (region availability)                                                           |
  | ----------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
  | [model-router](https://ai.azure.com/resource/models/model-router/version/2025-11-18/registry/azureml-routers/?cid=learnDocs)1 | chat-completion                                                          | More details in [Model router overview](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/model-router). - **Input:** text, image - **Output:** text (max output tokens varies2) **Context window:** 200,0003 - **Languages:** `en` | - Global standard (East US 2, Sweden Central) - Data Zone standard4 (East US 2, Sweden Central) |
  | `MAI-DS-R1`                                                                                                                   | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (163,840 tokens) - **Output:** text (163,840 tokens) - **Languages:** `en` and `zh` - **Tool calling:** No - **Response formats:** Text                                                                                          | - Global standard (all regions)                                                                 |

  1 **Model router version** `2025-11-18`. Earlier versions (`2025-08-07` and `2025-05-19`) are also available.

  2 **Max output tokens** varies for underlying models in the model router. For example, 32,768 (`GPT-4.1 series`), 100,000 (`o4-mini`), 128,000 (`gpt-5 reasoning models`), and 16,384 (`gpt-5-chat`).

  3 Larger **context windows** are compatible with *some* of the underlying models of the Model Router. That means an API call with a larger context succeeds only if the prompt gets routed to one of such models. Otherwise, the call fails.

  4 Billing for **Data Zone Standard** model router deployments begins no earlier than November 1, 2025.

  ## Mistral models sold directly by Azure

  | Model                      | Type            | Capabilities                                                                                                                                                                                 | Deployment type (region availability)                            |
  | -------------------------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------- |
  | `Mistral-Large-3`          | chat-completion | - **Input:** text, image - **Output:** text - **Languages:** `en`, `fr`, `de`, `es`, `it`, `pt`, `nl`, `zh`, `ja`, `ko`, and `ar` - **Tool calling:** Yes - **Response formats:** Text, JSON | - Global standard (West US 3)                                    |
  | `mistral-document-ai-2505` | Image-to-Text   | - **Input:** image or PDF pages (30 pages, max 30MB PDF file) - **Output:** text - **Languages:** `en` - **Tool calling:** no - **Response formats:** Text, JSON, Markdown                   | - Global standard (all regions) - Data zone standard (US and EU) |

  See [the Mistral model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Mistral+AI/?cid=learnDocs). You can also find several Mistral models available [from partners and community](models-from-partners#mistral-ai).

  ## Moonshot AI models sold directly by Azure

  Moonshot AI models include Kimi K2.5 and Kimi K2 Thinking. Kimi K2.5 is a multimodal reasoning model that accepts text and image input, while Kimi K2 Thinking is the latest, most capable version of open-source thinking model.

  | Model              | Type                                                                     | Capabilities                                                                                                                                                         | Deployment type (region availability) |
  | ------------------ | ------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------- |
  | `Kimi-K2.5`        | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text and image (262,144 tokens) - **Output:** text (262,144 tokens) - **Languages:** `en` and `zh` - **Tool calling:** Yes - **Response formats:** Text | - Global standard (all regions)       |
  | `Kimi-K2-Thinking` | chat-completion [(with reasoning content)](../how-to/use-chat-reasoning) | - **Input:** text (262,144 tokens) - **Output:** text (262,144 tokens) - **Languages:** `en` and `zh` - **Tool calling:** Yes - **Response formats:** Text           | - Global standard (all regions)       |

  See [this model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Moonshot+ai/?cid=learnDocs).

  ## xAI models sold directly by Azure

  xAI's Grok models in Foundry Models include a diverse set of reasoning and non-reasoning models designed for enterprise use cases such as data extraction, coding, text summarization, and agentic applications. [Registration is required for access to grok-code-fast-1](https://aka.ms/xai/grok-code-fast-1) and [grok-4](https://aka.ms/xai/grok-4).

  | Model                       | Type            | Capabilities                                                                                                                                             | Deployment type (region availability)                     |
  | --------------------------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- |
  | `grok-4`                    | chat-completion | - **Input:** text (262,000 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text          | - Global standard (all regions)                           |
  | `grok-4-fast-reasoning`     | chat-completion | - **Input:** text, image (128,000 tokens) - **Output:** text (128,000 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text | - Global standard (all regions) - Data zone standard (US) |
  | `grok-4-fast-non-reasoning` | chat-completion | - **Input:** text, image (128,000 tokens) - **Output:** text (128,000 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text | - Global standard (all regions) - Data zone standard (US) |
  | `grok-code-fast-1`          | chat-completion | - **Input:** text (256,000 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text          | - Global standard (all regions)                           |
  | `grok-3`                    | chat-completion | - **Input:** text (131,072 tokens) - **Output:** text (131,072 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text        | - Global standard (all regions) - Data zone standard (US) |
  | `grok-3-mini`               | chat-completion | - **Input:** text (131,072 tokens) - **Output:** text (131,072 tokens) - **Languages:** `en` - **Tool calling:** yes - **Response formats:** text        | - Global standard (all regions) - Data zone standard (US) |

  See [the xAI model collection in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=xAI/?cid=learnDocs).

  ## Model region availability by deployment type

  Foundry Models gives you choices for the hosting structure that fits your business and usage patterns. The service offers two main types of deployment:

  * **Standard**: Has a global deployment option, routing traffic globally to provide higher throughput.
  * **Provisioned**: Also has a global deployment option, allowing you to purchase and deploy provisioned throughput units across Azure global infrastructure.

  All deployments perform the same inference operations, but the billing, scale, and performance differ. For more information about deployment types, see [Deployment types in Foundry Models](deployment-types).

  <Tabs>
    <Tab title="Global Standard">
      ### Global Standard model availability

      | **Region**         | **DeepSeek-R1-0528** | **DeepSeek-R1** | **DeepSeek-V3-0324** | **DeepSeek-V3.1** | **FLUX.1-Kontext-pro** | **FLUX-1.1-pro** | **grok-4** | **grok-4-fast-reasoning** | **grok-4-fast-non-reasoning** | **grok-code-fast-1** | **grok-3** | **grok-3-mini** | **Llama-4-Maverick-17B-128E-Instruct-FP8** | **Llama-3.3-70B-Instruct** | **MAI-DS-R1** | **mistral-document-ai-2505** |
      | :----------------- | :------------------: | :-------------: | :------------------: | :---------------: | :--------------------: | :--------------: | :--------: | :-----------------------: | :---------------------------: | :------------------: | :--------: | :-------------: | :----------------------------------------: | :------------------------: | :-----------: | :--------------------------: |
      | australiaeast      |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | brazilsouth        |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | canadaeast         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | eastus             |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | eastus2            |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | francecentral      |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | germanywestcentral |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | italynorth         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | japaneast          |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | koreacentral       |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | northcentralus     |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | norwayeast         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | polandcentral      |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | southafricanorth   |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | southcentralus     |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | southindia         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | spaincentral       |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | swedencentral      |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | switzerlandnorth   |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | switzerlandwest    |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | uaenorth           |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | uksouth            |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | westeurope         |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | westus             |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
      | westus3            |           ✅          |        ✅        |           ✅          |         ✅         |            ✅           |         ✅        |      ✅     |             ✅             |               ✅               |           ✅          |      ✅     |        ✅        |                      ✅                     |              ✅             |       ✅       |               ✅              |
    </Tab>

    <Tab title="Global Provisioned managed">
      ### Global Provisioned managed model availability

      | **Region**         | **DeepSeek-R1-0528** | **DeepSeek-V3-0324** | **DeepSeek-R1** |
      | :----------------- | :------------------: | :------------------: | :-------------: |
      | australiaeast      |           ✅          |           ✅          |        ✅        |
      | brazilsouth        |           ✅          |           ✅          |        ✅        |
      | canadaeast         |           ✅          |           ✅          |        ✅        |
      | eastus             |           ✅          |           ✅          |        ✅        |
      | eastus2            |           ✅          |           ✅          |        ✅        |
      | francecentral      |           ✅          |           ✅          |        ✅        |
      | germanywestcentral |           ✅          |           ✅          |        ✅        |
      | italynorth         |           ✅          |           ✅          |        ✅        |
      | japaneast          |           ✅          |           ✅          |        ✅        |
      | koreacentral       |           ✅          |           ✅          |        ✅        |
      | northcentralus     |           ✅          |           ✅          |        ✅        |
      | norwayeast         |           ✅          |           ✅          |        ✅        |
      | polandcentral      |           ✅          |           ✅          |        ✅        |
      | southafricanorth   |           ✅          |           ✅          |        ✅        |
      | southcentralus     |           ✅          |           ✅          |        ✅        |
      | southindia         |           ✅          |           ✅          |        ✅        |
      | spaincentral       |           ✅          |           ✅          |        ✅        |
      | swedencentral      |           ✅          |           ✅          |        ✅        |
      | switzerlandnorth   |           ✅          |           ✅          |        ✅        |
      | switzerlandwest    |           ✅          |           ✅          |        ✅        |
      | uaenorth           |           ✅          |           ✅          |        ✅        |
      | uksouth            |           ✅          |           ✅          |        ✅        |
      | westeurope         |           ✅          |           ✅          |        ✅        |
      | westus             |           ✅          |           ✅          |        ✅        |
      | westus3            |           ✅          |           ✅          |        ✅        |
    </Tab>

    <Tab title="Data Zone Standard">
      ### Data Zone Standard model availability

      | **Region**         | **mistral-document-ai-2505** | **grok-4-fast-reasoning** | **grok-4-fast-non-reasoning** | **grok-3** | **grok-3-mini** |
      | :----------------- | :--------------------------: | :-----------------------: | :---------------------------: | :--------: | :-------------: |
      | eastus             |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | eastus2            |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | francecentral      |               ✅              |             -             |               -               |      -     |        -        |
      | germanywestcentral |               ✅              |             -             |               -               |      -     |        -        |
      | italynorth         |               ✅              |             -             |               -               |      -     |        -        |
      | northcentralus     |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | norwayeast         |               ✅              |             -             |               -               |      -     |        -        |
      | polandcentral      |               ✅              |             -             |               -               |      -     |        -        |
      | southcentralus     |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | spaincentral       |               ✅              |             -             |               -               |      -     |        -        |
      | swedencentral      |               ✅              |             -             |               -               |      -     |        -        |
      | switzerlandnorth   |               ✅              |             -             |               -               |      -     |        -        |
      | switzerlandwest    |               ✅              |             -             |               -               |      -     |        -        |
      | westeurope         |               ✅              |             -             |               -               |      -     |        -        |
      | westus             |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
      | westus3            |               ✅              |             ✅             |               ✅               |      ✅     |        ✅        |
    </Tab>
  </Tabs>

  ## Related content

  * [Foundry Models from partners and community](models-from-partners)
  * [Model deprecation and retirement for Foundry Models](../../concepts/model-lifecycle-retirement)
  * [Deployment overview for Foundry Models](../../concepts/deployments-overview)
  * [Add and configure models to Foundry Models](../how-to/create-model-deployments)
  * [Deployment types in Foundry Models](deployment-types)
</ZonePivot>

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

This article lists capabilities for a selection of Microsoft Foundry Models from partners and community. Most Foundry Model providers are trusted third-party organizations, partners, research labs, and community contributors. The selection of models that you see in Foundry depends on the [kind of project](https://learn.microsoft.com/en-us/azure/ai-foundry/../../what-is-foundry?view=foundry-classic\&preserve-view=true#types-of-projects) you use. To learn more about attributes of Foundry Models from partners and community, see [Explore Foundry Models](../../concepts/foundry-models-overview#models-from-partners-and-community).

<Callout type="note">
  For a list of models sold directly by Azure, see [Foundry Models sold directly by Azure](models-sold-directly-by-azure).

  For a list of Azure OpenAI models that are supported by the Foundry Agent Service, see [Models supported by Agent Service](../../agents/concepts/model-region-support).
</Callout>

## Anthropic

Anthropic's flagship product is Claude, a frontier AI model trusted by leading enterprises and millions of users worldwide for complex tasks including coding, agents, financial analysis, research, and office tasks. Claude delivers exceptional performance while maintaining high safety standards.

To work with Claude models in Foundry, see [Deploy and use Claude models in Microsoft Foundry](../how-to/use-foundry-models-claude).

<Callout type="important">
  To use Claude models in Microsoft Foundry, you need a paid Azure subscription with a billing account in a [country or region](../../how-to/deploy-models-serverless-availability#region-availability) where Anthropic offers the models for purchase. The following paid subscription types are currently restricted: Cloud Solution Providers (CSP), sponsored accounts with Azure credits, enterprise accounts in Singapore and South Korea, and Microsoft accounts.

  For a list of common subscription-related errors, see [Common error messages and solutions](https://learn.microsoft.com/en-us/marketplace/purchase-saas-offer-in-azure-portal#common-error-messages-and-solutions).
</Callout>

| Model                                    | Type     | Capabilities                                                                                                                                                                                                                                                                                                                                                                                             |
| ---------------------------------------- | -------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `claude-opus-4-6` &#xNAN;**(Preview)**   | Messages | - **Input:** text, image, and code - **Output:** text, image, and code (128,000 max tokens) - **Context window:** 1,000,000,000 (beta) - **Languages:** `en`, `fr`, `ar`, `zh`, `ja`, `ko`, `es`, `hi` - **Tool calling:** Yes (file search and code execution) - **Response formats:** Text in various formats (e.g., prose, lists, Markdown tables, JSON, HTML, code in various programming languages) |
| `claude-opus-4-5` &#xNAN;**(Preview)**   | Messages | - **Input:** text, image, and code - **Output:** text (64,000 max tokens) - **Context window:** 200,000 - **Languages:** `en`, `fr`, `ar`, `zh`, `ja`, `ko`, `es`, `hi` - **Tool calling:** Yes (file search and code execution) - **Response formats:** Text in various formats (e.g., prose, lists, Markdown tables, JSON, HTML, code in various programming languages)                                |
| `claude-opus-4-1` &#xNAN;**(Preview)**   | Messages | - **Input:** text, image, and code - **Output:** text (32,000 max tokens) - **Context window:** 200,000 - **Languages:** `en`, `fr`, `ar`, `zh`, `ja`, `ko`, `es`, `hi` - **Tool calling:** Yes (file search and code execution) - **Response formats:** Text in various formats (e.g., prose, lists, Markdown tables, JSON, HTML, code in various programming languages)                                |
| `claude-sonnet-4-5` &#xNAN;**(Preview)** | Messages | - **Input:** text, image, and code - **Output:** text (64,000 max tokens) - **Context window:** 200,000 - **Languages:** `en`, `fr`, `ar`, `zh`, `ja`, `ko`, `es`, `hi` - **Tool calling:** Yes (file search and code execution) - **Response formats:** Text in various formats (e.g., prose, lists, Markdown tables, JSON, HTML, code in various programming languages)                                |
| `claude-haiku-4-5` &#xNAN;**(Preview)**  | Messages | - **Input:** text and image - **Output:** text (64,000 max tokens) - **Context window:** 200,000 - **Languages:** `en`, `fr`, `ar`, `zh`, `ja`, `ko`, `es`, `hi` - **Tool calling:** Yes (file search and code execution) - **Response formats:** Text in various formats (e.g., prose, lists, Markdown tables, JSON, HTML, code in various programming languages)                                       |

See [Anthropic models in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=anthropic/?cid=learnDocs).

## Cohere

The Cohere family of models includes various models optimized for different use cases, including chat completions and embeddings. Cohere models are optimized for various use cases that include reasoning, summarization, and question answering.

To deploy Cohere models in Foundry, see [Deploy Microsoft Foundry Models in the Foundry portal](../how-to/deploy-foundry-models).

| Model                           | Type            | Capabilities                                                                                                                                                                                                          |
| ------------------------------- | --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Cohere-command-r-plus-08-2024` | chat-completion | - **Input:** text (131,072 tokens) - **Output:** text (4,096 tokens) - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `ko`, `zh-cn`, and `ar` - **Tool calling:** Yes - **Response formats:** Text, JSON |
| `Cohere-command-r-08-2024`      | chat-completion | - **Input:** text (131,072 tokens) - **Output:** text (4,096 tokens) - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `ko`, `zh-cn`, and `ar` - **Tool calling:** Yes - **Response formats:** Text, JSON |
| `Cohere-embed-v3-english`       | embeddings      | - **Input:** text and images (512 tokens) - **Output:** Vector (1024 dim.) - **Languages:** `en`                                                                                                                      |
| `Cohere-embed-v3-multilingual`  | embeddings      | - **Input:** text (512 tokens) - **Output:** Vector (1024 dim.) - **Languages:** `en`, `fr`, `es`, `it`, `de`, `pt-br`, `ja`, `ko`, `zh-cn`, and `ar`                                                                 |

See [Cohere models in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Cohere/?cid=learnDocs).

## Meta

Meta Llama models and tools are a collection of pretrained and fine-tuned generative AI text and image reasoning models. Meta models range in scale to include:

* Small language models (SLMs) like 1B and 3B Base and Instruct models for on-device and edge inferencing
* Mid-size large language models (LLMs) like 7B, 8B, and 70B Base and Instruct models
* High-performance models like Meta Llama 3.1-405B Instruct for synthetic data generation and distillation use cases.

To deploy Meta Llama models in Foundry, see [Deploy Microsoft Foundry Models in the Foundry portal](../how-to/deploy-foundry-models).

| Model                            | Type            | Capabilities                                                                                                                                                                                 |
| -------------------------------- | --------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Llama-3.2-11B-Vision-Instruct`  | chat-completion | - **Input:** text and image (128,000 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en` - **Tool calling:** No - **Response formats:** Text                                     |
| `Llama-3.2-90B-Vision-Instruct`  | chat-completion | - **Input:** text and image (128,000 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en` - **Tool calling:** No - **Response formats:** Text                                     |
| `Meta-Llama-3.1-405B-Instruct`   | chat-completion | - **Input:** text (131,072 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en`, `de`, `fr`, `it`, `pt`, `hi`, `es`, and `th` - **Tool calling:** No - **Response formats:** Text |
| `Meta-Llama-3.1-8B-Instruct`     | chat-completion | - **Input:** text (131,072 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en`, `de`, `fr`, `it`, `pt`, `hi`, `es`, and `th` - **Tool calling:** No - **Response formats:** Text |
| `Llama-4-Scout-17B-16E-Instruct` | chat-completion | - **Input:** text and image (128,000 tokens) - **Output:** text (8,192 tokens) - **Languages:** `en` - **Tool calling:** No - **Response formats:** Text                                     |

See [Meta models in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Meta/?cid=learnDocs). You can also find several Meta models available as [models sold directly by Azure](models-sold-directly-by-azure?pivots=azure-direct-others).

## Microsoft

Microsoft models include various model groups such as MAI models, Phi models, healthcare AI models, and more.

To deploy Microsoft models in Foundry, see [Deploy Microsoft Foundry Models in the Foundry portal](../how-to/deploy-foundry-models).

| Model                       | Type                                   | Capabilities                                                                                                                                                                                                                                                                                                                                                                                                               |
| --------------------------- | -------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Phi-4-mini-instruct`       | chat-completion                        | - **Input:** text (131,072 tokens) - **Output:** text (4,096 tokens) - **Languages:** `ar`, `zh`, `cs`, `da`, `nl`, `en`, `fi`, `fr`, `de`, `he`, `hu`, `it`, `ja`, `ko`, `no`, `pl`, `pt`, `ru`, `es`, `sv`, `th`, `tr`, and `uk` - **Tool calling:** No - **Response formats:** Text                                                                                                                                     |
| `Phi-4-multimodal-instruct` | chat-completion                        | - **Input:** text, images, and audio (131,072 tokens) - **Output:** text (4,096 tokens) - **Languages:** `ar`, `zh`, `cs`, `da`, `nl`, `en`, `fi`, `fr`, `de`, `he`, `hu`, `it`, `ja`, `ko`, `no`, `pl`, `pt`, `ru`, `es`, `sv`, `th`, `tr`, and `uk` - **Tool calling:** No - **Response formats:** Text                                                                                                                  |
| `Phi-4`                     | chat-completion                        | - **Input:** text (16,384 tokens) - **Output:** text (16,384 tokens) - **Languages:** `en`, `ar`, `bn`, `cs`, `da`, `de`, `el`, `es`, `fa`, `fi`, `fr`, `gu`, `ha`, `he`, `hi`, `hu`, `id`, `it`, `ja`, `jv`, `kn`, `ko`, `ml`, `mr`, `nl`, `no`, `or`, `pa`, `pl`, `ps`, `pt`, `ro`, `ru`, `sv`, `sw`, `ta`, `te`, `th`, `tl`, `tr`, `uk`, `ur`, `vi`, `yo`, and `zh` - **Tool calling:** No - **Response formats:** Text |
| `Phi-4-reasoning`           | chat-completion with reasoning content | - **Input:** text (32,768 tokens) - **Output:** text (32,768 tokens) - **Languages:** `en` - **Tool calling:** No - **Response formats:** Text                                                                                                                                                                                                                                                                             |
| `Phi-4-mini-reasoning`      | chat-completion with reasoning content | - **Input:** text (128,000 tokens) - **Output:** text (128,000 tokens) - **Languages:** `en` - **Tool calling:** No - **Response formats:** Text                                                                                                                                                                                                                                                                           |

See [Microsoft models in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Microsoft/?cid=learnDocs). Microsoft models are also available as [models sold directly by Azure](models-sold-directly-by-azure?pivots=azure-direct-others).

## Mistral AI

Mistral AI offers models for code generation, general-purpose chat, and multimodal tasks, including Codestral, Ministral, Mistral Small, and Mistral Medium.

To deploy Mistral AI models in Foundry, see [Deploy Microsoft Foundry Models in the Foundry portal](../how-to/deploy-foundry-models).

| Model                 | Type            | Capabilities                                                                                                                                                            |
| --------------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Codestral-2501`      | chat-completion | - **Input:** text (262,144 tokens) - **Output:** text (4,096 tokens) - **Languages:** en - **Tool calling:** No - **Response formats:** Text                            |
| `Ministral-3B`        | chat-completion | - **Input:** text (131,072 tokens) - **Output:** text (4,096 tokens) - **Languages:** fr, de, es, it, and en - **Tool calling:** Yes - **Response formats:** Text, JSON |
| `Mistral-small-2503`  | chat-completion | - **Input:** text (32,768 tokens) - **Output:** text (4,096 tokens) - **Languages:** fr, de, es, it, and en - **Tool calling:** Yes - **Response formats:** Text, JSON  |
| `Mistral-medium-2505` | chat-completion | - **Input:** text (128,000 tokens), image - **Output:** text (128,000 tokens) - **Tool calling:** No - **Response formats:** Text, JSON                                 |

See [Mistral AI models in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Mistral+AI/?cid=learnDocs). Mistral models are also available as [models sold directly by Azure](models-sold-directly-by-azure?pivots=azure-direct-others).

## Stability AI

The Stability AI collection of image generation models includes Stable Image Core, Stable Image Ultra, and Stable Diffusion 3.5 Large. Stable Diffusion 3.5 Large accepts both image and text input.

To deploy Stability AI models in Foundry, see [Deploy Microsoft Foundry Models in the Foundry portal](../how-to/deploy-foundry-models).

| Model                        | Type             | Capabilities                                                                                                                                      |
| ---------------------------- | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------- |
| `Stable Diffusion 3.5 Large` | Image generation | - **Input:** text and image (1,000 tokens and 1 image) - **Output:** One Image - **Tool calling:** No - **Response formats**: Image (PNG and JPG) |
| `Stable Image Core`          | Image generation | - **Input:** text (1,000 tokens) - **Output:** One Image - **Tool calling:** No - **Response formats:** Image (PNG and JPG)                       |
| `Stable Image Ultra`         | Image generation | - **Input:** text (1,000 tokens) - **Output:** One Image - **Tool calling:** No - **Response formats:** Image (PNG and JPG)                       |

See [Stability AI models in the Foundry portal](https://ai.azure.com/explore/models?\&selectedCollection=Stability+AI/?cid=learnDocs).

## Related content

* [Deployment overview for Foundry Models](../../concepts/deployments-overview)
* [Deploy Microsoft Foundry Models in the Foundry portal](../how-to/deploy-foundry-models)
* [Deployment types in Foundry Models](deployment-types)
* [Azure Marketplace requirements for Foundry Models from partners](../how-to/configure-marketplace)
* [Region availability for Foundry Models](../../how-to/deploy-models-serverless-availability)
* [Explore Foundry Models](../../concepts/foundry-models-overview)

Microsoft Foundry Models regularly release new model versions that incorporate the latest features and improvements from model providers. This article explains how model versioning works, what update policies are available for your deployments, and how Azure OpenAI and partner model versions are managed.

## How model versions work

You can choose to start with a particular model version and stay on it, or automatically update as new versions are released.

There are two different versions to consider when working with models:

* The version of the model itself.
* The version of the API used to consume a model deployment.

The version of a model is decided when you deploy it. You can choose an update policy, which can include the following options:

* Deployments set with a specific version or without offering an upgrade policy require a manual upgrade if a new version is released. When the model is retired, those deployments stop working.

* Deployments set to **Auto-update to default** automatically update to use the new default version.

* Deployments set to **Upgrade when expired** automatically update when its current version is retired.

<Callout type="note">
  Update policies are configured per deployment and **vary** by model and provider.
</Callout>

For example, a deployment of `gpt-4o` might target version `2024-08-06`. When version `2024-11-20` becomes available, deployments set to auto-update switch to the new version automatically.

You configure update policies when you deploy a model in the [Foundry portal](https://ai.azure.com). You can also change the policy later in the deployment settings. To check the current version of a deployment, open the deployment details in the Foundry portal or query the deployment via the REST API.

The API version indicates the contract that you use to interface with the model in code. When using REST APIs, you indicate the API version using the query parameter `api-version`. Azure SDK versions are usually paired with specific API versions, but you can specify the API version you want to use.

A given model deployment might support multiple API versions. The release of a new model version doesn't always require you to upgrade to a new API version, as is the case when there's an update to the model's weights. Azure maintains the previous major version of a model until its retirement date, so you can switch back to it if needed.

## Azure OpenAI model updates

Azure works closely with OpenAI to release new model versions. When a new version of a model is released, you can immediately test it in new deployments. Azure publishes when new versions of models are released, and notifies customers at least two weeks before a new version becomes the default version of the model.

### Prepare for Azure OpenAI model version upgrades

As a customer of Azure OpenAI models, you might notice some changes in the model behavior and compatibility after a version upgrade. These changes might affect your applications and workflows that rely on the models. Here are some tips to help you prepare for version upgrades and minimize the impact:

* Read [what's new](../../openai/whats-new) and [models](models-sold-directly-by-azure) to understand the changes and new features.
* Read the documentation on [model deployments](../../openai/how-to/create-resource) and [version upgrades](../../openai/how-to/working-with-models) to understand how to work with model versions.
* Test your applications and workflows with the new model version after release.
* Update your code and configuration to use the new features and capabilities of the new model version.

## Partner model updates

Azure works closely with model providers to release new model versions. When a new version of a model is released, you can immediately test it in new deployments.

New model versions might result in a new model ID being published. For example, `Llama-3.3-70B-Instruct`, `Meta-Llama-3.1-70B-Instruct`, and `Meta-Llama-3-70B-Instruct`. In some cases, all the model versions might be available in the same API version. In other cases, you might also need to adjust the API version used to consume the model in case the API contract has changed from one model to another.

## Related content

* [Working with Azure OpenAI models](../../openai/how-to/working-with-models)
* [Model deprecation and retirement for Microsoft Foundry Models](../../concepts/model-lifecycle-retirement)
* [Azure OpenAI in Microsoft Foundry model deprecations and retirements](../../openai/concepts/model-retirements)
* [Deploy Foundry Models](../how-to/deploy-foundry-models)
* [Deployment types in Foundry Models](deployment-types)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Certain Microsoft Foundry Models are offered directly by the model provider through the Azure Marketplace. This article explains the requirements to use Azure Marketplace if you plan to use such models in your workloads. Models sold directly by Azure, like DeepSeek, Black Forest Labs, or Azure OpenAI in Foundry Models, don't have this requirement.

## Permissions required to subscribe to Models from Partners and Community

[Foundry Models from partners and community](../concepts/models-from-partners) available for deployment (for example, Cohere models) require Azure Marketplace. Model providers define the license terms and set the price for use of their models using Azure Marketplace.

When deploying third-party models, ensure you have the following permissions in your account:

* On the Azure subscription:

  * `Microsoft.MarketplaceOrdering/agreements/offers/plans/read`
  * `Microsoft.MarketplaceOrdering/agreements/offers/plans/sign/action`
  * `Microsoft.MarketplaceOrdering/offerTypes/publishers/offers/plans/agreements/read`
  * `Microsoft.Marketplace/offerTypes/publishers/offers/plans/agreements/read`
  * `Microsoft.SaaS/register/action`

* On the resource group—to create and use the SaaS resource:

  * `Microsoft.SaaS/resources/read`
  * `Microsoft.SaaS/resources/write`

## Country/region availability

Users can access models from partners and community with pay-as-you-go billing only if their Azure subscription belongs to a billing account in a country/region or region where the model offer is available. Availability varies per model provider and model SKU. For more information, see [Region availability for models](../../how-to/deploy-models-serverless-availability).

## Troubleshooting

Use the following troubleshooting guide to find and solve errors when deploying third-party models in Foundry Models:

| Error                                                                                                                                                                                                                                            | Description                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| This offer is not made available by the provider in the country/region where your account and Azure Subscription are registered.                                                                                                                 | The model provider didn't make the specific model SKU available in the country/region where you registered your subscription. Each model provider decides which countries/regions to make the offer available in, and availability can vary by model SKU. You need to deploy the model to a subscription with billing in a supported country/region. See the list of countries/regions at [Region availability for models](../../how-to/deploy-models-serverless-availability). |
| Marketplace Subscription purchase eligibility check failed.                                                                                                                                                                                      | The model provider didn't make the specific model SKU available in the country/region where you registered your subscription, or the model isn't available in the region where you deployed the Foundry Tools resource. See [Region availability for models](../../how-to/deploy-models-serverless-availability).                                                                                                                                                               |
| Unable to create a model deployment for model "model-name". If the error persists, please contact [HIT (Human Intelligence Team)](https://go.microsoft.com/fwlink/?linkid=2101400\&clcid=0x409) and request to allowlist the Azure subscription. | Azure Marketplace rejects the request to create a model subscription. This rejection can happen for multiple reasons, including subscribing to the model offering too often or from multiple subscriptions at the same time. Contact support by using the provided link and include your subscription ID.                                                                                                                                                                       |
| This offer is not available for purchasing by subscriptions belonging to Microsoft Azure Cloud Solution Providers.                                                                                                                               | Cloud Solution Provider (CSP) subscriptions can't purchase third-party model offerings. Consider using models offered as first-party consumption service.                                                                                                                                                                                                                                                                                                                       |

GPT-5 is the first model from OpenAI that introduces four adjustable levels of thinking, controlling the amount of time and tokens the model uses when responding to a prompt. When selecting which model to use, or whether to use a reasoning model at all, it is important to consider your application’s priorities.

Scenarios like researching and producing a report involve the collection, processing, and generation of large amounts of data. Customers in these scenarios are typically willing to wait many minutes for a high-quality report to be generated. A reasoning model like GPT-5 with medium or high thinking is great for this use case.

Another example is a coding assistant, where you want to vary the amount of thinking based on the complexity of the coding task. Here, you want your customers to have control over the amount of time and level of effort the model exerts before providing a response. GPT-5 or GPT-5 mini with controllable thinking levels are a great solution.

In contrast, a customer service assistant that is answering customer questions live, retrieving information from a highly efficient search index, and providing human-like responses needs to be fast, friendly, and efficient. For these scenarios, OpenAI’s GPT-4.1 is a far better option.

Choosing the right model for your use case can be a challenging endeavor, so we’ve created this simple guide to help you pick between the two latest flagship models from OpenAI – GPT-5 and GPT-4.1.

Microsoft Foundry offers multiple variants of generative AI models to meet diverse customer needs. Two of the most widely used models—**GPT-5** and **GPT-4.1**—serve different purposes depending on your workload, latency sensitivity, and reasoning requirements.

* **GPT-5** is optimized for advanced enterprise use cases such as code generation and review, agentic tool calling, and business research. It excels in structured reasoning, multi-step logic, and planning tasks, making it ideal for Copilot-style applications that require deep understanding and orchestration. While it delivers significantly improved accuracy and contextual awareness, it may introduce higher latency due to its reasoning depth and model complexity.
* **GPT-4.1** is optimized for high-speed, high-throughput enterprise applications such as real-time chat, customer support, and lightweight summarization. It delivers fast, concise responses with low latency, making it ideal for latency-sensitive workloads and high-volume deployments. While it does not offer the deep reasoning capabilities of GPT-5, GPT-4.1 excels in responsiveness, cost efficiency, and predictable performance across a wide range of general-purpose tasks.

This guide helps you understand the differences and choose the right model for your use case.

## GPT-5 vs GPT-4.1 comparison

| **Feature**      | **GPT-5**                                                                              | **GPT-4.1**                                                                            |
| ---------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| **Model Type**   | Reasoning                                                                              | Non-reasoning, fast response                                                           |
| **Best For**     | Complex reasoning, multi-hop logic, thinking                                           | Real-time chat, short factual queries, high-throughput workloads                       |
| **Latency**      | Higher (due to deeper reasoning and longer outputs)                                    | Lower (optimized for speed and responsiveness)                                         |
| **Throughput**   | Moderate                                                                               | High                                                                                   |
| **Token Length** | 272K tokens in, 128K tokens out (400K total)                                           | 128 K (short context), up to 1M (long-context)                                         |
| **Perspective**  | Structured, analytical, step-by-step                                                   | Concise, fast, conversational                                                          |
| **Cost**         | [Cost](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) | [Cost](https://azure.microsoft.com/pricing/details/cognitive-services/openai-service/) |
| **Variants**     | GPT-5 GPT-5-mini GPT-5-nano                                                            | GPT-4.1 GPT-4.1-mini GPT-4.1-nano                                                      |

## GPT-5 thinking levels trade-offs

| **Reasoning Effort** | **Description**                                                                       | **Depth of Reasoning** | **Latency** | **Cost** | **Accuracy / Reliability** | **Typical Use Cases**                          |
| -------------------- | ------------------------------------------------------------------------------------- | ---------------------- | ----------- | -------- | -------------------------- | ---------------------------------------------- |
| **Minimal**          | Few or no internal reasoning tokens; optimized for throughput and time-to-first-token | Very shallow           | Fastest     | Lowest   | Lowest on complex tasks    | Bulk operations, simple transforms             |
| **Low**              | Light reasoning with quick judgment                                                   | Shallow to light       | Fast        | Low      | Moderate                   | Triage, short answers, simple edits            |
| **Medium (Default)** | Balanced depth vs. speed; safe general-purpose choice                                 | Moderate               | Moderate    | Medium   | Good for most tasks        | Content drafting, moderate coding, RAG Q\&A    |
| **High**             | Deep, multistep “think-through” for hardest problems                                  | Deep                   | Slowest     | Highest  | Highest                    | Complex planning, analysis, multihop reasoning |

**Notes:**

* The pattern above applies to GPT-5, GPT-5-mini, and GPT-5-nano; absolute latency and cost scale down with *mini* and *nano* but the tradeoffs are the same.
* **Parallel tool calls are not supported at Minimal reasoning\_effort.** If you need parallel tool use, choose **Low/Medium/High**.

## When to use GPT-5

Choose GPT-5 if your application requires:

* **Deep, multistep reasoning** for hard problems (planning, analysis, complex synthesis and summarization).
* **Reliability over raw speed**—GPT-5 delivers higher quality and fewer mistakes than prior generations in many tasks, particularly when reasoning is enabled.
* **Agentic workflows** for Copilot-style tools that must plan, call multiple tools, and act, benefit from GPT-5’s planning ("preamble") and robust tool use.
* **Nuanced intent understanding and structured follow-ups**: use **structured outputs** for predictable formats and **verbosity** to control response length.

*Example Use Cases:*

* Legal or financial document analysis
* Technical troubleshooting assistants
* Enterprise Copilots with multi-turn logic
* Research summarization and synthesis

## When to use GPT-4.1

Choose GPT-4.1 if your application needs:

* **Low latency**: Ideal for real-time interactions or user-facing chatbots.
* **High throughput**: Supports large-scale deployments with cost efficiency.
* **Long-context handling**: Use GPT-4.1 long-context for inputs up to 1M tokens.
* **Short, factual responses**: Great for Q\&A, search, and summarization of short content.

*Example Use Cases:*

* Customer support chatbots
* Real-time product recommendation engines
* High-volume summarization pipelines
* Lightweight assistants for internal tools

If you're unsure which model to choose, try [Model Router](https://ai.azure.com/catalog/models/model-router) in Foundry for a ready-to-use solution. Developers can use the model router in Foundry Models to maximize the capabilities of the GPT-5 family models (and other models in Foundry Models) while saving up to 60% on inferencing cost with comparable quality. [How to use model router for Foundry (preview) – Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/model-router)

## Latency considerations

Understanding the latency differences between GPT-5 and GPT-4.1 is key to selecting the right model for your needs. GPT-5 delivers powerful reasoning and deeper analysis, but this comes with slightly longer wait times before you see your first response, especially for shorter prompts. You may notice that interactions feel slower when accuracy and complex problem-solving are prioritized.

In contrast, GPT-4.1 offers a snappier and more responsive experience, making it ideal for real-time chats, quick Q\&A, and high-volume tasks where speed matters most. If your workflow requires instant feedback and low latency, GPT-4.1 is recommended. However, for tasks where advanced reasoning and accuracy are critical—even if responses take a bit longer—GPT-5 is the preferred choice. This trade-off ensures you get the right balance of speed and intelligence for your specific use case.

| **Metric**                     | **GPT-5**                                         | **GPT-4.1**                 |
| ------------------------------ | ------------------------------------------------- | --------------------------- |
| **TTFT (Time to First Token)** | Higher (due to deeper model layers and reasoning) | Lower                       |
| **TBT (Time Between Tokens)**  | Moderate to high                                  | Low                         |
| **User Perception**            | May feel slower, especially for short prompts     | Feels snappy and responsive |

If you wish to utilize the advanced features of GPT-5 while ensuring consistent latency, we recommend selecting the [Provisioned Throughput](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/provisioned-throughput?context=%2Fazure%2Fai-foundry%2Fcontext%2Fcontext\&tabs=global-ptum) deployment type. This option provides specific latency service level agreements (SLAs) for latency and is well-suited to use cases where latency sensitivity is critical. [Get started with Provisioned Throughput.](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/provisioned-get-started?context=%2Fazure%2Fai-foundry%2Fcontext%2Fcontext)

Azure OpenAI in Microsoft Foundry Models is powered by a diverse set of models with different capabilities and price points. [Model availability varies by region](../../foundry-models/concepts/models-sold-directly-by-azure).

You can get a list of models that are available for both inference and fine-tuning by your Azure OpenAI resource by using the [Models List API](https://learn.microsoft.com/en-us/rest/api/azureopenai/models/list).

This article shows you how to:

* Configure automatic model updates.
* View and update a deployment's version upgrade policy.
* Update a deployed model version by using the Azure Resource Manager API.
* Migrate provisioned deployments to a different model version or family.

## Prerequisites

* An Azure subscription with an Azure OpenAI models.

## Model updates

Azure OpenAI supports automatic updates for select model deployments. On models where automatic update support is available, a model version upgrade policy drop-down is available.

You can learn more about Azure OpenAI model versions and how they work in the [Azure OpenAI model versions](../concepts/model-versions) article.

<Callout type="note">
  Automatic model updates are only supported for Standard deployment types. For more information on how to manage model updates and migrations on provisioned deployment types, refer to the section on [managing models on provisioned deployment types](working-with-models#managing-models-on-provisioned-deployment-types)
</Callout>

### Auto update to default

When you set your deployment to **Auto-update to default**, your model deployment is automatically updated within two weeks of a change in the default version. For a preview version, it updates automatically when a new preview version is available starting two weeks after the new preview version is released.

If you're still in the early testing phases for inference models, we recommend deploying models with **auto-update to default** set whenever it's available.

### Specific model version

As your use of Azure OpenAI evolves, and you start to build and integrate with applications you might want to manually control model updates. You can first test and validate that your application behavior is consistent for your use case before upgrading.

When you select a specific model version for a deployment, this version remains selected until you either choose to manually update yourself, or once you reach the retirement date for the model. When the retirement date is reached the model will automatically upgrade to the default version at the time of retirement.

## Model deployment upgrade configuration

You can check what model upgrade options are set for previously deployed models using REST, Azure CLI, and Azure PowerShell, as well as with the Foundry portal.

The corresponding property can also be accessed via [REST](working-with-models#model-deployment-upgrade-configuration), [Azure PowerShell](https://learn.microsoft.com/en-us/powershell/module/az.cognitiveservices/get-azcognitiveservicesaccountdeployment), and [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/cognitiveservices/account/deployment#az-cognitiveservices-account-deployment-show).

| Option                                                                                                                                     | Read                                                                | Update                                                                  |
| ------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------- | ----------------------------------------------------------------------- |
| [REST](working-with-models#model-deployment-upgrade-configuration)                                                                         | Yes. If `versionUpgradeOption` isn't returned, it means it's `null` | Yes                                                                     |
| [Azure PowerShell](https://learn.microsoft.com/en-us/powershell/module/az.cognitiveservices/get-azcognitiveservicesaccountdeployment)      | Yes.`VersionUpgradeOption` can be checked for `$null`               | Yes                                                                     |
| [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/cognitiveservices/account/deployment#az-cognitiveservices-account-deployment-show) | Yes. It shows `null` if `versionUpgradeOption` isn't set.           | *No.* It's currently not possible to update the version upgrade option. |

There are three distinct model deployment upgrade options:

| Name                             | Description                                                                                                                                                                                                                    |
| -------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| `OnceNewDefaultVersionAvailable` | Once a new version is designated as the default, the model deployment automatically upgrades to the default version within two weeks of that designation change being made.                                                    |
| `OnceCurrentVersionExpired`      | Once the retirement date is reached the model deployment automatically upgrades to the current default version.                                                                                                                |
| `NoAutoUpgrade`                  | The model deployment never automatically upgrades. Once the retirement date is reached the model deployment stops working. You need to update your code referencing that deployment to point to a nonexpired model deployment. |

<Callout type="note">
  `null` is equivalent to `OnceCurrentVersionExpired`. If the **Version update policy** option isn't present in the properties for a model that supports model upgrades this indicates the value is currently `null`. Once you explicitly modify this value, the property is visible in the studio properties page as well as via the REST API.
</Callout>

### Examples

<Tabs>
  <Tab title="PowerShell">
    Review the Azure PowerShell [getting started guide](https://learn.microsoft.com/en-us/powershell/azure/get-started-azureps) to install Azure PowerShell locally or you can use the [Azure Cloud Shell](https://learn.microsoft.com/en-us/azure/cloud-shell/overview).

    The steps below demonstrate checking the `VersionUpgradeOption` option property as well as updating it:

    ```powershell
    # Step 1: Get deployment
    $deployment = Get-AzCognitiveServicesAccountDeployment -ResourceGroupName {ResourceGroupName} -AccountName {AccountName} -Name {DeploymentName}

    # Step 2: Show VersionUpgradeOption
    $deployment.Properties.VersionUpgradeOption

    # VersionUpgradeOption can be null. One way to check is:
    $null -eq $deployment.Properties.VersionUpgradeOption

    # Step 3: Update VersionUpgradeOption
    $deployment.Properties.VersionUpgradeOption = "NoAutoUpgrade"
    New-AzCognitiveServicesAccountDeployment -ResourceGroupName {ResourceGroupName} -AccountName {AccountName} -Name {DeploymentName} -Properties $deployment.Properties -Sku $deployment.Sku

    # Repeat steps 1 and 2 to confirm the change.
    # If you aren't sure about the deployment name, list all deployments under an account:
    Get-AzCognitiveServicesAccountDeployment -ResourceGroupName {ResourceGroupName} -AccountName {AccountName}
    ```

    ```powershell
    # Update to a new model version

    # Step 1: Get deployment
    $deployment = Get-AzCognitiveServicesAccountDeployment -ResourceGroupName {ResourceGroupName} -AccountName {AccountName} -Name {DeploymentName}

    # Step 2: Show the current model version
    $deployment.Properties.Model.Version

    # Step 3: Update the model version
    $deployment.Properties.Model.Version = "0613"
    New-AzCognitiveServicesAccountDeployment -ResourceGroupName {ResourceGroupName} -AccountName {AccountName} -Name {DeploymentName} -Properties $deployment.Properties -Sku $deployment.Sku

    # Repeat steps 1 and 2 to confirm the change.
    ```
  </Tab>

  <Tab title="REST">
    To query the current model deployment settings including the deployment upgrade configuration for a given resource use [`Deployments List`](https://learn.microsoft.com/en-us/rest/api/aiservices/accountmanagement/deployments/list?tabs=HTTP#code-try-0). If the value is null, you won't see a `versionUpgradeOption` property.

    ```http
    GET https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/deployments?api-version=2023-05-01
    ```

    **Path parameters**

    | Parameter           | Type   | Required? | Description                                                                    |
    | ------------------- | ------ | --------- | ------------------------------------------------------------------------------ |
    | `accountName`       | string | Required  | The name of your Azure OpenAI resource.                                        |
    | `resourceGroupName` | string | Required  | The name of the associated resource group for this model deployment.           |
    | `subscriptionId`    | string | Required  | Subscription ID for the associated subscription.                               |
    | `api-version`       | string | Required  | The API version to use for this operation. This follows the YYYY-MM-DD format. |

    **Supported versions**

    * `2025-06-01` [Swagger spec](https://github.com/Azure/azure-rest-api-specs/blob/main/specification/cognitiveservices/resource-manager/Microsoft.CognitiveServices/stable/2025-06-01/cognitiveservices.json)

    ### Example response

    ```json
    {
      "value": [
        {
          "id": "/subscriptions/aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeeb/resourceGroups/az-test-openai/providers/Microsoft.CognitiveServices/accounts/aztestopenai001/deployments/gpt-35-turbo",
          "type": "Microsoft.CognitiveServices/accounts/deployments",
          "name": "gpt-35-turbo",
          "sku": {
            "name": "Standard",
            "capacity": 80
          },
          "properties": {
            "model": {
              "format": "OpenAI",
              "name": "gpt-35-turbo",
              "version": "0301"
            },
            "versionUpgradeOption": "OnceNewDefaultVersionAvailable",
            "capabilities": {
              "completion": "true",
              "chatCompletion": "true"
            },
            "raiPolicyName": "Microsoft.Default",
            "provisioningState": "Succeeded",
            "rateLimits": [
              {
                "key": "request",
                "renewalPeriod": 10,
                "count": 80
              },
              {
                "key": "token",
                "renewalPeriod": 60,
                "count": 80000
              }
            ]
          },
          "systemData": {
            "createdBy": "docs@contoso.com",
            "createdByType": "User",
            "createdAt": "2023-07-31T16:45:32.622404Z",
            "lastModifiedBy": "docs@contoso.com",
            "lastModifiedByType": "User",
            "lastModifiedAt": "2023-10-31T13:59:34.4978286Z"
          },
          "etag": "\"aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee\""
        }
      ]
    }
    ```

    You can then take the settings from this list to construct an update model REST API call as described below if you want to modify the deployment upgrade configuration.
  </Tab>
</Tabs>

## Update & deploy models via the API

```http
PUT https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/deployments/{deploymentName}?api-version=2025-06-01
```

**Path parameters**

| Parameter           | Type   | Required? | Description                                                                                                                  |
| ------------------- | ------ | --------- | ---------------------------------------------------------------------------------------------------------------------------- |
| `accountName`       | string | Required  | The name of your Azure OpenAI resource.                                                                                      |
| `deploymentName`    | string | Required  | The deployment name you chose when you deployed an existing model or the name you would like a new model deployment to have. |
| `resourceGroupName` | string | Required  | The name of the associated resource group for this model deployment.                                                         |
| `subscriptionId`    | string | Required  | Subscription ID for the associated subscription.                                                                             |
| `api-version`       | string | Required  | The API version to use for this operation. This follows the YYYY-MM-DD format.                                               |

**Supported versions**

* `2025-06-01` [Swagger spec](https://github.com/Azure/azure-rest-api-specs/blob/main/specification/cognitiveservices/resource-manager/Microsoft.CognitiveServices/stable/2025-06-01/cognitiveservices.json)

**Request body**

This is only a subset of the available request body parameters. For the full list of the parameters, you can refer to the [REST API reference documentation](https://learn.microsoft.com/en-us/rest/api/aiservices/accountmanagement/deployments/create-or-update).

| Parameter            | Type    | Description                                                                                                                         |
| -------------------- | ------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| versionUpgradeOption | String  | Deployment model version upgrade options: `OnceNewDefaultVersionAvailable` `OnceCurrentVersionExpired` `NoAutoUpgrade`              |
| capacity             | integer | This represents the amount of [quota](quota) you're assigning to this deployment. A value of 1 equals 1,000 Tokens per Minute (TPM) |

#### Example request

```bash
curl -X PUT https://management.azure.com/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/resource-group-temp/providers/Microsoft.CognitiveServices/accounts/docs-openai-test-001/deployments/gpt-35-turbo?api-version=2025-06-01 \
  -H "Content-Type: application/json" \
  -H 'Authorization: Bearer YOUR_AUTH_TOKEN' \
  -d '{"sku":{"name":"Standard","capacity":120},"properties": {"model": {"format": "OpenAI","name": "gpt-35-turbo","version": "0613"},"versionUpgradeOption":"OnceCurrentVersionExpired"}}'
```

<Callout type="note">
  There are multiple ways to generate an authorization token. The easiest method for initial testing is to launch the Cloud Shell from the [Azure portal](https://portal.azure.com). Then run [`az account get-access-token`](https://learn.microsoft.com/en-us/cli/azure/account?view=azure-cli-latest#az-account-get-access-token\&preserve-view=true). You can use this token as your temporary authorization token for API testing.
</Callout>

#### Example response

```json
 {
  "id": "/subscriptions/{subscription-id}/resourceGroups/resource-group-temp/providers/Microsoft.CognitiveServices/accounts/docs-openai-test-001/deployments/gpt-35-turbo",
  "type": "Microsoft.CognitiveServices/accounts/deployments",
  "name": "gpt-35-turbo",
  "sku": {
    "name": "Standard",
    "capacity": 120
  },
  "properties": {
    "model": {
      "format": "OpenAI",
      "name": "gpt-35-turbo",
      "version": "0613"
    },
    "versionUpgradeOption": "OnceCurrentVersionExpired",
    "capabilities": {
      "chatCompletion": "true"
    },
    "provisioningState": "Succeeded",
    "rateLimits": [
      {
        "key": "request",
        "renewalPeriod": 10,
        "count": 120
      },
      {
        "key": "token",
        "renewalPeriod": 60,
        "count": 120000
      }
    ]
  },
  "systemData": {
    "createdBy": "docs@contoso.com",
    "createdByType": "User",
    "createdAt": "2023-02-28T02:57:15.8951706Z",
    "lastModifiedBy": "docs@contoso.com",
    "lastModifiedByType": "User",
    "lastModifiedAt": "2023-10-31T15:35:53.082912Z"
  },
  "etag": "\"GUID\""
}
```

## Managing models on provisioned deployment types

Provisioned deployments support distinct model management practices. Provisioned deployment model management practices are intended to give you the greatest control over when and how you migrate between model versions and model families. Currently, there are two approaches available to manage models on provisioned deployments: (1) in-place migrations and (2) multi-deployment migrations.

### Prerequisites

* Validate that the target model version or model family is supported for your existing deployment type. Migrations can only occur between provisioned deployments of the same deployment type. For more information on deployment types, review the [deployment type documentation](../../foundry-models/concepts/deployment-types).
* Validate capacity availability for your target model version or model family prior to attempting a migration. For more information on determining capacity availability, review the [capacity transparency documentation](../concepts/provisioned-throughput#capacity-transparency).
* For multi-deployment migrations, validate that you have sufficient quota to support multiple deployments simultaneously. For more information on how to validate quota for each provisioned deployment type, review the [provisioned throughput cost documentation](provisioned-throughput-onboarding).

### In-place migrations for provisioned deployments

In-place migrations allow you to maintain the same provisioned deployment name and size while changing the model version or model family assigned to that deployment. With in-place migrations, Azure OpenAI takes care of migrating any existing traffic between model versions or model families throughout the migration over a 20-30 minute window. Throughout the migration window, your provisioned deployment will display an "updating" provisioned state. You can continue to use your provisioned deployment as you normally would. Once the in-place migration is complete, the provisioned state will be updated to "succeeded", indicating that all traffic has been migrated over to the target model version or model family.

#### In-place migration: model version update

In-place migrations that target updating an existing provisioned deployment to a new model version within the same model family are supported through Foundry, REST API, and Azure CLI. To perform an in-place migration targeting a model version update within Foundry, select **Deployments** > under the deployment name column select the deployment name of the provisioned deployment you would like to migrate.

Selecting a deployment name opens the **Properties** for the model deployment. From this view, select the **edit** button, which will show the **Update deployment** dialogue box. Select the model version dropdown to set a new model version for the provisioned deployment. As noted, the provisioning state will change to "updating" during the migration and will revert to "succeeded" once the migration is complete.

#### In-place migration: model family change

In-place migrations that target updating an existing provisioned deployment to a new model family are supported through REST API and Azure CLI. To perform an in-place migration targeting a model family change, use the example request below as a guide. In the request, you'll need to update the model name and model version for the target model you're migrating to.

```bash
curl -X PUT https://management.azure.com/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/resource-group-temp/providers/Microsoft.CognitiveServices/accounts/docs-openai-test-001/deployments/gpt-4o-ptu-deployment?api-version=2024-10-01 \
  -H "Content-Type: application/json" \
  -H 'Authorization: Bearer YOUR_AUTH_TOKEN' \
  -d '{"sku":{"name":"GlobalProvisionedManaged","capacity":100},"properties": {"model": {"format": "OpenAI","name": "gpt-4o-mini","version": "2024-07-18"}}}'
```

#### Example response

```json
{
  "id": "/subscriptions/{subscription-id}/resourceGroups/resource-group-temp/providers/Microsoft.CognitiveServices/accounts/docs-openai-test-001/deployments/gpt-4o-ptu-deployment",
  "type": "Microsoft.CognitiveServices/accounts/deployments",
  "name": "gpt-4o-ptu-deployment",
  "sku": {
    "name": "GlobalProvisionedManaged",
    "capacity": 100
  },
  "properties": {
    "model": {
      "format": "OpenAI",
      "name": "gpt-4o-mini",
      "version": "2024-07-18"
    },
    "versionUpgradeOption": "OnceCurrentVersionExpired",
    "currentCapacity": 100,
    "capabilities": {
      "area": "EUR",
      "chatCompletion": "true",
      "jsonObjectResponse": "true",
      "maxContextToken": "128000",
      "maxOutputToken": "16834",
      "assistants": "true"
    },
    "provisioningState": "Updating",
    "rateLimits": [
      {
        "key": "request",
        "renewalPeriod": 10,
        "count": 300
      }
    ]
  },
  "systemData": {
    "createdBy": "docs@contoso.com",
    "createdByType": "User",
    "createdAt": "2025-01-28T02:57:15.8951706Z",
    "lastModifiedBy": "docs@contoso.com",
    "lastModifiedByType": "User",
    "lastModifiedAt": "2025-01-29T15:35:53.082912Z"
  },
  "etag": "\"GUID\""
}
```

<Callout type="note">
  There are multiple ways to generate an authorization token. The easiest method for initial testing is to launch the Cloud Shell from the [Azure portal](https://portal.azure.com). Then run [`az account get-access-token`](https://learn.microsoft.com/en-us/cli/azure/account?view=azure-cli-latest#az-account-get-access-token\&preserve-view=true). You can use this token as your temporary authorization token for API testing.
</Callout>

### Multi-deployment migrations for provisioned deployments

Multi-deployment migrations allow you to have greater control over the model migration process. With multi-deployment migrations, you can dictate how quickly you would like to migrate your existing traffic to the target model version or model family on a new provisioned deployment. The process to migrate to a new model version or model family using the multi-deployment migration approach is as follows:

* Create a new provisioned deployment. For this new deployment, you can choose to maintain the same provisioned deployment type as your existing deployment or select a new deployment type if desired.
* Transition traffic from the existing provisioned deployment to the newly created provisioned deployment with your target model version or model family until all traffic is offloaded from the original deployment.
* Once traffic is migrated over to the new deployment, validate that there are no inference requests being processed on the previous provisioned deployment by ensuring the Azure OpenAI Requests metric doesn't show any API calls made within 5-10 minutes of the inference traffic being migrated over to the new deployment. For more information on this metric, [see the Monitor Azure OpenAI documentation](https://aka.ms/aoai/docs/monitor-azure-openai).
* Once you confirm that no inference calls have been made, delete the original provisioned deployment.

## Troubleshooting

### You get 401 or 403 responses from the Azure Resource Manager API

* Confirm your access token is valid and unexpired.
* Confirm you have permission to read and update deployments for the resource.

Azure OpenAI offers a variety of models for different use cases. The following models are no longer available for deployment.

## Retired models

These models are retired and are no longer available for use or for new deployments.

| Model                                    | Deprecation date  | Retirement date   | Suggested replacement                                                                                 |
| ---------------------------------------- | ----------------- | ----------------- | ----------------------------------------------------------------------------------------------------- |
| `o1-preview`                             |                   | July 28, 2025     | `o1`                                                                                                  |
| `gpt-4.5-preview`                        |                   | July 14, 2025     | `gpt-4.1` version: `2025-04-14`                                                                       |
| `gpt-4o-realtime-preview` - `2024-10-01` | February 25, 2025 | March 26, 2025    | `gpt-4o-realtime-preview` (version 2024-12-17) or `gpt-4o-mini-realtime-preview` (version 2024-12-17) |
| `gpt-35-turbo` - 0301                    |                   | February 13, 2025 | `gpt-35-turbo` (0125) `gpt-4o-mini`                                                                   |
| `gpt-35-turbo` - 0613                    |                   | February 13, 2025 | `gpt-35-turbo` (0125) `gpt-4o-mini`                                                                   |
| `gpt-4` `gpt-4-32k` - 0314               |                   | June 6, 2025      | `gpt-4o` version: `2024-11-20`                                                                        |
| `gpt-4` `gpt-4-32k` - 0613               |                   | June 6, 2025      | `gpt-4o` version: `2024-11-20`                                                                        |
| `gpt-35-turbo-16k` - 0613                |                   | April 30, 2025    | `gpt-4.1-mini` version: `2025-04-14`                                                                  |
| `babbage-002`                            |                   | January 27, 2025  |                                                                                                       |
| `davinci-002`                            |                   | January 27, 2025  |                                                                                                       |
| `dall-e-2`                               |                   | January 27, 2025  | `dall-e-3`                                                                                            |
| `ada`                                    | July 6, 2023      | June 14, 2024     |                                                                                                       |
| `babbage`                                | July 6, 2023      | June 14, 2024     |                                                                                                       |
| `curie`                                  | July 6, 2023      | June 14, 2024     |                                                                                                       |
| `davinci`                                | July 6, 2023      | June 14, 2024     |                                                                                                       |
| `text-ada-001`                           | July 6, 2023      | June 14, 2024     | gpt-35-turbo-instruct                                                                                 |
| `text-babbage-001`                       | July 6, 2023      | June 14, 2024     | gpt-35-turbo-instruct                                                                                 |
| `text-curie-001`                         | July 6, 2023      | June 14, 2024     | gpt-35-turbo-instruct                                                                                 |
| `text-davinci-002`                       | July 6, 2023      | June 14, 2024     | gpt-35-turbo-instruct                                                                                 |
| `text-davinci-003`                       | July 6, 2023      | June 14, 2024     | gpt-35-turbo-instruct                                                                                 |
| `code-cushman-001`                       | July 6, 2023      | June 14, 2024     | gpt-35-turbo-instruct                                                                                 |
| `code-davinci-002`                       | July 6, 2023      | June 14, 2024     | gpt-35-turbo-instruct                                                                                 |
| `text-similarity-ada-001`                | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-similarity-babbage-001`            | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-similarity-curie-001`              | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-similarity-davinci-001`            | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-search-ada-doc-001`                | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-search-ada-query-001`              | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-search-babbage-doc-001`            | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-search-babbage-query-001`          | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-search-curie-doc-001`              | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-search-curie-query-001`            | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-search-davinci-doc-001`            | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `text-search-davinci-query-001`          | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `code-search-ada-code-001`               | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `code-search-ada-text-001`               | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `code-search-babbage-code-001`           | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |
| `code-search-babbage-text-001`           | July 6, 2023      | June 14, 2024     | text-embedding-3-small                                                                                |

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Model router is a trained language model that intelligently routes your prompts in real time to the most suitable large language model (LLM). You deploy model router like any other Foundry model. Thus, it delivers high performance while saving on costs, reducing latencies, and increasing responsiveness, while maintaining comparable quality, all packaged as a single model deployment.

<Callout type="note">
  You do not need to separately deploy the supported LLMs for use with model router, with the exception of the Claude models. To use model router with your Claude models, first deploy them from the model catalog. The deployments are invoked by model router if they're selected for routing.
</Callout>

To try model router quickly, follow [How to use model router](../how-to/model-router). After you deploy model router, send a request to the deployment. Model router selects an underlying model for each request based on your routing settings.

## How model router works

As a trained language model, model router analyzes your prompts in real time based on complexity, reasoning, task type, and other attributes. It does not store your prompts. It routes only to eligible models based on your access and deployment types, honoring data zone boundaries.

<Callout type="important">
  The effective context window is limited by the smallest underlying model. For larger contexts, use [model subset](#model-subset) to select models that support your requirements.
</Callout>

* In Balanced mode (default), it considers all underlying models within a small quality range (for example, 1% to 2% compared with the highest-quality model for that prompt) and picks the most cost-effective model.
* In Cost mode, it considers a larger quality band (for example, 5% to 6% compared with the highest-quality model for that prompt) and chooses the most cost-effective model.
* In Quality mode, it picks the highest quality rated model for the prompt, ignoring the cost.

## Why use model router?

Model router optimizes costs and latencies while maintaining comparable quality. Smaller and cheaper models are used when they're sufficient for the task, but larger and more expensive models are available for more complex tasks. Also, reasoning models are available for tasks that require complex reasoning, and non-reasoning models are used otherwise. Model router provides a single deployment and chat experience that combines the best features from all of the underlying chat models.

The latest version, `2025-11-18` adds several capabilities:

1. Support Global Standard and Data Zone Standard deployments.
2. Adds support for new models: `grok-4`, `grok-4-fast-reasoning`, `DeepSeek-V3.1`, `gpt-oss-120b`, `Llama-4-Maverick-17B-128E-Instruct-FP8`, `claude-haiku-4-5`, `claude-opus-4-1`, and `claude-sonnet-4-5`.
3. Quick deploy or Custom deploy with **routing mode** and **model subset** options.
4. **Routing mode**: Optimize the routing logic for your needs. Supported options: `Quality`, `Cost`, `Balanced` (default).
5. **Model subset**: Select your preferred models to create your model subset for routing.
6. Support for agentic scenarios including tools so you can now use it in the Foundry Agent Service.

## Versioning

Each version of model router is associated with a specific set of underlying models and their versions. This set is fixed—only newer versions of model router can expose new underlying models.

If you select **Auto-update** at the deployment step (see [Model updates](../how-to/working-with-models#model-updates)), then your model router model automatically updates when new versions become available. When that happens, the set of underlying models also changes, which could affect the overall performance of the model and costs.

## Supported underlying models

With the `2025-11-18` version, Model Router adds nine new models including Anthropic's Claude, DeepSeek, Llama, Grok models to support a total of 18 models available for routing your prompts.

<Callout type="note">
  You don't need to separately deploy the supported LLMs for use with model router, with the exception of the Claude models. To use model router with your Claude models, first deploy them from the model catalog. The deployments will get invoked by Model router if they're selected for routing.
</Callout>

| Model router version | Underlying models                                                                                                                                                                                                                                   |                                                                                   Underlying model version                                                                                   |
| :------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|     `2025-11-18`     | `gpt-4.1` `gpt-4.1-mini` `gpt-4.1-nano` `o4-mini` `gpt-5-nano` `gpt-5-mini` `gpt-5`1 `gpt-5-chat` `Deepseek-v3.1`2 `gpt-oss-120b`2 `llama4-maverick-instruct`2 `grok-4`2 `grok-4-fast`2 `claude-haiku-4-5`3 `claude-opus-4-1`3 `claude-sonnet-4-5`3 | `2025-04-14` `2025-04-14` `2025-04-14` `2025-04-16` `2025-08-07` `2025-08-07` `2025-08-07` `2025-08-07` N/A N/A N/A N/A N/A `2024-11-20` `2024-07-18` `2025-10-01` `2025-08-05` `2025-09-29` |
|     `2025-08-07`     | `gpt-4.1` `gpt-4.1-mini` `gpt-4.1-nano` `o4-mini` `gpt-5`1 `gpt-5-mini` `gpt-5-nano` `gpt-5-chat`                                                                                                                                                   |                                            `2025-04-14` `2025-04-14` `2025-04-14` `2025-04-16` `2025-08-07` `2025-08-07` `2025-08-07` `2025-08-07`                                           |
|     `2025-05-19`     | `gpt-4.1` `gpt-4.1-mini` `gpt-4.1-nano` `o4-mini`                                                                                                                                                                                                   |                                                                      `2025-04-14` `2025-04-14` `2025-04-14` `2025-04-16`                                                                     |

* 1Requires registration.
* 2Model router support is in preview.
* 3Model router support is in preview. Requires deployment of model for use with Model router.

## Routing mode

With the latest version, if you choose custom deployment, you can select the **routing mode** to optimize for quality or cost while maintaining a baseline level of performance. Setting a routing mode is optional, and if you don’t set one, your deployment defaults to the Balanced mode.

Available routing modes:

| Mode               | Description                                                                          |
| ------------------ | ------------------------------------------------------------------------------------ |
| Balanced (default) | Considers both cost and quality dynamically. Perfect for general-purpose scenarios   |
| Quality            | Prioritizes for maximum accuracy. Best for complex reasoning or critical outputs     |
| Cost               | Prioritizes for more cost savings. Ideal for high-volume, budget-sensitive workloads |

## Model subset

The latest version of model router supports model subsets: For custom deployments, you can specify which underlying models to include in routing decisions. This gives you more control over cost, compliance, and performance characteristics.

When new base models become available, they're not included in your selection unless you explicitly add them to your deployment's inclusion list.

## Limitations

### Resource limitations

| Region         | Deployment types supported          |
| -------------- | ----------------------------------- |
| East US 2      | Global Standard, Data Zone Standard |
| Sweden Central | Global Standard, Data Zone Standard |

Also see [Azure OpenAI in Microsoft Foundry models](../../foundry-models/concepts/models-sold-directly-by-azure) for current region availability.

### Rate limits

|             Model             |  Deployment Type | Default RPM | Default TPM | Enterprise and MCA-E RPM | Enterprise and MCA-E TPM |
| :---------------------------: | ---------------: | :---------: | :---------: | :----------------------- | :----------------------: |
| `model-router` `(2025-11-18)` | DataZoneStandard |     150     |   150,000   | 300                      |          300,000         |
| `model-router` `(2025-11-18)` |   GlobalStandard |     250     |   250,000   | 400                      |          400,000         |

Also see [Quotas and limits](../quotas-limits) for rate limit information.

To overcome the limits on context window and parameters, use the Model subset feature to select your models for routing that support your desired properties.

<Callout type="note">
  The context window limit listed for model router is the limit of the smallest underlying model. Other underlying models are compatible with larger context windows, which means an API call with a larger context will succeed only if the prompt happens to be routed to the right model. To review context windows for the underlying models, see [Azure OpenAI in Microsoft Foundry models](../../foundry-models/concepts/models-sold-directly-by-azure).

  To shorten the context window, you can do one of the following:

  * Summarize the prompt before passing it to the model
  * Truncate the prompt into more relevant parts
  * Use document embeddings and have the chat model retrieve relevant sections. For more information, see [What is Azure AI Search?](../../../search/search-what-is-azure-search)
</Callout>

Model router accepts image inputs for [Vision enabled chats](../how-to/gpt-with-vision) (all of the underlying models can accept image input), but the routing decision is based on the text input only.

Model router doesn't process audio input.

## Troubleshooting

| Issue                      | Resolution                                                                                 |
| -------------------------- | ------------------------------------------------------------------------------------------ |
| Deployment fails           | Verify your Foundry resource is in East US 2 or Sweden Central.                            |
| Claude models not routing  | Ensure Claude models are deployed separately before enabling in model router.              |
| Context exceeded error     | Reduce prompt size or use model subset to select models with larger context windows.       |
| Unexpected model selection | Review your routing mode setting (Balanced, Cost, Quality) and model subset configuration. |

For detailed deployment troubleshooting, see [How to use model router](../how-to/model-router).

## Billing information

Starting November 2025, the model router usage will be charged for input prompts at the rate listed on the pricing page.

You can monitor the costs of your model router deployment in the Azure portal.

## Next step

<Callout type="nextstepaction">
  [How to use model router](../how-to/model-router)
</Callout>

This article provides a summary of the latest releases and major documentation updates for Azure model router, including new supported models, routing features, and deployment options.

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

## November 2025

### Anthropic models added

Version `2025-11-18` of model router adds support for three Anthropic models: `claude-haiku-4-5`, `claude-opus-4-1`, and `claude-sonnet-4-5`. To include these in your model router deployment, you need to first deploy them yourself to your Foundry resource (see [Deploy and use Claude models](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/how-to/use-foundry-models-claude?view=foundry\&tabs=python)). Then enable them with [model subset configuration](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/model-router?view=foundry) in your model router deployment.

### Model router GA version

A new model router model is now available. Version `2025-11-18` includes support for all underlying models in previous versions, as well as 10 new language models.

It also includes new features that make it more versatile and effective.

* **Routing profiles** let you skew model router's choices to optimize for quality or cost while maintaining a baseline level of performance.
* Model router supports **custom subsets**: you can specify which underlying models to include in routing decisions. This gives you more control over cost, compliance, and performance characteristics.
* Model router supports **Global Standard** and **Data Zone Standard** deployment types.

For more information on model router and its capabilities, see the [Model router concepts guide](../openai/concepts/model-router).

## August 2025

### New version of model router (preview)

* Model router now supports GPT-5 series models.

* Model router for Microsoft Foundry is a deployable AI chat model that automatically selects the best underlying chat model to respond to a given prompt. For more information on how model router works and its advantages and limitations, see the [Model router concepts guide](../openai/concepts/model-router). To use model router with the Completions API, follow the [How-to guide](../openai/how-to/model-router).

## May 2025

### Model router (preview)

Model router for Foundry is a deployable AI chat model that automatically selects the best underlying chat model to respond to a given prompt. For more information on how model router works and its advantages and limitations, see the [Model router concepts guide](../openai/concepts/model-router). To use model router with the Completions API, follow the [How-to guide](../openai/how-to/model-router).

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Model router for Microsoft Foundry is a deployable AI chat model that selects the best large language model (LLM) to respond to a prompt in real time. It uses different preexisting models to deliver high performance and save on compute costs, all in one model deployment. To learn more about how model router works, its advantages, and limitations, see the [Model router concepts guide](../concepts/model-router).

Use model router through the Chat Completions API like you'd use a single base model such as GPT-4. Follow the same steps as in the [Chat completions guide](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/chatgpt).

## Supported underlying models

With the `2025-11-18` version, Model Router adds nine new models including Anthropic's Claude, DeepSeek, Llama, Grok models to support a total of 18 models available for routing your prompts.

<Callout type="note">
  You don't need to separately deploy the supported LLMs for use with model router, with the exception of the Claude models. To use model router with your Claude models, first deploy them from the model catalog. The deployments will get invoked by Model router if they're selected for routing.
</Callout>

| Model router version | Underlying models                                                                                                                                                                                                                                   |                                                                                   Underlying model version                                                                                   |
| :------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|     `2025-11-18`     | `gpt-4.1` `gpt-4.1-mini` `gpt-4.1-nano` `o4-mini` `gpt-5-nano` `gpt-5-mini` `gpt-5`1 `gpt-5-chat` `Deepseek-v3.1`2 `gpt-oss-120b`2 `llama4-maverick-instruct`2 `grok-4`2 `grok-4-fast`2 `claude-haiku-4-5`3 `claude-opus-4-1`3 `claude-sonnet-4-5`3 | `2025-04-14` `2025-04-14` `2025-04-14` `2025-04-16` `2025-08-07` `2025-08-07` `2025-08-07` `2025-08-07` N/A N/A N/A N/A N/A `2024-11-20` `2024-07-18` `2025-10-01` `2025-08-05` `2025-09-29` |
|     `2025-08-07`     | `gpt-4.1` `gpt-4.1-mini` `gpt-4.1-nano` `o4-mini` `gpt-5`1 `gpt-5-mini` `gpt-5-nano` `gpt-5-chat`                                                                                                                                                   |                                            `2025-04-14` `2025-04-14` `2025-04-14` `2025-04-16` `2025-08-07` `2025-08-07` `2025-08-07` `2025-08-07`                                           |
|     `2025-05-19`     | `gpt-4.1` `gpt-4.1-mini` `gpt-4.1-nano` `o4-mini`                                                                                                                                                                                                   |                                                                      `2025-04-14` `2025-04-14` `2025-04-14` `2025-04-16`                                                                     |

* 1Requires registration.
* 2Model router support is in preview.
* 3Model router support is in preview. Requires deployment of model for use with Model router.

## Deploy a model router model

Model router is packaged as a single Foundry model that you deploy. Start by following the steps in the [resource deployment guide](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/create-resource).

In the model catalog, find `model-router` in the **Models** list and select it. Choose **Default settings** for the **Balanced** routing mode and route between all supported models. To enable more configuration options, choose **Custom settings**.

![Screenshot of model router deploy screen.](https://learn.microsoft.com/azure/ai-foundry/media/working-with-models/model-router-deploy.png)

<Callout type="note">
  Your deployment settings apply to all underlying chat models that model router uses.

  * Don't deploy the underlying chat models separately. Model router works independently of your other deployed models.
  * Select a content filter when you deploy the model router model or apply a filter later. The content filter applies to all content passed to and from the model router; don't set content filters for each underlying chat model.
  * Your tokens-per-minute rate limit setting applies to all activity to and from the model router; don't set rate limits for each underlying chat model.
</Callout>

### Select a routing mode

<Callout type="note">
  Changes to the routing mode can take up to five minutes to take effect.
</Callout>

Use the **Routing mode** dropdown to select a routing profile. This sets the routing logic for your deployment.

![Screenshot of model router routing mode selection.](https://learn.microsoft.com/azure/ai-foundry/media/working-with-models/model-router-routing-mode.png)

**When to use each mode:**

* **Balanced** (default): Most workloads. Optimizes cost while maintaining quality.
* **Quality**: Critical tasks like legal review, medical summaries, or complex reasoning.
* **Cost**: High-volume, budget-sensitive workloads like content classification or simple Q\&A.

### Select your model subset

<Callout type="note">
  Changes to the model subset can take up to five minutes to take effect.
</Callout>

The latest version of model router supports custom subsets: you can specify which underlying models to include in routing decisions. This gives you more control over cost, compliance, and performance characteristics.

In the model router deployment pane, select **Route to a subset of models**. Then select the underlying models you want to enable.

![Screenshot of model router subset selection.](https://learn.microsoft.com/azure/ai-foundry/media/working-with-models/model-router-model-subset.png)

<Callout type="important">
  To include models by Anthropic (Claude) in your model router deployment, you need to deploy them yourself to your Foundry resource. See [Deploy and use Claude models](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/how-to/use-foundry-models-claude?view=foundry\&preserve-view=true).
</Callout>

<Callout type="note">
  You must select at least one model for routing. If no models are selected, the deployment uses the default model set for your routing mode.
</Callout>

New models introduced later are excluded by default until explicitly added.

## Test model router with the Completions API

You can use model router through the [chat completions API](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/chatgpt-quickstart) in the same way you'd use other OpenAI chat models. Set the `model` parameter to the name of our model router deployment, and set the `messages` parameter to the messages you want to send to the model.

## Test model router in the playground

In the [Foundry portal](https://ai.azure.com/?cid=learnDocs), go to your model router deployment on the **Models + endpoints** page and select it to open the model playground. In the playground, enter messages and see the model's responses. Each response shows which underlying model the router selected.

<Callout type="important">
  You can set the `Temperature` and `Top_P` parameters to the values you prefer (see the [concepts guide](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering?tabs=chat#temperature-and-top_p-parameters)), but note that reasoning models (o-series) don't support these parameters. If model router selects a reasoning model for your prompt, it ignores the `Temperature` and `Top_P` input parameters.

  The parameters `stop`, `presence_penalty`, `frequency_penalty`, `logit_bias`, and `logprobs` are similarly dropped for o-series models but used otherwise.
</Callout>

<Callout type="important">
  Starting with the `2025-11-18` version, the `reasoning_effort` parameter (see the [Reasoning models guide](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/reasoning?tabs=python-secure#reasoning-effort)) is now **supported** in model router. If the model router selects a reasoning model for your prompt, it will use your `reasoning_effort` input value with the underlying model.
</Callout>

## Connect model router to a Foundry agent

If you've created an AI agent in Foundry, you can connect your model router deployment to be used as the agent's base model. Select it from the **model** dropdown menu in the agent playground. Your agent will have all the tools and instructions you've configured for it, but the underlying model that processes its responses will be selected by model router.

<Callout type="important">
  If you use Agent service tools in your flows, only OpenAI models will be used for routing.
</Callout>

### Output format

The JSON response you receive from a model router model is identical to the standard chat completions API response. Note that the `"model"` field reveals which underlying model was selected to respond to the prompt.

The following example response was generated using API version `2025-11-18`:

```json
{
  "choices": [
    {
      "content_filter_results": {
        "hate": {
          "filtered": "False",
          "severity": "safe"
        },
        "protected_material_code": {
          "detected": "False",
          "filtered": "False"
        },
        "protected_material_text": {
          "detected": "False",
          "filtered": "False"
        },
        "self_harm": {
          "filtered": "False",
          "severity": "safe"
        },
        "sexual": {
          "filtered": "False",
          "severity": "safe"
        },
        "violence": {
          "filtered": "False",
          "severity": "safe"
        }
      },
      "finish_reason": "stop",
      "index": 0,
      "logprobs": "None",
      "message": {
        "content": "I'm doing well, thank you! How can I assist you today?",
        "refusal": "None",
        "role": "assistant"
      }
    }
  ],
  "created": 1745308617,
  "id": "xxxx-yyyy-zzzz",
  "model": "gpt-4.1-nano-2025-04-14",
  "object": "chat.completion",
  "prompt_filter_results": [
    {
      "content_filter_results": {
        "hate": {
          "filtered": "False",
          "severity": "safe"
        },
        "jailbreak": {
          "detected": "False",
          "filtered": "False"
        },
        "self_harm": {
          "filtered": "False",
          "severity": "safe"
        },
        "sexual": {
          "filtered": "False",
          "severity": "safe"
        },
        "violence": {
          "filtered": "False",
          "severity": "safe"
        }
      },
      "prompt_index": 0
    }
  ],
  "system_fingerprint": "xxxx",
  "usage": {
    "completion_tokens": 15,
    "completion_tokens_details": {
      "accepted_prediction_tokens": 0,
      "audio_tokens": 0,
      "reasoning_tokens": 0,
      "rejected_prediction_tokens": 0
    },
    "prompt_tokens": 21,
    "prompt_tokens_details": {
      "audio_tokens": 0,
      "cached_tokens": 0
    },
    "total_tokens": 36
  }
}
```

## Monitor model router metrics

### Monitor performance

Monitor the performance of your model router deployment in Azure Monitor (AzMon) in the Azure portal.

1. Go to the **Monitoring** > **Metrics** page for your Azure OpenAI resource in the Azure portal.
2. Filter by the deployment name of your model router model.
3. Split the metrics by underlying models if needed.

### Monitor costs

You can monitor the costs of model router, which is the sum of the costs incurred by the underlying models.

1. Visit the **Resource Management** -> **Cost analysis** page in the Azure portal.
2. If needed, filter by Azure resource.
3. Then, filter by deployment name: Filter by "Tag", select **Deployment** as the type of the tag, and then select your model router deployment name as the value.

## Troubleshoot model router

### Common issues

| Issue                      | Cause                                                | Resolution                                                                     |
| -------------------------- | ---------------------------------------------------- | ------------------------------------------------------------------------------ |
| Rate limit exceeded        | Too many requests to model router deployment         | Increase tokens-per-minute quota or implement retry with exponential backoff   |
| Unexpected model selection | Routing logic selected different model than expected | Review routing mode settings; consider using model subset to constrain options |
| High latency               | Router overhead plus underlying model processing     | Use Cost mode for latency-sensitive workloads; smaller models respond faster   |
| Claude model not routing   | Claude models require separate deployment            | Deploy Claude models from model catalog before enabling in subset              |

### Error codes

For API error codes and troubleshooting, see the [Azure OpenAI REST API reference](../reference).

## Next steps

* [Model router concepts](../concepts/model-router) - Learn how routing modes work
* [Quotas and limits](../quotas-limits) - Rate limits for model router
* [Create an agent](../../agents/quickstart) - Use model router with Foundry agents

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

This article explains how to generate text responses for Foundry Models, such as Microsoft AI, DeepSeek, and Grok models, by using the Responses API. For a full list of the Foundry Models that support use of the Responses API, see [Supported Foundry Models](#supported-foundry-models).

## Prerequisites

To use the Responses API with deployed models in your application, you need:

* An Azure subscription. If you're using [GitHub Models](https://docs.github.com/en/github-models/), you can upgrade your experience and create an Azure subscription in the process. Read [Upgrade from GitHub Models to Microsoft Foundry Models](quickstart-github-models) if that's your case.

* A Foundry project. This kind of project is managed under a Foundry resource. If you don't have a Foundry project, see [Create a project for Microsoft Foundry](../../how-to/create-projects).

* Your Foundry project's endpoint URL, which is of the form `https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME`.

* A deployment of a Foundry Model, such as the `MAI-DS-R1` model used in this article. If you don't have a deployment already, see [Add and configure Foundry Models](create-model-deployments) to a model deployment to your resource.

## Use the Responses API to generate text

Use the code in this section to make Responses API calls for Foundry Models. In the code samples, you create the client to consume the model and then send it a basic request.

<Callout type="important">
  Some of the SDK packages used in these samples are currently in preview. API surface and behavior might change before general availability.
</Callout>

<Callout type="tip">
  When you deploy a model in the Foundry portal, you assign it a deployment name. Use this deployment name (not the model catalog ID) in the `model` parameter of your API calls.
</Callout>

<Callout type="note">
  Use keyless authentication with **Microsoft Entra ID**. To learn more about keyless authentication, see [What is Microsoft Entra authentication?](https://learn.microsoft.com/en-us/entra/identity/authentication/overview-authentication) and [DefaultAzureCredential](https://learn.microsoft.com/en-us/azure/developer/python/sdk/authentication/overview#defaultazurecredential).
</Callout>

<Tabs>
  <Tab title="Python">
    1. Install libraries, including the Azure Identity client library:

       ```bash
       pip install azure-identity
       pip install openai
       pip install --pre azure-ai-projects>=2.0.0b1
       ```

    2. Use the following code to configure the OpenAI client object in the project route, specify your deployment, and generate responses.

       ```python
       import os
       from azure.identity import DefaultAzureCredential
       from azure.ai.projects import AIProjectClient

       project_client = AIProjectClient(
           endpoint="https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME",
           credential=DefaultAzureCredential(),
       )

       openai_client = project_client.get_openai_client()

       response = openai_client.responses.create(
           model="MAI-DS-R1", # Replace with your deployment name, not the model ID
           input="What is the capital/major city of France?",
       )

       print(response.model_dump_json(indent=2))
       ```
  </Tab>

  <Tab title="C#">
    1. Install the Azure Identity client library:

       ```dotnetcli
       dotnet add package Azure.Identity
       ```

    2. Use the following code to configure the OpenAI client object in the project route, specify your deployment, and generate responses.

       ```csharp
       using Azure.Identity;
       using Azure.AI.Projects;
       using Azure.AI.Projects.OpenAI;
       using OpenAI.Responses;

       #pragma warning disable OPENAI001

       const string deploymentName = "MAIDSR1"; // Replace with your deployment name, not the model ID
       const string endpoint = "https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME";

       AIProjectClient projectClient = new(new Uri(endpoint), new DefaultAzureCredential());

       ProjectResponsesClient responseClient = projectClient.OpenAI.GetProjectResponsesClientForModel(deploymentName);

       ResponseResult response = responseClient.CreateResponse("What is the capital/major city of France?");

       Console.WriteLine($"[ASSISTANT]: {response.GetOutputText()}");
       ```
  </Tab>

  <Tab title="JavaScript">
    [JavaScript v1 examples](../../openai/supported-languages)

    1. Install the Azure Identity client library before you can use DefaultAzureCredential:

       ```bash
       npm install @azure/identity
       ```

    2. Use the following code to configure the OpenAI client object in the project route, specify your deployment, and generate responses.

       ```javascript
       import { AIProjectClient } from "@azure/ai-projects";
       import { DefaultAzureCredential } from "@azure/identity";

       const endpoint = "https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME";
       const deploymentName = "MAI-DS-R1"; // Replace with your deployment name, not the model ID

       async function main() {
           const projectClient = new AIProjectClient(endpoint, new DefaultAzureCredential());
           const openAIClient = await projectClient.getOpenAIClient();

           const response = await openAIClient.responses.create({
               model: deploymentName,
               input: "What is the capital/major city of France?"
           });
           console.log(response.output_text);
       }

       main();
       ```
  </Tab>

  <Tab title="Java">
    Authentication with Microsoft Entra ID requires some initial setup. First, install the Azure Identity client library. For more options on how to install this library, see [Azure Identity client library for Java](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/identity/azure-identity/README.md#include-the-package).

    1. Add the Azure Identity client library:

       ```xml
       <dependency>
           <groupId>com.azure</groupId>
           <artifactId>azure-identity</artifactId>
           <version>1.18.0</version>
       </dependency>
       ```

       After setup, choose which type of credential from `azure.identity` to use. For example, use `DefaultAzureCredential` to authenticate the client. Authentication is easiest with `DefaultAzureCredential`, as it finds the best credential to use in its running environment.

    2. Use the following code to configure the OpenAI client object in the project route, specify your deployment, and generate responses.

       ```java
       import com.azure.ai.agents;
       import com.azure.core.util.Configuration;
       import com.azure.identity.DefaultAzureCredentialBuilder;
       import com.openai.models.responses.Response;
       import com.openai.models.responses.ResponseCreateParams;

       public class Sample {
           public static void main(String[] args) {
               String endpoint = "https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME";
               String deploymentName = "MAI-DS-R1"; // Replace with your deployment name, not the model ID

               ResponsesClient responsesClient = new AgentsClientBuilder()
                       .credential(new DefaultAzureCredentialBuilder().build())
                       .endpoint(endpoint)
                       .serviceVersion(AgentsServiceVersion.V2025_11_15_PREVIEW)
                       .buildResponsesClient();

               ResponseCreateParams responseRequest = new ResponseCreateParams.Builder()
                       .input("What is the capital/major city of France?")
                       .model(deploymentName)
                       .build();

               Response response = responsesClient.getResponseService().create(responseRequest);
           }
       }
       ```
  </Tab>

  <Tab title="REST">
    ```bash
    curl -X POST https://YOUR-RESOURCE-NAME.services.ai.azure.com/api/projects/YOUR_PROJECT_NAME/openai/responses?api-version={{API_VERSION}} \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $AZURE_OPENAI_AUTH_TOKEN" \
    -d '{
        "model": "MAI-DS-R1",
        "input": "What is the capital/major city of France?"
    }'
    ```
  </Tab>
</Tabs>

The response includes the generated text along with model and usage metadata.

## Supported Foundry Models

A selection of Foundry Models are supported for use with the Responses API.

### View supported models in the Foundry portal

To see a full list of the supported models in the Foundry portal:

1. Sign in to [Microsoft Foundry](https://ai.azure.com/?cid=learnDocs). Make sure the **New Foundry** toggle is on. These steps refer to **Foundry (new)**.![](https://learn.microsoft.com/azure/ai-foundry/media/version-banner/new-foundry.png)
2. Select **Discover** in the upper-right navigation, then **Models** in the left pane.
3. Open the **Capabilities** dropdown and select the **Agent supported** filter.

### List of supported models

This section lists some of the Foundry Models that are supported for use with the Responses API. For the Azure OpenAI models that are supported, see [Available Azure OpenAI models](../../agents/concepts/model-region-support#available-models).

**Models sold directly by Azure:**

* **MAI-DS-R1**: Deterministic, precision-focused reasoning.
* **grok-4**: Frontier-scale reasoning for complex, multiple-step problem solving.
* **grok-4-fast-reasoning**: Accelerated agentic reasoning optimized for workflow automation.
* **grok-4-fast-non-reasoning**: High-throughput, low-latency generation and system routing.
* **grok-3**: Strong reasoning for complex, system-level workflows.
* **grok-3-mini**: Lightweight model optimized for interactive, high-volume use cases.
* **Llama-3.3-70B-Instruct**: Versatile model for enterprise Q\&A, decision support, and system orchestration.
* **Llama-4-Maverick-17B-128E-Instruct-FP8**: FP8-optimized model that delivers fast, cost-efficient inference.
* **DeepSeek-V3-0324**: Multimodal understanding across text and images.
* **DeepSeek-V3.1**: Enhanced multimodal reasoning and grounded retrieval.
* **DeepSeek-R1-0528**: Advanced long-form and multiple-step reasoning.
* **gpt-oss-120b**: Open-ecosystem model that supports transparency and reproducibility.

## Troubleshoot common errors

| Error                   | Cause                               | Resolution                                                                                                                |
| ----------------------- | ----------------------------------- | ------------------------------------------------------------------------------------------------------------------------- |
| 401 Unauthorized        | Invalid or expired credential       | Verify your `DefaultAzureCredential` has the **Cognitive Services OpenAI User** role assigned on the resource.            |
| 404 Not Found           | Wrong endpoint or deployment name   | Confirm your endpoint URL includes `/api/projects/YOUR_PROJECT_NAME` and the deployment name matches your Foundry portal. |
| 400 Model not supported | Model doesn't support Responses API | Check the [supported models list](#supported-foundry-models) and verify your deployment uses a compatible model.          |

## Related content

* [Migrate from Azure AI Inference SDK to OpenAI SDK](../../how-to/model-inference-to-openai-migration)
* [Azure OpenAI supported programming languages](../../openai/supported-languages)
* [Switch between OpenAI and Azure OpenAI endpoints](https://learn.microsoft.com/en-us/azure/developer/ai/how-to/switching-endpoints)
* [Model support for v1 Azure OpenAI API](../../openai/api-version-lifecycle#model-support)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

The [configurable Guardrails and controls](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/content-filters) available in Microsoft Foundry are sufficient for most content moderation needs. However, you might need to filter terms specific to your use case—such as competitor names, internal project names, or domain-specific sensitive terms. For this, you can create custom block lists that automatically filter content containing your specified terms.

In this article, you learn how to:

* Create and manage custom blocklists
* Add terms using exact match or regex patterns
* Apply blocklists to your content filters
* Test blocklist behavior with your deployments

## Prerequisites

* An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* Once you have your Azure subscription, create an Azure OpenAI resource in the Azure portal to get your token, key, and endpoint. Enter a unique name for your resource, select the subscription you entered on the application form, select a resource group, supported region, and supported pricing tier. Then select **Create**.
  * The resource takes a few minutes to deploy. After it finishes, select **go to resource**. In the left pane, under **Resource Management**, select **Subscription Key and Endpoint**. The endpoint and either of the keys are used to call APIs.
* [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) version 2.50 or later
* [cURL](https://curl.haxx.se/) installed

## Use block lists

<Tabs>
  <Tab title="Azure OpenAI API">
    You can create block lists with the Azure OpenAI API. The following steps help you get started.

    ### Get your token

    First, you need to get a token for accessing the APIs for creating, editing, and deleting block lists. You can get this token using the following Azure CLI command:

    ```bash
    az account get-access-token
    ```

    ### Create or modify a blocklist

    Copy the cURL command below to a text editor and make the following changes:

    1. Replace \{subscriptionId} with your subscription ID.
    2. Replace \{resourceGroupName} with your resource group name.
    3. Replace \{accountName} with your resource name.
    4. Replace \{raiBlocklistName} (in the URL) with a custom name for your list. Allowed characters: `0-9, A-Z, a-z, - . _ ~`.
    5. Replace \{token} with the token you got from the "Get your token" step above.
    6. Optionally replace the value of the "description" field with a custom description.

    ```bash
    curl --location --request PUT 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiBlocklists/{raiBlocklistName}?api-version=2024-04-01-preview' \
    --header 'Authorization: Bearer {token}' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "properties": {
            "description": "This is a prompt blocklist"
        }
    }'
    ```

    The response code should be `201` (created a new list) or `200` (updated an existing list).

    ### Apply a blocklist to a content filter

    If you haven't yet created a content filter, you can do so in [Foundry](https://ai.azure.com/?cid=learnDocs). See [Content filtering](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/content-filters#create-a-content-filter-in-azure-ai-foundry).

    To apply a **completion** blocklist to a content filter, use the following cURL command:

    1. Replace \{subscriptionId} with your sub ID.
    2. Replace \{resourceGroupName} with your resource group name.
    3. Replace \{accountName} with your resource name.
    4. Replace \{raiPolicyName} with the name of your Content Filter
    5. Replace \{token} with the token you got from the "Get your token" step above.
    6. Optionally change the `"completionBlocklists"` title to `"promptBlocklists"` if you want the blocklist to apply to user prompts instead of AI model completions.
    7. Replace `"raiBlocklistName"` in the body with a custom name for your list. Allowed characters: `0-9, A-Z, a-z, - . _ ~`.

    ```bash
    curl --location --request PUT 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiPolicies/{raiPolicyName}?api-version=2024-04-01-preview' \
    --header 'Authorization: Bearer {token}' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "properties": {
            "basePolicyName": "Microsoft.Default",
            "completionBlocklists": [{
                "blocklistName": "raiBlocklistName",
                "blocking": true
            }],
            "contentFilters": [ ]
        }
    }'
    ```

    ### Add blockItems to the list

    <Callout type="note">
      There's a maximum limit of 10,000 terms allowed in one list.
    </Callout>

    Copy the cURL command below to a text editor and make the following changes:

    1. Replace \{subscriptionId} with your sub ID.
    2. Replace \{resourceGroupName} with your resource group name.
    3. Replace \{accountName} with your resource name.
    4. Replace \{raiBlocklistName} (in the URL) with a custom name for your list. Allowed characters: `0-9, A-Z, a-z, - . _ ~`.
    5. Replace \{raiBlocklistItemName} with a custom name for your list item.
    6. Replace \{token} with the token you got from the "Get your token" step above.
    7. Replace the value of the `"pattern"` field with the item you'd like to add to your blocklist. The maximum length of a blockItem is 1,000 characters. Also specify whether the pattern is regex or exact match.

    ```bash
    curl --location --request PUT 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiBlocklists/{raiBlocklistName}/raiBlocklistItems/{raiBlocklistItemName}?api-version=2024-04-01-preview' \
    --header 'Authorization: Bearer {token}' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "properties": {
            "pattern": "blocking pattern",
            "isRegex": false
        }
    }'
    ```

    <Callout type="note">
      It can take around 5 minutes for a new term to be added to the blocklist. Test the blocklist after 5 minutes.
    </Callout>

    The response code should be `200`.

    ```json
    {
      "name": "raiBlocklistItemName",
      "id": "/subscriptions/subscriptionId/resourceGroups/resourceGroupName/providers/Microsoft.CognitiveServices/accounts/accountName/raiBlocklists/raiBlocklistName/raiBlocklistItems/raiBlocklistItemName",
      "properties": {
        "pattern": "blocking pattern",
        "isRegex": false
      }
    }
    ```

    ### Analyze text with a blocklist

    Now you can test out your deployment that has the blocklist. For instructions on calling the Azure OpenAI endpoints, visit the [Quickstart](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/quickstart).

    In the below example, a GPT-35-Turbo deployment with a blocklist is blocking the prompt. The response returns a `400` error.

    ```json
    {
      "error": {
        "message": "The response was filtered due to the prompt triggering Azure OpenAI's content management policy. Please modify your prompt and retry. To learn more about our content filtering policies please read our documentation: https://go.microsoft.com/fwlink/?linkid=2198766",
        "type": null,
        "param": "prompt",
        "code": "content_filter",
        "status": 400,
        "innererror": {
          "code": "ResponsibleAIPolicyViolation",
          "content_filter_result": {
            "custom_blocklists": {
              "details": [{ "filtered": true, "id": "pizza" }],
              "filtered": true
            }
          }
        }
      }
    }
    ```

    If the completion itself is blocked, the response returns `200`, as the completion only cuts off when the blocklist content is matched. The annotations show that a blocklist item was matched.

    ```json
    {
        "id": "chatcmpl-85NkyY0AkeBMunOjyxivQSiTaxGAl",
        "object": "chat.completion",
        "created": 1696293652,
        "model": "gpt-35-turbo",
        "prompt_filter_results": [
            {
                "prompt_index": 0,
                "content_filter_results": {
                    "hate": {
                        "filtered": false,
                        "severity": "safe"
                    },
                    "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                    },
                    "sexual": {
                        "filtered": false,
                        "severity": "safe"
                    },
                    "violence": {
                        "filtered": false,
                        "severity": "safe"
                    }
                }
            }
        ],
        "choices": [
            {
                "index": 0,
                "finish_reason": "content_filter",
                "message": {
                    "role": "assistant"
                },
                "content_filter_results": {
                    "custom_blocklists": [
                        {
                            "filtered": true,
                            "id": "myBlocklistName"
                        }
                    ],
                    "hate": {
                        "filtered": false,
                        "severity": "safe"
                    },
                    "self_harm": {
                        "filtered": false,
                        "severity": "safe"
                    },
                    "sexual": {
                        "filtered": false,
                        "severity": "safe"
                    },
                    "violence": {
                        "filtered": false,
                        "severity": "safe"
                    }
                }
            }
        ],
        "usage": {
            "completion_tokens": 75,
            "prompt_tokens": 27,
            "total_tokens": 102
        }
    }
    ```

    **Key fields in the response:**

    * `finish_reason: "content_filter"` indicates the completion was stopped by a blocklist match
    * `custom_blocklists[].id` shows which blocklist triggered the filter
    * `custom_blocklists[].filtered: true` confirms the content was blocked

    ### Delete a blocklist item

    To delete a blocklist item, use the following cURL command:

    1. Replace \{subscriptionId} with your subscription ID.
    2. Replace \{resourceGroupName} with your resource group name.
    3. Replace \{accountName} with your resource name.
    4. Replace \{raiBlocklistName} with the name of your blocklist.
    5. Replace \{raiBlocklistItemName} with the name of the item to delete.
    6. Replace \{token} with the token you got from the "Get your token" step above.

    ```bash
    curl --location --request DELETE 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiBlocklists/{raiBlocklistName}/raiBlocklistItems/{raiBlocklistItemName}?api-version=2024-10-01' \
    --header 'Authorization: Bearer {token}'
    ```

    The response code should be `200` (item deleted) or `204` (no content).

    ### Delete a blocklist

    To delete an entire blocklist, use the following cURL command:

    1. Replace \{subscriptionId} with your subscription ID.
    2. Replace \{resourceGroupName} with your resource group name.
    3. Replace \{accountName} with your resource name.
    4. Replace \{raiBlocklistName} with the name of the blocklist to delete.
    5. Replace \{token} with the token you got from the "Get your token" step above.

    ```bash
    curl --location --request DELETE 'https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.CognitiveServices/accounts/{accountName}/raiBlocklists/{raiBlocklistName}?api-version=2024-10-01' \
    --header 'Authorization: Bearer {token}'
    ```

    <Callout type="warning">
      Deleting a blocklist removes all items in that list and cannot be undone.
    </Callout>
  </Tab>

  <Tab title="Foundry">
    <Callout type="note">
      This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
    </Callout>

    ## Create a blocklist

    Go to [Foundry](https://ai.azure.com/) and navigate to your project. Then select the **Guardrails + controls** page in the left navigation. Select the **Create a custom blocklist** link, and follow the instructions.
  </Tab>
</Tabs>

## Troubleshooting

### 403 Forbidden error

Ensure your Azure AD token has the correct permissions. The account must have **Cognitive Services Contributor** or **Owner** role on the Azure OpenAI resource.

### Blocklist not taking effect

New blocklist terms can take up to 5 minutes to propagate. Wait and test again. If the issue persists, verify the blocklist is correctly applied to your content filter.

### Pattern not matching expected content

If using regex patterns, ensure the pattern syntax is valid. Test your regex pattern separately before adding it to the blocklist. Common issues include:

* Unescaped special characters
* Case sensitivity (patterns are case-sensitive by default)
* Anchors (`^` and `$`) might not behave as expected in streaming scenarios

### Quota limit reached

Each blocklist can contain a maximum of 10,000 terms. If you need more, create more blocklists and apply them to the same content filter.

## Related content

* Learn more about Responsible AI practices for Azure OpenAI: [Overview of Responsible AI practices for Azure OpenAI models](https://learn.microsoft.com/en-us/azure/ai-foundry/responsible-ai/openai/overview).

* Read more about [content filtering categories and severity levels](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/content-filter?tabs=python) with Azure OpenAI in Foundry models.

* Learn more about red teaming from our: [Introduction to red teaming large language models (LLMs)](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/red-teaming) article.

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

OpenAI's image generation models create images from user-provided text prompts and optional images. This article explains how to use these models, configure options, and benefit from advanced image generation capabilities in Azure.

## Prerequisites

* An Azure subscription. You can [create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
* An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability).
* Deploy a `dall-e-3` or `gpt-image-1`-series model with your Azure OpenAI resource. For more information on deployments, see [Create a resource and deploy a model with Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/create-resource).
  * GPT-image-1 series models are newer and feature a number of improvements over DALL-E 3. They are available in limited access: [Apply for GPT-image-1 access](https://aka.ms/oai/gptimage1access); [Apply for GPT-image-1.5 access](https://aka.ms/oai/gptimage1.5access).
* Python 3.8 or later.
  * Install the required packages: `pip install openai azure-identity`

## Overview

* Use image generation via [image generation API](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/dall-e-quickstart) or [responses API](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/responses?tabs=python-key)
* Experiment with image generation in the [Foundry portal](https://ai.azure.com)
* Learn about [image generation tokens](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-models/concepts/models-sold-directly-by-azure#image-generation-models)

| Aspect                                 | GPT-Image-1.5                                                                                        | GPT-Image-1                                                                         | GPT-Image-1-Mini                                                                       | DALL·E 3                                                                                                  |
| -------------------------------------- | ---------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| **Input / Output Modalities & Format** | Accepts **text + image** inputs; outputs images only in **base64** (no URL option).                  | Accepts **text + image** inputs; outputs images only in **base64** (no URL option). | Accepts **text + image** inputs; outputs images only in **base64** (no URL option).    | Accepts **text (primary)** input; limited image editing inputs (with mask). Outputs as **URL or base64**. |
| **Image Sizes / Resolutions**          | 1024×1024, 1024×1536, 1536×1024                                                                      | 1024×1024, 1024×1536, 1536×1024                                                     | 1024×1024, 1024×1536, 1536×1024                                                        | 1024×1024, 1024×1792, 1792×1024                                                                           |
| **Quality Options**                    | `low`, `medium`, `high` (default = high)                                                             | `low`, `medium`, `high` (default = high)                                            | `low`, `medium`, `high` (default = medium)                                             | `standard`, `hd`; style options: `natural`, `vivid`                                                       |
| **Number of Images per Request**       | 1–10 images per request (`n` parameter)                                                              | 1–10 images per request (`n` parameter)                                             | 1–10 images per request (`n` parameter)                                                | Only **1 image** per request (`n` must be 1)                                                              |
| **Editing (inpainting / variations)**  | Yes — supports inpainting and variations with mask + prompt                                          | Yes — supports inpainting and variations with mask + prompt                         | Yes — supports inpainting and variations with mask + prompt                            | Yes — supports inpainting and variations                                                                  |
| **Face Preservation**                  | ✅ Advanced **face preservation** for realistic, consistent results                                   | ✅ Advanced **face preservation** for realistic, consistent results                  | ❌ No dedicated face preservation; better for **non-portrait/general creative** imagery | ❌ No dedicated face preservation                                                                          |
| **Performance & Cost**                 | High-fidelity, **realism-optimized** model; improved efficiency and latency over GPT-Image-1         | High-fidelity, **realism-optimized** model; higher latency and cost                 | **Cost-efficient** and **faster** for large-scale or iterative generation              | Balanced performance; higher latency on complex prompts                                                   |
| **Strengths**                          | Best for **realism**, **instruction following**, **multimodal context**, and **improved speed/cost** | Best for **realism**, **instruction following**, and **multimodal context**         | Best for **fast prototyping**, **bulk generation**, or **cost-sensitive** use cases    | Strong **prompt adherence**, **natural text rendering**, and **stylistic diversity**                      |

## Responsible AI and Image Generation

Azure OpenAI's image generation models include built-in Responsible AI (RAI) protections to help ensure safe and compliant use.

In addition, Azure provides input and output moderation across all image generation models, along with Azure-specific safeguards such as content filtering and abuse monitoring. These systems help detect and prevent the generation or misuse of harmful, unsafe, or policy-violating content.

Customers can learn more about these safeguards and how to customize them here:

* Learn more: Explore [content filtering](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/content-filter)
* Request customization: Apply to [opt out of content filtering](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUMlBQNkZMR0lFRldORTdVQzQ0TEI5Q1ExOSQlQCN0PWcu)

### Special considerations for generating images of minors

Photorealistic images of minors are blocked by default. Customers can [request access](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUQVFQRDhQRjVPNllLMVZCSVNYVUs4MzhNMyQlQCN0PWcu) to this model capability. Enterprise-tier customers are automatically approved.

## Quotas and limits

Image generation has default rate limits per deployment:

| Model              | Default quota (images/min) |
| ------------------ | -------------------------- |
| DALL-E 3           | 2                          |
| GPT-image-1 series | 5                          |

To view your current quota or request an increase, see [Manage Azure OpenAI quotas](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/quota).

## Call the image generation API

The following command shows the most basic way to use an image model with code. If this is your first time using these models programmatically, start with the [quickstart](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/dall-e-quickstart).

<Tabs>
  <Tab title="GPT-image-1 series">
    Send a POST request to:

    ```
    https://<your_resource_name>.openai.azure.com/openai/deployments/<your_deployment_name>/images/generations?api-version=<api_version>
    ```

    **URL**:

    Replace the following values:

    * `<your_resource_name>` is the name of your Azure OpenAI resource.
    * `<your_deployment_name>` is the name of your GPT-image-1 series model deployment.
    * `<api_version>` is the version of the API you want to use. For example, `2025-04-01-preview`.

    **Required headers**:

    * `Content-Type`: `application/json`
    * `api-key`: `<your_API_key>`

    **Body**:

    The following is a sample request body. You specify a number of options, defined in later sections.

    ```json
    {
        "prompt": "A multi-colored umbrella on the beach, disposable camera",
        "model": "gpt-image-1.5",
        "size": "1024x1024",
        "n": 1,
        "quality": "high"
    }
    ```
  </Tab>

  <Tab title="DALL-E 3">
    Send a POST request to:

    ```
    https://<your_resource_name>.openai.azure.com/openai/deployments/<your_deployment_name>/images/generations?api-version=<api_version>
    ```

    **URL**:

    Replace the following values:

    * `<your_resource_name>` is the name of your Azure OpenAI resource.
    * `<your_deployment_name>` is the name of your DALL-E 3 model deployment.
    * `<api_version>` is the version of the API you want to use. For example, `2024-02-01`.

    **Required headers**:

    * `Content-Type`: `application/json`
    * `api-key`: `<your_API_key>`

    **Body**:

    The following is a sample request body. You specify a number of options, defined in later sections.

    ```json
    {
        "prompt": "A multi-colored umbrella on the beach, disposable camera",
        "size": "1024x1024",
        "n": 1,
        "quality": "hd",
        "style": "vivid"
    }
    ```
  </Tab>
</Tabs>

<Callout type="tip">
  For image generation token costs, see [Image tokens](../../foundry-models/concepts/models-sold-directly-by-azure).
</Callout>

### Output

<Tabs>
  <Tab title="GPT-image-1 series">
    The response from a successful image generation API call looks like the following example. The `b64_json` field contains the output image data.

    ```json
    {
        "created": 1698116662,
        "data": [
            {
                "b64_json": "<base64 image data>"
            }
        ]
    }
    ```

    <Callout type="note">
      The `response_format` parameter isn't supported for GPT-image-1 series models, which always return base64-encoded images.
    </Callout>
  </Tab>

  <Tab title="DALL-E 3">
    The response from a successful image generation API call looks like the following example. The `url` field contains a URL where you can download the generated image. The URL stays active for 24 hours.

    ```json
    {
        "created": 1698116662,
        "data": [
            {
                "url": "<URL_to_generated_image>",
                "revised_prompt": "<prompt_that_was_used>"
            }
        ]
    }
    ```
  </Tab>
</Tabs>

### Streaming

Streaming lets you receive partial images as they're generated, providing faster visual feedback for your users. This is useful for applications where you want to show generation progress. The `partial_images` parameter (1-3) controls how many intermediate images are returned before the final result.

You can stream image generation requests to `gpt-image-1`-series models by setting the `stream` parameter to `true`, and setting the `partial_images` parameter to a value between 0 and 3.

```python
import base64
from openai import OpenAI
from azure.identity import DefaultAzureCredential, get_bearer_token_provider

token_provider = get_bearer_token_provider(
    DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default"
)

client = OpenAI(
  base_url = "https://RESOURCE-NAME-HERE/openai/v1/",
  api_key=token_provider,
  default_headers={"api_version":"preview"}
)

stream = client.images.generate(
    model="gpt-image-1.5",
    prompt="A cute baby sea otter",
    n=1,
    size="1024x1024",
    stream=True,
    partial_images = 2
)

for event in stream:
    if event.type == "image_generation.partial_image":
        idx = event.partial_image_index
        image_base64 = event.b64_json
        image_bytes = base64.b64decode(image_base64)
        with open(f"river{idx}.png", "wb") as f:
            f.write(image_bytes)

```

### API call rejection

Prompts and images are filtered based on our content policy. The API returns an error when a prompt or image is flagged.

If your prompt is flagged, the `error.code` value in the message is set to `contentFilter`. Here's an example:

```json
{
    "created": 1698435368,
    "error":
    {
        "code": "contentFilter",
        "message": "Your task failed as a result of our safety system."
    }
}
```

It's also possible that the generated image itself is filtered. In this case, the error message is set to *Generated image was filtered as a result of our safety system*. Here's an example:

```json
{
    "created": 1698435368,
    "error":
    {
        "code": "contentFilter",
        "message": "Generated image was filtered as a result of our safety system."
    }
}
```

### Write effective text-to-image prompts

Your prompts should describe the content you want to see in the image and the visual style of the image.

When you write prompts, consider that the Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](../concepts/content-filter).

<Callout type="tip">
  For a thorough look at how you can tweak your text prompts to generate different kinds of images, see the [Image prompt engineering guide](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/gpt-4-v-prompt-engineering).
</Callout>

### Specify API options

The following API body parameters are available for image generation models.

<Tabs>
  <Tab title="GPT-image-1 series">
    #### Size

    Specify the size of the generated images. Must be one of `1024x1024`, `1024x1536`, or `1536x1024` for GPT-image-1 series models. Square images are faster to generate.

    #### Quality

    There are three options for image quality: `low`, `medium`, and `high`. Lower quality images can be generated faster.

    The default value is `high`.

    #### Number

    You can generate between one and 10 images in a single API call. The default value is `1`.

    #### User ID

    Use the *user* parameter to specify a unique identifier for the user making the request. This identifier is useful for tracking and monitoring usage patterns. The value can be any string, such as a user ID or email address.

    #### Output format

    Use the *output\_format* parameter to specify the format of the generated image. Supported formats are `PNG` and `JPEG`. The default is `PNG`.

    <Callout type="note">
      WEBP images aren't supported in the Azure OpenAI in Microsoft Foundry Models.
    </Callout>

    #### Compression

    Use the *output\_compression* parameter to specify the compression level for the generated image. Input an integer between `0` and `100`, where `0` is no compression and `100` is maximum compression. The default is `100`.

    #### Streaming

    Use the *stream* parameter to enable streaming responses. When set to `true`, the API returns partial images as they're generated. This feature provides faster visual feedback for users and improves perceived latency. Set the *partial\_images* parameter to control how many partial images are generated (1-3).

    #### Transparency

    Set the *background* parameter to `transparent` and *output\_format* to `PNG` on an image generate request to get an image with a transparent background.
  </Tab>

  <Tab title="DALL-E 3">
    #### Size

    Specify the size of the generated images. Must be one of `1024x1024`, `1792x1024`, or `1024x1792` for DALL-E 3 models. Square images are faster to generate.

    #### Style

    DALL-E 3 offers two style options: `natural` and `vivid`. The natural style is more similar to the default style of older models, while the vivid style generates more hyper-real and cinematic images.

    The natural style is useful in cases where DALL-E 3 over-exaggerates or confuses a subject that's meant to be more simple, subdued, or realistic.

    The default value is `vivid`.

    #### Quality

    There are two options for image quality: `hd` and `standard`. The hd option creates images with finer details and greater consistency across the image. Standard images are faster to generate.

    The default value is `standard`.

    #### Number

    With DALL-E 3, you can't generate more than one image in a single API call: the `n` parameter must be set to *1*. To generate multiple images at once, make parallel requests.

    #### Response format

    The format in which DALL-E 3 returns generated images. Must be one of `url` or `b64_json`. This parameter isn't supported for GPT-image-1 series models, which always return base64-encoded images.
  </Tab>
</Tabs>

## Call the image edit API

The Image Edit API enables you to modify existing images based on text prompts you provide. The API call is similar to the image generation API call, but you also need to provide an input image.

<Tabs>
  <Tab title="GPT-image-1 series">
    <Callout type="important">
      The input image must be less than 50 MB in size and must be a PNG or JPG file.
    </Callout>

    Send a POST request to:

    ```
    https://<your_resource_name>.openai.azure.com/openai/deployments/<your_deployment_name>/images/edits?api-version=<api_version>
    ```

    **URL**:

    Replace the following values:

    * `<your_resource_name>` is the name of your Azure OpenAI resource.
    * `<your_deployment_name>` is the name of your GPT-image-1 series model deployment.
    * `<api_version>` is the version of the API you want to use. For example, `2025-04-01-preview`.

    **Required headers**:

    * `Content-Type`: `multipart/form-data`
    * `api-key`: `<your_API_key>`

    **Body**:

    The following is a sample request body. You specify a number of options, defined in later sections.

    <Callout type="important">
      The Image Edit API takes multipart/form data, not JSON data. The example below shows sample form data that would be attached to a cURL request.
    </Callout>

    ```
    -F "image[]=@beach.png" \
    -F 'prompt=Add a beach ball in the center' \
    -F "model=gpt-image-1" \
    -F "size=1024x1024" \
    -F "n=1" \
    -F "quality=high"
    ```

    ### API response output

    The response from a successful image editing API call looks like the following example. The `b64_json` field contains the output image data.

    ```json
    {
        "created": 1698116662,
        "data": [
            {
                "b64_json": "<base64 image data>"
            }
        ]
    }
    ```

    ### Specify image edit API options

    The following API body parameters are available for image editing models, in addition to the ones available for image generation models.

    #### Image

    The *image* value indicates the image file you want to edit.

    #### Input fidelity

    The *input\_fidelity* parameter controls how much effort the model puts into matching the style and features, especially facial features, of input images.

    This parameter lets you make subtle edits to an image without changing unrelated areas. When you use high input fidelity, faces are preserved more accurately than in standard mode.

    <Callout type="important">
      Input fidelity is not supported by the `gpt-image-1-mini` model.
    </Callout>

    #### Mask

    The *mask* parameter uses the same type as the main *image* input parameter. It defines the area of the image that you want the model to edit, using fully transparent pixels (alpha of zero) in those areas. The mask must be a PNG file and have the same dimensions as the input image.

    #### Streaming

    Use the *stream* parameter to enable streaming responses. When set to `true`, the API returns partial images as they're generated. This feature provides faster visual feedback for users and improves perceived latency. Set the *partial\_images* parameter to control how many partial images are generated (1-3).

    #### Transparency

    Set the *background* parameter to `transparent` and *output\_format* to `PNG` on an image generate request to get an image with a transparent background.
  </Tab>

  <Tab title="DALL-E 3">
    DALL-E models don't support the Image Edit API.
  </Tab>
</Tabs>

## Troubleshooting

### Rate limit errors

If you receive a 429 error, you've exceeded your rate limit. Wait before retrying or request a quota increase in the Azure portal.

### Authentication errors

If you receive a 401 error:

* **API key auth**: Verify your API key is correct and not expired.
* **Managed identity**: Ensure your identity has the **Cognitive Services OpenAI User** role on the resource.

### Timeout errors

Image generation can take up to 60 seconds for complex prompts. If you experience timeouts:

* Use streaming to get partial results sooner.
* Simplify your prompt.
* Try a smaller image size.

## Related content

* [What is Azure OpenAI?](../../foundry-models/concepts/models-sold-directly-by-azure)
* [Quickstart: Generate images with Azure OpenAI](../dall-e-quickstart)
* [Image API reference](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference#image-generation)
* [Image API (preview) reference](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/reference-preview)

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

<ZonePivot pivot="rest-api">
  Use this guide to get started calling the Azure OpenAI in Microsoft Foundry Models image generation REST APIs by using Python.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/).
  * The following Python libraries installed: `os`, `requests`, `json`.
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability).
  * Then, you need to deploy a `gpt-image-1`-series or `dalle3` model with your Azure resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Setup

  ### Retrieve key and endpoint

  To successfully call the Azure OpenAI APIs, you need the following information about your Azure OpenAI resource:

  | Variable     | Name       | Value                                                                                                                                                                                                                                              |
  | ------------ | ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | **Endpoint** | `api_base` | The endpoint value is located under **Keys and Endpoint** for your resource in the Azure portal. You can also find the endpoint via the **Deployments** page in Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | **Key**      | `api_key`  | The key value is also located under **Keys and Endpoint** for your resource in the Azure portal. Azure generates two keys for your resource. You can use either value.                                                                             |

  Go to your resource in the Azure portal. On the navigation pane, select **Keys and Endpoint** under **Resource Management**. Copy the **Endpoint** value and an access key value. You can use either the **KEY 1** or **KEY 2** value. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot that shows the Keys and Endpoint page for an Azure OpenAI resource in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    We recommend Microsoft Entra ID authentication with [managed identities for Azure resources](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to avoid storing credentials with your applications that run in the cloud.

    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys regularly, and restrict access to Azure Key Vault using role based access control and network access restrictions. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```cmd
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```bash
      export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>
  </Tabs>

  ## Create a new Python application

  Create a new Python file named *quickstart.py*. Open the new file in your preferred editor or IDE.

  <Tabs>
    <Tab title="GPT-image-1 series">
      1. Replace the contents of *quickstart.py* with the following code. Change the value of `prompt` to your preferred text. Also set `deployment` to the deployment name you chose when you deployed the GPT-image-1 series model.

         ```python
         import os
         import requests
         import base64
         from PIL import Image
         from io import BytesIO

         # set environment variables
         endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
         subscription_key = os.getenv("AZURE_OPENAI_API_KEY")

         deployment = "gpt-image-1.5" # the name of your GPT-image-1 series deployment
         api_version = "2025-04-01-preview" # or later version

         def decode_and_save_image(b64_data, output_filename):
           image = Image.open(BytesIO(base64.b64decode(b64_data)))
           image.show()
           image.save(output_filename)

         def save_all_images_from_response(response_data, filename_prefix):
           for idx, item in enumerate(response_data['data']):
             b64_img = item['b64_json']
             filename = f"{filename_prefix}_{idx+1}.png"
             decode_and_save_image(b64_img, filename)
             print(f"Image saved to: '{filename}'")

         base_path = f'openai/deployments/{deployment}/images'
         params = f'?api-version={api_version}'

         generation_url = f"{endpoint}{base_path}/generations{params}"
         generation_body = {
           "prompt": "girl falling asleep",
           "n": 1,
           "size": "1024x1024",
           "quality": "medium",
           "output_format": "png"
         }
         generation_response = requests.post(
           generation_url,
           headers={
             'Api-Key': subscription_key,
             'Content-Type': 'application/json',
           },
           json=generation_body
         ).json()
         save_all_images_from_response(generation_response, "generated_image")

         # In addition to generating images, you can edit them.
         edit_url = f"{endpoint}{base_path}/edits{params}"
         edit_body = {
           "prompt": "girl falling asleep",
           "n": 1,
           "size": "1024x1024",
           "quality": "medium"
         }
         files = {
           "image": ("generated_image_1.png", open("generated_image_1.png", "rb"), "image/png"),
           # You can use a mask to specify which parts of the image you want to edit.
           # The mask must be the same size as the input image.
           # "mask": ("mask.png", open("mask.png", "rb"), "image/png"),
         }
         edit_response = requests.post(
           edit_url,
           headers={'Api-Key': subscription_key},
           data=edit_body,
           files=files
         ).json()
         save_all_images_from_response(edit_response, "edited_image")
         ```

         The script makes a synchronous image generation API call.

         <Callout type="important">
           Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
         </Callout>

      2. Run the application with the `python` command:

         ```console
         python quickstart.py
         ```

         Wait a few moments to get the response.
    </Tab>

    <Tab title="DALL-E">
      1. Replace the contents of *quickstart.py* with the following code. Change the value of `prompt` to your preferred text.

         You also need to replace `<dalle3>` in the URL with the deployment name you chose when you deployed the DALL-E 3 model. Entering the model name will result in an error unless you chose a deployment name that is identical to the underlying model name. If you encounter an error, double check to make sure that you don't have a doubling of the `/` at the separation between your endpoint and `/openai/deployments`.

         ```python
         import requests
         import time
         import os
         api_base = os.environ['AZURE_OPENAI_ENDPOINT']  # Enter your endpoint here
         api_key = os.environ['AZURE_OPENAI_API_KEY']         # Enter your API key here

         api_version = '2024-02-01'
         url = f"{api_base}/openai/deployments/<dalle3>/images/generations?api-version={api_version}"
         headers= { "api-key": api_key, "Content-Type": "application/json" }
         body = {
             # Enter your prompt text here
             "prompt": "A multi-colored umbrella on the beach, disposable camera",
             "size": "1024x1024", # supported values are “1792x1024”, “1024x1024” and “1024x1792”
             "n": 1, #The number of images to generate. Only n=1 is supported for DALL-E 3.
             "quality": "hd", # Options are “hd” and “standard”; defaults to standard
             "style": "vivid" # Options are “natural” and “vivid”; defaults to “vivid”
         }
         submission = requests.post(url, headers=headers, json=body)

         image_url = submission.json()['data'][0]['url']

         print(image_url)
         ```

         The script makes a synchronous image generation API call.

         <Callout type="important">
           Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
         </Callout>

      2. Run the application with the `python` command:

         ```console
         python quickstart.py
         ```

         Wait a few moments to get the response.
    </Tab>
  </Tabs>

  ## Output

  The output from a successful image generation API call looks like the following example. The `url` field contains a URL where you can download the generated image. The URL stays active for 24 hours.

  ```json
  {
      "created": 1698116662,
      "data": [
          {
              "url": "<URL_to_generated_image>",
              "revised_prompt": "<prompt_that_was_used>"
          }
      ]
  }
  ```

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter). For examples of error responses, see the [Image generation how-to guide](how-to/dall-e).

  The system returns an operation status of `Failed` and the `error.code` value in the message is set to `contentFilter`. Here's an example:

  ```json
  {
      "created": 1698435368,
      "error":
      {
          "code": "contentFilter",
          "message": "Your task failed as a result of our safety system."
      }
  }
  ```

  It's also possible that the generated image itself is filtered. In this case, the error message is set to `Generated image was filtered as a result of our safety system.`. Here's an example:

  ```json
  {
      "created": 1698435368,
      "error":
      {
          "code": "contentFilter",
          "message": "Generated image was filtered as a result of our safety system."
      }
  }
  ```

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  Use this guide to get started generating images with the Azure OpenAI SDK for Python.

  [Library source code](https://github.com/openai/openai-python/tree/main/src/openai) | [Package](https://github.com/openai/openai-python) | [Samples](https://github.com/openai/openai-python/tree/main/examples)

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/).
  * An Azure OpenAI resource created in a compatible region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability).
    * Access the Azure OpenAI resource endpoint and keys in the Azure portal.
  * Then, you need to deploy a `dalle3` model with your Azure resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Setup

  ### Retrieve key and endpoint

  To successfully call the Azure OpenAI APIs, you need the following information about your Azure OpenAI resource:

  | Variable     | Name       | Value                                                                                                                                                                                                                                                        |
  | ------------ | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
  | **Endpoint** | `api_base` | The endpoint value is located under **Keys and Endpoint** for your resource in the Azure portal. You can also find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | **Key**      | `api_key`  | The key value is also located under **Keys and Endpoint** for your resource in the Azure portal. Azure generates two keys for your resource. You can use either value.                                                                                       |

  Go to your resource in the Azure portal. On the navigation pane, select **Keys and Endpoint** under **Resource Management**. Copy the **Endpoint** value and an access key value. You can use either the **KEY 1** or **KEY 2** value. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot that shows the Keys and Endpoint page for an Azure OpenAI resource in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    We recommend Microsoft Entra ID authentication with [managed identities for Azure resources](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to avoid storing credentials with your applications that run in the cloud.

    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys regularly, and restrict access to Azure Key Vault using role based access control and network access restrictions. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```cmd
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```bash
      export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>
  </Tabs>

  ## Install the Python SDK

  Open a command prompt and browse to your project folder. Install the OpenAI Python SDK by using the following command:

  ```bash
  pip install openai
  ```

  Install the following libraries as well:

  ```bash
  pip install requests
  pip install pillow
  ```

  ## Generate images with DALL-E

  Create a new python file, *quickstart.py*. Open it in your preferred editor or IDE.

  Replace the contents of *quickstart.py* with the following code.

  ```python
  from openai import AzureOpenAI
  import os
  import requests
  from PIL import Image
  import json

  client = AzureOpenAI(
      api_version="2024-02-01",
      api_key=os.environ["AZURE_OPENAI_API_KEY"],
      azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT']
  )

  result = client.images.generate(
      model="dalle3", # the name of your DALL-E 3 deployment
      prompt="a close-up of a bear walking through the forest",
      n=1
  )

  json_response = json.loads(result.model_dump_json())

  # Set the directory for the stored image
  image_dir = os.path.join(os.curdir, 'images')

  # If the directory doesn't exist, create it
  if not os.path.isdir(image_dir):
      os.mkdir(image_dir)

  # Initialize the image path (note the filetype should be png)
  image_path = os.path.join(image_dir, 'generated_image.png')

  # Retrieve the generated image
  image_url = json_response["data"][0]["url"]  # extract image URL from response
  generated_image = requests.get(image_url).content  # download the image
  with open(image_path, "wb") as image_file:
      image_file.write(generated_image)

  # Display the image in the default image viewer
  image = Image.open(image_path)
  image.show()
  ```

  1. Make sure the `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_API_KEY` environment variables are set.
  2. Change the value of `prompt` to your preferred text.
  3. Change the value of `model` to the name of your deployed DALL-E 3 model.

  <Callout type="important">
    Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
  </Callout>

  Run the application with the `python` command:

  ```console
  python quickstart.py
  ```

  Wait a few moments to get the response.

  ## Output

  Azure OpenAI stores the output image in the *generated\_image.png* file in your specified directory. The script also displays the image in your default image viewer.

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter).

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<ZonePivot pivot="programming-language-csharp">
  Use this guide to get started generating images with the Azure OpenAI SDK for C#.

  [Library source code](https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI) | [Package (NuGet)](https://www.nuget.org/packages/Azure.AI.OpenAI/) | [Samples](https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI/tests/Samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * The [.NET 7 SDK](https://dotnet.microsoft.com/download/dotnet/7.0)
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create a new console application with the following command:

     ```shell
     dotnet new console
     ```

  3. Install the [OpenAI .NET client library](https://www.nuget.org/packages/Azure.AI.OpenAI/) with the [dotnet add package](https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-add-package) command:

     ```console
     dotnet add package Azure.AI.OpenAI --version 1.0.0-beta.6
     ```

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the [Azure.Identity](https://www.nuget.org/packages/Azure.Identity) package with:

     ```console
     dotnet add package Azure.Identity
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
      ```
    </Tab>

    <Tab title="API key">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));
      ```
    </Tab>
  </Tabs>

  To run the quickstart, follow these steps:

  1. Replace the contents of `Program.cs` with the following code and update the placeholder values with your own.

     ```csharp
     using Azure;
     using Azure.AI.OpenAI;
     using OpenAI.Images;
     using static System.Environment;

     string endpoint = Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT") ?? "https://<your-resource-name>.openai.azure.com/";
     string key = Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY") ?? "<your-key>";

     // Use the recommended keyless credential instead of the AzureKeyCredential credential.
     AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
     //AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));

     // This must match the custom deployment name you chose for your model
     ImageClient chatClient = openAIClient.GetImageClient("dalle-3");

     var imageGeneration = await chatClient.GenerateImageAsync(
             "a happy monkey sitting in a tree, in watercolor",
             new ImageGenerationOptions()
             {
                 Size = GeneratedImageSize.W1024xH1024
             }
         );

     Console.WriteLine(imageGeneration.Value.ImageUri);
     ```

  2. Run the application using the `dotnet run` command or the run button at the top of Visual Studio:

     ```dotnetcli
     dotnet run
     ```

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-java">
  Use this guide to get started generating images with the Azure OpenAI SDK for Java.

  [Library source code](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/openai/azure-ai-openai) | [Artifact (Maven)](https://central.sonatype.com/artifact/com.azure/azure-ai-openai/1.0.0-beta.3) | [Samples](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/openai/azure-ai-openai/src/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * The current version of the [Java Development Kit (JDK)](https://www.microsoft.com/openjdk)
  * Install [Apache Maven](https://maven.apache.org/install.html).
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Install [Apache Maven](https://maven.apache.org/install.html). Then run `mvn -v` to confirm successful installation.

  3. Create a new `pom.xml` file in the root of your project, and copy the following code into it:

     ```xml
     <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
          <modelVersion>4.0.0</modelVersion>
          <groupId>com.azure.samples</groupId>
          <artifactId>quickstart-dall-e</artifactId>
          <version>1.0.0-SNAPSHOT</version>
          <build>
              <sourceDirectory>src</sourceDirectory>
              <plugins>
              <plugin>
                  <artifactId>maven-compiler-plugin</artifactId>
                  <version>3.7.0</version>
                  <configuration>
                  <source>1.8</source>
                  <target>1.8</target>
                  </configuration>
              </plugin>
              </plugins>
          </build>
          <dependencies>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-ai-openai</artifactId>
                  <version>1.0.0-beta.3</version>
              </dependency>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-core</artifactId>
                  <version>1.53.0</version>
              </dependency>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-identity</artifactId>
                  <version>1.15.1</version>
              </dependency>
              <dependency>
                  <groupId>org.slf4j</groupId>
                  <artifactId>slf4j-simple</artifactId>
                  <version>1.7.9</version>
              </dependency>
          </dependencies>
      </project>
     ```

  4. Install the Azure OpenAI SDK and dependencies.

     ```console
     mvn clean dependency:copy-dependencies
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the app

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```java
      OpenAIAsyncClient client = new OpenAIClientBuilder()
          .endpoint(endpoint)
          .credential(new DefaultAzureCredentialBuilder().build())
          .buildAsyncClient();
      ```
    </Tab>

    <Tab title="API key">
      ```java
      OpenAIAsyncClient client = new OpenAIClientBuilder()
          .endpoint(endpoint)
          .credential(new AzureKeyCredential(key))
          .buildAsyncClient();
      ```
    </Tab>
  </Tabs>

  <Tabs>
    <Tab title="Microsoft Entra ID">
      Follow these steps to create a console application for image generation.

      1. Create a new file named *Quickstart.java* in the same project root directory.

      2. Copy the following code into *Quickstart.java*:

         ```java
         import com.azure.ai.openai.OpenAIAsyncClient;
         import com.azure.ai.openai.OpenAIClientBuilder;
         import com.azure.ai.openai.models.ImageGenerationOptions;
         import com.azure.ai.openai.models.ImageLocation;
         import com.azure.core.credential.AzureKeyCredential;
         import com.azure.core.models.ResponseError;

         import java.util.concurrent.TimeUnit;

         public class Quickstart {

             public static void main(String[] args) throws InterruptedException {

                 String endpoint = System.getenv("AZURE_OPENAI_ENDPOINT");

                 // Use the recommended keyless credential instead of the AzureKeyCredential credential.

                 OpenAIAsyncClient client = new OpenAIClientBuilder()
                     .endpoint(endpoint)
                     .credential(new DefaultAzureCredentialBuilder().build())
                     .buildAsyncClient();

                 ImageGenerationOptions imageGenerationOptions = new ImageGenerationOptions(
                     "A drawing of the Seattle skyline in the style of Van Gogh");
                 client.getImages(imageGenerationOptions).subscribe(
                     images -> {
                         for (ImageLocation imageLocation : images.getData()) {
                             ResponseError error = imageLocation.getError();
                             if (error != null) {
                                 System.out.printf("Image generation operation failed. Error code: %s, error message: %s.%n",
                                     error.getCode(), error.getMessage());
                             } else {
                                 System.out.printf(
                                     "Image location URL that provides temporary access to download the generated image is %s.%n",
                                     imageLocation.getUrl());
                             }
                         }
                     },
                     error -> System.err.println("There was an error getting images." + error),
                     () -> System.out.println("Completed getImages."));

                 // The .subscribe() creation and assignment isn't a blocking call.
                 // The thread sleeps so the program does not end before the send operation is complete.
                 // Use .block() instead of .subscribe() for a synchronous call.
                 TimeUnit.SECONDS.sleep(10);
             }
         }
         ```

      3. Run your new console application to generate an image:

         ```console
         javac Quickstart.java -cp ".;target\dependency\*"
         java -cp ".;target\dependency\*" Quickstart
         ```
    </Tab>

    <Tab title="API key">
      Follow these steps to create a console application for image generation.

      1. Create a new file named *Quickstart.java* in the same project root directory.

      2. Copy the following code into *Quickstart.java*:

         ```java
         import com.azure.ai.openai.OpenAIAsyncClient;
         import com.azure.ai.openai.OpenAIClientBuilder;
         import com.azure.ai.openai.models.ImageGenerationOptions;
         import com.azure.ai.openai.models.ImageLocation;
         import com.azure.identity.DefaultAzureCredentialBuilder;
         import com.azure.core.models.ResponseError;

         import java.util.concurrent.TimeUnit;

         public class Quickstart {

             public static void main(String[] args) throws InterruptedException {

                 String key = System.getenv("AZURE_OPENAI_API_KEY");
                 String endpoint = System.getenv("AZURE_OPENAI_ENDPOINT");

                 OpenAIAsyncClient client = new OpenAIClientBuilder()
                     .endpoint(endpoint)
                     .credential(new AzureKeyCredential(key))
                     .buildAsyncClient();

                 ImageGenerationOptions imageGenerationOptions = new ImageGenerationOptions(
                     "A drawing of the Seattle skyline in the style of Van Gogh");
                 client.getImages(imageGenerationOptions).subscribe(
                     images -> {
                         for (ImageLocation imageLocation : images.getData()) {
                             ResponseError error = imageLocation.getError();
                             if (error != null) {
                                 System.out.printf("Image generation operation failed. Error code: %s, error message: %s.%n",
                                     error.getCode(), error.getMessage());
                             } else {
                                 System.out.printf(
                                     "Image location URL that provides temporary access to download the generated image is %s.%n",
                                     imageLocation.getUrl());
                             }
                         }
                     },
                     error -> System.err.println("There was an error getting images." + error),
                     () -> System.out.println("Completed getImages."));

                 // The .subscribe() creation and assignment isn't a blocking call.
                 // The thread sleeps so the program does not end before the send operation is complete.
                 // Use .block() instead of .subscribe() for a synchronous call.
                 TimeUnit.SECONDS.sleep(10);
             }
         }
         ```

      3. Run your new console application to generate an image:

         ```console
         javac Quickstart.java -cp ".;target\dependency\*"
         java -cp ".;target\dependency\*" Quickstart
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  Image location URL that provides temporary access to download the generated image is <SAS URL>.
  Completed getImages.
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai)
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  Use this guide to get started generating images with the Azure OpenAI SDK for JavaScript.

  [Reference documentation](https://platform.openai.com/docs/api-reference/images/create) | [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  4. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Generate images with DALL-E

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { AzureOpenAI } = require("openai");
         const {
             DefaultAzureCredential,
             getBearerTokenProvider
         } = require("@azure/identity");

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { AzureOpenAI } = require("openai");

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         function getClient() {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  == Batch Image Generation ==
  Image generation result URL: <SAS URL>
  Image generation result URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  Use this guide to get started generating images with the Azure OpenAI SDK for JavaScript.

  [Reference documentation](https://platform.openai.com/docs/api-reference/images/create) | [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [TypeScript](https://www.typescriptlang.org/download/)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```shell
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  5. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Generate images with DALL-E

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { AzureOpenAI } from "openai";
         import {
             DefaultAzureCredential,
             getBearerTokenProvider
         } from "@azure/identity";

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      5. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  == Batch Image Generation ==
  Image generation result URL: <SAS URL>
  Image generation result URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/azure-openai-samples).
</ZonePivot>

<ZonePivot pivot="programming-language-go">
  Use this guide to get started generating images with the Azure OpenAI SDK for Go.

  [Library source code](https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai) | [Package](https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai) | [Samples](https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [Go 1.8+](https://go.dev/doc/install)
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `dall-e-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir dall-e-quickstart && cd dall-e-quickstart
     ```

  2. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `NewDefaultAzureCredential` implementation with `NewKeyCredential`.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```go
      azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
      credential, err := azidentity.NewDefaultAzureCredential(nil)
      client, err := azopenai.NewClient(azureOpenAIEndpoint, credential, nil)
      ```
    </Tab>

    <Tab title="API key">
      ```go
      azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
      azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
      credential := azcore.NewKeyCredential(azureOpenAIKey)
      client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)
      ```
    </Tab>
  </Tabs>

  <Tabs>
    <Tab title="Microsoft Entra ID">
      To run the sample:

      1. Create a new file named *quickstart.go*. Copy the following code into the *quickstart.go* file.

         ```go
          package main

         import (
         	"context"
         	"fmt"
         	"net/http"
         	"os"
         	"log"

         	"github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
         	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
         )

         func main() {
         	azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
         	modelDeploymentID := "dall-e-3"

         	credential, err := azidentity.NewDefaultAzureCredential(nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	client, err := azopenai.NewClient(
         		azureOpenAIEndpoint, credential, nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	resp, err := client.GetImageGenerations(context.TODO(), azopenai.ImageGenerationOptions{
         		Prompt:         to.Ptr("A painting of a cat in the style of Dali."),
         		ResponseFormat: to.Ptr(azopenai.ImageGenerationResponseFormatURL),
         		DeploymentName: to.Ptr(modelDeploymentID),
         	}, nil)

         	if err != nil {
         		// Implement application specific error handling logic.
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	for _, generatedImage := range resp.Data {
         		// The underlying type for the generatedImage is determined by the value of
         		// ImageGenerationOptions.ResponseFormat.
         		// In this example we use `azopenai.ImageGenerationResponseFormatURL`,
         		// so the underlying type will be ImageLocation.

         		resp, err := http.Head(*generatedImage.URL)

         		if err != nil {
         			// Implement application specific error handling logic.
         			log.Printf("ERROR: %s", err)
         			return
         		}

         		fmt.Fprintf(os.Stderr, "Image generated, HEAD request on URL returned %d\nImage URL: %s\n", resp.StatusCode, *generatedImage.URL)
         	}
         }
         ```

      2. Run the following command to create a new Go module:

         ```shell
          go mod init quickstart.go
         ```

      3. Run `go mod tidy` to install the required dependencies:

         ```cmd
         go mod tidy
         ```

      4. Run the following command to run the sample:

         ```shell
          go run quickstart.go
         ```
    </Tab>

    <Tab title="API key">
      To run the sample:

      1. Create a new file named *quickstart.go*. Copy the following code into the *quickstart.go* file.

         ```go
         package main

         import (
         	"context"
         	"fmt"
         	"net/http"
         	"os"
         	"log"

         	"github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
         )

         func main() {
         	azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
         	modelDeploymentID := "dall-e-3"

         	azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
         	credential := azcore.NewKeyCredential(azureOpenAIKey)

         	client, err := azopenai.NewClientWithKeyCredential(
         		azureOpenAIEndpoint, credential, nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	resp, err := client.GetImageGenerations(context.TODO(), azopenai.ImageGenerationOptions{
         		Prompt:         to.Ptr("A painting of a cat in the style of Dali."),
         		ResponseFormat: to.Ptr(azopenai.ImageGenerationResponseFormatURL),
         		DeploymentName: to.Ptr(modelDeploymentID),
         	}, nil)

         	if err != nil {
         		// Implement application specific error handling logic.
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	for _, generatedImage := range resp.Data {
         		// The underlying type for the generatedImage is determined by the value of
         		// ImageGenerationOptions.ResponseFormat.
         		// In this example we use `azopenai.ImageGenerationResponseFormatURL`,
         		// so the underlying type will be ImageLocation.

         		resp, err := http.Head(*generatedImage.URL)

         		if err != nil {
         			// Implement application specific error handling logic.
         			log.Printf("ERROR: %s", err)
         			return
         		}

         		fmt.Fprintf(os.Stderr, "Image generated, HEAD request on URL returned %d\nImage URL: %s\n", resp.StatusCode, *generatedImage.URL)
         	}
         }
         ```

      2. Run the following command to create a new Go module:

         ```shell
          go mod init quickstart.go
         ```

      3. Run `go mod tidy` to install the required dependencies:

         ```cmd
         go mod tidy
         ```

      4. Run the following command to run the sample:

         ```shell
          go run quickstart.go
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  Image generated, HEAD request on URL returned 200
  Image URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-powershell">
  Use this guide to get started calling the Azure OpenAI in Microsoft Foundry Models image generation APIs with PowerShell.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * For this task, [the latest version of PowerShell 7](https://aka.ms/installpowershell) is recommended because the examples use new features not available in Windows PowerShell 5.1.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Generate images

  1. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```powershell
     az login
     ```

  2. Get an Azure OpenAI auth token and set it as an environment variable for the current PowerShell session:

     ```powershell
     $Env:DEFAULT_AZURE_CREDENTIAL_TOKEN = az account get-access-token --resource https://cognitiveservices.azure.com --query accessToken -o tsv
     ```

  3. Create a new PowerShell file called *quickstart.ps1*. Then open it up in your preferred editor or IDE.

  4. Replace the contents of *quickstart.ps1* with the following code. Make sure `AZURE_OPENAI_ENDPOINT` is set, and change the value of `prompt` to your preferred text.

     To use API key authentication instead of keyless authentication, set `AZURE_OPENAI_API_KEY` and uncomment the `'api-key'` line.

     ```powershell
      # Azure OpenAI metadata variables
      $openai = @{
          api_base    = $Env:AZURE_OPENAI_ENDPOINT
          api_version = '2023-06-01-preview' # This can change in the future.
      }

      # Use the recommended keyless authentication via bearer token.
      $headers = [ordered]@{
          #'api-key' = $Env:AZURE_OPENAI_API_KEY
          'Authorization' = "Bearer $($Env:DEFAULT_AZURE_CREDENTIAL_TOKEN)"
      }

      # Text to describe image
      $prompt = 'A painting of a dog'

      # Adjust these values to fine-tune completions
      $body = [ordered]@{
          prompt = $prompt
          size   = '1024x1024'
          n      = 1
      } | ConvertTo-Json

      # Call the API to generate the image and retrieve the response
      $url = "$($openai.api_base)/openai/images/generations:submit?api-version=$($openai.api_version)"

      $submission = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json' -ResponseHeadersVariable submissionHeaders

      $operation_location = $submissionHeaders['operation-location'][0]
      $status = ''
      while ($status -ne 'succeeded') {
          Start-Sleep -Seconds 1
          $response = Invoke-RestMethod -Uri $operation_location -Headers $headers
          $status   = $response.status
      }

      # Set the directory for the stored image
      $image_dir = Join-Path -Path $pwd -ChildPath 'images'

      # If the directory doesn't exist, create it
      if (-not(Resolve-Path $image_dir -ErrorAction Ignore)) {
          New-Item -Path $image_dir -ItemType Directory
      }

      # Initialize the image path (note the filetype should be png)
      $image_path = Join-Path -Path $image_dir -ChildPath 'generated_image.png'

      # Retrieve the generated image
      $image_url = $response.result.data[0].url  # extract image URL from response
      $generated_image = Invoke-WebRequest -Uri $image_url -OutFile $image_path  # download the image
      return $image_path
     ```

     <Callout type="important">
       For production, use a secure way of storing and accessing your credentials like [The PowerShell Secret Management with Azure Key Vault](https://learn.microsoft.com/en-us/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information about credential security, see this [security](https://learn.microsoft.com/en-us/azure/ai-services/security-features) article.
     </Callout>

  5. Run the script using PowerShell:

     ```powershell
     ./quickstart.ps1
     ```

     The script loops until the generated image is ready.

  ## Output

  PowerShell requests the image from Azure OpenAI and stores the output image in the *generated\_image.png* file in your specified directory. For convenience, the full path for the file is returned at the end of the script.

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter).

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure PowerShell](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azpowershell#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).

  - Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-studio">
  Use this guide to get started generating images with Azure OpenAI in your browser with Microsoft Foundry.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Go to Foundry

  Browse to [Foundry](https://ai.azure.com/?cid=learnDocs) and sign in with the credentials associated with your Azure OpenAI resource. During or after the sign-in workflow, select the appropriate directory, Azure subscription, and Azure OpenAI resource.

  From the Foundry landing page, create or select a new project. Navigate to the **Models + endpoints** page on the left nav. Select **Deploy model** and then choose one of the DALL-E models from the list. Complete the deployment process.

  On the model's page, select **Open in playground**.

  ## Try out image generation

  Start exploring Azure OpenAI capabilities with a no-code approach through the **Images playground**. Enter your image prompt into the text box and select **Generate**. When the AI-generated image is ready, it appears on the page.

  <Callout type="note">
    The Image APIs come with a content moderation filter. If Azure OpenAI recognizes your prompt as harmful content, it doesn't return a generated image. For more information, see [Content filtering](concepts/content-filter).
  </Callout>

  In the **Images playground**, you can also view Python and cURL code samples, which are prefilled according to your settings. Select **View code** near the top of the page. You can use this code to write an application that completes the same task.

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

<ZonePivot pivot="rest-api">
  Use this guide to get started calling the Azure OpenAI in Microsoft Foundry Models image generation REST APIs by using Python.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/).
  * The following Python libraries installed: `os`, `requests`, `json`.
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability).
  * Then, you need to deploy a `gpt-image-1`-series or `dalle3` model with your Azure resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Setup

  ### Retrieve key and endpoint

  To successfully call the Azure OpenAI APIs, you need the following information about your Azure OpenAI resource:

  | Variable     | Name       | Value                                                                                                                                                                                                                                              |
  | ------------ | ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | **Endpoint** | `api_base` | The endpoint value is located under **Keys and Endpoint** for your resource in the Azure portal. You can also find the endpoint via the **Deployments** page in Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | **Key**      | `api_key`  | The key value is also located under **Keys and Endpoint** for your resource in the Azure portal. Azure generates two keys for your resource. You can use either value.                                                                             |

  Go to your resource in the Azure portal. On the navigation pane, select **Keys and Endpoint** under **Resource Management**. Copy the **Endpoint** value and an access key value. You can use either the **KEY 1** or **KEY 2** value. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot that shows the Keys and Endpoint page for an Azure OpenAI resource in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    We recommend Microsoft Entra ID authentication with [managed identities for Azure resources](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to avoid storing credentials with your applications that run in the cloud.

    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys regularly, and restrict access to Azure Key Vault using role based access control and network access restrictions. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```cmd
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```bash
      export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>
  </Tabs>

  ## Create a new Python application

  Create a new Python file named *quickstart.py*. Open the new file in your preferred editor or IDE.

  <Tabs>
    <Tab title="GPT-image-1 series">
      1. Replace the contents of *quickstart.py* with the following code. Change the value of `prompt` to your preferred text. Also set `deployment` to the deployment name you chose when you deployed the GPT-image-1 series model.

         ```python
         import os
         import requests
         import base64
         from PIL import Image
         from io import BytesIO

         # set environment variables
         endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
         subscription_key = os.getenv("AZURE_OPENAI_API_KEY")

         deployment = "gpt-image-1.5" # the name of your GPT-image-1 series deployment
         api_version = "2025-04-01-preview" # or later version

         def decode_and_save_image(b64_data, output_filename):
           image = Image.open(BytesIO(base64.b64decode(b64_data)))
           image.show()
           image.save(output_filename)

         def save_all_images_from_response(response_data, filename_prefix):
           for idx, item in enumerate(response_data['data']):
             b64_img = item['b64_json']
             filename = f"{filename_prefix}_{idx+1}.png"
             decode_and_save_image(b64_img, filename)
             print(f"Image saved to: '{filename}'")

         base_path = f'openai/deployments/{deployment}/images'
         params = f'?api-version={api_version}'

         generation_url = f"{endpoint}{base_path}/generations{params}"
         generation_body = {
           "prompt": "girl falling asleep",
           "n": 1,
           "size": "1024x1024",
           "quality": "medium",
           "output_format": "png"
         }
         generation_response = requests.post(
           generation_url,
           headers={
             'Api-Key': subscription_key,
             'Content-Type': 'application/json',
           },
           json=generation_body
         ).json()
         save_all_images_from_response(generation_response, "generated_image")

         # In addition to generating images, you can edit them.
         edit_url = f"{endpoint}{base_path}/edits{params}"
         edit_body = {
           "prompt": "girl falling asleep",
           "n": 1,
           "size": "1024x1024",
           "quality": "medium"
         }
         files = {
           "image": ("generated_image_1.png", open("generated_image_1.png", "rb"), "image/png"),
           # You can use a mask to specify which parts of the image you want to edit.
           # The mask must be the same size as the input image.
           # "mask": ("mask.png", open("mask.png", "rb"), "image/png"),
         }
         edit_response = requests.post(
           edit_url,
           headers={'Api-Key': subscription_key},
           data=edit_body,
           files=files
         ).json()
         save_all_images_from_response(edit_response, "edited_image")
         ```

         The script makes a synchronous image generation API call.

         <Callout type="important">
           Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
         </Callout>

      2. Run the application with the `python` command:

         ```console
         python quickstart.py
         ```

         Wait a few moments to get the response.
    </Tab>

    <Tab title="DALL-E">
      1. Replace the contents of *quickstart.py* with the following code. Change the value of `prompt` to your preferred text.

         You also need to replace `<dalle3>` in the URL with the deployment name you chose when you deployed the DALL-E 3 model. Entering the model name will result in an error unless you chose a deployment name that is identical to the underlying model name. If you encounter an error, double check to make sure that you don't have a doubling of the `/` at the separation between your endpoint and `/openai/deployments`.

         ```python
         import requests
         import time
         import os
         api_base = os.environ['AZURE_OPENAI_ENDPOINT']  # Enter your endpoint here
         api_key = os.environ['AZURE_OPENAI_API_KEY']         # Enter your API key here

         api_version = '2024-02-01'
         url = f"{api_base}/openai/deployments/<dalle3>/images/generations?api-version={api_version}"
         headers= { "api-key": api_key, "Content-Type": "application/json" }
         body = {
             # Enter your prompt text here
             "prompt": "A multi-colored umbrella on the beach, disposable camera",
             "size": "1024x1024", # supported values are “1792x1024”, “1024x1024” and “1024x1792”
             "n": 1, #The number of images to generate. Only n=1 is supported for DALL-E 3.
             "quality": "hd", # Options are “hd” and “standard”; defaults to standard
             "style": "vivid" # Options are “natural” and “vivid”; defaults to “vivid”
         }
         submission = requests.post(url, headers=headers, json=body)

         image_url = submission.json()['data'][0]['url']

         print(image_url)
         ```

         The script makes a synchronous image generation API call.

         <Callout type="important">
           Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
         </Callout>

      2. Run the application with the `python` command:

         ```console
         python quickstart.py
         ```

         Wait a few moments to get the response.
    </Tab>
  </Tabs>

  ## Output

  The output from a successful image generation API call looks like the following example. The `url` field contains a URL where you can download the generated image. The URL stays active for 24 hours.

  ```json
  {
      "created": 1698116662,
      "data": [
          {
              "url": "<URL_to_generated_image>",
              "revised_prompt": "<prompt_that_was_used>"
          }
      ]
  }
  ```

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter). For examples of error responses, see the [Image generation how-to guide](how-to/dall-e).

  The system returns an operation status of `Failed` and the `error.code` value in the message is set to `contentFilter`. Here's an example:

  ```json
  {
      "created": 1698435368,
      "error":
      {
          "code": "contentFilter",
          "message": "Your task failed as a result of our safety system."
      }
  }
  ```

  It's also possible that the generated image itself is filtered. In this case, the error message is set to `Generated image was filtered as a result of our safety system.`. Here's an example:

  ```json
  {
      "created": 1698435368,
      "error":
      {
          "code": "contentFilter",
          "message": "Generated image was filtered as a result of our safety system."
      }
  }
  ```

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  Use this guide to get started generating images with the Azure OpenAI SDK for Python.

  [Library source code](https://github.com/openai/openai-python/tree/main/src/openai) | [Package](https://github.com/openai/openai-python) | [Samples](https://github.com/openai/openai-python/tree/main/examples)

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/).
  * An Azure OpenAI resource created in a compatible region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability).
    * Access the Azure OpenAI resource endpoint and keys in the Azure portal.
  * Then, you need to deploy a `dalle3` model with your Azure resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Setup

  ### Retrieve key and endpoint

  To successfully call the Azure OpenAI APIs, you need the following information about your Azure OpenAI resource:

  | Variable     | Name       | Value                                                                                                                                                                                                                                                        |
  | ------------ | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
  | **Endpoint** | `api_base` | The endpoint value is located under **Keys and Endpoint** for your resource in the Azure portal. You can also find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | **Key**      | `api_key`  | The key value is also located under **Keys and Endpoint** for your resource in the Azure portal. Azure generates two keys for your resource. You can use either value.                                                                                       |

  Go to your resource in the Azure portal. On the navigation pane, select **Keys and Endpoint** under **Resource Management**. Copy the **Endpoint** value and an access key value. You can use either the **KEY 1** or **KEY 2** value. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot that shows the Keys and Endpoint page for an Azure OpenAI resource in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    We recommend Microsoft Entra ID authentication with [managed identities for Azure resources](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to avoid storing credentials with your applications that run in the cloud.

    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys regularly, and restrict access to Azure Key Vault using role based access control and network access restrictions. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```cmd
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```bash
      export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>
  </Tabs>

  ## Install the Python SDK

  Open a command prompt and browse to your project folder. Install the OpenAI Python SDK by using the following command:

  ```bash
  pip install openai
  ```

  Install the following libraries as well:

  ```bash
  pip install requests
  pip install pillow
  ```

  ## Generate images with DALL-E

  Create a new python file, *quickstart.py*. Open it in your preferred editor or IDE.

  Replace the contents of *quickstart.py* with the following code.

  ```python
  from openai import AzureOpenAI
  import os
  import requests
  from PIL import Image
  import json

  client = AzureOpenAI(
      api_version="2024-02-01",
      api_key=os.environ["AZURE_OPENAI_API_KEY"],
      azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT']
  )

  result = client.images.generate(
      model="dalle3", # the name of your DALL-E 3 deployment
      prompt="a close-up of a bear walking through the forest",
      n=1
  )

  json_response = json.loads(result.model_dump_json())

  # Set the directory for the stored image
  image_dir = os.path.join(os.curdir, 'images')

  # If the directory doesn't exist, create it
  if not os.path.isdir(image_dir):
      os.mkdir(image_dir)

  # Initialize the image path (note the filetype should be png)
  image_path = os.path.join(image_dir, 'generated_image.png')

  # Retrieve the generated image
  image_url = json_response["data"][0]["url"]  # extract image URL from response
  generated_image = requests.get(image_url).content  # download the image
  with open(image_path, "wb") as image_file:
      image_file.write(generated_image)

  # Display the image in the default image viewer
  image = Image.open(image_path)
  image.show()
  ```

  1. Make sure the `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_API_KEY` environment variables are set.
  2. Change the value of `prompt` to your preferred text.
  3. Change the value of `model` to the name of your deployed DALL-E 3 model.

  <Callout type="important">
    Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
  </Callout>

  Run the application with the `python` command:

  ```console
  python quickstart.py
  ```

  Wait a few moments to get the response.

  ## Output

  Azure OpenAI stores the output image in the *generated\_image.png* file in your specified directory. The script also displays the image in your default image viewer.

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter).

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<ZonePivot pivot="programming-language-csharp">
  Use this guide to get started generating images with the Azure OpenAI SDK for C#.

  [Library source code](https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI) | [Package (NuGet)](https://www.nuget.org/packages/Azure.AI.OpenAI/) | [Samples](https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI/tests/Samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * The [.NET 7 SDK](https://dotnet.microsoft.com/download/dotnet/7.0)
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create a new console application with the following command:

     ```shell
     dotnet new console
     ```

  3. Install the [OpenAI .NET client library](https://www.nuget.org/packages/Azure.AI.OpenAI/) with the [dotnet add package](https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-add-package) command:

     ```console
     dotnet add package Azure.AI.OpenAI --version 1.0.0-beta.6
     ```

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the [Azure.Identity](https://www.nuget.org/packages/Azure.Identity) package with:

     ```console
     dotnet add package Azure.Identity
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
      ```
    </Tab>

    <Tab title="API key">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));
      ```
    </Tab>
  </Tabs>

  To run the quickstart, follow these steps:

  1. Replace the contents of `Program.cs` with the following code and update the placeholder values with your own.

     ```csharp
     using Azure;
     using Azure.AI.OpenAI;
     using OpenAI.Images;
     using static System.Environment;

     string endpoint = Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT") ?? "https://<your-resource-name>.openai.azure.com/";
     string key = Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY") ?? "<your-key>";

     // Use the recommended keyless credential instead of the AzureKeyCredential credential.
     AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
     //AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));

     // This must match the custom deployment name you chose for your model
     ImageClient chatClient = openAIClient.GetImageClient("dalle-3");

     var imageGeneration = await chatClient.GenerateImageAsync(
             "a happy monkey sitting in a tree, in watercolor",
             new ImageGenerationOptions()
             {
                 Size = GeneratedImageSize.W1024xH1024
             }
         );

     Console.WriteLine(imageGeneration.Value.ImageUri);
     ```

  2. Run the application using the `dotnet run` command or the run button at the top of Visual Studio:

     ```dotnetcli
     dotnet run
     ```

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-java">
  Use this guide to get started generating images with the Azure OpenAI SDK for Java.

  [Library source code](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/openai/azure-ai-openai) | [Artifact (Maven)](https://central.sonatype.com/artifact/com.azure/azure-ai-openai/1.0.0-beta.3) | [Samples](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/openai/azure-ai-openai/src/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * The current version of the [Java Development Kit (JDK)](https://www.microsoft.com/openjdk)
  * Install [Apache Maven](https://maven.apache.org/install.html).
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Install [Apache Maven](https://maven.apache.org/install.html). Then run `mvn -v` to confirm successful installation.

  3. Create a new `pom.xml` file in the root of your project, and copy the following code into it:

     ```xml
     <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
          <modelVersion>4.0.0</modelVersion>
          <groupId>com.azure.samples</groupId>
          <artifactId>quickstart-dall-e</artifactId>
          <version>1.0.0-SNAPSHOT</version>
          <build>
              <sourceDirectory>src</sourceDirectory>
              <plugins>
              <plugin>
                  <artifactId>maven-compiler-plugin</artifactId>
                  <version>3.7.0</version>
                  <configuration>
                  <source>1.8</source>
                  <target>1.8</target>
                  </configuration>
              </plugin>
              </plugins>
          </build>
          <dependencies>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-ai-openai</artifactId>
                  <version>1.0.0-beta.3</version>
              </dependency>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-core</artifactId>
                  <version>1.53.0</version>
              </dependency>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-identity</artifactId>
                  <version>1.15.1</version>
              </dependency>
              <dependency>
                  <groupId>org.slf4j</groupId>
                  <artifactId>slf4j-simple</artifactId>
                  <version>1.7.9</version>
              </dependency>
          </dependencies>
      </project>
     ```

  4. Install the Azure OpenAI SDK and dependencies.

     ```console
     mvn clean dependency:copy-dependencies
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the app

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```java
      OpenAIAsyncClient client = new OpenAIClientBuilder()
          .endpoint(endpoint)
          .credential(new DefaultAzureCredentialBuilder().build())
          .buildAsyncClient();
      ```
    </Tab>

    <Tab title="API key">
      ```java
      OpenAIAsyncClient client = new OpenAIClientBuilder()
          .endpoint(endpoint)
          .credential(new AzureKeyCredential(key))
          .buildAsyncClient();
      ```
    </Tab>
  </Tabs>

  <Tabs>
    <Tab title="Microsoft Entra ID">
      Follow these steps to create a console application for image generation.

      1. Create a new file named *Quickstart.java* in the same project root directory.

      2. Copy the following code into *Quickstart.java*:

         ```java
         import com.azure.ai.openai.OpenAIAsyncClient;
         import com.azure.ai.openai.OpenAIClientBuilder;
         import com.azure.ai.openai.models.ImageGenerationOptions;
         import com.azure.ai.openai.models.ImageLocation;
         import com.azure.core.credential.AzureKeyCredential;
         import com.azure.core.models.ResponseError;

         import java.util.concurrent.TimeUnit;

         public class Quickstart {

             public static void main(String[] args) throws InterruptedException {

                 String endpoint = System.getenv("AZURE_OPENAI_ENDPOINT");

                 // Use the recommended keyless credential instead of the AzureKeyCredential credential.

                 OpenAIAsyncClient client = new OpenAIClientBuilder()
                     .endpoint(endpoint)
                     .credential(new DefaultAzureCredentialBuilder().build())
                     .buildAsyncClient();

                 ImageGenerationOptions imageGenerationOptions = new ImageGenerationOptions(
                     "A drawing of the Seattle skyline in the style of Van Gogh");
                 client.getImages(imageGenerationOptions).subscribe(
                     images -> {
                         for (ImageLocation imageLocation : images.getData()) {
                             ResponseError error = imageLocation.getError();
                             if (error != null) {
                                 System.out.printf("Image generation operation failed. Error code: %s, error message: %s.%n",
                                     error.getCode(), error.getMessage());
                             } else {
                                 System.out.printf(
                                     "Image location URL that provides temporary access to download the generated image is %s.%n",
                                     imageLocation.getUrl());
                             }
                         }
                     },
                     error -> System.err.println("There was an error getting images." + error),
                     () -> System.out.println("Completed getImages."));

                 // The .subscribe() creation and assignment isn't a blocking call.
                 // The thread sleeps so the program does not end before the send operation is complete.
                 // Use .block() instead of .subscribe() for a synchronous call.
                 TimeUnit.SECONDS.sleep(10);
             }
         }
         ```

      3. Run your new console application to generate an image:

         ```console
         javac Quickstart.java -cp ".;target\dependency\*"
         java -cp ".;target\dependency\*" Quickstart
         ```
    </Tab>

    <Tab title="API key">
      Follow these steps to create a console application for image generation.

      1. Create a new file named *Quickstart.java* in the same project root directory.

      2. Copy the following code into *Quickstart.java*:

         ```java
         import com.azure.ai.openai.OpenAIAsyncClient;
         import com.azure.ai.openai.OpenAIClientBuilder;
         import com.azure.ai.openai.models.ImageGenerationOptions;
         import com.azure.ai.openai.models.ImageLocation;
         import com.azure.identity.DefaultAzureCredentialBuilder;
         import com.azure.core.models.ResponseError;

         import java.util.concurrent.TimeUnit;

         public class Quickstart {

             public static void main(String[] args) throws InterruptedException {

                 String key = System.getenv("AZURE_OPENAI_API_KEY");
                 String endpoint = System.getenv("AZURE_OPENAI_ENDPOINT");

                 OpenAIAsyncClient client = new OpenAIClientBuilder()
                     .endpoint(endpoint)
                     .credential(new AzureKeyCredential(key))
                     .buildAsyncClient();

                 ImageGenerationOptions imageGenerationOptions = new ImageGenerationOptions(
                     "A drawing of the Seattle skyline in the style of Van Gogh");
                 client.getImages(imageGenerationOptions).subscribe(
                     images -> {
                         for (ImageLocation imageLocation : images.getData()) {
                             ResponseError error = imageLocation.getError();
                             if (error != null) {
                                 System.out.printf("Image generation operation failed. Error code: %s, error message: %s.%n",
                                     error.getCode(), error.getMessage());
                             } else {
                                 System.out.printf(
                                     "Image location URL that provides temporary access to download the generated image is %s.%n",
                                     imageLocation.getUrl());
                             }
                         }
                     },
                     error -> System.err.println("There was an error getting images." + error),
                     () -> System.out.println("Completed getImages."));

                 // The .subscribe() creation and assignment isn't a blocking call.
                 // The thread sleeps so the program does not end before the send operation is complete.
                 // Use .block() instead of .subscribe() for a synchronous call.
                 TimeUnit.SECONDS.sleep(10);
             }
         }
         ```

      3. Run your new console application to generate an image:

         ```console
         javac Quickstart.java -cp ".;target\dependency\*"
         java -cp ".;target\dependency\*" Quickstart
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  Image location URL that provides temporary access to download the generated image is <SAS URL>.
  Completed getImages.
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai)
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  Use this guide to get started generating images with the Azure OpenAI SDK for JavaScript.

  [Reference documentation](https://platform.openai.com/docs/api-reference/images/create) | [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  4. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Generate images with DALL-E

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { AzureOpenAI } = require("openai");
         const {
             DefaultAzureCredential,
             getBearerTokenProvider
         } = require("@azure/identity");

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { AzureOpenAI } = require("openai");

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         function getClient() {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  == Batch Image Generation ==
  Image generation result URL: <SAS URL>
  Image generation result URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  Use this guide to get started generating images with the Azure OpenAI SDK for JavaScript.

  [Reference documentation](https://platform.openai.com/docs/api-reference/images/create) | [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [TypeScript](https://www.typescriptlang.org/download/)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```shell
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  5. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Generate images with DALL-E

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { AzureOpenAI } from "openai";
         import {
             DefaultAzureCredential,
             getBearerTokenProvider
         } from "@azure/identity";

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      5. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  == Batch Image Generation ==
  Image generation result URL: <SAS URL>
  Image generation result URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/azure-openai-samples).
</ZonePivot>

<ZonePivot pivot="programming-language-go">
  Use this guide to get started generating images with the Azure OpenAI SDK for Go.

  [Library source code](https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai) | [Package](https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai) | [Samples](https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [Go 1.8+](https://go.dev/doc/install)
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `dall-e-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir dall-e-quickstart && cd dall-e-quickstart
     ```

  2. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `NewDefaultAzureCredential` implementation with `NewKeyCredential`.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```go
      azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
      credential, err := azidentity.NewDefaultAzureCredential(nil)
      client, err := azopenai.NewClient(azureOpenAIEndpoint, credential, nil)
      ```
    </Tab>

    <Tab title="API key">
      ```go
      azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
      azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
      credential := azcore.NewKeyCredential(azureOpenAIKey)
      client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)
      ```
    </Tab>
  </Tabs>

  <Tabs>
    <Tab title="Microsoft Entra ID">
      To run the sample:

      1. Create a new file named *quickstart.go*. Copy the following code into the *quickstart.go* file.

         ```go
          package main

         import (
         	"context"
         	"fmt"
         	"net/http"
         	"os"
         	"log"

         	"github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
         	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
         )

         func main() {
         	azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
         	modelDeploymentID := "dall-e-3"

         	credential, err := azidentity.NewDefaultAzureCredential(nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	client, err := azopenai.NewClient(
         		azureOpenAIEndpoint, credential, nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	resp, err := client.GetImageGenerations(context.TODO(), azopenai.ImageGenerationOptions{
         		Prompt:         to.Ptr("A painting of a cat in the style of Dali."),
         		ResponseFormat: to.Ptr(azopenai.ImageGenerationResponseFormatURL),
         		DeploymentName: to.Ptr(modelDeploymentID),
         	}, nil)

         	if err != nil {
         		// Implement application specific error handling logic.
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	for _, generatedImage := range resp.Data {
         		// The underlying type for the generatedImage is determined by the value of
         		// ImageGenerationOptions.ResponseFormat.
         		// In this example we use `azopenai.ImageGenerationResponseFormatURL`,
         		// so the underlying type will be ImageLocation.

         		resp, err := http.Head(*generatedImage.URL)

         		if err != nil {
         			// Implement application specific error handling logic.
         			log.Printf("ERROR: %s", err)
         			return
         		}

         		fmt.Fprintf(os.Stderr, "Image generated, HEAD request on URL returned %d\nImage URL: %s\n", resp.StatusCode, *generatedImage.URL)
         	}
         }
         ```

      2. Run the following command to create a new Go module:

         ```shell
          go mod init quickstart.go
         ```

      3. Run `go mod tidy` to install the required dependencies:

         ```cmd
         go mod tidy
         ```

      4. Run the following command to run the sample:

         ```shell
          go run quickstart.go
         ```
    </Tab>

    <Tab title="API key">
      To run the sample:

      1. Create a new file named *quickstart.go*. Copy the following code into the *quickstart.go* file.

         ```go
         package main

         import (
         	"context"
         	"fmt"
         	"net/http"
         	"os"
         	"log"

         	"github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
         )

         func main() {
         	azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
         	modelDeploymentID := "dall-e-3"

         	azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
         	credential := azcore.NewKeyCredential(azureOpenAIKey)

         	client, err := azopenai.NewClientWithKeyCredential(
         		azureOpenAIEndpoint, credential, nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	resp, err := client.GetImageGenerations(context.TODO(), azopenai.ImageGenerationOptions{
         		Prompt:         to.Ptr("A painting of a cat in the style of Dali."),
         		ResponseFormat: to.Ptr(azopenai.ImageGenerationResponseFormatURL),
         		DeploymentName: to.Ptr(modelDeploymentID),
         	}, nil)

         	if err != nil {
         		// Implement application specific error handling logic.
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	for _, generatedImage := range resp.Data {
         		// The underlying type for the generatedImage is determined by the value of
         		// ImageGenerationOptions.ResponseFormat.
         		// In this example we use `azopenai.ImageGenerationResponseFormatURL`,
         		// so the underlying type will be ImageLocation.

         		resp, err := http.Head(*generatedImage.URL)

         		if err != nil {
         			// Implement application specific error handling logic.
         			log.Printf("ERROR: %s", err)
         			return
         		}

         		fmt.Fprintf(os.Stderr, "Image generated, HEAD request on URL returned %d\nImage URL: %s\n", resp.StatusCode, *generatedImage.URL)
         	}
         }
         ```

      2. Run the following command to create a new Go module:

         ```shell
          go mod init quickstart.go
         ```

      3. Run `go mod tidy` to install the required dependencies:

         ```cmd
         go mod tidy
         ```

      4. Run the following command to run the sample:

         ```shell
          go run quickstart.go
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  Image generated, HEAD request on URL returned 200
  Image URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-powershell">
  Use this guide to get started calling the Azure OpenAI in Microsoft Foundry Models image generation APIs with PowerShell.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * For this task, [the latest version of PowerShell 7](https://aka.ms/installpowershell) is recommended because the examples use new features not available in Windows PowerShell 5.1.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Generate images

  1. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```powershell
     az login
     ```

  2. Get an Azure OpenAI auth token and set it as an environment variable for the current PowerShell session:

     ```powershell
     $Env:DEFAULT_AZURE_CREDENTIAL_TOKEN = az account get-access-token --resource https://cognitiveservices.azure.com --query accessToken -o tsv
     ```

  3. Create a new PowerShell file called *quickstart.ps1*. Then open it up in your preferred editor or IDE.

  4. Replace the contents of *quickstart.ps1* with the following code. Make sure `AZURE_OPENAI_ENDPOINT` is set, and change the value of `prompt` to your preferred text.

     To use API key authentication instead of keyless authentication, set `AZURE_OPENAI_API_KEY` and uncomment the `'api-key'` line.

     ```powershell
      # Azure OpenAI metadata variables
      $openai = @{
          api_base    = $Env:AZURE_OPENAI_ENDPOINT
          api_version = '2023-06-01-preview' # This can change in the future.
      }

      # Use the recommended keyless authentication via bearer token.
      $headers = [ordered]@{
          #'api-key' = $Env:AZURE_OPENAI_API_KEY
          'Authorization' = "Bearer $($Env:DEFAULT_AZURE_CREDENTIAL_TOKEN)"
      }

      # Text to describe image
      $prompt = 'A painting of a dog'

      # Adjust these values to fine-tune completions
      $body = [ordered]@{
          prompt = $prompt
          size   = '1024x1024'
          n      = 1
      } | ConvertTo-Json

      # Call the API to generate the image and retrieve the response
      $url = "$($openai.api_base)/openai/images/generations:submit?api-version=$($openai.api_version)"

      $submission = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json' -ResponseHeadersVariable submissionHeaders

      $operation_location = $submissionHeaders['operation-location'][0]
      $status = ''
      while ($status -ne 'succeeded') {
          Start-Sleep -Seconds 1
          $response = Invoke-RestMethod -Uri $operation_location -Headers $headers
          $status   = $response.status
      }

      # Set the directory for the stored image
      $image_dir = Join-Path -Path $pwd -ChildPath 'images'

      # If the directory doesn't exist, create it
      if (-not(Resolve-Path $image_dir -ErrorAction Ignore)) {
          New-Item -Path $image_dir -ItemType Directory
      }

      # Initialize the image path (note the filetype should be png)
      $image_path = Join-Path -Path $image_dir -ChildPath 'generated_image.png'

      # Retrieve the generated image
      $image_url = $response.result.data[0].url  # extract image URL from response
      $generated_image = Invoke-WebRequest -Uri $image_url -OutFile $image_path  # download the image
      return $image_path
     ```

     <Callout type="important">
       For production, use a secure way of storing and accessing your credentials like [The PowerShell Secret Management with Azure Key Vault](https://learn.microsoft.com/en-us/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information about credential security, see this [security](https://learn.microsoft.com/en-us/azure/ai-services/security-features) article.
     </Callout>

  5. Run the script using PowerShell:

     ```powershell
     ./quickstart.ps1
     ```

     The script loops until the generated image is ready.

  ## Output

  PowerShell requests the image from Azure OpenAI and stores the output image in the *generated\_image.png* file in your specified directory. For convenience, the full path for the file is returned at the end of the script.

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter).

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure PowerShell](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azpowershell#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).

  - Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-studio">
  Use this guide to get started generating images with Azure OpenAI in your browser with Microsoft Foundry.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Go to Foundry

  Browse to [Foundry](https://ai.azure.com/?cid=learnDocs) and sign in with the credentials associated with your Azure OpenAI resource. During or after the sign-in workflow, select the appropriate directory, Azure subscription, and Azure OpenAI resource.

  From the Foundry landing page, create or select a new project. Navigate to the **Models + endpoints** page on the left nav. Select **Deploy model** and then choose one of the DALL-E models from the list. Complete the deployment process.

  On the model's page, select **Open in playground**.

  ## Try out image generation

  Start exploring Azure OpenAI capabilities with a no-code approach through the **Images playground**. Enter your image prompt into the text box and select **Generate**. When the AI-generated image is ready, it appears on the page.

  <Callout type="note">
    The Image APIs come with a content moderation filter. If Azure OpenAI recognizes your prompt as harmful content, it doesn't return a generated image. For more information, see [Content filtering](concepts/content-filter).
  </Callout>

  In the **Images playground**, you can also view Python and cURL code samples, which are prefilled according to your settings. Select **View code** near the top of the page. You can use this code to write an application that completes the same task.

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

<ZonePivot pivot="rest-api">
  Use this guide to get started calling the Azure OpenAI in Microsoft Foundry Models image generation REST APIs by using Python.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/).
  * The following Python libraries installed: `os`, `requests`, `json`.
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability).
  * Then, you need to deploy a `gpt-image-1`-series or `dalle3` model with your Azure resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Setup

  ### Retrieve key and endpoint

  To successfully call the Azure OpenAI APIs, you need the following information about your Azure OpenAI resource:

  | Variable     | Name       | Value                                                                                                                                                                                                                                              |
  | ------------ | ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | **Endpoint** | `api_base` | The endpoint value is located under **Keys and Endpoint** for your resource in the Azure portal. You can also find the endpoint via the **Deployments** page in Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | **Key**      | `api_key`  | The key value is also located under **Keys and Endpoint** for your resource in the Azure portal. Azure generates two keys for your resource. You can use either value.                                                                             |

  Go to your resource in the Azure portal. On the navigation pane, select **Keys and Endpoint** under **Resource Management**. Copy the **Endpoint** value and an access key value. You can use either the **KEY 1** or **KEY 2** value. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot that shows the Keys and Endpoint page for an Azure OpenAI resource in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    We recommend Microsoft Entra ID authentication with [managed identities for Azure resources](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to avoid storing credentials with your applications that run in the cloud.

    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys regularly, and restrict access to Azure Key Vault using role based access control and network access restrictions. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```cmd
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```bash
      export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>
  </Tabs>

  ## Create a new Python application

  Create a new Python file named *quickstart.py*. Open the new file in your preferred editor or IDE.

  <Tabs>
    <Tab title="GPT-image-1 series">
      1. Replace the contents of *quickstart.py* with the following code. Change the value of `prompt` to your preferred text. Also set `deployment` to the deployment name you chose when you deployed the GPT-image-1 series model.

         ```python
         import os
         import requests
         import base64
         from PIL import Image
         from io import BytesIO

         # set environment variables
         endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
         subscription_key = os.getenv("AZURE_OPENAI_API_KEY")

         deployment = "gpt-image-1.5" # the name of your GPT-image-1 series deployment
         api_version = "2025-04-01-preview" # or later version

         def decode_and_save_image(b64_data, output_filename):
           image = Image.open(BytesIO(base64.b64decode(b64_data)))
           image.show()
           image.save(output_filename)

         def save_all_images_from_response(response_data, filename_prefix):
           for idx, item in enumerate(response_data['data']):
             b64_img = item['b64_json']
             filename = f"{filename_prefix}_{idx+1}.png"
             decode_and_save_image(b64_img, filename)
             print(f"Image saved to: '{filename}'")

         base_path = f'openai/deployments/{deployment}/images'
         params = f'?api-version={api_version}'

         generation_url = f"{endpoint}{base_path}/generations{params}"
         generation_body = {
           "prompt": "girl falling asleep",
           "n": 1,
           "size": "1024x1024",
           "quality": "medium",
           "output_format": "png"
         }
         generation_response = requests.post(
           generation_url,
           headers={
             'Api-Key': subscription_key,
             'Content-Type': 'application/json',
           },
           json=generation_body
         ).json()
         save_all_images_from_response(generation_response, "generated_image")

         # In addition to generating images, you can edit them.
         edit_url = f"{endpoint}{base_path}/edits{params}"
         edit_body = {
           "prompt": "girl falling asleep",
           "n": 1,
           "size": "1024x1024",
           "quality": "medium"
         }
         files = {
           "image": ("generated_image_1.png", open("generated_image_1.png", "rb"), "image/png"),
           # You can use a mask to specify which parts of the image you want to edit.
           # The mask must be the same size as the input image.
           # "mask": ("mask.png", open("mask.png", "rb"), "image/png"),
         }
         edit_response = requests.post(
           edit_url,
           headers={'Api-Key': subscription_key},
           data=edit_body,
           files=files
         ).json()
         save_all_images_from_response(edit_response, "edited_image")
         ```

         The script makes a synchronous image generation API call.

         <Callout type="important">
           Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
         </Callout>

      2. Run the application with the `python` command:

         ```console
         python quickstart.py
         ```

         Wait a few moments to get the response.
    </Tab>

    <Tab title="DALL-E">
      1. Replace the contents of *quickstart.py* with the following code. Change the value of `prompt` to your preferred text.

         You also need to replace `<dalle3>` in the URL with the deployment name you chose when you deployed the DALL-E 3 model. Entering the model name will result in an error unless you chose a deployment name that is identical to the underlying model name. If you encounter an error, double check to make sure that you don't have a doubling of the `/` at the separation between your endpoint and `/openai/deployments`.

         ```python
         import requests
         import time
         import os
         api_base = os.environ['AZURE_OPENAI_ENDPOINT']  # Enter your endpoint here
         api_key = os.environ['AZURE_OPENAI_API_KEY']         # Enter your API key here

         api_version = '2024-02-01'
         url = f"{api_base}/openai/deployments/<dalle3>/images/generations?api-version={api_version}"
         headers= { "api-key": api_key, "Content-Type": "application/json" }
         body = {
             # Enter your prompt text here
             "prompt": "A multi-colored umbrella on the beach, disposable camera",
             "size": "1024x1024", # supported values are “1792x1024”, “1024x1024” and “1024x1792”
             "n": 1, #The number of images to generate. Only n=1 is supported for DALL-E 3.
             "quality": "hd", # Options are “hd” and “standard”; defaults to standard
             "style": "vivid" # Options are “natural” and “vivid”; defaults to “vivid”
         }
         submission = requests.post(url, headers=headers, json=body)

         image_url = submission.json()['data'][0]['url']

         print(image_url)
         ```

         The script makes a synchronous image generation API call.

         <Callout type="important">
           Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
         </Callout>

      2. Run the application with the `python` command:

         ```console
         python quickstart.py
         ```

         Wait a few moments to get the response.
    </Tab>
  </Tabs>

  ## Output

  The output from a successful image generation API call looks like the following example. The `url` field contains a URL where you can download the generated image. The URL stays active for 24 hours.

  ```json
  {
      "created": 1698116662,
      "data": [
          {
              "url": "<URL_to_generated_image>",
              "revised_prompt": "<prompt_that_was_used>"
          }
      ]
  }
  ```

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter). For examples of error responses, see the [Image generation how-to guide](how-to/dall-e).

  The system returns an operation status of `Failed` and the `error.code` value in the message is set to `contentFilter`. Here's an example:

  ```json
  {
      "created": 1698435368,
      "error":
      {
          "code": "contentFilter",
          "message": "Your task failed as a result of our safety system."
      }
  }
  ```

  It's also possible that the generated image itself is filtered. In this case, the error message is set to `Generated image was filtered as a result of our safety system.`. Here's an example:

  ```json
  {
      "created": 1698435368,
      "error":
      {
          "code": "contentFilter",
          "message": "Generated image was filtered as a result of our safety system."
      }
  }
  ```

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  Use this guide to get started generating images with the Azure OpenAI SDK for Python.

  [Library source code](https://github.com/openai/openai-python/tree/main/src/openai) | [Package](https://github.com/openai/openai-python) | [Samples](https://github.com/openai/openai-python/tree/main/examples)

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/).
  * An Azure OpenAI resource created in a compatible region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability).
    * Access the Azure OpenAI resource endpoint and keys in the Azure portal.
  * Then, you need to deploy a `dalle3` model with your Azure resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Setup

  ### Retrieve key and endpoint

  To successfully call the Azure OpenAI APIs, you need the following information about your Azure OpenAI resource:

  | Variable     | Name       | Value                                                                                                                                                                                                                                                        |
  | ------------ | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
  | **Endpoint** | `api_base` | The endpoint value is located under **Keys and Endpoint** for your resource in the Azure portal. You can also find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | **Key**      | `api_key`  | The key value is also located under **Keys and Endpoint** for your resource in the Azure portal. Azure generates two keys for your resource. You can use either value.                                                                                       |

  Go to your resource in the Azure portal. On the navigation pane, select **Keys and Endpoint** under **Resource Management**. Copy the **Endpoint** value and an access key value. You can use either the **KEY 1** or **KEY 2** value. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot that shows the Keys and Endpoint page for an Azure OpenAI resource in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    We recommend Microsoft Entra ID authentication with [managed identities for Azure resources](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to avoid storing credentials with your applications that run in the cloud.

    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys regularly, and restrict access to Azure Key Vault using role based access control and network access restrictions. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```cmd
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```bash
      export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>
  </Tabs>

  ## Install the Python SDK

  Open a command prompt and browse to your project folder. Install the OpenAI Python SDK by using the following command:

  ```bash
  pip install openai
  ```

  Install the following libraries as well:

  ```bash
  pip install requests
  pip install pillow
  ```

  ## Generate images with DALL-E

  Create a new python file, *quickstart.py*. Open it in your preferred editor or IDE.

  Replace the contents of *quickstart.py* with the following code.

  ```python
  from openai import AzureOpenAI
  import os
  import requests
  from PIL import Image
  import json

  client = AzureOpenAI(
      api_version="2024-02-01",
      api_key=os.environ["AZURE_OPENAI_API_KEY"],
      azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT']
  )

  result = client.images.generate(
      model="dalle3", # the name of your DALL-E 3 deployment
      prompt="a close-up of a bear walking through the forest",
      n=1
  )

  json_response = json.loads(result.model_dump_json())

  # Set the directory for the stored image
  image_dir = os.path.join(os.curdir, 'images')

  # If the directory doesn't exist, create it
  if not os.path.isdir(image_dir):
      os.mkdir(image_dir)

  # Initialize the image path (note the filetype should be png)
  image_path = os.path.join(image_dir, 'generated_image.png')

  # Retrieve the generated image
  image_url = json_response["data"][0]["url"]  # extract image URL from response
  generated_image = requests.get(image_url).content  # download the image
  with open(image_path, "wb") as image_file:
      image_file.write(generated_image)

  # Display the image in the default image viewer
  image = Image.open(image_path)
  image.show()
  ```

  1. Make sure the `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_API_KEY` environment variables are set.
  2. Change the value of `prompt` to your preferred text.
  3. Change the value of `model` to the name of your deployed DALL-E 3 model.

  <Callout type="important">
    Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
  </Callout>

  Run the application with the `python` command:

  ```console
  python quickstart.py
  ```

  Wait a few moments to get the response.

  ## Output

  Azure OpenAI stores the output image in the *generated\_image.png* file in your specified directory. The script also displays the image in your default image viewer.

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter).

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<ZonePivot pivot="programming-language-csharp">
  Use this guide to get started generating images with the Azure OpenAI SDK for C#.

  [Library source code](https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI) | [Package (NuGet)](https://www.nuget.org/packages/Azure.AI.OpenAI/) | [Samples](https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI/tests/Samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * The [.NET 7 SDK](https://dotnet.microsoft.com/download/dotnet/7.0)
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create a new console application with the following command:

     ```shell
     dotnet new console
     ```

  3. Install the [OpenAI .NET client library](https://www.nuget.org/packages/Azure.AI.OpenAI/) with the [dotnet add package](https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-add-package) command:

     ```console
     dotnet add package Azure.AI.OpenAI --version 1.0.0-beta.6
     ```

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the [Azure.Identity](https://www.nuget.org/packages/Azure.Identity) package with:

     ```console
     dotnet add package Azure.Identity
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
      ```
    </Tab>

    <Tab title="API key">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));
      ```
    </Tab>
  </Tabs>

  To run the quickstart, follow these steps:

  1. Replace the contents of `Program.cs` with the following code and update the placeholder values with your own.

     ```csharp
     using Azure;
     using Azure.AI.OpenAI;
     using OpenAI.Images;
     using static System.Environment;

     string endpoint = Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT") ?? "https://<your-resource-name>.openai.azure.com/";
     string key = Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY") ?? "<your-key>";

     // Use the recommended keyless credential instead of the AzureKeyCredential credential.
     AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
     //AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));

     // This must match the custom deployment name you chose for your model
     ImageClient chatClient = openAIClient.GetImageClient("dalle-3");

     var imageGeneration = await chatClient.GenerateImageAsync(
             "a happy monkey sitting in a tree, in watercolor",
             new ImageGenerationOptions()
             {
                 Size = GeneratedImageSize.W1024xH1024
             }
         );

     Console.WriteLine(imageGeneration.Value.ImageUri);
     ```

  2. Run the application using the `dotnet run` command or the run button at the top of Visual Studio:

     ```dotnetcli
     dotnet run
     ```

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-java">
  Use this guide to get started generating images with the Azure OpenAI SDK for Java.

  [Library source code](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/openai/azure-ai-openai) | [Artifact (Maven)](https://central.sonatype.com/artifact/com.azure/azure-ai-openai/1.0.0-beta.3) | [Samples](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/openai/azure-ai-openai/src/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * The current version of the [Java Development Kit (JDK)](https://www.microsoft.com/openjdk)
  * Install [Apache Maven](https://maven.apache.org/install.html).
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Install [Apache Maven](https://maven.apache.org/install.html). Then run `mvn -v` to confirm successful installation.

  3. Create a new `pom.xml` file in the root of your project, and copy the following code into it:

     ```xml
     <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
          <modelVersion>4.0.0</modelVersion>
          <groupId>com.azure.samples</groupId>
          <artifactId>quickstart-dall-e</artifactId>
          <version>1.0.0-SNAPSHOT</version>
          <build>
              <sourceDirectory>src</sourceDirectory>
              <plugins>
              <plugin>
                  <artifactId>maven-compiler-plugin</artifactId>
                  <version>3.7.0</version>
                  <configuration>
                  <source>1.8</source>
                  <target>1.8</target>
                  </configuration>
              </plugin>
              </plugins>
          </build>
          <dependencies>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-ai-openai</artifactId>
                  <version>1.0.0-beta.3</version>
              </dependency>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-core</artifactId>
                  <version>1.53.0</version>
              </dependency>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-identity</artifactId>
                  <version>1.15.1</version>
              </dependency>
              <dependency>
                  <groupId>org.slf4j</groupId>
                  <artifactId>slf4j-simple</artifactId>
                  <version>1.7.9</version>
              </dependency>
          </dependencies>
      </project>
     ```

  4. Install the Azure OpenAI SDK and dependencies.

     ```console
     mvn clean dependency:copy-dependencies
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the app

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```java
      OpenAIAsyncClient client = new OpenAIClientBuilder()
          .endpoint(endpoint)
          .credential(new DefaultAzureCredentialBuilder().build())
          .buildAsyncClient();
      ```
    </Tab>

    <Tab title="API key">
      ```java
      OpenAIAsyncClient client = new OpenAIClientBuilder()
          .endpoint(endpoint)
          .credential(new AzureKeyCredential(key))
          .buildAsyncClient();
      ```
    </Tab>
  </Tabs>

  <Tabs>
    <Tab title="Microsoft Entra ID">
      Follow these steps to create a console application for image generation.

      1. Create a new file named *Quickstart.java* in the same project root directory.

      2. Copy the following code into *Quickstart.java*:

         ```java
         import com.azure.ai.openai.OpenAIAsyncClient;
         import com.azure.ai.openai.OpenAIClientBuilder;
         import com.azure.ai.openai.models.ImageGenerationOptions;
         import com.azure.ai.openai.models.ImageLocation;
         import com.azure.core.credential.AzureKeyCredential;
         import com.azure.core.models.ResponseError;

         import java.util.concurrent.TimeUnit;

         public class Quickstart {

             public static void main(String[] args) throws InterruptedException {

                 String endpoint = System.getenv("AZURE_OPENAI_ENDPOINT");

                 // Use the recommended keyless credential instead of the AzureKeyCredential credential.

                 OpenAIAsyncClient client = new OpenAIClientBuilder()
                     .endpoint(endpoint)
                     .credential(new DefaultAzureCredentialBuilder().build())
                     .buildAsyncClient();

                 ImageGenerationOptions imageGenerationOptions = new ImageGenerationOptions(
                     "A drawing of the Seattle skyline in the style of Van Gogh");
                 client.getImages(imageGenerationOptions).subscribe(
                     images -> {
                         for (ImageLocation imageLocation : images.getData()) {
                             ResponseError error = imageLocation.getError();
                             if (error != null) {
                                 System.out.printf("Image generation operation failed. Error code: %s, error message: %s.%n",
                                     error.getCode(), error.getMessage());
                             } else {
                                 System.out.printf(
                                     "Image location URL that provides temporary access to download the generated image is %s.%n",
                                     imageLocation.getUrl());
                             }
                         }
                     },
                     error -> System.err.println("There was an error getting images." + error),
                     () -> System.out.println("Completed getImages."));

                 // The .subscribe() creation and assignment isn't a blocking call.
                 // The thread sleeps so the program does not end before the send operation is complete.
                 // Use .block() instead of .subscribe() for a synchronous call.
                 TimeUnit.SECONDS.sleep(10);
             }
         }
         ```

      3. Run your new console application to generate an image:

         ```console
         javac Quickstart.java -cp ".;target\dependency\*"
         java -cp ".;target\dependency\*" Quickstart
         ```
    </Tab>

    <Tab title="API key">
      Follow these steps to create a console application for image generation.

      1. Create a new file named *Quickstart.java* in the same project root directory.

      2. Copy the following code into *Quickstart.java*:

         ```java
         import com.azure.ai.openai.OpenAIAsyncClient;
         import com.azure.ai.openai.OpenAIClientBuilder;
         import com.azure.ai.openai.models.ImageGenerationOptions;
         import com.azure.ai.openai.models.ImageLocation;
         import com.azure.identity.DefaultAzureCredentialBuilder;
         import com.azure.core.models.ResponseError;

         import java.util.concurrent.TimeUnit;

         public class Quickstart {

             public static void main(String[] args) throws InterruptedException {

                 String key = System.getenv("AZURE_OPENAI_API_KEY");
                 String endpoint = System.getenv("AZURE_OPENAI_ENDPOINT");

                 OpenAIAsyncClient client = new OpenAIClientBuilder()
                     .endpoint(endpoint)
                     .credential(new AzureKeyCredential(key))
                     .buildAsyncClient();

                 ImageGenerationOptions imageGenerationOptions = new ImageGenerationOptions(
                     "A drawing of the Seattle skyline in the style of Van Gogh");
                 client.getImages(imageGenerationOptions).subscribe(
                     images -> {
                         for (ImageLocation imageLocation : images.getData()) {
                             ResponseError error = imageLocation.getError();
                             if (error != null) {
                                 System.out.printf("Image generation operation failed. Error code: %s, error message: %s.%n",
                                     error.getCode(), error.getMessage());
                             } else {
                                 System.out.printf(
                                     "Image location URL that provides temporary access to download the generated image is %s.%n",
                                     imageLocation.getUrl());
                             }
                         }
                     },
                     error -> System.err.println("There was an error getting images." + error),
                     () -> System.out.println("Completed getImages."));

                 // The .subscribe() creation and assignment isn't a blocking call.
                 // The thread sleeps so the program does not end before the send operation is complete.
                 // Use .block() instead of .subscribe() for a synchronous call.
                 TimeUnit.SECONDS.sleep(10);
             }
         }
         ```

      3. Run your new console application to generate an image:

         ```console
         javac Quickstart.java -cp ".;target\dependency\*"
         java -cp ".;target\dependency\*" Quickstart
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  Image location URL that provides temporary access to download the generated image is <SAS URL>.
  Completed getImages.
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai)
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  Use this guide to get started generating images with the Azure OpenAI SDK for JavaScript.

  [Reference documentation](https://platform.openai.com/docs/api-reference/images/create) | [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  4. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Generate images with DALL-E

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { AzureOpenAI } = require("openai");
         const {
             DefaultAzureCredential,
             getBearerTokenProvider
         } = require("@azure/identity");

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { AzureOpenAI } = require("openai");

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         function getClient() {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  == Batch Image Generation ==
  Image generation result URL: <SAS URL>
  Image generation result URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  Use this guide to get started generating images with the Azure OpenAI SDK for JavaScript.

  [Reference documentation](https://platform.openai.com/docs/api-reference/images/create) | [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [TypeScript](https://www.typescriptlang.org/download/)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```shell
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  5. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Generate images with DALL-E

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { AzureOpenAI } from "openai";
         import {
             DefaultAzureCredential,
             getBearerTokenProvider
         } from "@azure/identity";

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      5. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  == Batch Image Generation ==
  Image generation result URL: <SAS URL>
  Image generation result URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/azure-openai-samples).
</ZonePivot>

<ZonePivot pivot="programming-language-go">
  Use this guide to get started generating images with the Azure OpenAI SDK for Go.

  [Library source code](https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai) | [Package](https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai) | [Samples](https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [Go 1.8+](https://go.dev/doc/install)
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `dall-e-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir dall-e-quickstart && cd dall-e-quickstart
     ```

  2. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `NewDefaultAzureCredential` implementation with `NewKeyCredential`.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```go
      azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
      credential, err := azidentity.NewDefaultAzureCredential(nil)
      client, err := azopenai.NewClient(azureOpenAIEndpoint, credential, nil)
      ```
    </Tab>

    <Tab title="API key">
      ```go
      azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
      azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
      credential := azcore.NewKeyCredential(azureOpenAIKey)
      client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)
      ```
    </Tab>
  </Tabs>

  <Tabs>
    <Tab title="Microsoft Entra ID">
      To run the sample:

      1. Create a new file named *quickstart.go*. Copy the following code into the *quickstart.go* file.

         ```go
          package main

         import (
         	"context"
         	"fmt"
         	"net/http"
         	"os"
         	"log"

         	"github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
         	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
         )

         func main() {
         	azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
         	modelDeploymentID := "dall-e-3"

         	credential, err := azidentity.NewDefaultAzureCredential(nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	client, err := azopenai.NewClient(
         		azureOpenAIEndpoint, credential, nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	resp, err := client.GetImageGenerations(context.TODO(), azopenai.ImageGenerationOptions{
         		Prompt:         to.Ptr("A painting of a cat in the style of Dali."),
         		ResponseFormat: to.Ptr(azopenai.ImageGenerationResponseFormatURL),
         		DeploymentName: to.Ptr(modelDeploymentID),
         	}, nil)

         	if err != nil {
         		// Implement application specific error handling logic.
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	for _, generatedImage := range resp.Data {
         		// The underlying type for the generatedImage is determined by the value of
         		// ImageGenerationOptions.ResponseFormat.
         		// In this example we use `azopenai.ImageGenerationResponseFormatURL`,
         		// so the underlying type will be ImageLocation.

         		resp, err := http.Head(*generatedImage.URL)

         		if err != nil {
         			// Implement application specific error handling logic.
         			log.Printf("ERROR: %s", err)
         			return
         		}

         		fmt.Fprintf(os.Stderr, "Image generated, HEAD request on URL returned %d\nImage URL: %s\n", resp.StatusCode, *generatedImage.URL)
         	}
         }
         ```

      2. Run the following command to create a new Go module:

         ```shell
          go mod init quickstart.go
         ```

      3. Run `go mod tidy` to install the required dependencies:

         ```cmd
         go mod tidy
         ```

      4. Run the following command to run the sample:

         ```shell
          go run quickstart.go
         ```
    </Tab>

    <Tab title="API key">
      To run the sample:

      1. Create a new file named *quickstart.go*. Copy the following code into the *quickstart.go* file.

         ```go
         package main

         import (
         	"context"
         	"fmt"
         	"net/http"
         	"os"
         	"log"

         	"github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
         )

         func main() {
         	azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
         	modelDeploymentID := "dall-e-3"

         	azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
         	credential := azcore.NewKeyCredential(azureOpenAIKey)

         	client, err := azopenai.NewClientWithKeyCredential(
         		azureOpenAIEndpoint, credential, nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	resp, err := client.GetImageGenerations(context.TODO(), azopenai.ImageGenerationOptions{
         		Prompt:         to.Ptr("A painting of a cat in the style of Dali."),
         		ResponseFormat: to.Ptr(azopenai.ImageGenerationResponseFormatURL),
         		DeploymentName: to.Ptr(modelDeploymentID),
         	}, nil)

         	if err != nil {
         		// Implement application specific error handling logic.
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	for _, generatedImage := range resp.Data {
         		// The underlying type for the generatedImage is determined by the value of
         		// ImageGenerationOptions.ResponseFormat.
         		// In this example we use `azopenai.ImageGenerationResponseFormatURL`,
         		// so the underlying type will be ImageLocation.

         		resp, err := http.Head(*generatedImage.URL)

         		if err != nil {
         			// Implement application specific error handling logic.
         			log.Printf("ERROR: %s", err)
         			return
         		}

         		fmt.Fprintf(os.Stderr, "Image generated, HEAD request on URL returned %d\nImage URL: %s\n", resp.StatusCode, *generatedImage.URL)
         	}
         }
         ```

      2. Run the following command to create a new Go module:

         ```shell
          go mod init quickstart.go
         ```

      3. Run `go mod tidy` to install the required dependencies:

         ```cmd
         go mod tidy
         ```

      4. Run the following command to run the sample:

         ```shell
          go run quickstart.go
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  Image generated, HEAD request on URL returned 200
  Image URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-powershell">
  Use this guide to get started calling the Azure OpenAI in Microsoft Foundry Models image generation APIs with PowerShell.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * For this task, [the latest version of PowerShell 7](https://aka.ms/installpowershell) is recommended because the examples use new features not available in Windows PowerShell 5.1.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Generate images

  1. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```powershell
     az login
     ```

  2. Get an Azure OpenAI auth token and set it as an environment variable for the current PowerShell session:

     ```powershell
     $Env:DEFAULT_AZURE_CREDENTIAL_TOKEN = az account get-access-token --resource https://cognitiveservices.azure.com --query accessToken -o tsv
     ```

  3. Create a new PowerShell file called *quickstart.ps1*. Then open it up in your preferred editor or IDE.

  4. Replace the contents of *quickstart.ps1* with the following code. Make sure `AZURE_OPENAI_ENDPOINT` is set, and change the value of `prompt` to your preferred text.

     To use API key authentication instead of keyless authentication, set `AZURE_OPENAI_API_KEY` and uncomment the `'api-key'` line.

     ```powershell
      # Azure OpenAI metadata variables
      $openai = @{
          api_base    = $Env:AZURE_OPENAI_ENDPOINT
          api_version = '2023-06-01-preview' # This can change in the future.
      }

      # Use the recommended keyless authentication via bearer token.
      $headers = [ordered]@{
          #'api-key' = $Env:AZURE_OPENAI_API_KEY
          'Authorization' = "Bearer $($Env:DEFAULT_AZURE_CREDENTIAL_TOKEN)"
      }

      # Text to describe image
      $prompt = 'A painting of a dog'

      # Adjust these values to fine-tune completions
      $body = [ordered]@{
          prompt = $prompt
          size   = '1024x1024'
          n      = 1
      } | ConvertTo-Json

      # Call the API to generate the image and retrieve the response
      $url = "$($openai.api_base)/openai/images/generations:submit?api-version=$($openai.api_version)"

      $submission = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json' -ResponseHeadersVariable submissionHeaders

      $operation_location = $submissionHeaders['operation-location'][0]
      $status = ''
      while ($status -ne 'succeeded') {
          Start-Sleep -Seconds 1
          $response = Invoke-RestMethod -Uri $operation_location -Headers $headers
          $status   = $response.status
      }

      # Set the directory for the stored image
      $image_dir = Join-Path -Path $pwd -ChildPath 'images'

      # If the directory doesn't exist, create it
      if (-not(Resolve-Path $image_dir -ErrorAction Ignore)) {
          New-Item -Path $image_dir -ItemType Directory
      }

      # Initialize the image path (note the filetype should be png)
      $image_path = Join-Path -Path $image_dir -ChildPath 'generated_image.png'

      # Retrieve the generated image
      $image_url = $response.result.data[0].url  # extract image URL from response
      $generated_image = Invoke-WebRequest -Uri $image_url -OutFile $image_path  # download the image
      return $image_path
     ```

     <Callout type="important">
       For production, use a secure way of storing and accessing your credentials like [The PowerShell Secret Management with Azure Key Vault](https://learn.microsoft.com/en-us/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information about credential security, see this [security](https://learn.microsoft.com/en-us/azure/ai-services/security-features) article.
     </Callout>

  5. Run the script using PowerShell:

     ```powershell
     ./quickstart.ps1
     ```

     The script loops until the generated image is ready.

  ## Output

  PowerShell requests the image from Azure OpenAI and stores the output image in the *generated\_image.png* file in your specified directory. For convenience, the full path for the file is returned at the end of the script.

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter).

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure PowerShell](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azpowershell#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).

  - Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-studio">
  Use this guide to get started generating images with Azure OpenAI in your browser with Microsoft Foundry.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Go to Foundry

  Browse to [Foundry](https://ai.azure.com/?cid=learnDocs) and sign in with the credentials associated with your Azure OpenAI resource. During or after the sign-in workflow, select the appropriate directory, Azure subscription, and Azure OpenAI resource.

  From the Foundry landing page, create or select a new project. Navigate to the **Models + endpoints** page on the left nav. Select **Deploy model** and then choose one of the DALL-E models from the list. Complete the deployment process.

  On the model's page, select **Open in playground**.

  ## Try out image generation

  Start exploring Azure OpenAI capabilities with a no-code approach through the **Images playground**. Enter your image prompt into the text box and select **Generate**. When the AI-generated image is ready, it appears on the page.

  <Callout type="note">
    The Image APIs come with a content moderation filter. If Azure OpenAI recognizes your prompt as harmful content, it doesn't return a generated image. For more information, see [Content filtering](concepts/content-filter).
  </Callout>

  In the **Images playground**, you can also view Python and cURL code samples, which are prefilled according to your settings. Select **View code** near the top of the page. You can use this code to write an application that completes the same task.

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

<ZonePivot pivot="rest-api">
  Use this guide to get started calling the Azure OpenAI in Microsoft Foundry Models image generation REST APIs by using Python.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/).
  * The following Python libraries installed: `os`, `requests`, `json`.
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability).
  * Then, you need to deploy a `gpt-image-1`-series or `dalle3` model with your Azure resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Setup

  ### Retrieve key and endpoint

  To successfully call the Azure OpenAI APIs, you need the following information about your Azure OpenAI resource:

  | Variable     | Name       | Value                                                                                                                                                                                                                                              |
  | ------------ | ---------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
  | **Endpoint** | `api_base` | The endpoint value is located under **Keys and Endpoint** for your resource in the Azure portal. You can also find the endpoint via the **Deployments** page in Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | **Key**      | `api_key`  | The key value is also located under **Keys and Endpoint** for your resource in the Azure portal. Azure generates two keys for your resource. You can use either value.                                                                             |

  Go to your resource in the Azure portal. On the navigation pane, select **Keys and Endpoint** under **Resource Management**. Copy the **Endpoint** value and an access key value. You can use either the **KEY 1** or **KEY 2** value. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot that shows the Keys and Endpoint page for an Azure OpenAI resource in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    We recommend Microsoft Entra ID authentication with [managed identities for Azure resources](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to avoid storing credentials with your applications that run in the cloud.

    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys regularly, and restrict access to Azure Key Vault using role based access control and network access restrictions. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```cmd
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```bash
      export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>
  </Tabs>

  ## Create a new Python application

  Create a new Python file named *quickstart.py*. Open the new file in your preferred editor or IDE.

  <Tabs>
    <Tab title="GPT-image-1 series">
      1. Replace the contents of *quickstart.py* with the following code. Change the value of `prompt` to your preferred text. Also set `deployment` to the deployment name you chose when you deployed the GPT-image-1 series model.

         ```python
         import os
         import requests
         import base64
         from PIL import Image
         from io import BytesIO

         # set environment variables
         endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
         subscription_key = os.getenv("AZURE_OPENAI_API_KEY")

         deployment = "gpt-image-1.5" # the name of your GPT-image-1 series deployment
         api_version = "2025-04-01-preview" # or later version

         def decode_and_save_image(b64_data, output_filename):
           image = Image.open(BytesIO(base64.b64decode(b64_data)))
           image.show()
           image.save(output_filename)

         def save_all_images_from_response(response_data, filename_prefix):
           for idx, item in enumerate(response_data['data']):
             b64_img = item['b64_json']
             filename = f"{filename_prefix}_{idx+1}.png"
             decode_and_save_image(b64_img, filename)
             print(f"Image saved to: '{filename}'")

         base_path = f'openai/deployments/{deployment}/images'
         params = f'?api-version={api_version}'

         generation_url = f"{endpoint}{base_path}/generations{params}"
         generation_body = {
           "prompt": "girl falling asleep",
           "n": 1,
           "size": "1024x1024",
           "quality": "medium",
           "output_format": "png"
         }
         generation_response = requests.post(
           generation_url,
           headers={
             'Api-Key': subscription_key,
             'Content-Type': 'application/json',
           },
           json=generation_body
         ).json()
         save_all_images_from_response(generation_response, "generated_image")

         # In addition to generating images, you can edit them.
         edit_url = f"{endpoint}{base_path}/edits{params}"
         edit_body = {
           "prompt": "girl falling asleep",
           "n": 1,
           "size": "1024x1024",
           "quality": "medium"
         }
         files = {
           "image": ("generated_image_1.png", open("generated_image_1.png", "rb"), "image/png"),
           # You can use a mask to specify which parts of the image you want to edit.
           # The mask must be the same size as the input image.
           # "mask": ("mask.png", open("mask.png", "rb"), "image/png"),
         }
         edit_response = requests.post(
           edit_url,
           headers={'Api-Key': subscription_key},
           data=edit_body,
           files=files
         ).json()
         save_all_images_from_response(edit_response, "edited_image")
         ```

         The script makes a synchronous image generation API call.

         <Callout type="important">
           Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
         </Callout>

      2. Run the application with the `python` command:

         ```console
         python quickstart.py
         ```

         Wait a few moments to get the response.
    </Tab>

    <Tab title="DALL-E">
      1. Replace the contents of *quickstart.py* with the following code. Change the value of `prompt` to your preferred text.

         You also need to replace `<dalle3>` in the URL with the deployment name you chose when you deployed the DALL-E 3 model. Entering the model name will result in an error unless you chose a deployment name that is identical to the underlying model name. If you encounter an error, double check to make sure that you don't have a doubling of the `/` at the separation between your endpoint and `/openai/deployments`.

         ```python
         import requests
         import time
         import os
         api_base = os.environ['AZURE_OPENAI_ENDPOINT']  # Enter your endpoint here
         api_key = os.environ['AZURE_OPENAI_API_KEY']         # Enter your API key here

         api_version = '2024-02-01'
         url = f"{api_base}/openai/deployments/<dalle3>/images/generations?api-version={api_version}"
         headers= { "api-key": api_key, "Content-Type": "application/json" }
         body = {
             # Enter your prompt text here
             "prompt": "A multi-colored umbrella on the beach, disposable camera",
             "size": "1024x1024", # supported values are “1792x1024”, “1024x1024” and “1024x1792”
             "n": 1, #The number of images to generate. Only n=1 is supported for DALL-E 3.
             "quality": "hd", # Options are “hd” and “standard”; defaults to standard
             "style": "vivid" # Options are “natural” and “vivid”; defaults to “vivid”
         }
         submission = requests.post(url, headers=headers, json=body)

         image_url = submission.json()['data'][0]['url']

         print(image_url)
         ```

         The script makes a synchronous image generation API call.

         <Callout type="important">
           Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
         </Callout>

      2. Run the application with the `python` command:

         ```console
         python quickstart.py
         ```

         Wait a few moments to get the response.
    </Tab>
  </Tabs>

  ## Output

  The output from a successful image generation API call looks like the following example. The `url` field contains a URL where you can download the generated image. The URL stays active for 24 hours.

  ```json
  {
      "created": 1698116662,
      "data": [
          {
              "url": "<URL_to_generated_image>",
              "revised_prompt": "<prompt_that_was_used>"
          }
      ]
  }
  ```

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter). For examples of error responses, see the [Image generation how-to guide](how-to/dall-e).

  The system returns an operation status of `Failed` and the `error.code` value in the message is set to `contentFilter`. Here's an example:

  ```json
  {
      "created": 1698435368,
      "error":
      {
          "code": "contentFilter",
          "message": "Your task failed as a result of our safety system."
      }
  }
  ```

  It's also possible that the generated image itself is filtered. In this case, the error message is set to `Generated image was filtered as a result of our safety system.`. Here's an example:

  ```json
  {
      "created": 1698435368,
      "error":
      {
          "code": "contentFilter",
          "message": "Generated image was filtered as a result of our safety system."
      }
  }
  ```

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<ZonePivot pivot="programming-language-python">
  Use this guide to get started generating images with the Azure OpenAI SDK for Python.

  [Library source code](https://github.com/openai/openai-python/tree/main/src/openai) | [Package](https://github.com/openai/openai-python) | [Samples](https://github.com/openai/openai-python/tree/main/examples)

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/).
  * An Azure OpenAI resource created in a compatible region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability).
    * Access the Azure OpenAI resource endpoint and keys in the Azure portal.
  * Then, you need to deploy a `dalle3` model with your Azure resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Setup

  ### Retrieve key and endpoint

  To successfully call the Azure OpenAI APIs, you need the following information about your Azure OpenAI resource:

  | Variable     | Name       | Value                                                                                                                                                                                                                                                        |
  | ------------ | ---------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
  | **Endpoint** | `api_base` | The endpoint value is located under **Keys and Endpoint** for your resource in the Azure portal. You can also find the endpoint via the **Deployments** page in Microsoft Foundry portal. An example endpoint is: `https://docs-test-001.openai.azure.com/`. |
  | **Key**      | `api_key`  | The key value is also located under **Keys and Endpoint** for your resource in the Azure portal. Azure generates two keys for your resource. You can use either value.                                                                                       |

  Go to your resource in the Azure portal. On the navigation pane, select **Keys and Endpoint** under **Resource Management**. Copy the **Endpoint** value and an access key value. You can use either the **KEY 1** or **KEY 2** value. Always having two keys allows you to securely rotate and regenerate keys without causing a service disruption.

  ![Screenshot that shows the Keys and Endpoint page for an Azure OpenAI resource in the Azure portal.](https://learn.microsoft.com/azure/ai-foundry/media/quickstarts/endpoint.png)

  ### Environment variables

  Create and assign persistent environment variables for your key and endpoint.

  <Callout type="important">
    We recommend Microsoft Entra ID authentication with [managed identities for Azure resources](https://learn.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview) to avoid storing credentials with your applications that run in the cloud.

    Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If using API keys, store them securely in Azure Key Vault, rotate the keys regularly, and restrict access to Azure Key Vault using role based access control and network access restrictions. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

    For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
  </Callout>

  <Tabs>
    <Tab title="Command Line">
      ```cmd
      setx AZURE_OPENAI_API_KEY "REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      setx AZURE_OPENAI_ENDPOINT "REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>

    <Tab title="PowerShell">
      ```powershell
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_API_KEY', 'REPLACE_WITH_YOUR_KEY_VALUE_HERE', 'User')
      [System.Environment]::SetEnvironmentVariable('AZURE_OPENAI_ENDPOINT', 'REPLACE_WITH_YOUR_ENDPOINT_HERE', 'User')
      ```
    </Tab>

    <Tab title="Bash">
      ```bash
      export AZURE_OPENAI_API_KEY="REPLACE_WITH_YOUR_KEY_VALUE_HERE"
      export AZURE_OPENAI_ENDPOINT="REPLACE_WITH_YOUR_ENDPOINT_HERE"
      ```
    </Tab>
  </Tabs>

  ## Install the Python SDK

  Open a command prompt and browse to your project folder. Install the OpenAI Python SDK by using the following command:

  ```bash
  pip install openai
  ```

  Install the following libraries as well:

  ```bash
  pip install requests
  pip install pillow
  ```

  ## Generate images with DALL-E

  Create a new python file, *quickstart.py*. Open it in your preferred editor or IDE.

  Replace the contents of *quickstart.py* with the following code.

  ```python
  from openai import AzureOpenAI
  import os
  import requests
  from PIL import Image
  import json

  client = AzureOpenAI(
      api_version="2024-02-01",
      api_key=os.environ["AZURE_OPENAI_API_KEY"],
      azure_endpoint=os.environ['AZURE_OPENAI_ENDPOINT']
  )

  result = client.images.generate(
      model="dalle3", # the name of your DALL-E 3 deployment
      prompt="a close-up of a bear walking through the forest",
      n=1
  )

  json_response = json.loads(result.model_dump_json())

  # Set the directory for the stored image
  image_dir = os.path.join(os.curdir, 'images')

  # If the directory doesn't exist, create it
  if not os.path.isdir(image_dir):
      os.mkdir(image_dir)

  # Initialize the image path (note the filetype should be png)
  image_path = os.path.join(image_dir, 'generated_image.png')

  # Retrieve the generated image
  image_url = json_response["data"][0]["url"]  # extract image URL from response
  generated_image = requests.get(image_url).content  # download the image
  with open(image_path, "wb") as image_file:
      image_file.write(generated_image)

  # Display the image in the default image viewer
  image = Image.open(image_path)
  image.show()
  ```

  1. Make sure the `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_API_KEY` environment variables are set.
  2. Change the value of `prompt` to your preferred text.
  3. Change the value of `model` to the name of your deployed DALL-E 3 model.

  <Callout type="important">
    Remember to remove the key from your code when you're done, and never post your key publicly. For production, use a secure way of storing and accessing your credentials. For more information, see [Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/overview).
  </Callout>

  Run the application with the `python` command:

  ```console
  python quickstart.py
  ```

  Wait a few moments to get the response.

  ## Output

  Azure OpenAI stores the output image in the *generated\_image.png* file in your specified directory. The script also displays the image in your default image viewer.

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter).

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<ZonePivot pivot="programming-language-csharp">
  Use this guide to get started generating images with the Azure OpenAI SDK for C#.

  [Library source code](https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI) | [Package (NuGet)](https://www.nuget.org/packages/Azure.AI.OpenAI/) | [Samples](https://github.com/Azure/azure-sdk-for-net/tree/main/sdk/openai/Azure.AI.OpenAI/tests/Samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * The [.NET 7 SDK](https://dotnet.microsoft.com/download/dotnet/7.0)
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create a new console application with the following command:

     ```shell
     dotnet new console
     ```

  3. Install the [OpenAI .NET client library](https://www.nuget.org/packages/Azure.AI.OpenAI/) with the [dotnet add package](https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-add-package) command:

     ```console
     dotnet add package Azure.AI.OpenAI --version 1.0.0-beta.6
     ```

  4. For the **recommended** keyless authentication with Microsoft Entra ID, install the [Azure.Identity](https://www.nuget.org/packages/Azure.Identity) package with:

     ```console
     dotnet add package Azure.Identity
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
      ```
    </Tab>

    <Tab title="API key">
      ```csharp
      AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));
      ```
    </Tab>
  </Tabs>

  To run the quickstart, follow these steps:

  1. Replace the contents of `Program.cs` with the following code and update the placeholder values with your own.

     ```csharp
     using Azure;
     using Azure.AI.OpenAI;
     using OpenAI.Images;
     using static System.Environment;

     string endpoint = Environment.GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT") ?? "https://<your-resource-name>.openai.azure.com/";
     string key = Environment.GetEnvironmentVariable("AZURE_OPENAI_API_KEY") ?? "<your-key>";

     // Use the recommended keyless credential instead of the AzureKeyCredential credential.
     AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new DefaultAzureCredential());
     //AzureOpenAIClient openAIClient = new AzureOpenAIClient(new Uri(endpoint), new AzureKeyCredential(key));

     // This must match the custom deployment name you chose for your model
     ImageClient chatClient = openAIClient.GetImageClient("dalle-3");

     var imageGeneration = await chatClient.GenerateImageAsync(
             "a happy monkey sitting in a tree, in watercolor",
             new ImageGenerationOptions()
             {
                 Size = GeneratedImageSize.W1024xH1024
             }
         );

     Console.WriteLine(imageGeneration.Value.ImageUri);
     ```

  2. Run the application using the `dotnet run` command or the run button at the top of Visual Studio:

     ```dotnetcli
     dotnet run
     ```

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-java">
  Use this guide to get started generating images with the Azure OpenAI SDK for Java.

  [Library source code](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/openai/azure-ai-openai) | [Artifact (Maven)](https://central.sonatype.com/artifact/com.azure/azure-ai-openai/1.0.0-beta.3) | [Samples](https://github.com/Azure/azure-sdk-for-java/tree/main/sdk/openai/azure-ai-openai/src/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * The current version of the [Java Development Kit (JDK)](https://www.microsoft.com/openjdk)
  * Install [Apache Maven](https://maven.apache.org/install.html).
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Install [Apache Maven](https://maven.apache.org/install.html). Then run `mvn -v` to confirm successful installation.

  3. Create a new `pom.xml` file in the root of your project, and copy the following code into it:

     ```xml
     <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
          <modelVersion>4.0.0</modelVersion>
          <groupId>com.azure.samples</groupId>
          <artifactId>quickstart-dall-e</artifactId>
          <version>1.0.0-SNAPSHOT</version>
          <build>
              <sourceDirectory>src</sourceDirectory>
              <plugins>
              <plugin>
                  <artifactId>maven-compiler-plugin</artifactId>
                  <version>3.7.0</version>
                  <configuration>
                  <source>1.8</source>
                  <target>1.8</target>
                  </configuration>
              </plugin>
              </plugins>
          </build>
          <dependencies>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-ai-openai</artifactId>
                  <version>1.0.0-beta.3</version>
              </dependency>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-core</artifactId>
                  <version>1.53.0</version>
              </dependency>
              <dependency>
                  <groupId>com.azure</groupId>
                  <artifactId>azure-identity</artifactId>
                  <version>1.15.1</version>
              </dependency>
              <dependency>
                  <groupId>org.slf4j</groupId>
                  <artifactId>slf4j-simple</artifactId>
                  <version>1.7.9</version>
              </dependency>
          </dependencies>
      </project>
     ```

  4. Install the Azure OpenAI SDK and dependencies.

     ```console
     mvn clean dependency:copy-dependencies
     ```

  5. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the app

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `DefaultAzureCredential` object with an `AzureKeyCredential` object.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```java
      OpenAIAsyncClient client = new OpenAIClientBuilder()
          .endpoint(endpoint)
          .credential(new DefaultAzureCredentialBuilder().build())
          .buildAsyncClient();
      ```
    </Tab>

    <Tab title="API key">
      ```java
      OpenAIAsyncClient client = new OpenAIClientBuilder()
          .endpoint(endpoint)
          .credential(new AzureKeyCredential(key))
          .buildAsyncClient();
      ```
    </Tab>
  </Tabs>

  <Tabs>
    <Tab title="Microsoft Entra ID">
      Follow these steps to create a console application for image generation.

      1. Create a new file named *Quickstart.java* in the same project root directory.

      2. Copy the following code into *Quickstart.java*:

         ```java
         import com.azure.ai.openai.OpenAIAsyncClient;
         import com.azure.ai.openai.OpenAIClientBuilder;
         import com.azure.ai.openai.models.ImageGenerationOptions;
         import com.azure.ai.openai.models.ImageLocation;
         import com.azure.core.credential.AzureKeyCredential;
         import com.azure.core.models.ResponseError;

         import java.util.concurrent.TimeUnit;

         public class Quickstart {

             public static void main(String[] args) throws InterruptedException {

                 String endpoint = System.getenv("AZURE_OPENAI_ENDPOINT");

                 // Use the recommended keyless credential instead of the AzureKeyCredential credential.

                 OpenAIAsyncClient client = new OpenAIClientBuilder()
                     .endpoint(endpoint)
                     .credential(new DefaultAzureCredentialBuilder().build())
                     .buildAsyncClient();

                 ImageGenerationOptions imageGenerationOptions = new ImageGenerationOptions(
                     "A drawing of the Seattle skyline in the style of Van Gogh");
                 client.getImages(imageGenerationOptions).subscribe(
                     images -> {
                         for (ImageLocation imageLocation : images.getData()) {
                             ResponseError error = imageLocation.getError();
                             if (error != null) {
                                 System.out.printf("Image generation operation failed. Error code: %s, error message: %s.%n",
                                     error.getCode(), error.getMessage());
                             } else {
                                 System.out.printf(
                                     "Image location URL that provides temporary access to download the generated image is %s.%n",
                                     imageLocation.getUrl());
                             }
                         }
                     },
                     error -> System.err.println("There was an error getting images." + error),
                     () -> System.out.println("Completed getImages."));

                 // The .subscribe() creation and assignment isn't a blocking call.
                 // The thread sleeps so the program does not end before the send operation is complete.
                 // Use .block() instead of .subscribe() for a synchronous call.
                 TimeUnit.SECONDS.sleep(10);
             }
         }
         ```

      3. Run your new console application to generate an image:

         ```console
         javac Quickstart.java -cp ".;target\dependency\*"
         java -cp ".;target\dependency\*" Quickstart
         ```
    </Tab>

    <Tab title="API key">
      Follow these steps to create a console application for image generation.

      1. Create a new file named *Quickstart.java* in the same project root directory.

      2. Copy the following code into *Quickstart.java*:

         ```java
         import com.azure.ai.openai.OpenAIAsyncClient;
         import com.azure.ai.openai.OpenAIClientBuilder;
         import com.azure.ai.openai.models.ImageGenerationOptions;
         import com.azure.ai.openai.models.ImageLocation;
         import com.azure.identity.DefaultAzureCredentialBuilder;
         import com.azure.core.models.ResponseError;

         import java.util.concurrent.TimeUnit;

         public class Quickstart {

             public static void main(String[] args) throws InterruptedException {

                 String key = System.getenv("AZURE_OPENAI_API_KEY");
                 String endpoint = System.getenv("AZURE_OPENAI_ENDPOINT");

                 OpenAIAsyncClient client = new OpenAIClientBuilder()
                     .endpoint(endpoint)
                     .credential(new AzureKeyCredential(key))
                     .buildAsyncClient();

                 ImageGenerationOptions imageGenerationOptions = new ImageGenerationOptions(
                     "A drawing of the Seattle skyline in the style of Van Gogh");
                 client.getImages(imageGenerationOptions).subscribe(
                     images -> {
                         for (ImageLocation imageLocation : images.getData()) {
                             ResponseError error = imageLocation.getError();
                             if (error != null) {
                                 System.out.printf("Image generation operation failed. Error code: %s, error message: %s.%n",
                                     error.getCode(), error.getMessage());
                             } else {
                                 System.out.printf(
                                     "Image location URL that provides temporary access to download the generated image is %s.%n",
                                     imageLocation.getUrl());
                             }
                         }
                     },
                     error -> System.err.println("There was an error getting images." + error),
                     () -> System.out.println("Completed getImages."));

                 // The .subscribe() creation and assignment isn't a blocking call.
                 // The thread sleeps so the program does not end before the send operation is complete.
                 // Use .block() instead of .subscribe() for a synchronous call.
                 TimeUnit.SECONDS.sleep(10);
             }
         }
         ```

      3. Run your new console application to generate an image:

         ```console
         javac Quickstart.java -cp ".;target\dependency\*"
         java -cp ".;target\dependency\*" Quickstart
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  Image location URL that provides temporary access to download the generated image is <SAS URL>.
  Completed getImages.
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai)
</ZonePivot>

<ZonePivot pivot="programming-language-javascript">
  Use this guide to get started generating images with the Azure OpenAI SDK for JavaScript.

  [Reference documentation](https://platform.openai.com/docs/api-reference/images/create) | [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  4. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Generate images with DALL-E

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { AzureOpenAI } = require("openai");
         const {
             DefaultAzureCredential,
             getBearerTokenProvider
         } = require("@azure/identity");

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      3. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.js` file with the following code:

         ```javascript
         const { AzureOpenAI } = require("openai");

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         function getClient() {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Run the JavaScript file.

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  == Batch Image Generation ==
  Image generation result URL: <SAS URL>
  Image generation result URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-typescript">
  Use this guide to get started generating images with the Azure OpenAI SDK for JavaScript.

  [Reference documentation](https://platform.openai.com/docs/api-reference/images/create) | [Source code](https://github.com/openai/openai-node) | [Package (npm)](https://www.npmjs.com/package/openai) | [Samples](https://github.com/Azure/azure-sdk-for-js/tree/main/sdk/openai/openai/samples)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [LTS versions of Node.js](https://github.com/nodejs/release#release-schedule)
  * [TypeScript](https://www.typescriptlang.org/download/)
  * [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `image-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir image-quickstart && cd image-quickstart
     ```

  2. Create the `package.json` with the following command:

     ```shell
     npm init -y
     ```

  3. Update the `package.json` to ECMAScript with the following command:

     ```shell
     npm pkg set type=module
     ```

  4. Install the OpenAI client library for JavaScript with:

     ```console
     npm install openai
     ```

  5. For the **recommended** passwordless authentication:

     ```console
     npm install @azure/identity
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  <Callout type="caution">
    To use the recommended keyless authentication with the SDK, make sure that the `AZURE_OPENAI_API_KEY` environment variable isn't set.
  </Callout>

  ## Generate images with DALL-E

  <Tabs>
    <Tab title="Microsoft Entra ID">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { AzureOpenAI } from "openai";
         import {
             DefaultAzureCredential,
             getBearerTokenProvider
         } from "@azure/identity";

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // keyless authentication
         const credential = new DefaultAzureCredential();
         const scope = "https://cognitiveservices.azure.com/.default";
         const azureADTokenProvider = getBearerTokenProvider(credential, scope);

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             azureADTokenProvider,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Sign in to Azure with the following command:

         ```shell
         az login
         ```

      5. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>

    <Tab title="API key">
      1. Create the `index.ts` file with the following code:

         ```typescript
         import { AzureOpenAI } from "openai";

         // You will need to set these environment variables or edit the following values
         const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
         const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";

         // Required Azure OpenAI deployment name and API version
         const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01";
         const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "dall-e-3";

         // The prompt to generate images from
         const prompt = "a monkey eating a banana";
         const numberOfImagesToGenerate = 1;

         function getClient(): AzureOpenAI {
           return new AzureOpenAI({
             endpoint,
             apiKey,
             apiVersion,
             deployment: deploymentName,
           });
         }
         async function main() {
           console.log("== Image Generation ==");

           const client = getClient();

           const results = await client.images.generate({
             prompt,
             size: "1024x1024",
             n: numberOfImagesToGenerate,
             model: "",
             style: "vivid", // or "natural"
           });

           for (const image of results.data) {
             console.log(`Image generation result URL: ${image.url}`);
           }
         }

         main().catch((err) => {
           console.error("The sample encountered an error:", err);
         });
         ```

      2. Create the `tsconfig.json` file to transpile the TypeScript code and copy the following code for ECMAScript.

         ```json
         {
             "compilerOptions": {
               "module": "NodeNext",
               "target": "ES2022", // Supports top-level await
               "moduleResolution": "NodeNext",
               "skipLibCheck": true, // Avoid type errors from node_modules
               "strict": true // Enable strict type-checking options
             },
             "include": ["*.ts"]
         }
         ```

      3. Transpile from TypeScript to JavaScript.

         ```shell
         tsc
         ```

      4. Run the code with the following command:

         ```shell
         node index.js
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  == Batch Image Generation ==
  Image generation result URL: <SAS URL>
  Image generation result URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/azure-openai-samples).
</ZonePivot>

<ZonePivot pivot="programming-language-go">
  Use this guide to get started generating images with the Azure OpenAI SDK for Go.

  [Library source code](https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai) | [Package](https://pkg.go.dev/github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai) | [Samples](https://github.com/Azure/azure-sdk-for-go/tree/main/sdk/ai/azopenai)

  ## Prerequisites

  * An Azure subscription - [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn)
  * [Go 1.8+](https://go.dev/doc/install)
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `dall-e-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir dall-e-quickstart && cd dall-e-quickstart
     ```

  2. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```console
     az login
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Run the quickstart

  The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace the `NewDefaultAzureCredential` implementation with `NewKeyCredential`.

  <Tabs>
    <Tab title="Microsoft Entra ID">
      ```go
      azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
      credential, err := azidentity.NewDefaultAzureCredential(nil)
      client, err := azopenai.NewClient(azureOpenAIEndpoint, credential, nil)
      ```
    </Tab>

    <Tab title="API key">
      ```go
      azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
      azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
      credential := azcore.NewKeyCredential(azureOpenAIKey)
      client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)
      ```
    </Tab>
  </Tabs>

  <Tabs>
    <Tab title="Microsoft Entra ID">
      To run the sample:

      1. Create a new file named *quickstart.go*. Copy the following code into the *quickstart.go* file.

         ```go
          package main

         import (
         	"context"
         	"fmt"
         	"net/http"
         	"os"
         	"log"

         	"github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
         	"github.com/Azure/azure-sdk-for-go/sdk/azidentity"
         )

         func main() {
         	azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
         	modelDeploymentID := "dall-e-3"

         	credential, err := azidentity.NewDefaultAzureCredential(nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	client, err := azopenai.NewClient(
         		azureOpenAIEndpoint, credential, nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	resp, err := client.GetImageGenerations(context.TODO(), azopenai.ImageGenerationOptions{
         		Prompt:         to.Ptr("A painting of a cat in the style of Dali."),
         		ResponseFormat: to.Ptr(azopenai.ImageGenerationResponseFormatURL),
         		DeploymentName: to.Ptr(modelDeploymentID),
         	}, nil)

         	if err != nil {
         		// Implement application specific error handling logic.
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	for _, generatedImage := range resp.Data {
         		// The underlying type for the generatedImage is determined by the value of
         		// ImageGenerationOptions.ResponseFormat.
         		// In this example we use `azopenai.ImageGenerationResponseFormatURL`,
         		// so the underlying type will be ImageLocation.

         		resp, err := http.Head(*generatedImage.URL)

         		if err != nil {
         			// Implement application specific error handling logic.
         			log.Printf("ERROR: %s", err)
         			return
         		}

         		fmt.Fprintf(os.Stderr, "Image generated, HEAD request on URL returned %d\nImage URL: %s\n", resp.StatusCode, *generatedImage.URL)
         	}
         }
         ```

      2. Run the following command to create a new Go module:

         ```shell
          go mod init quickstart.go
         ```

      3. Run `go mod tidy` to install the required dependencies:

         ```cmd
         go mod tidy
         ```

      4. Run the following command to run the sample:

         ```shell
          go run quickstart.go
         ```
    </Tab>

    <Tab title="API key">
      To run the sample:

      1. Create a new file named *quickstart.go*. Copy the following code into the *quickstart.go* file.

         ```go
         package main

         import (
         	"context"
         	"fmt"
         	"net/http"
         	"os"
         	"log"

         	"github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore"
         	"github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
         )

         func main() {
         	azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
         	modelDeploymentID := "dall-e-3"

         	azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
         	credential := azcore.NewKeyCredential(azureOpenAIKey)

         	client, err := azopenai.NewClientWithKeyCredential(
         		azureOpenAIEndpoint, credential, nil)
         	if err != nil {
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	resp, err := client.GetImageGenerations(context.TODO(), azopenai.ImageGenerationOptions{
         		Prompt:         to.Ptr("A painting of a cat in the style of Dali."),
         		ResponseFormat: to.Ptr(azopenai.ImageGenerationResponseFormatURL),
         		DeploymentName: to.Ptr(modelDeploymentID),
         	}, nil)

         	if err != nil {
         		// Implement application specific error handling logic.
         		log.Printf("ERROR: %s", err)
         		return
         	}

         	for _, generatedImage := range resp.Data {
         		// The underlying type for the generatedImage is determined by the value of
         		// ImageGenerationOptions.ResponseFormat.
         		// In this example we use `azopenai.ImageGenerationResponseFormatURL`,
         		// so the underlying type will be ImageLocation.

         		resp, err := http.Head(*generatedImage.URL)

         		if err != nil {
         			// Implement application specific error handling logic.
         			log.Printf("ERROR: %s", err)
         			return
         		}

         		fmt.Fprintf(os.Stderr, "Image generated, HEAD request on URL returned %d\nImage URL: %s\n", resp.StatusCode, *generatedImage.URL)
         	}
         }
         ```

      2. Run the following command to create a new Go module:

         ```shell
          go mod init quickstart.go
         ```

      3. Run `go mod tidy` to install the required dependencies:

         ```cmd
         go mod tidy
         ```

      4. Run the following command to run the sample:

         ```shell
          go run quickstart.go
         ```
    </Tab>
  </Tabs>

  ## Output

  The URL of the generated image is printed to the console.

  ```console
  Image generated, HEAD request on URL returned 200
  Image URL: <SAS URL>
  ```

  <Callout type="note">
    The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it won't return a generated image. For more information, see the [content filter](concepts/content-filter) article.
  </Callout>

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * For more examples check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-powershell">
  Use this guide to get started calling the Azure OpenAI in Microsoft Foundry Models image generation APIs with PowerShell.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * For this task, [the latest version of PowerShell 7](https://aka.ms/installpowershell) is recommended because the examples use new features not available in Windows PowerShell 5.1.
  * An Azure OpenAI resource created in a supported region (see [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability)). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ### Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Generate images

  1. For the **recommended** keyless authentication with Microsoft Entra ID, sign in to Azure with the following command:

     ```powershell
     az login
     ```

  2. Get an Azure OpenAI auth token and set it as an environment variable for the current PowerShell session:

     ```powershell
     $Env:DEFAULT_AZURE_CREDENTIAL_TOKEN = az account get-access-token --resource https://cognitiveservices.azure.com --query accessToken -o tsv
     ```

  3. Create a new PowerShell file called *quickstart.ps1*. Then open it up in your preferred editor or IDE.

  4. Replace the contents of *quickstart.ps1* with the following code. Make sure `AZURE_OPENAI_ENDPOINT` is set, and change the value of `prompt` to your preferred text.

     To use API key authentication instead of keyless authentication, set `AZURE_OPENAI_API_KEY` and uncomment the `'api-key'` line.

     ```powershell
      # Azure OpenAI metadata variables
      $openai = @{
          api_base    = $Env:AZURE_OPENAI_ENDPOINT
          api_version = '2023-06-01-preview' # This can change in the future.
      }

      # Use the recommended keyless authentication via bearer token.
      $headers = [ordered]@{
          #'api-key' = $Env:AZURE_OPENAI_API_KEY
          'Authorization' = "Bearer $($Env:DEFAULT_AZURE_CREDENTIAL_TOKEN)"
      }

      # Text to describe image
      $prompt = 'A painting of a dog'

      # Adjust these values to fine-tune completions
      $body = [ordered]@{
          prompt = $prompt
          size   = '1024x1024'
          n      = 1
      } | ConvertTo-Json

      # Call the API to generate the image and retrieve the response
      $url = "$($openai.api_base)/openai/images/generations:submit?api-version=$($openai.api_version)"

      $submission = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json' -ResponseHeadersVariable submissionHeaders

      $operation_location = $submissionHeaders['operation-location'][0]
      $status = ''
      while ($status -ne 'succeeded') {
          Start-Sleep -Seconds 1
          $response = Invoke-RestMethod -Uri $operation_location -Headers $headers
          $status   = $response.status
      }

      # Set the directory for the stored image
      $image_dir = Join-Path -Path $pwd -ChildPath 'images'

      # If the directory doesn't exist, create it
      if (-not(Resolve-Path $image_dir -ErrorAction Ignore)) {
          New-Item -Path $image_dir -ItemType Directory
      }

      # Initialize the image path (note the filetype should be png)
      $image_path = Join-Path -Path $image_dir -ChildPath 'generated_image.png'

      # Retrieve the generated image
      $image_url = $response.result.data[0].url  # extract image URL from response
      $generated_image = Invoke-WebRequest -Uri $image_url -OutFile $image_path  # download the image
      return $image_path
     ```

     <Callout type="important">
       For production, use a secure way of storing and accessing your credentials like [The PowerShell Secret Management with Azure Key Vault](https://learn.microsoft.com/en-us/powershell/utility-modules/secretmanagement/how-to/using-azure-keyvault). For more information about credential security, see this [security](https://learn.microsoft.com/en-us/azure/ai-services/security-features) article.
     </Callout>

  5. Run the script using PowerShell:

     ```powershell
     ./quickstart.ps1
     ```

     The script loops until the generated image is ready.

  ## Output

  PowerShell requests the image from Azure OpenAI and stores the output image in the *generated\_image.png* file in your specified directory. For convenience, the full path for the file is returned at the end of the script.

  The Image APIs come with a content moderation filter. If the service recognizes your prompt as harmful content, it doesn't generate an image. For more information, see [Content filtering](concepts/content-filter).

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure PowerShell](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azpowershell#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).

  - Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
</ZonePivot>

<ZonePivot pivot="programming-language-studio">
  Use this guide to get started generating images with Azure OpenAI in your browser with Microsoft Foundry.

  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Go to Foundry

  Browse to [Foundry](https://ai.azure.com/?cid=learnDocs) and sign in with the credentials associated with your Azure OpenAI resource. During or after the sign-in workflow, select the appropriate directory, Azure subscription, and Azure OpenAI resource.

  From the Foundry landing page, create or select a new project. Navigate to the **Models + endpoints** page on the left nav. Select **Deploy model** and then choose one of the DALL-E models from the list. Complete the deployment process.

  On the model's page, select **Open in playground**.

  ## Try out image generation

  Start exploring Azure OpenAI capabilities with a no-code approach through the **Images playground**. Enter your image prompt into the text box and select **Generate**. When the AI-generated image is ready, it appears on the page.

  <Callout type="note">
    The Image APIs come with a content moderation filter. If Azure OpenAI recognizes your prompt as harmful content, it doesn't return a generated image. For more information, see [Content filtering](concepts/content-filter).
  </Callout>

  In the **Images playground**, you can also view Python and cURL code samples, which are prefilled according to your settings. Select **View code** near the top of the page. You can use this code to write an application that completes the same task.

  ## Clean up resources

  If you want to clean up and remove an Azure OpenAI resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.

  * [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal#clean-up-resources)
  * [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli#clean-up-resources)

  ## Next steps

  * Explore the Image APIs in more depth with the [Image API how-to guide](how-to/dall-e).
  * Try examples in the [Azure OpenAI Samples GitHub repository](https://github.com/Azure-Samples/openai).
  * See the [API reference](reference#image-generation)
</ZonePivot>

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this quickstart, you generate video clips using the Azure OpenAI service. The example uses the Sora model, which is a video generation model that creates realistic and imaginative video scenes from text instructions and/or image or video inputs. This guide shows you how to create a video generation job, poll for its status, and retrieve the generated video.

For more information on video generation, see [Video generation concepts](concepts/video-generation).

<ZonePivot pivot="rest-api">
  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/). We recommend using Python 3.10 or later, but having at least Python 3.8 is required. If you don't have a suitable version of Python installed, you can follow the instructions in the [VS Code Python Tutorial](https://code.visualstudio.com/docs/python/python-tutorial#_install-a-python-interpreter) for the easiest way of installing Python on your operating system.
  * An Azure OpenAI resource created in one of the supported regions. For more information about region availability, see the [models and versions documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#video-generation-models).
  * Then, you need to deploy a `sora` model with your Azure OpenAI resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `video-generation-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir video-generation-quickstart && cd video-generation-quickstart
     ```

  2. Create a virtual environment. If you already have Python 3.10 or higher installed, you can create a virtual environment using the following commands:

     **Windows**

     ```bash
     py -3 -m venv .venv
     .venv\scripts\activate
     ```

     **Linux**

     ```bash
     python3 -m venv .venv
     source .venv/bin/activate
     ```

     **macOS**

     ```bash
     python3 -m venv .venv
     source .venv/bin/activate
     ```

     Activating the Python environment means that when you run `python` or `pip` from the command line, you then use the Python interpreter contained in the `.venv` folder of your application. You can use the `deactivate` command to exit the python virtual environment, and can later reactivate it when needed.

     <Callout type="tip">
       We recommend that you create and activate a new Python environment to use to install the packages you need for this tutorial. Don't install packages into your global python installation. You should always use a virtual or conda environment when installing python packages, otherwise you can break your global installation of Python.
     </Callout>

  3. Install the required packages.

     **Microsoft Entra ID**

     ```console
     pip install requests azure-identity
     ```

     **API key**

     ```console
     pip install requests
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Generate video with Sora

  You can generate a video with the Sora model by creating a video generation job, polling for its status, and retrieving the generated video. The following code shows how to do this via the REST API using Python.

  1. Create the `sora-quickstart.py` file and add the following code to authenticate your resource:

     **Microsoft Entra ID**

     ```python
     import json
     import requests
     import time
     import os
     from azure.identity import DefaultAzureCredential

     # Set environment variables or edit the corresponding values here.
     endpoint = os.environ.get('AZURE_OPENAI_ENDPOINT')
     deployment_name = os.environ.get('AZURE_OPENAI_DEPLOYMENT_NAME')
     if not endpoint or not deployment_name:
         raise ValueError("Set AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_DEPLOYMENT_NAME.")

     # Keyless authentication
     credential = DefaultAzureCredential()
     token = credential.get_token("https://cognitiveservices.azure.com/.default")

     api_version = 'preview'
     headers= { "Authorization": f"Bearer {token.token}", "Content-Type": "application/json" }
     ```

     **API key**

     ```python
     import json
     import requests
     import time
     import os

     # Set environment variables or edit the corresponding values here.
     endpoint = os.environ.get('AZURE_OPENAI_ENDPOINT')
     deployment_name = os.environ.get('AZURE_OPENAI_DEPLOYMENT_NAME')
     api_key = os.environ.get('AZURE_OPENAI_API_KEY')
     if not endpoint or not deployment_name or not api_key:
         raise ValueError(
             "Set AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME, and AZURE_OPENAI_API_KEY."
         )

     api_version = 'preview'
     headers= { "api-key": api_key, "Content-Type": "application/json" }
     ```

  2. Create the video generation job. You can create it from a text prompt only, or from an input image and text prompt.

     **Text prompt**

     ```python
     # 1. Create a video generation job
     create_url = f"{endpoint}/openai/v1/video/generations/jobs?api-version={api_version}"
     body = {
         "prompt": "A cat playing piano in a jazz bar.",
         "width": 480,
         "height": 480,
         "n_seconds": 5,
         "model": deployment_name
     }
     response = requests.post(create_url, headers=headers, json=body)
     response.raise_for_status()
     print("Full response JSON:", response.json())
     job_id = response.json()["id"]
     print(f"Job created: {job_id}")

     # 2. Poll for job status
     status_url = f"{endpoint}/openai/v1/video/generations/jobs/{job_id}?api-version={api_version}"
     status=None
     while status not in ("succeeded", "failed", "cancelled"):
         time.sleep(5)  # Wait before polling again
         status_response = requests.get(status_url, headers=headers).json()
         status = status_response.get("status")
         print(f"Job status: {status}")

     # 3. Retrieve generated video
     if status == "succeeded":
         generations = status_response.get("generations", [])
         if generations:
             print(f"✅ Video generation succeeded.")
             generation_id = generations[0].get("id")
             video_url = f"{endpoint}/openai/v1/video/generations/{generation_id}/content/video?api-version={api_version}"
             video_response = requests.get(video_url, headers=headers)
             if video_response.ok:
                 output_filename = "output.mp4"
                 with open(output_filename, "wb") as file:
                     file.write(video_response.content)
                     print(f'Generated video saved as "{output_filename}"')
         else:
             raise Exception("No generations found in job result.")
     else:
         raise Exception(f"Job didn't succeed. Status: {status}")
     ```

     **Image prompt**

     Replace the `"file_name"` field in `"inpaint_items"` with the name of your input image file. Also replace the construction of the `files` array, which associates the path to the actual file with the filename that the API uses.

     Use the `"crop_bounds"` data (image crop distances, from each direction, as a fraction of the total image dimensions) to specify which part of the image should be used in video generation.

     You can optionally set the `"frame_index"` to the frame in the generated video where your image should appear (the default is 0, the start of the video).

     ```python
     # 1. Create a video generation job with image inpainting (multipart upload)
     create_url = f"{endpoint}/openai/v1/video/generations/jobs?api-version={api_version}"

     # Flatten the body for multipart/form-data
     data = {
         "prompt": "A serene forest scene transitioning into autumn",
         "height": str(1080),
         "width": str(1920),
         "n_seconds": str(10),
         "n_variants": str(1),
         "model": deployment_name,
         # inpaint_items must be JSON string
         "inpaint_items": json.dumps([
             {
                 "frame_index": 0,
                 "type": "image",
                 "file_name": "dog_swimming.jpg",
                 "crop_bounds": {
                     "left_fraction": 0.1,
                     "top_fraction": 0.1,
                     "right_fraction": 0.9,
                     "bottom_fraction": 0.9
                 }
             }
         ])
     }

     # Replace with your own image file path
     with open("dog_swimming.jpg", "rb") as image_file:
         files = [
             ("files", ("dog_swimming.jpg", image_file, "image/jpeg"))
         ]
         multipart_headers = {k: v for k, v in headers.items() if k.lower() != "content-type"}
         response = requests.post(
             create_url,
             headers=multipart_headers,
             data=data,
             files=files
         )

     if not response.ok:
         print("Error response:", response.status_code, response.text)
         response.raise_for_status()
     print("Full response JSON:", response.json())
     job_id = response.json()["id"]
     print(f"Job created: {job_id}")

     # 2. Poll for job status
     status_url = f"{endpoint}/openai/v1/video/generations/jobs/{job_id}?api-version={api_version}"
     status = None
     while status not in ("succeeded", "failed", "cancelled"):
         time.sleep(5)
         status_response = requests.get(status_url, headers=headers).json()
         status = status_response.get("status")
         print(f"Job status: {status}")

     # 3. Retrieve generated video
     if status == "succeeded":
         generations = status_response.get("generations", [])
         if generations:
             generation_id = generations[0].get("id")
             video_url = f"{endpoint}/openai/v1/video/generations/{generation_id}/content/video?api-version={api_version}"
             video_response = requests.get(video_url, headers=headers)
             if video_response.ok:
                 output_filename = "output.mp4"
                 with open(output_filename, "wb") as file:
                     file.write(video_response.content)
                     print(f'✅ Generated video saved as "{output_filename}"')
         else:
             raise Exception("No generations found in job result.")
     else:
         raise Exception(f"Job didn't succeed. Status: {status}")
     ```

     **Video prompt**

     Replace the `"file_name"` field in `"inpaint_items"` with the name of your input video file. Also replace the construction of the `files` array, which associates the path to the actual file with the filename that the API uses.

     Use the `"crop_bounds"` data (image crop distances, from each direction, as a fraction of the total frame dimensions) to specify which part of the video frame should be used in video generation.

     You can optionally set the `"frame_index"` to the frame in the generated video where your input video should start (the default is 0, the beginning).

     ```python
     # 1. Create a video generation job with video inpainting (multipart upload)
     create_url = f"{endpoint}/openai/v1/video/generations/jobs?api-version={api_version}"

     # Flatten the body for multipart/form-data
     data = {
         "prompt": "A serene forest scene transitioning into autumn",
         "height": str(1080),
         "width": str(1920),
         "n_seconds": str(10),
         "n_variants": str(1),
         "model": deployment_name,
         # inpaint_items must be JSON string
         "inpaint_items": json.dumps([
             {
                 "frame_index": 0,
                 "type": "video",
                 "file_name": "dog_swimming.mp4",
                 "crop_bounds": {
                     "left_fraction": 0.1,
                     "top_fraction": 0.1,
                     "right_fraction": 0.9,
                     "bottom_fraction": 0.9
                 }
             }
         ])
     }

     # Replace with your own video file path
     with open("dog_swimming.mp4", "rb") as video_file:
         files = [
             ("files", ("dog_swimming.mp4", video_file, "video/mp4"))
         ]
         multipart_headers = {k: v for k, v in headers.items() if k.lower() != "content-type"}
         response = requests.post(
             create_url,
             headers=multipart_headers,
             data=data,
             files=files
         )

     if not response.ok:
         print("Error response:", response.status_code, response.text)
         response.raise_for_status()
     print("Full response JSON:", response.json())
     job_id = response.json()["id"]
     print(f"Job created: {job_id}")

     # 2. Poll for job status
     status_url = f"{endpoint}/openai/v1/video/generations/jobs/{job_id}?api-version={api_version}"
     status = None
     while status not in ("succeeded", "failed", "cancelled"):
         time.sleep(5)
         status_response = requests.get(status_url, headers=headers).json()
         status = status_response.get("status")
         print(f"Job status: {status}")

     # 3. Retrieve generated video
     if status == "succeeded":
         generations = status_response.get("generations", [])
         if generations:
             generation_id = generations[0].get("id")
             video_url = f"{endpoint}/openai/v1/video/generations/{generation_id}/content/video?api-version={api_version}"
             video_response = requests.get(video_url, headers=headers)
             if video_response.ok:
                 output_filename = "output.mp4"
                 with open(output_filename, "wb") as file:
                     file.write(video_response.content)
                     print(f'✅ Generated video saved as "{output_filename}"')
         else:
             raise Exception("No generations found in job result.")
     else:
         raise Exception(f"Job didn't succeed. Status: {status}")
     ```

  3. Run the Python file.

     ```shell
     python sora-quickstart.py
     ```

     Wait a few moments to get the response.

  ### Output

  The output will show the full response JSON from the video generation job creation request, including the job ID and status.

  ```json
  {
      "object": "video.generation.job",
      "id": "task_01jwcet0eje35tc5jy54yjax5q",
      "status": "queued",
      "created_at": 1748469875,
      "finished_at": null,
      "expires_at": null,
      "generations": [],
      "prompt": "A cat playing piano in a jazz bar.",
      "model": "<your-deployment-name>",
      "n_variants": 1,
      "n_seconds": 5,
      "height": 480,
      "width": 480,
      "failure_reason": null
  }
  ```

  The generated video will be saved as `output.mp4` in the current directory.

  ```text
  Job created: task_01jwcet0eje35tc5jy54yjax5q
  Job status: preprocessing
  Job status: running
  Job status: processing
  Job status: succeeded
  ✅ Video generation succeeded.
  Generated video saved as "output.mp4"
  ```
</ZonePivot>

<ZonePivot pivot="ai-foundry-portal">
  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Go to Microsoft Foundry portal

  Browse to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and sign in with the credentials associated with your Azure OpenAI resource. During or after the sign-in workflow, select the appropriate directory, Azure subscription, and Azure OpenAI resource.

  From the Foundry landing page, create or select a new project. Navigate to the **Models + endpoints** page on the left nav. Select **Deploy model** and then choose the Sora video generation model from the list. Complete the deployment process.

  On the model's page, select **Open in playground**.

  ## Try out video generation

  Start exploring Sora video generation with a no-code approach through the **Video playground**. Enter your prompt into the text box and select **Generate**. When the AI-generated video is ready, it appears on the page.

  <Callout type="note">
    The content generation APIs come with a content moderation filter. If Azure OpenAI recognizes your prompt as harmful content, it doesn't return a generated video. For more information, see [Content filtering](concepts/content-filter).
  </Callout>

  In the **Video playground**, you can also view Python and cURL code samples, which are prefilled according to your settings. Select the code button at the top of your video playback pane. You can use this code to write an application that completes the same task.
</ZonePivot>

## Clean up resources

If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

* [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal)
* [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli)

## Related content

* Learn more about Azure OpenAI [deployment types](../foundry-models/concepts/deployment-types).
* Learn more about Azure OpenAI [quotas and limits](quotas-limits).

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

In this quickstart, you generate video clips using the Azure OpenAI service. The example uses the Sora model, which is a video generation model that creates realistic and imaginative video scenes from text instructions and/or image or video inputs. This guide shows you how to create a video generation job, poll for its status, and retrieve the generated video.

For more information on video generation, see [Video generation concepts](concepts/video-generation).

<ZonePivot pivot="rest-api">
  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * [Python 3.8 or later version](https://www.python.org/). We recommend using Python 3.10 or later, but having at least Python 3.8 is required. If you don't have a suitable version of Python installed, you can follow the instructions in the [VS Code Python Tutorial](https://code.visualstudio.com/docs/python/python-tutorial#_install-a-python-interpreter) for the easiest way of installing Python on your operating system.
  * An Azure OpenAI resource created in one of the supported regions. For more information about region availability, see the [models and versions documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#video-generation-models).
  * Then, you need to deploy a `sora` model with your Azure OpenAI resource. For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Microsoft Entra ID prerequisites

  For the recommended keyless authentication with Microsoft Entra ID, you need to:

  * Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli) used for keyless authentication with Microsoft Entra ID.
  * Assign the `Cognitive Services User` role to your user account. You can assign roles in the Azure portal under **Access control (IAM)** > **Add role assignment**.

  ## Set up

  1. Create a new folder `video-generation-quickstart` and go to the quickstart folder with the following command:

     ```shell
     mkdir video-generation-quickstart && cd video-generation-quickstart
     ```

  2. Create a virtual environment. If you already have Python 3.10 or higher installed, you can create a virtual environment using the following commands:

     **Windows**

     ```bash
     py -3 -m venv .venv
     .venv\scripts\activate
     ```

     **Linux**

     ```bash
     python3 -m venv .venv
     source .venv/bin/activate
     ```

     **macOS**

     ```bash
     python3 -m venv .venv
     source .venv/bin/activate
     ```

     Activating the Python environment means that when you run `python` or `pip` from the command line, you then use the Python interpreter contained in the `.venv` folder of your application. You can use the `deactivate` command to exit the python virtual environment, and can later reactivate it when needed.

     <Callout type="tip">
       We recommend that you create and activate a new Python environment to use to install the packages you need for this tutorial. Don't install packages into your global python installation. You should always use a virtual or conda environment when installing python packages, otherwise you can break your global installation of Python.
     </Callout>

  3. Install the required packages.

     **Microsoft Entra ID**

     ```console
     pip install requests azure-identity
     ```

     **API key**

     ```console
     pip install requests
     ```

  ## Retrieve resource information

  You need to retrieve the following information to authenticate your application with your Azure OpenAI resource:

  <Tabs>
    <Tab title="Microsoft Entra ID">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [keyless authentication](https://learn.microsoft.com/en-us/azure/ai-services/authentication) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).
    </Tab>

    <Tab title="API key">
      | Variable name                  | Value                                                                                                                                                                                                     |
      | ------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
      | `AZURE_OPENAI_ENDPOINT`        | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal.                                                                                          |
      | `AZURE_OPENAI_API_KEY`         | This value can be found in the **Keys and Endpoint** section when examining your resource from the Azure portal. You can use either `KEY1` or `KEY2`.                                                     |
      | `AZURE_OPENAI_DEPLOYMENT_NAME` | This value will correspond to the custom name you chose for your deployment when you deployed a model. This value can be found under **Resource Management** > **Model Deployments** in the Azure portal. |

      Learn more about [finding API keys](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables) and [setting environment variables](https://learn.microsoft.com/en-us/azure/ai-services/cognitive-services-environment-variables).

      <Callout type="important">
        Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, see [API keys with Azure Key Vault](https://learn.microsoft.com/en-us/azure/key-vault/general/apps-api-keys-secrets).

        For more information about AI services security, see [Authenticate requests to Azure AI services](https://learn.microsoft.com/en-us/azure/ai-services/authentication).
      </Callout>
    </Tab>
  </Tabs>

  ## Generate video with Sora

  You can generate a video with the Sora model by creating a video generation job, polling for its status, and retrieving the generated video. The following code shows how to do this via the REST API using Python.

  1. Create the `sora-quickstart.py` file and add the following code to authenticate your resource:

     **Microsoft Entra ID**

     ```python
     import json
     import requests
     import time
     import os
     from azure.identity import DefaultAzureCredential

     # Set environment variables or edit the corresponding values here.
     endpoint = os.environ.get('AZURE_OPENAI_ENDPOINT')
     deployment_name = os.environ.get('AZURE_OPENAI_DEPLOYMENT_NAME')
     if not endpoint or not deployment_name:
         raise ValueError("Set AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_DEPLOYMENT_NAME.")

     # Keyless authentication
     credential = DefaultAzureCredential()
     token = credential.get_token("https://cognitiveservices.azure.com/.default")

     api_version = 'preview'
     headers= { "Authorization": f"Bearer {token.token}", "Content-Type": "application/json" }
     ```

     **API key**

     ```python
     import json
     import requests
     import time
     import os

     # Set environment variables or edit the corresponding values here.
     endpoint = os.environ.get('AZURE_OPENAI_ENDPOINT')
     deployment_name = os.environ.get('AZURE_OPENAI_DEPLOYMENT_NAME')
     api_key = os.environ.get('AZURE_OPENAI_API_KEY')
     if not endpoint or not deployment_name or not api_key:
         raise ValueError(
             "Set AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME, and AZURE_OPENAI_API_KEY."
         )

     api_version = 'preview'
     headers= { "api-key": api_key, "Content-Type": "application/json" }
     ```

  2. Create the video generation job. You can create it from a text prompt only, or from an input image and text prompt.

     **Text prompt**

     ```python
     # 1. Create a video generation job
     create_url = f"{endpoint}/openai/v1/video/generations/jobs?api-version={api_version}"
     body = {
         "prompt": "A cat playing piano in a jazz bar.",
         "width": 480,
         "height": 480,
         "n_seconds": 5,
         "model": deployment_name
     }
     response = requests.post(create_url, headers=headers, json=body)
     response.raise_for_status()
     print("Full response JSON:", response.json())
     job_id = response.json()["id"]
     print(f"Job created: {job_id}")

     # 2. Poll for job status
     status_url = f"{endpoint}/openai/v1/video/generations/jobs/{job_id}?api-version={api_version}"
     status=None
     while status not in ("succeeded", "failed", "cancelled"):
         time.sleep(5)  # Wait before polling again
         status_response = requests.get(status_url, headers=headers).json()
         status = status_response.get("status")
         print(f"Job status: {status}")

     # 3. Retrieve generated video
     if status == "succeeded":
         generations = status_response.get("generations", [])
         if generations:
             print(f"✅ Video generation succeeded.")
             generation_id = generations[0].get("id")
             video_url = f"{endpoint}/openai/v1/video/generations/{generation_id}/content/video?api-version={api_version}"
             video_response = requests.get(video_url, headers=headers)
             if video_response.ok:
                 output_filename = "output.mp4"
                 with open(output_filename, "wb") as file:
                     file.write(video_response.content)
                     print(f'Generated video saved as "{output_filename}"')
         else:
             raise Exception("No generations found in job result.")
     else:
         raise Exception(f"Job didn't succeed. Status: {status}")
     ```

     **Image prompt**

     Replace the `"file_name"` field in `"inpaint_items"` with the name of your input image file. Also replace the construction of the `files` array, which associates the path to the actual file with the filename that the API uses.

     Use the `"crop_bounds"` data (image crop distances, from each direction, as a fraction of the total image dimensions) to specify which part of the image should be used in video generation.

     You can optionally set the `"frame_index"` to the frame in the generated video where your image should appear (the default is 0, the start of the video).

     ```python
     # 1. Create a video generation job with image inpainting (multipart upload)
     create_url = f"{endpoint}/openai/v1/video/generations/jobs?api-version={api_version}"

     # Flatten the body for multipart/form-data
     data = {
         "prompt": "A serene forest scene transitioning into autumn",
         "height": str(1080),
         "width": str(1920),
         "n_seconds": str(10),
         "n_variants": str(1),
         "model": deployment_name,
         # inpaint_items must be JSON string
         "inpaint_items": json.dumps([
             {
                 "frame_index": 0,
                 "type": "image",
                 "file_name": "dog_swimming.jpg",
                 "crop_bounds": {
                     "left_fraction": 0.1,
                     "top_fraction": 0.1,
                     "right_fraction": 0.9,
                     "bottom_fraction": 0.9
                 }
             }
         ])
     }

     # Replace with your own image file path
     with open("dog_swimming.jpg", "rb") as image_file:
         files = [
             ("files", ("dog_swimming.jpg", image_file, "image/jpeg"))
         ]
         multipart_headers = {k: v for k, v in headers.items() if k.lower() != "content-type"}
         response = requests.post(
             create_url,
             headers=multipart_headers,
             data=data,
             files=files
         )

     if not response.ok:
         print("Error response:", response.status_code, response.text)
         response.raise_for_status()
     print("Full response JSON:", response.json())
     job_id = response.json()["id"]
     print(f"Job created: {job_id}")

     # 2. Poll for job status
     status_url = f"{endpoint}/openai/v1/video/generations/jobs/{job_id}?api-version={api_version}"
     status = None
     while status not in ("succeeded", "failed", "cancelled"):
         time.sleep(5)
         status_response = requests.get(status_url, headers=headers).json()
         status = status_response.get("status")
         print(f"Job status: {status}")

     # 3. Retrieve generated video
     if status == "succeeded":
         generations = status_response.get("generations", [])
         if generations:
             generation_id = generations[0].get("id")
             video_url = f"{endpoint}/openai/v1/video/generations/{generation_id}/content/video?api-version={api_version}"
             video_response = requests.get(video_url, headers=headers)
             if video_response.ok:
                 output_filename = "output.mp4"
                 with open(output_filename, "wb") as file:
                     file.write(video_response.content)
                     print(f'✅ Generated video saved as "{output_filename}"')
         else:
             raise Exception("No generations found in job result.")
     else:
         raise Exception(f"Job didn't succeed. Status: {status}")
     ```

     **Video prompt**

     Replace the `"file_name"` field in `"inpaint_items"` with the name of your input video file. Also replace the construction of the `files` array, which associates the path to the actual file with the filename that the API uses.

     Use the `"crop_bounds"` data (image crop distances, from each direction, as a fraction of the total frame dimensions) to specify which part of the video frame should be used in video generation.

     You can optionally set the `"frame_index"` to the frame in the generated video where your input video should start (the default is 0, the beginning).

     ```python
     # 1. Create a video generation job with video inpainting (multipart upload)
     create_url = f"{endpoint}/openai/v1/video/generations/jobs?api-version={api_version}"

     # Flatten the body for multipart/form-data
     data = {
         "prompt": "A serene forest scene transitioning into autumn",
         "height": str(1080),
         "width": str(1920),
         "n_seconds": str(10),
         "n_variants": str(1),
         "model": deployment_name,
         # inpaint_items must be JSON string
         "inpaint_items": json.dumps([
             {
                 "frame_index": 0,
                 "type": "video",
                 "file_name": "dog_swimming.mp4",
                 "crop_bounds": {
                     "left_fraction": 0.1,
                     "top_fraction": 0.1,
                     "right_fraction": 0.9,
                     "bottom_fraction": 0.9
                 }
             }
         ])
     }

     # Replace with your own video file path
     with open("dog_swimming.mp4", "rb") as video_file:
         files = [
             ("files", ("dog_swimming.mp4", video_file, "video/mp4"))
         ]
         multipart_headers = {k: v for k, v in headers.items() if k.lower() != "content-type"}
         response = requests.post(
             create_url,
             headers=multipart_headers,
             data=data,
             files=files
         )

     if not response.ok:
         print("Error response:", response.status_code, response.text)
         response.raise_for_status()
     print("Full response JSON:", response.json())
     job_id = response.json()["id"]
     print(f"Job created: {job_id}")

     # 2. Poll for job status
     status_url = f"{endpoint}/openai/v1/video/generations/jobs/{job_id}?api-version={api_version}"
     status = None
     while status not in ("succeeded", "failed", "cancelled"):
         time.sleep(5)
         status_response = requests.get(status_url, headers=headers).json()
         status = status_response.get("status")
         print(f"Job status: {status}")

     # 3. Retrieve generated video
     if status == "succeeded":
         generations = status_response.get("generations", [])
         if generations:
             generation_id = generations[0].get("id")
             video_url = f"{endpoint}/openai/v1/video/generations/{generation_id}/content/video?api-version={api_version}"
             video_response = requests.get(video_url, headers=headers)
             if video_response.ok:
                 output_filename = "output.mp4"
                 with open(output_filename, "wb") as file:
                     file.write(video_response.content)
                     print(f'✅ Generated video saved as "{output_filename}"')
         else:
             raise Exception("No generations found in job result.")
     else:
         raise Exception(f"Job didn't succeed. Status: {status}")
     ```

  3. Run the Python file.

     ```shell
     python sora-quickstart.py
     ```

     Wait a few moments to get the response.

  ### Output

  The output will show the full response JSON from the video generation job creation request, including the job ID and status.

  ```json
  {
      "object": "video.generation.job",
      "id": "task_01jwcet0eje35tc5jy54yjax5q",
      "status": "queued",
      "created_at": 1748469875,
      "finished_at": null,
      "expires_at": null,
      "generations": [],
      "prompt": "A cat playing piano in a jazz bar.",
      "model": "<your-deployment-name>",
      "n_variants": 1,
      "n_seconds": 5,
      "height": 480,
      "width": 480,
      "failure_reason": null
  }
  ```

  The generated video will be saved as `output.mp4` in the current directory.

  ```text
  Job created: task_01jwcet0eje35tc5jy54yjax5q
  Job status: preprocessing
  Job status: running
  Job status: processing
  Job status: succeeded
  ✅ Video generation succeeded.
  Generated video saved as "output.mp4"
  ```
</ZonePivot>

<ZonePivot pivot="ai-foundry-portal">
  ## Prerequisites

  * An Azure subscription. [Create one for free](https://azure.microsoft.com/pricing/purchase-options/azure-account?cid=msft_learn).
  * An Azure OpenAI resource created in a supported region. See [Region availability](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/models#model-summary-table-and-region-availability). For more information, see [Create a resource and deploy a model with Azure OpenAI](how-to/create-resource).

  ## Go to Microsoft Foundry portal

  Browse to the [Foundry portal](https://ai.azure.com/?cid=learnDocs) and sign in with the credentials associated with your Azure OpenAI resource. During or after the sign-in workflow, select the appropriate directory, Azure subscription, and Azure OpenAI resource.

  From the Foundry landing page, create or select a new project. Navigate to the **Models + endpoints** page on the left nav. Select **Deploy model** and then choose the Sora video generation model from the list. Complete the deployment process.

  On the model's page, select **Open in playground**.

  ## Try out video generation

  Start exploring Sora video generation with a no-code approach through the **Video playground**. Enter your prompt into the text box and select **Generate**. When the AI-generated video is ready, it appears on the page.

  <Callout type="note">
    The content generation APIs come with a content moderation filter. If Azure OpenAI recognizes your prompt as harmful content, it doesn't return a generated video. For more information, see [Content filtering](concepts/content-filter).
  </Callout>

  In the **Video playground**, you can also view Python and cURL code samples, which are prefilled according to your settings. Select the code button at the top of your video playback pane. You can use this code to write an application that completes the same task.
</ZonePivot>

## Clean up resources

If you want to clean up and remove an Azure OpenAI resource, you can delete the resource. Before deleting the resource, you must first delete any deployed models.

* [Azure portal](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azportal)
* [Azure CLI](https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?pivots=azcli)

## Related content

* Learn more about Azure OpenAI [deployment types](../foundry-models/concepts/deployment-types).
* Learn more about Azure OpenAI [quotas and limits](quotas-limits).

<Callout type="note">
  This article refers to the [Microsoft Foundry (new)](../../what-is-foundry#microsoft-foundry-portals) portal.
</Callout>

Vision-enabled chat models are large multimodal models (LMM) developed by OpenAI that can analyze images and provide textual responses to questions about them. They incorporate both natural language processing and visual understanding. The current vision-enabled models are the [o-series reasoning models](reasoning), GPT-5 series, GPT-4.1 series, GPT-4.5, GPT-4o series.

The vision-enabled models can answer general questions about what's present in the images you upload.

<Callout type="tip">
  To use vision-enabled models, you call the Chat Completion API on a supported model that you have deployed. If you're not familiar with the Chat Completion API, see the [Vision-enabled chat how-to guide](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/how-to/chatgpt?tabs=python\&pivots=programming-language-chat-completions).
</Callout>

## Call the Chat Completion APIs

The following command shows the most basic way to use a vision-enabled chat model with code. If this is your first time using these models programmatically, we recommend starting with our [Chat with images quickstart](../gpt-v-quickstart).

<Tabs>
  <Tab title="REST">
    Send a POST request to `https://{RESOURCE_NAME}.openai.azure.com/openai/v1/chat/completions` where

    * RESOURCE\_NAME is the name of your Azure OpenAI resource

    **Required headers**:

    * `Content-Type`: application/json
    * `api-key`: \{API\_KEY}

    **Body**: The following is a sample request body. The format is the same as the chat completions API for GPT-4o, except that the message content can be an array containing text and images (either a valid publicly accessible HTTP or HTTPS URL to an image, or a base-64-encoded image).

    <Callout type="important">
      Remember to set a `"max_tokens"`, or `max_completion_tokens` value or the return output will be cut off.
    </Callout>

    <Callout type="important">
      When uploading images, there is a limit of 10 images per chat request.
    </Callout>

    ```json
    {
        "model": "MODEL-DEPLOYMENT-NAME",
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": [
    	            {
    	                "type": "text",
    	                "text": "Describe this picture:"
    	            },
    	            {
    	                "type": "image_url",
    	                "image_url": {
                            "url": "<image URL>"
                        }
                    }
               ]
            }
        ],
        "max_tokens": 100,
        "stream": false
    }
    ```
  </Tab>

  <Tab title="Python">
    1. Define your Azure OpenAI `base_url` and `api-key`.

    2. Create a client object using those values.

       ```python
       import os
       from openai import OpenAI

       client = OpenAI(
           api_key=os.getenv("AZURE_OPENAI_API_KEY"),
           base_url="https://YOUR-RESOURCE-NAME.openai.azure.com/openai/v1/",
       )
       ```

    3. Then call the client's **create** method. The following code shows a sample request body. The format is the same as the chat completions API for GPT-4o, except that the message content can be an array containing text and images (either a valid HTTP or HTTPS URL to an image, or a base-64-encoded image).

       <Callout type="important">
         Remember to set a `"max_tokens"`, or `max_completion_tokens` value or the return output will be cut off.
       </Callout>

       ```python
       response = client.chat.completions.create(
           model="MODEL-DEPLOYMENT-NAME",
           messages=[
               { "role": "system", "content": "You are a helpful assistant." },
               { "role": "user", "content": [
                   {
                       "type": "text",
                       "text": "Describe this picture:"
                   },
                   {
                       "type": "image_url",
                       "image_url": {
                           "url": "<image URL>"
                       }
                   }
               ] }
           ],
           max_tokens=2000
       )
       print(response)
       ```
  </Tab>
</Tabs>

<Callout type="tip">
  ### Use a local image

  If you want to use a local image, you can use the following Python code to convert it to base64 so it can be passed to the API. Alternative file conversion tools are available online.

  ```python
  import base64
  from mimetypes import guess_type

  # Function to encode a local image into data URL
  def local_image_to_data_url(image_path):
      # Guess the MIME type of the image based on the file extension
      mime_type, _ = guess_type(image_path)
      if mime_type is None:
          mime_type = 'application/octet-stream'  # Default MIME type if none is found

      # Read and encode the image file
      with open(image_path, "rb") as image_file:
          base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')

      # Construct the data URL
      return f"data:{mime_type};base64,{base64_encoded_data}"

  # Example usage
  image_path = '<path_to_image>'
  data_url = local_image_to_data_url(image_path)
  print("Data URL:", data_url)
  ```

  When your base64 image data is ready, you can pass it to the API in the request body like this:

  ```json
  ...
  "type": "image_url",
  "image_url": {
     "url": "data:image/jpeg;base64,<your_image_data>"
  }
  ...
  ```
</Callout>

### Detail parameter settings

You can optionally define a `"detail"` parameter in the `"image_url"` field. Choose one of three values, `low`, `high`, or `auto`, to adjust the way the model interprets and processes images.

* `auto` setting: The default setting. The model decides between low or high based on the size of the image input.
* `low` setting: the model does not activate the "high res" mode, instead processes a lower resolution 512x512 version, resulting in quicker responses and reduced token consumption for scenarios where fine detail isn't crucial.
* `high` setting: the model activates "high res" mode. Here, the model initially views the low-resolution image and then generates detailed 512x512 segments from the input image. Each segment uses double the token budget, allowing for a more detailed interpretation of the image.

You set the value using the format shown in this example:

```json
{
    "type": "image_url",
    "image_url": {
        "url": "<image URL>",
        "detail": "high"
    }
}
```

For details on how the image parameters impact tokens used and pricing please see - [What is Azure OpenAI? Image Tokens](../../foundry-models/concepts/models-sold-directly-by-azure)

## Output

The API response should look like the following.

```json
{
    "id": "chatcmpl-8VAVx58veW9RCm5K1ttmxU6Cm4XDX",
    "object": "chat.completion",
    "created": 1702439277,
    "model": "gpt-4o",
    "prompt_filter_results": [
        {
            "prompt_index": 0,
            "content_filter_results": {
                "hate": {
                    "filtered": false,
                    "severity": "safe"
                },
                "self_harm": {
                    "filtered": false,
                    "severity": "safe"
                },
                "sexual": {
                    "filtered": false,
                    "severity": "safe"
                },
                "violence": {
                    "filtered": false,
                    "severity": "safe"
                }
            }
        }
    ],
    "choices": [
        {
            "finish_reason":"stop",
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "The picture shows an individual dressed in formal attire, which includes a black tuxedo with a black bow tie. There is an American flag on the left lapel of the individual's jacket. The background is predominantly blue with white text that reads \"THE KENNEDY PROFILE IN COURAGE AWARD\" and there are also visible elements of the flag of the United States placed behind the individual."
            },
            "content_filter_results": {
                "hate": {
                    "filtered": false,
                    "severity": "safe"
                },
                "self_harm": {
                    "filtered": false,
                    "severity": "safe"
                },
                "sexual": {
                    "filtered": false,
                    "severity": "safe"
                },
                "violence": {
                    "filtered": false,
                    "severity": "safe"
                }
            }
        }
    ],
    "usage": {
        "prompt_tokens": 1156,
        "completion_tokens": 80,
        "total_tokens": 1236
    }
}
```

Every response includes a `"finish_reason"` field. It has the following possible values:

* `stop`: API returned complete model output.
* `length`: Incomplete model output due to the `max_tokens` input parameter or model's token limit.
* `content_filter`: Omitted content due to a flag from our content filters.

### Output

The chat responses you receive from the model should now include enhanced information about the image, such as object labels and bounding boxes, and OCR results. The API response should look like the following.

```json
{
    "id": "chatcmpl-8UyuhLfzwTj34zpevT3tWlVIgCpPg",
    "object": "chat.completion",
    "created": 1702394683,
    "model": "gpt-4o",
    "choices":
    [
        {
            "finish_reason": {
                "type": "stop",
                "stop": "<|fim_suffix|>"
            },
            "index": 0,
            "message":
            {
                "role": "assistant",
                "content": "The image shows a close-up of an individual with dark hair and what appears to be a short haircut. The person has visible ears and a bit of their neckline. The background is a neutral light color, providing a contrast to the dark hair."
            }
        }
    ],
    "usage":
    {
        "prompt_tokens": 816,
        "completion_tokens": 49,
        "total_tokens": 865
    }
}
```

Every response includes a `"finish_reason"` field. It has the following possible values:

* `stop`: API returned complete model output.
* `length`: Incomplete model output due to the `max_tokens` input parameter or model's token limit.
* `content_filter`: Omitted content due to a flag from our content filters.

## Related content

* [Learn more about Azure OpenAI](../../foundry-models/concepts/models-sold-directly-by-azure).
* [Vision-enabled chats quickstart](../gpt-v-quickstart)
* [Vision chats frequently asked questions](../faq#gpt-4-turbo-with-vision)
* [Chat completions API reference](https://aka.ms/gpt-v-api-ref)

